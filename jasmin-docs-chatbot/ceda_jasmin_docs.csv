title,contents,page_url,char_count,word_count
About this site,"**We hope you like our new-look help docs** which make use of richer formatting to make the articles easier to read, particularly those with code snippets or command-line instructions. We've also changed the way we manage the source content so it's easier for us to maintain. There's still more to do, to re-organise some of the content and add new pages to reflect how JASMIN is evolving.
This site should redirect you from any previous URLs you may have stored, but please use the following features to help you navigate around the site:
- main navigation bar along the top of the site, including search tool
- breadcrumbs menu, to show you where you are in the site
- collapsible sidebar, left side panel (docs pages only)
- icons, to identify articles within the same section
- table of contents, right side panel
- hover over sub-headings to reveal a bookmark-able link to that heading
- {{< icon fas tag >}} {{<link url=""/tags""  >}}tags{{</link>}} to group together articles covering similar topics
- links between articles
",https://help.jasmin.ac.uk/docs/about-this-site#about-this-site,1030,175
Issues with this site,"If you spot any broken links or incorrect information, please let us know by opening an issue in the GitHub repository for this site's source code:
{{<button icon=""fab github"" cue=false order=""first"" href=""https://github.com/cedadev/jasmin-help-hugo-hinode/issues"" >}}Report issues with this site{{</button>}}
The date of update date and commit message for each page should be just above the footer of each page.
",https://help.jasmin.ac.uk/docs/about-this-site#issues-with-this-site,413,57
Other issues,"For all other reports of problems, or for any information you can't find (don't forget the other JASMIN sites linked in the footer!), please use the contact form via ""Ask"" in the JASMIN Help beacon (bottom right, orange button) and use the contact form to send a message to the helpdesk: this is the best method to get in touch.
Note that the beacon no longer contains the links to the help docs themselves, but still provides the contact form. Hopefully the other navigation features described above should enable you to find what you need.
",https://help.jasmin.ac.uk/docs/about-this-site#other-issues,542,95
Multiple account types,"This article defines the types of account available on JASMIN and their
purpose. It covers:
- STANDARD accounts (with note about training accounts)
- SHARED accounts
- SERVICE accounts
",https://help.jasmin.ac.uk/docs/getting-started/multiple-account-types,185,29
Introduction,"For some time, we have been asked by user communities to cater for legitimate
use cases where accounts need to be shared by a small, known and pre-arranged
set of users, or by services or functions.
To maintain a secure approach, we have brought these together into a clearly-
defined set of account types for each purpose.
",https://help.jasmin.ac.uk/docs/getting-started/multiple-account-types#introduction,324,57
Definitions,,https://help.jasmin.ac.uk/docs/getting-started/multiple-account-types#definitions,0,0
STANDARD accounts,"A standard account:
  * is for use by one human individual user only.
  * can login to the JASMIN accounts portal to (re)set a password, store an SSH key and apply for access roles.
  * has a unique SSH key, traceable to its owner.
**Training accounts** are a special type of STANDARD account, issued on a
short-term basis and preconfigured with certain access roles as required for
training events.
A **standard account** holder may act as a **responsible user** on one more
**service** or **shared accounts.**
",https://help.jasmin.ac.uk/docs/getting-started/multiple-account-types#standard-accounts,512,86
SHARED accounts,"A shared account:
  * is for use by a small, defined set of **responsible users** , each associated by their **standard account** username
  * has a set of SSH public keys, one for each **responsible user**. The **shared account** itself does not have a key, and users do not share keys. The set of keys associated with the **shared account** is updated automatically in the event that any individual **responsible user** changes their the SSH key on their own **standard account**.
  * can log in to the JASMIN accounts portal using the shared account username to apply for roles and can (re)set a a password, which may be shared securely** and only between the set of responsible users. The accounts portal profile for the shared account will display, but not allow editing of, the public keys of the responsible users.
  * can be used by individual **responsible users** to login via SSH, but using their own individual SSH private key which must not be shared with any other user, and should be kept locally, i.e. not uploaded to anywhere on JASMIN.
  * by default, emails originating from the JASMIN accounts portal destined for **shared accounts** are instead sent to all their **responsible users**. An optional email address for the **shared account** itself may be specified in the accounts portal profile for the account.
  * can perform any action in the system that a standard account can, including but not limited to the following (and subject to membership of relevant access roles): 
    * becoming a member of a group workspace
    * using elastic tape / JDMA
    * submitting a job to the LOTUS batch processing cluster
    * obtaining an short-lived credential for use with a high-performance transfer method
  * may be requested by a user or group of users via the JASMIN helpdesk, but the decision as to whether to grant the request is at the discretion of the JASMIN team, after scrutiny of the request, its justification and the past JASMIN behaviour of the individual users proposed to be responsible for the **shared account.**
**An example of a secure means of sharing a password is to use Keeper (or
similar password manager system) to share a securely-stored entry with a
specific list of other individuals in an encrypted form. Password sharing via
unencrypted means (such as a text file, email or post-it note) is not
permitted.
",https://help.jasmin.ac.uk/docs/getting-started/multiple-account-types#shared-accounts,2359,396
SERVICE accounts,"A service account:
  * is for use by a service or function only
  * has one or more **responsible users** , each associated by their **standard account** username
  * can never log in to the JASMIN accounts portal or (re)set a password.
  * may be granted roles by arrangement with the JASMIN team
  * has no SSH key
  * emails originating from the JASMIN accounts portal destined for **service accounts** are instead sent to all their **responsible users.** An optional email address for the **service account** itself may be specified in the accounts portal profile for the account.
  * may be requested by a user or group of users via the JASMIN helpdesk, but the decision as to whether to grant the request is at the discretion of the JASMIN team, after scrutiny of the request, its justification and the past JASMIN behaviour of the individual users proposed to be responsible for the **service account.**
**NOTES:**
  * With the implementation of these new account types, existing setups will be examined and discussed with their ""owners"" and moved over to either **service** or **shared account** types as appropriate.
  * Users of a **shared** or **service account** are jointly responsible for actions performed by the account. This requires coordination and communication between responsible users, which should be done independently of the JASMIN system.
  * Membership of a **shared** or **service account** , and availability of the account itself, may be withdrawn if behaviour falls outside the [JASMIN Terms and Conditions](https://accounts.jasmin.ac.uk/account/conditions/). In serious cases, individual users may be barred from further use of JASMIN altogether. Users are reminded to familiarise themselves with the **Terms and Conditions** and have a responsibility to keep up to date with them as they change. Users must also pay attention to service announcements made by the JASMIN team by email and other means.
Requests for shared or service accounts should be sent to
the JASMIN helpdesk with ""shared account
request"" or ""service account request"" in the subject line.
",https://help.jasmin.ac.uk/docs/getting-started/multiple-account-types#service-accounts,2093,330
Get Started with JASMIN,"This article explains the steps involved for most users to gain
access to the JASMIN environment.
",https://help.jasmin.ac.uk/docs/getting-started/get-started-with-jasmin,98,16
Start here,"For most users, the ""front door"" to JASMIN is via {{<abbr SSH>}} access to
the JASMIN login servers.
Other services are only available once these basic steps have been
completed.
{{<alert type=""info"">}}
The overview presentation of the [JASMIN workshop](https://github.com/cedadev/jasmin-workshop) training materials gives a good introduction to the range of other services available"". Once you're up & running after following the steps below, we recommend new users to try the accompanying
exercises.
{{</alert>}}
**Table 1.** Steps involved to gain login access to JASMIN.
",https://help.jasmin.ac.uk/docs/getting-started/get-started-with-jasmin#start-here,575,82
Essential steps,"Step  |  Details  |  Comments  
---|---|---  
1  |  [Generate an SSH key]({{% ref ""generate-ssh-key-pair"" %}}) |  Create this on your laptop/desktop, ready to upload the public part of it to your JASMIN account.
2  |  [Get a JASMIN portal account]({{% ref ""get-jasmin-portal-account"" %}}) | Register for an account: do this on the {{<link ""jasmin_accounts_portal"" >}}JASMIN Accounts Portal{{</link>}}. However, this simply creates a user profile to store your basic information and SSH key: it does not give you privileges to access any services yet: you will need to apply for access to the services you require, see below.
3  |  [Request ""jasmin-login"" access]({{% ref ""get-login-account"" %}}) | Apply for the `jasmin-login` service, which will create you a system account and allow you to connect to it using {{<abbr SSH>}}.
4  |  [How to login]({{% ref ""how-to-login"" %}}) |  Follow these steps for logging in to JASMIN via {{<abbr SSH>}}.
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/getting-started/get-started-with-jasmin#essential-steps,968,150
Optional further steps,"Step  |  Details  |  Comments
---|---|---
5  |  Apply for access to additional services on JASMIN<br>(optional) |  JASMIN has a range of additional services, access to which is managed via the Accounts Portal. Search and apply for any services you require in the portal. In most cases, users will ""belong"" to a particular scientific project which may already have a presence on JASMIN, often in the form of a [Group Workspace]({{% ref ""short-term-project-storage"" %}}). See here how to [Apply for access to a Group Workspace]({{% ref ""apply-for-access-to-a-gws"" %}}).
6  |  [Get a CEDA account]({{% ref ""ceda-archive"" %}})<br>(optional) |  If you will need to access data in the {{<link ""ceda_archive"">}}CEDA Archive{{</link>}} for your work, it's accessible read-only throughout JASMIN,  Some datasets on the CEDA Archive require specific agreements, and to apply for access to these, you will need a CEDA account.  
7  |  [Link your JASMIN and CEDA accounts]({{% ref ""update-a-jasmin-account"" %}}) (optional)  |  The final step is to link your CEDA account to your JASMIN account. This lets you access CEDA data on JASMIN, with the same dataset access permissions that your linked CEDA account has.
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/getting-started/get-started-with-jasmin#optional-further-steps,1225,188
Further information,"* Use the navigation menu on the left to find about other services on JASMIN.
* Try exercises in the JASMIN workshop training materials
* The CEDA team also hosts training workshops and events, see CEDA
{{<link ""ceda_events"">}}events{{</link>}} and {{<link ""ceda_news"">}}news{{</link>}}.
* Keep an eye on the {{<link ""ceda_status"">}}status{{</link>}} info (also provided in the terminal when you log in).
",https://help.jasmin.ac.uk/docs/getting-started/get-started-with-jasmin#further-information,405,59
Access to storage,"**IMPORTANT:** Please see also
[Understanding new JASMIN storage]({{% ref""understanding-new-jasmin-storage"" %}})
which explains more about the different types of storage as of Phase 4 of JASMIN's history.
",https://help.jasmin.ac.uk/docs/getting-started/storage,205,26
Home directory,"Every JASMIN user is allocated a HOME directory located
at `/home/users/<username>`. This directory is available across most of the
interactive and batch computing resources, including the JASMIN login and
transfer servers.
{{< alert color=""info"" >}}
In the commands on this page, please replace `<username>` with your username, or use the environment variable `${USER}`. 
For example, `/home/users/<username>`  becomes `/home/users/joebloggs`.
{{</alert>}}
Each home directory has a default **quota of 100 GB**. Although you can't 
directly check usage against your quota, you can find out the current size
of your home directory as follows (the `pdu` command is a parallel variant
of the `du` command, designed to work with the particular storage used for home
directories on JASMIN).
{{<command>}}
pdu -sh /home/users/<username>
(out)#                   ^^^^^^^^^^ replace with your username
{{</command>}}
{{<alert type=""danger"">}}You are only allowed to exceed the 100 GB quota for a very
brief period of time. If you continue to exceed the limit, you will be
unable to add any more files, which means that jobs may fail, and other things may
stop working for you. You will need to reduce your usage below the 100GB quota to resolve this.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/getting-started/storage#home-directory,1257,189
Backups of your home directory,"Your home directory is backed up using a daily snapshot which provides a quick, **self-service** method for you to restore files or directories that have been accidentally deleted. Snapshot backups are kept for 1-2 weeks before being deleted.
",https://help.jasmin.ac.uk/docs/getting-started/storage#backups-of-your-home-directory,243,38
Recovering snapshots of your home directory data,"Users can access snapshots to recover files/directories that have been
accidentally deleted. These are stored in
`/home/users/.snapshot/homeusers.<snapshotid>/<username>`
The most recent backup is the one with the highest snapshot id number.
Find the ones relevant to your username with a command line this:
{{<command>}}
ls -ld /home/users/.snapshot/homeusers2.*/<username>  ## replace <username> with your own, as before
{{</command>}}
There should be up to 14 directories like this: 
drwx------ 113 joeblogs users 0 Jan 26 15:00 /home/users/.snapshot/homeusers2.2024_01_28_02_01/joebloggs
drwx------ 113 joeblogs users 0 Jan 26 15:00 /home/users/.snapshot/homeusers2.2024_01_29_02_01/joebloggs
drwx------ 113 joeblogs users 0 Jan 29 09:51 /home/users/.snapshot/homeusers2.2024_01_30_02_01/joebloggs
drwx------ 113 joeblogs users 0 Jan 30 09:29 /home/users/.snapshot/homeusers2.2024_01_31_02_01/joebloggs
drwx------ 113 joeblogs users 0 Jan 30 09:29 /home/users/.snapshot/homeusers2.2024_02_01_02_01/joebloggs
Each of these snapshot directories effectively contains your home directory as it was on that date. You can copy files back from them (yourself) to their original location.
{{<command>}}
ls -l /home/users/.snapshot/homeusers2.45678/joebloggs/
(out)total 1170964
(out)-rw-r--r-- 1 joebloggs users              104857600 Jun 26  2017 100M.dat
(out)-rw-r--r-- 1 joebloggs users             1024000000 Feb  1  2017 1G.dat
(out)-rw-r--r-- 1 joebloggs users                      0 Dec 18 12:09 6181791.err
cp /home/users/.snapshot/homeusers2.45678/joebloggs/100M.dat ~/100M.dat
{{</command>}}
A snapshot backup is also provided for `/gws/smf` volumes (similar allocations of
SSD storage for GWS groups to share): snapshots in this case are made hourly
and kept for 10 hours, then daily snapshots are kept for 2 weeks. These can
be retrieved in a similar manner to that shown above. In this case the
relevant directories should be found at
{{<command>}}
    /gws/smf/jNN/<gwsname>/.snapshot
{{</command>}}
(where `NN` = `04` or `07` depending on where the volume is located)
{{<alert type=""danger"">}}All other group workspace volumes are **not backed up**. The only exception to this is the snapshot backups for `smf` SSD volumes just described.
{{</alert>}}
Please also note the {{<link ""#advice-on-inter-volume-symlinks-in-jasmin-storage"">}}advice on inter-volume symlinks{{</link>}}, below: these are to be avoided.
",https://help.jasmin.ac.uk/docs/getting-started/storage#recovering-snapshots-of-your-home-directory-data,2424,292
JASMIN disk mounts,"There is a common file system layout that underpins most of the JASMIN
infrastructure. However, access to different parts of the file system will
depend on where you are logged in. Table 1 outlines the key disk mounts, where
they are accessible from and the type of access (read and/or write).
**Table 1.** List of common disk mounts, types of storage and their
availability on JASMIN
Disk mount  
location |  login  |  sci  |  transfer  |  LOTUS  |  Type  |  Parallel-write  
---|---|---|---|---|---|---  
/home/users  |  R/W  |  R/W  |  R/W  |  R/W  |  SSD  |  no
/gws/pw/j07<br>/gws/nopw/j04 (see note 1 below)<br>/gws/smf/j0[4,7] |  no<br>no<br>no | R/W<br>R/W<br>R/W | R/W<br>R/W<br>R/W | R/W<br>R/W<br>R/W | PFS<br>SOF<br>SSD | yes (hence ""pw"")<br>no (hence ""nopw"")<br>no
/work/xfc/volX (see note 2 below) |  no<br>no  |  R/W  |  R/W |  R/W  | PFS  | yes
/work/scratch-pw[2,3]<br>/work/scratch-nopw  |  no<br>no  |  R/W<br>R/W  |  no<br>no |  R/W<br>R/W  | PFS<br>SSD  | yes<br>no
/apps/contrib  |  No  |  RO  |  No  |  RO  |  n/a  |  n/a  
/badc, /neodc (archives)  |  No  |  RO  |  RO  |  RO  |  n/a  |  n/a  
{.table .table-striped}
login = {{<link ""../interactive-computing/login-servers"">}}login servers{{</link>}}: login[1-4].jasmin.ac.uk  
sci = {{<link ""../interactive-computing/sci-servers"">}}scientific analysis servers{{</link>}}:
sci[1-6,8].jasmin.ac.uk  
transfer = {{<link ""../interactive-computing/transfer-servers"">}}data transfer servers{{</link>}}:
xfer[1-2].jasmin.ac.uk  
LOTUS = {{<link ""../batch-computing/lotus-overview"">}}LOTUS batch processing cluster{{</link>}}(all cluster
nodes)  
Disks are mounted read/write ("" **R/W** "") or read-only ("" **RO** "").
**Note 1:** Please refer to issues related to writing small files and NetCDF3
to SOF storage [here]({{% ref ""faqs-storage"" %}})
**Note 2:** For details of how to use the Transfer Cache (XFC) service please see [here]({{% ref ""xfc"" %}})
",https://help.jasmin.ac.uk/docs/getting-started/storage#jasmin-disk-mounts,1921,256
Where to write data,"As indicated in table 1 there are three main disk mounts where data can be
written. Please follow these general principles when deciding where to write
your data:
- HOME directories (`/home/users`) are relatively small (100GB) and should NOT be used for storing large data volumes or for sharing data with other users.
0 Group Workspaces (mostly `/gws/nopw/*/<project>` but some `/gws/pw/*/<project`) are **usually the correct place to write your data**, although they are **not backed up**. Please refer to the [Group Workspace]({{% ref ""short-term-project-storage"" %}}) documentation for details.
  - `/gws/pw/j07` volumes are parallel-write-capable storage from Phase 7 (onwards) of JASMIN
  - `/gws/nopw/j04` volumes are ""Scale out Filesystem"" (SOF) from Phase 4 (onwaards) of JASMIN: this storage is not parallel-write-capable
- The ""scratch"" areas (`/work/scratch-pw2`, `/work/scratch-pw3` and `/work/scratch-nopw`) are available as a temporary file space for jobs running on [LOTUS]({{% ref ""lotus-overview"" %}}) (see next section below).
- The `/tmp` directory is **not usually an appropriate location to write your data (see next section below).**
",https://help.jasmin.ac.uk/docs/getting-started/storage#where-to-write-data,1157,164
How to use the temporary disk space,,https://help.jasmin.ac.uk/docs/getting-started/storage#how-to-use-the-temporary-disk-space,0,0
Scratch,"The scratch areas `/work/scratch-pw2`, `/work/scratch-pw3` and
`/work/scratch-nopw` are a temporary file space shared across the entire LOTUS
cluster and the scientific analysis servers.
These scratch areas are ideal for processes that generate _intermediate_ data
files that are consumed by other parts of the processing before being deleted.
Please remember that these volumes are resources shared between all users, so
consider other users and remember to clean up after your jobs. **** Any data
that you wish to keep should be written to a Group Workspace (but remember to
change the group-ownership of the data if you do).
There are 2 types of scratch storage available:
- **PFS scratch** (lots of it, fast, less good for small files) as 2 x 1 PB volumes `/work/scratch-pw[2,3]` and particularly suitable for users with a need for storage capabale of shared-file writes with MPI-IO, but good for most purposes.
- **SSD scratch** (less of it, very fast, good for small files) `/work/scratch-nopw2` as 1 x 220 TB volume. Do not use for operations that attempt to write to multiple parts of a file simultaneously. Please be aware of this if your code (perhaps inadvertently?) writes to a shared log file.
When using the ""scratch"" areas, please create a sub-directory (e.g.
`/work/scratch-????/<username>`) labelled with your username and write your data there.
",https://help.jasmin.ac.uk/docs/getting-started/storage#scratch,1363,216
/tmp,"In contrast to the ""scratch"" space, `/tmp` directories on LOTUS nodes 
and physical sci machines are all local to the machine. These can be used
to store _small_ volumes of temporary data for a job that only needs to be
read by the local process. But `/tmp` on virtual sci machines, are not local and
therefore should not usually be used by users.
",https://help.jasmin.ac.uk/docs/getting-started/storage#/tmp,348,63
Cleaning up the scratch and tmp directories,"**Please** make sure that your jobs delete any files under the `/tmp`and scratch
directories when they are complete ( _especially_ if jobs have not been
completed normally!).
Please do this yourself so that you are not taken by surprise when automated 
deletion processes clear up any residual data:
{{<alert type=""danger"">}}
Automated cleanup processes run daily and
delete files that are older than 28 days from the last time of being
accessed. This applies to `/work/scratch-pw2`, `/work/scratch-pw3` and
`/work/scratch-nopw2`
Please remember that shared temporary storage is for the use of all 2,000 users
of JASMIN, not just you. If you persistently store large amounts (100s of TB) of data in scratch 
for long periods of tine, you deny use of that storage to other users (so expect action from the JASMIN team).
Please be a good JASMIN citizen!
{{</alert>}}
Any important data for keeping should be written to
a [Group Workspace]({{% ref ""introduction-to-group-workspaces"" %}})
or to your home directory if appropriate.**
The `/work/scratch-pw[2,3]` and `/work/scratch-nopw` areas are not available on
the xfer, login or nx-login servers.
",https://help.jasmin.ac.uk/docs/getting-started/storage#cleaning-up-the-scratch-and-tmp-directories,1146,175
Avoid inadvertently writing to /tmp,"Sometimes software is configured by default to write to `/tmp`. Where possible, you 
should over-ride this and use your group workspace or a username-labelled directory 
within the scratch space instead.
To do this, please add the following lines (or
similar) to your $HOME/.bashrc file:
export TMPDIR=/<path-to-your-GWS-or-scratch>/<your_username>/tmp
# create the directory if needed
[ -d $TMPDIR ] || mkdir -p $TMPDIR
...but please check that location regularly to clear it out!
",https://help.jasmin.ac.uk/docs/getting-started/storage#avoid-inadvertently-writing-to-/tmp,482,70
Access to the CEDA archive,"The CEDA Archive is mounted read-only under paths refleting the NERC data centres
which merged to form CEDA, i.e.
- `/badc` (British Atmospheric Data Centre)
- `/neodc` (NERC Earth Observation Data Centre).
(other data centre paths now also exist at that level: see {{<link ""ceda_helpdocs"">}}CEDA Help docs{{</link>}} for more info)
The Archive includes a range of data sets that are provided under varying licences. Access
to these groups is managed through standard Unix groups. Information about the
data and their access restrictions is available from the {{<link ""ceda_catalogue"">}}CEDA Catalogue{{</link>}}. 
{{<alert type=""info"">}}
As a JASMIN user, it is your
responsibility to ensure that you have the correct permissions to access data
any data in CEDA Archive from within JASMIN, even if file system permissions permit access.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/getting-started/storage#access-to-the-ceda-archive,851,126
Tape access,"Group workspace managers also have access to near-line storage on tape, see [Elastic Tape
service]({{% ref ""secondary-copy-using-elastic-tape"" %}}) for making
secondary copies and managing storage between online and near-line storage.
",https://help.jasmin.ac.uk/docs/getting-started/storage#tape-access,235,30
Number of files in a single directory,"It is highly recommended that you do not exceed more than 100,000 files in a
single directory on any type of storage on JASMIN. Large numbers of files
place unnecessary load on components of the file system and can be the source
of slow performance for you and other storage volumes in the system. To count
the number of files, please note the advice in {{<link ""#slow-ls-response"">}}slow `ls` response{{</link>}} below, or
use an alternative command e.g. `find`.
",https://help.jasmin.ac.uk/docs/getting-started/storage#number-of-files-in-a-single-directory,464,77
Slow 'ls' response,"This can be due to a number of reasons (see above advice regarding number of
files in a single directory, and below regarding inter-volume symlinks). To
speed up the response (useful if you want to count the number of files) it
often helps to un-alias ls, e.g. by placing a backslash in front of the
command: `\ls`.
",https://help.jasmin.ac.uk/docs/getting-started/storage#slow-'ls'-response,316,57
Advice on inter-volume symlinks in JASMIN storage,"We highly recommend users _not_ to use symbolic links in their home
directories to other parts of the JASMIN file systems, such as GWSs or scratch
areas. There are a number of conditions when the petabyte-scale JASMIN storage
can become unusable for all users due to these links. There is a more
technical explanation below. We would advise **path substitutions using
environment variables** instead.
Symlinks in users' home directories that point to other volumes (for example
group workspaces) make matters worse _when_ there are problems on the
`sci*.jasmin.ac.uk` servers and other shared machines, and/or when the
metadata servers responsible for particular storage volumes themselves become
overloaded. The simplest advice we can currently give is to **avoid** using
symlinks.
**In more detail:**
This issue is particularly apparent when `ls` is aliassed to `ls --color` (as
is the default on 99% of JASMIN systems) AND one of the colorisation options
specified is for an orphaned link. The `ls` on symlinks causes the metadata
servers at the end of the symlink to be called (to provided the `stat`
filesystem metadata), in addition to the metadata server for the home
directory. If those metadata servers at the far end are under load, or have
some other problem, the `ls` to the home directory can hang, but this also
hangs other users who may be trying to ls their own home directory (even if
theirs contains no symlinks). The situation can then escalate out of control
as more and more users try and fail.
This is happens especially where one or more of the volumes involved
contains large numbers of small files.
",https://help.jasmin.ac.uk/docs/getting-started/storage#advice-on-inter-volume-symlinks-in-jasmin-storage,1624,267
Get a login account,"A user with a `jasmin-login` grant is allocated a `HOME` directory of
**100GB** and can access the shared JASMIN servers (Scientific servers, data
transfer servers) and the LOTUS batch cluster. Sign in into the
{{<link ""jasmin_accounts_portal"">}}JASMIN accounts portal{{</link>}} where you can apply for a JASMIN login
account representing the JASMIN access role appropriate to your affiliation.
{{<image src=""img/docs/get-login-account/login-not-applied.png"" caption=""The 'My Services' page showing a message that the user has not been granted any services yet"">}}
**Step 1:** Select Login services and navigate to the 'More information' on
this service
{{<image src=""img/docs/get-login-account/login-services.png"" caption=""More information"">}}
**Step 2:** Apply for access to jasmin-login service
{{<image src=""img/docs/get-login-account/login-more-info.png"" caption=""Apply for jasmin-login"">}}
**Step 3:** Provide supporting information
{{<image src=""img/docs/get-login-account/login-apply.png"" caption=""Provide supporting information"">}}
**Step 4:** Your request is pending for approval
{{<image src=""img/docs/get-login-account/login-submitted.png"" caption=""Request pending"">}}
**Step 4:** Upon approval, a notification email is sent to you. Additionally,
a notification counter in the bell will appear on the top left corner of the
menu bar on your JASMIN account portal page. You have now access to the login
server and to the scientific servers
{{<image src=""img/docs/get-login-account/login-granted.png"" caption=""Notification"">}}
Under 'My Services' you can view all the services that you currently have
access to or have requested access for. Note: Every time a notification is
acknowledged the counter is reset or decremented.
Now you can proceed to [How to login]({{% ref ""how-to-login"" %}})
",https://help.jasmin.ac.uk/docs/getting-started/get-login-account#get-a-login-account,1803,223
tips-for-new-users,"These tips for new users are based on users' queries encountered by our helpdesk. They are not exhaustive but may help solve some initial problems and set out best practice.
- Sci machines
- LOTUS
- Xfer servers
- How to report an issue
",https://help.jasmin.ac.uk/docs/getting-started/tips-for-new-users,237,44
"""Sci"" machines usage guidelines","1. Check the current load and number of users on the sci machines, as shown by the login servers, to select a less-used sci machine. [The available Sci machines and their specifications are listed in the table of this help page]({{% ref ""sci-servers"" %}})
2. The sci machines are not for running large, long-running tasks, or scripts that spawn multiple child processes. The batch processing cluster LOTUS is available for heavier processing. The sci machines are for development, testing, and light interactive use. Overloading these with processing seriously impairs performance for interactive use by others.
3. Do not write to the temporary partition `/tmp`on sci machines. [Use your home directory, a scratch volume or a Group Workspace ]({{% ref ""understanding-new-jasmin-storage"" %}}). Any temporary data files can reside in a subdirectory of your group workspace instead of `/tmp`. To do this, please add the following lines (or similar) to your `$HOME/.bashrc` file:
export TMPDIR=/group_workspaces/jasmin/<your_project>/<your_username>/tmp
## create the directory if needed
[ -d $TMPDIR ] || mkdir -p $TMPDIR
4. If a process hangs, do not simply close the terminal window. Please contact the helpdesk and alert the team so that the process can be shut down. Otherwise hung processes build up and contribute to machine overloading.
5. Do not “hog” IDL development licenses. A limited number of these are available for _development_ and compilation of IDL code which should then be run on LOTUS [using IDL runtime licenses]({{% ref ""idl"" %}}), of which there are many more.
6. Do not use sci machines for data transfer: [xfer hosts are provided for this purpose]({{% ref ""transfer-servers"" %}}).
","https://help.jasmin.ac.uk/docs/getting-started/tips-for-new-users#""sci""-machines-usage-guidelines",1704,265
LOTUS usage guidelines,"1. Do not use [IDL development licences]({{% ref ""idl"" %}}) on LOTUS. There are many **runtime** licenses available, but the **development** licenses are for interactive use on the sci machines, where IDL code can be compiled, then run on LOTUS using a **runtime** license.
2. Beware of inadvertently filling up `/tmp` on LOTUS nodes. This can take nodes out of action (perhaps for other users who still have jobs running on the same node) if `/tmp` fills up. Design your code to clean up as it goes along, and use environment variables to control where your applications write temporary data, ideally to storage which is not specific to a LOTUS node. If your job crashes, check which nodes were involved and clean up after yourself.
3. Do not store data in scratch areas for long periods of time. Move data away to group workspaces once your processing has finished.
",https://help.jasmin.ac.uk/docs/getting-started/tips-for-new-users#lotus-usage-guidelines,868,149
Xfer servers guidelines,"1. Do not run a large number (>16) of rsync or scp transfer processes in parallel.
2. Do not run processing on xfer servers: they are provided for data transfer only
3. For heavy/high-performance data transfers, use Globus or one of the `hpxfer` servers.
",https://help.jasmin.ac.uk/docs/getting-started/tips-for-new-users#xfer-servers-guidelines,255,44
How to report an issue,"When you do experience an issue, please;
1. Make it clear whether you are simply advising the helpdesk of a general issue (which will be noted, but not necessarily investigated for a specific response), or
2. Provide FULL and SPECIFIC details of your problem so that it can be investigated. JASMIN is a complex infrastructure with many hundreds of hosts and storage volumes, so reporting that “JASMIN” or “Storage” is slow, is not sufficient.
3. If you are experiencing difficulties accessing a particular storage volume from a particular sci machine, **please** state:
    - the **full path** to the data you are trying to access:
    - the **full hostname** of the machine (but please try the same access from at least one other machine to help establish whether it’s related to the machine or the storage)
    - the **date and time** of the issue (for matching up with system reports/log files. Using the date and time of the email is not sufficient: please be specific in your report)
4. Be patient: sometimes, queries will take longer to resolve if they are complex or if relevant staff are not available.
5. Some issues will only be resolved by strategic improvements which are planned as part of phased upgrades to JASMIN accompanied by capital procurements followed by integration work, all carried out by the same, small team.
",https://help.jasmin.ac.uk/docs/getting-started/tips-for-new-users#how-to-report-an-issue,1336,224
Generate an SSH key pair,"This article explains how to create an SSH key pair for logging in to JASMIN.
You can also use this procedure to update an existing SSH key pair for JASMIN.
However, if you are experiencing problems logging in to JASMIN you are advised
to first check {{<link ""../interactive-computing/login-problems"">}}Login problems{{</link>}} before changing your
key. Once you have created your SSH key pair it will need to be uploaded to
the JASMIN accounts portal. If this is the first time you have created a key
pair then this will be done when you create an account on the portal (Step 2
of {{<link ""get-started-with-jasmin"">}}Get Started with JASMIN{{</link>}}). If you
are updating your key for an existing account then you will need to update it
in your
{{<link ""https://accounts.jasmin.ac.uk/account/login/?next=/account/profile/"">}}JASMIN profile{{</link>}}.
",https://help.jasmin.ac.uk/docs/getting-started/generate-ssh-key-pair,856,127
The shell terminal,"Generating an SSH key pair requires an SSH client and a Shell terminal. Linux
and Mac users can use a standard terminal which is very likely to have SSH
installed. Windows users are advised to {{<link ""../uncategorized/mobaxterm"">}}install the MobaXterm
application{{</link>}} which provides a linux-style terminal
with all the relevant utilities included. Figures 1 and 2 show example
terminal windows on a Mac and Windows (using MobaXterm).
{{<image src=""img/docs/generate-ssh-key-pair/file-QrkL51B5fW.png"" caption=""Mac terminal"" >}}
{{<image src=""img/docs/generate-ssh-key-pair/file-jmOb6PSApE.png"" caption=""Terminal using Mobaxterm client on Windows"">}}
",https://help.jasmin.ac.uk/docs/getting-started/generate-ssh-key-pair#the-shell-terminal,658,79
Using ssh-keygen to create an SSH key pair,"The Linux command `ssh-keygen` should be used to generate your SSH key pair.
Open a terminal and generate your public and private key, as follows (replace
the e-mail address with your own):
{{<command user=""localuser"" host=""localhost"">}}
ssh-keygen -t rsa -b 2048 -C ""me@somewhere.ac.uk"" -f ~/.ssh/id_rsa_jasmin
{{</command>}}
At the prompt, type a **secure passphrase** to protect your SSH private key.
**This is a requirement for access to JASMIN machines. Use a new, different
passphrase whenever you generate a new key.** Note that nothing is echoed to
the screen when you enter your passphrase, so it may look like it is not
working.
The output will look something like this:
{{<command user=""localuser"" host=""localhost"">}}
(out)Generating public/private rsa key pair.
(out)Enter passphrase (empty for no passphrase): <ADD PASSPHRASE HERE>
(out)Enter same passphrase again: <REPEAT PASSPHRASE HERE>
(out)Your identification has been saved in /home/users/meuser/.ssh/id_rsa_jasmin.
(out)Your public key has been saved in /home/users/meuser/.ssh/id_rsa_jasmin.pub.
(out)The key fingerprint is:
(out)74:14:95:8a:31:73:cc:5c:af:be:91:04:01:c2:39:0b me@somewhere.ac.uk
{{</command>}}
Running `ssh-keygen` will generate two files in your `$HOME/.ssh/` directory:
- `id_rsa_jasmin` -  private key file (which should have permission ""400"", i.e. readable only by you)
- `id_rsa_jasmin.pub` - public key file
The **public** key file is the part that you need to share in order to access
JASMIN. The **private** key file should be protected and not shared with
others.
",https://help.jasmin.ac.uk/docs/getting-started/generate-ssh-key-pair#using-ssh-keygen-to-create-an-ssh-key-pair,1563,214
Converting a PuTTYGen SSH private key for use with MobaXterm (Windows only),"If you have previously used the
{{<link ""https://www.chiark.greenend.org.uk/~sgtatham/putty/"">}}PuTTY{{</link>}} utilities to
login to JASMIN and you wish to move over to using {{<link ""../uncategorized/mobaxterm"">}}MobaXterm{{</link>}}
then please see these {{<link ""https://docs.oseems.com/general/application/putty/convert-ppk-to-ssh-key"">}}
instructions to convert{{</link>}} your SSH
private key from the PuTTYGen format to the OpenSSH format (as used by Linux/Mac).
Please save your resulting OpenSSH key as `id_rsa_jasmin` in your `$HOME/.ssh/` directory.
",https://help.jasmin.ac.uk/docs/getting-started/generate-ssh-key-pair#converting-a-puttygen-ssh-private-key-for-use-with-mobaxterm-(windows-only),563,60
Get a JASMIN portal account,"This article explains how to register on the JASMIN accounts portal.
Having a JASMIN portal account does not by itself provide you with any access to JASMIN machines or services.
",https://help.jasmin.ac.uk/docs/getting-started/get-jasmin-portal-account,179,30
JASMIN accounts portal,"The [JASMIN accounts portal](https://accounts.jasmin.ac.uk/) is the place where you manage your JASMIN account and can apply for access to the many services which you may want to use on JASMIN.
{{<image src=""img/docs/get-jasmin-portal-account/jasmin-accounts-welcome.png"" caption=""JASMIN accounts portal"">}}
",https://help.jasmin.ac.uk/docs/getting-started/get-jasmin-portal-account#jasmin-accounts-portal,308,35
Apply for a new JASMIN account,"To apply for a JASMIN account you need to use an email address affiliated with your academic institution. This is the preferred option — otherwise your email address may not be immediately approved. Then proceed as follows:
**Step 1** : On [JASMIN accounts portal](https://accounts.jasmin.ac.uk/)
select ""Apply for a new JASMIN account"". This will take you to the following
page to enter your details.
{{<image src=""img/docs/get-jasmin-portal-account/application-details.png"" caption=""Application details page"">}}
**Step 2** : Select your research discipline.
{{<image src=""img/docs/get-jasmin-portal-account/application-research-discipline.png"" caption=""Research discipline"">}}
**Step 3** : Select the institution you are affiliated with. If your
institution is not listed, you can add new institution details by clicking the
plus button. Remember to provide supporting information to assess your
eligibility for a JASMIN account and then submit your application.
{{<image src=""img/docs/get-jasmin-portal-account/application-select-institution.png"" caption=""Select institution"">}}
**Step 4** : After pressing **Submit application**, follow the URL link sent to your email address.
{{<image src=""img/docs/get-jasmin-portal-account/application-email-verification.png"" caption=""Email verification"">}}
**Step 5** : Once your email has been verified, you will receive a second email 
with a subject 'Application approved' inviting you to complete the account creation. 
This link will take you to the following page where you have to choose your JASMIN account
credentials, register your SSH public key and then click **Create
account**:
{{<image src=""img/docs/get-jasmin-portal-account/application-credentials.png"" caption=""Choose credentials and add public key"">}}
**Step 6** : Agree to the JASMIN Terms and Conditions of Access:
{{<image src=""img/docs/get-jasmin-portal-account/application-terms.png"" caption=""Accept terms of use"">}}
**Step 7** : Your JASMIN account is created and you can log in using your
credentials:
{{<image src=""img/docs/get-jasmin-portal-account/application-complete.png"" caption=""Application complete"">}}
",https://help.jasmin.ac.uk/docs/getting-started/get-jasmin-portal-account#apply-for-a-new-jasmin-account,2129,249
What are training accounts?,"JASMIN training accounts are TEMPORARY accounts that provide a person with
access to JASMIN for a specific event (such as a training workshop) organised
in advance.
The benefits of using these accounts are:
- permissions/services are consistent across all training accounts 
- these permissions include (but are limited to) those normally anticipated for use within training/hackathon events
- they provide access to someone who may not be eligible for a full account - setting up access for multiple users in advance of an event is more efficient for all involved than the usual registration process
",https://help.jasmin.ac.uk/docs/getting-started/jasmin-training-accounts#what-are-training-accounts?,601,95
When should they be used?,"- By participants of training events led by the JASMIN team
- (By special arrangement with the event organiser) by participants of other events such as training workshops or hackathons which require temporary access to JASMIN
In both cases, use of the training accounts is only for the duration of the
event. Use beyond the event requires the normal registration process.
",https://help.jasmin.ac.uk/docs/getting-started/jasmin-training-accounts#when-should-they-be-used?,372,61
Who can request these accounts?,"- Organisers of training events / hackathons, by contacting the JASMIN team.
",https://help.jasmin.ac.uk/docs/getting-started/jasmin-training-accounts#who-can-request-these-accounts?,77,12
For event organisers,,https://help.jasmin.ac.uk/docs/getting-started/jasmin-training-accounts#for-event-organisers,0,0
How to request training accounts for your event,"If you think these accounts could be useful for you, please contact the
helpdesk with the following information:
- Name/date of event 
- Estimated number of attendees (note: we only have 150 such accounts, and events sometimes overlap)
- What services the accounts will need access to e.g. sci machines, GWS, CEDA Archive, Notebook Service, Transfer Servers, LOTUS, or any other special services on JASMIN 
- Any special requests for accessing resources
  - By default, the training accounts have access to the following services. 
  - Any request for other services beyond these would need to be considered by the JASMIN team: 
    - Login, nx-login, sci and xfer servers
    - `hpxfer` service
    - `workshop` group workspace (`/gws/pw/j07/workshop`)
    - use of LOTUS via the `workshop` Slurm queue
    - Jupyter Notebooks service (requires users to set password)
Once the JASMIN team have confirmed that we can support your event, you will
need to collect and supply the following **at least 2 weeks** before your
event:
- Email addresses for all attendees who need a training account. 
  - These addresses cannot already be associated with an existing JASMIN account. They must be different.
Please note that we cannot guarantee the following during your event:
- no login problems
- uptime of services
- no problems due to usage at scale (particularly for use of sci servers and/or Notebook Service)
",https://help.jasmin.ac.uk/docs/getting-started/jasmin-training-accounts#how-to-request-training-accounts-for-your-event,1408,225
What to tell your event attendees,"The JASMIN team will set up the accounts and send credentials to attendees
approximately 1 week before your event.
Please ensure you have given the following information to attendees:
- A link to this help page. Tell them all information they need is under the 'for event attendees' section.
- Tell users what services they have been granted access to (see above) and any additional information e.g. full path to any GWSs, whether they will need to use the JASMIN Notebook Service or accounts portal. 
- Tell the users when the training accounts will be closed down. This is usually within 24 hours of the end of the event. They would be responsible for copying any important data and/or code elsewhere if it is important that these are not lost when the accounts are wiped.
***
",https://help.jasmin.ac.uk/docs/getting-started/jasmin-training-accounts#what-to-tell-your-event-attendees,779,136
For event attendees,,https://help.jasmin.ac.uk/docs/getting-started/jasmin-training-accounts#for-event-attendees,0,0
How to set up your temporary account,"1. Training account credentials will be emailed to you via OneDrive. Sometimes the OneDrive email ends up in junk/spam folders, please check here. If you still can't find the email, please contact the helpdesk ASAP. To access the training account credentials, you must:
    1. Click on the OneDrive link.
    1. Enter your email address (if you already have a JASMIN account, this will be the alternative email address you provided).
    1. If you are having difficulty opening the link, please sign out of any alternative OneDrive accounts and then try again.
    1. You will then be emailed a verification code.
    1. Enter this code on OneDrive.
    1. Download the files. These contain your training account credentials.
1. Next, you need to use these training account credentials to set up your own machine for accessing JASMIN. Follow the instructions in [Ex00](https://github.com/cedadev/jasmin-workshop/tree/master/exercises/ex00). You MUST do this before your event.
1. You may need access to the JASMIN Notebook Service and Accounts Portal. Your event organiser will let you know whether this is needed for your particular event. If you do need it, you will need to follow the steps in the section below.
    1. For the JASMIN Beginners workshop: you do not need this.
    1. For the JASMIN Advanced workshop: you do need this. Please follow the steps in the section below.
1. Before the event, familiarise yourself with the [JASMIN help documentation site]({{% ref ""/"" %}}). This should answer most questions you may have.
If you are attending a 3rd-party-led event, we suggest taking a look at the
[other JASMIN workshop training materials](https://github.com/cedadev/jasmin-workshop)
\- especially if you are a new user. Your event organiser should
provide full details of how the training account should be used for that
particular event - please contact them if you have any issues.
",https://help.jasmin.ac.uk/docs/getting-started/jasmin-training-accounts#how-to-set-up-your-temporary-account,1899,298
JASMIN Notebook Service and Accounts Portal access,"Access to these services requires a password. This is not sent in the OneDrive
link. If you need access to these services, you must:
- Go to the {{<link ""https://accounts.jasmin.ac.uk/account/password_reset/"" >}}Reset Account Password function of the JASMIN Accounts portal{{</link>}}
- Enter your email address (NB: the one to be used for the training account)
- You will then receive a password reset email (don’t forget to check your spam folder). Follow-
- You can now use this new password for accessing the notebook service and the accounts portal.
",https://help.jasmin.ac.uk/docs/getting-started/jasmin-training-accounts#jasmin-notebook-service-and-accounts-portal-access,555,88
Update info,"**Step 1** : Navigate to your {{<link ""https://accounts.jasmin.ac.uk/account/profile/"">}}profile page{{</link>}} and click on ""Update info"".
{{<image src=""img/docs/update-a-jasmin-account/update-profile.png"" caption=""Update JASMIN profile page before changing any details"">}}
**Step 2** : Once the discipline and degree are updated, click ""Update profile"".
{{<image src=""img/docs/update-a-jasmin-account/update-profile-changed.png"" caption=""Update JASMIN profile page after updating your discipline and degree"">}}
**Step 3** : The updated details are displayed.
{{<image src=""img/docs/update-a-jasmin-account/profile-updated.png"" caption=""The profile page shows the updated discipline and degree"">}}
",https://help.jasmin.ac.uk/docs/getting-started/update-a-jasmin-account#update-info,700,68
Update SSH public key,"To update your SSH public key, first you need to generate a new SSH key pair as described here: [Generate an SSH key pair]({{% ref ""generate-ssh-key-pair"" %}}). This should be done on
your local machine (e.g. Windows / Linux / Mac). You MUST protect your key
with a strong passphrase. Then follow these steps to update your SSH
public key.
The system will reject any key that it recognises has been used before (across all users) so you must generate a fresh key each time: you cannot recycle an old key.
**Step 1** : Navigate to your {{<link ""https://accounts.jasmin.ac.uk/account/profile/"">}}profile page{{</link>}}, and click on ""Update key"".
{{<image src=""img/docs/update-a-jasmin-account/update-ssh-key.png"" caption=""The form to enter your new SSH public key"">}}
**Step 2** : Paste in the new SSH public key, and click ""Update SSH key"".
{{<image src=""img/docs/update-a-jasmin-account/ssh-key-updated.png"" caption=""The profile page showing your updated SSH public key"">}}
**Step 3** : A box confirming you've updated your key is displayed.
Note: Please allow 15 minutes before attempting to log in again, while the new key becomes active on the JASMIN system.
If you get the message ""not a valid ssh public key"" please check that you have
copied the text from the `.pub` file and that no newline characters are
included: the public key should be a single line of text.
It can be difficult to see this as the text automatically wraps
itself to fit in the text box.
",https://help.jasmin.ac.uk/docs/getting-started/update-a-jasmin-account#update-ssh-public-key,1468,234
Change password,"**Step 1** : Go to your username in the top-right corner of the navigation bar.
Select ""Change password"" from the drop-down menu.
{{<image src=""img/docs/update-a-jasmin-account/change-password.png"" caption=""Select change password from your user menu"">}}
**Step 2** : Enter the new password which must conform to the rules stated. You should ideally generate a strongly random string
in an encrypted password manager and store it securely. Make sure it is NOT the same as your SSH key passphrase.
Re-enter the new password to confirm, then click ""Change password""
{{<image src=""img/docs/update-a-jasmin-account/change-password-form.png"" caption=""Change password form with old and new passwords filled in"">}}
",https://help.jasmin.ac.uk/docs/getting-started/update-a-jasmin-account#change-password,707,96
Link CEDA account,"Linking your CEDA account to your JASMIN account allows you filesystem access
to data on CEDA Archive. If you need to access data on the CEDA Archive and
you do not have an account, you will need to apply for a
[CEDA account]({{% ref ""ceda-archive#register"" %}}).
**Step 1** : On the profile page, select ""Link now"" which is next to the
field ""Linked CEDA Account"". This will take you to the CEDA accounts portal
page where you need to login.
{{<image src=""img/docs/update-a-jasmin-account/ceda-account-login.png"" caption=""CEDA accounts portal login page"">}}
**Step 2** : You will be directed to the page below to authorise the CEDA
accounts portal to link your JASMIN account.
{{<image src=""img/docs/update-a-jasmin-account/ceda-link-account.png"" caption=""CEDA accounts portal linking page"">}}
**Step 3** : Then you will be sent to the page below to authorise the JASMIN accounts portal to grant the CEDA accounts portal information about your JASMIN profile.
{{<image src=""img/docs/update-a-jasmin-account/jasmin-link-account.png"" caption=""JASMIN accounts portal linking page"">}}
**Step 4** : Finally, a box confirming you've linked your accounts is shown in the CEDA accounts portal. As shown below, a field stating ""Linked to JASMIN account: `<JASMIN username>`"" should show the name of the account you have linked.
{{<image src=""img/docs/update-a-jasmin-account/ceda-link-success.png"" caption=""CEDA accounts portal showing a successful link"">}}
",https://help.jasmin.ac.uk/docs/getting-started/update-a-jasmin-account#link-ceda-account,1450,201
beginners training workshop - materials,"The [CEDA team](https://jasmin.ac.uk/about/team/) regularly run hands-on
interactive training workshops for users of JASMIN. The workshop makes use of
exercises and tutorials to help users to become familiar with the JASMIN
environment.
There are two types of workshop activities; exercises (ex) and tutorials (t).
Exercises are interactive task-based activities based on different common
scenarios of using JASMIN. Whereas tutorials provide in-depth explanations and
demonstrations (but are not interactive). All workshop resources are [freely
available on GitHub](https://github.com/cedadev/jasmin-workshop) and have
supporting videos on
[YouTube](https://www.youtube.com/playlist?list=PLhF74Yhqhjqn8NDgU7xfKGLGP8h-FQ1lt).
Anyone can complete the exercises in their own time with the resources linked,
but joining an in-person workshop is a great chance for you to interact with
the team, to discuss your research needs, and to provide feedback on the
JASMIN service. Details about future workshops will be advertised on the [CEDA
events](https://www.ceda.ac.uk/events/) page - these are currently being run
virtually due to the Covid-19 pandemic.
Most of the exercises are stand-alone and do not necessarily need to be
completed in order - however, if you are a new user, then we recommend you
follow the exercises in order as it will help to explain the general workflows
that you may encounter on JASMIN.
**Note:** If you are using the materials individually (outside of an organised
workshop event) then you will need to use your own JASMIN account and a group
workspace which you already belong to, rather than the training accounts, and
`workshop` group workspace, respectively. Instead of the `workshop` LOTUS
queue, please use the `test` queue in this case.
",https://help.jasmin.ac.uk/docs/getting-started/beginners-training-workshop,1768,252
Overall aims of the workshop:,"- To engage novice to intermediate users in best practice for working on JASMIN via hands-on exercises
- To increase understanding about: 
  - which parts of JASMIN are suited for different tasks
  - software available  on JASMIN
- To provide a face-to-face environment where the JASMIN Team can offer support and feedback on a range of issues/problems
- To gather feedback on the gaps in current provision of documentation
",https://help.jasmin.ac.uk/docs/getting-started/beginners-training-workshop#overall-aims-of-the-workshop:,424,69
Present your SSH key,"There are 2 main ways to present your SSH key when connecting via SSH-based methods:
1. specifying the path to the private key, and entering the passphrase each time
1. **(recommended)** loading the key into an `ssh-agent`, which stores the key ready for any subsequent connections you want to make.
**(2)** is more convenient, because you don't have to repeat the process each time you want to make a new connection, but **(1)** is useful to know for
testing and troubleshooting.
",https://help.jasmin.ac.uk/docs/getting-started/present-ssh-key,481,81
1\. Specifying the key location each time,"This simply involves including the `-i` option in the SSH command to specify the location of your private key:
(You would type this command in a terminal window on your local computer i.e. desktop/laptop). This might be:
- Windows
  - PowerShell terminal window (no additional software needed)
  - MobaXterm (a 3rd party linux terminal emulator for windows, licence required for continued use)
- Mac: ""Terminal"" or similar applications
- Linux: ""Terminal"" or similar applications
{{<command user=""user"" host=""localhost"">}}
ssh -i path_to/my_private_key user@remotehost
{{</command>}}
",https://help.jasmin.ac.uk/docs/getting-started/present-ssh-key#1\.-specifying-the-key-location-each-time,584,82
2\. Loading your key into an agent,"We'll demonstrate the following methods:
- Windows (option 1): use the built-in ssh-agent in Windows OpenSSH Client
- Windows (option 2): MobaXterm (a linux terminal emulator for Windows)
- Mac: ""Terminal"" application
- Linux: ""Terminal"" or similar application
",https://help.jasmin.ac.uk/docs/getting-started/present-ssh-key#2\.-loading-your-key-into-an-agent,261,38
Methods to load your key,"{{< nav type=""tabs"" id=""tabs-methods"" >}}
  {{< nav-item header=""Windows (option 1: built-in OpenSSH client)"" show=""true"">}}
  There are two ways to do this:
  - with graphical tools in Windows
  - via the Windows PowerShell (as administrator)
  The video below shows how to do it via graphical tools:
  [Setting up OpenSSH in Windows](https://youtu.be/Tl631gh4DOU)
  The equivalent steps in Powershell are as follows:
  - Check that the OpenSSH client installed with, either by
    - locating ""optional features"" and finding ""OpenSSH Client""
    - if it's not installed, tick the box to select it, then click ""Next"", then ""Add"" (and wait: NB this can be slow!)
  - or
    - typing this command in a PowerShell window, as administrator
  {{< command prompt=""PS C:\Users\User>"" shell=""powershell"" >}}
  Get-WindowsCapability -Online | Where-Object Name -like 'OpenSSH.Client*'
  {{< /command >}}
  if it **IS** installed, you'll see something like this:
  {{< command prompt=""PS C:\Users\User>"" shell=""powershell"" >}}
  (out)Name  : OpenSSH.Client~~~~0.0.1.0
  (out)State : Installed
  {{< /command >}}
  if it's **NOT** installed, you'll see
  {{< command prompt=""PS C:\Users\User>"" shell=""powershell"" >}}
  (out)Name  : OpenSSH.Client~~~~0.0.1.0
  (out)State : NotPresent
  {{< /command >}}
  in which case, note the name and version, and use them in this command:
  {{< command prompt=""PS C:\Users\User>"" shell=""powershell"" >}}
  Add-WindowsCapability -Online -Name OpenSSH.Client~~~~0.0.1.0
  {{< /command >}}
  Eventually (this can be slow!) you should see:
  {{< command prompt=""PS C:\Users\User>"" shell=""powershell"" >}}
  (out)Path          :
  (out)Online        : True
  (out)RestartNeeded : False
  {{< /command >}}
  Now you can set up the OpenSSH client:
  - Set the ssh-agent service so that it starts manually, and start it on this occasion:
  {{< command prompt=""PS C:\Users\User>"" shell=""powershell"" >}}
  Get-Service ssh-agent
  Set-Service ssh-agent -StartupType Manual
  Start-Service ssh-agent
  {{</command>}}
  (once you're confident that it's working correctly, you could set `-StartupType Automatic`)
  - Load your key into the ssh-agent
  {{< command prompt=""PS C:\Users\User>"" shell=""powershell"" >}}
  ssh-add <path to key>
  (out)Enter passphrase for <path to key>: ## right-click to paste your passphrase, then press return
  (out)2048 SHA256:1WgYUGSqffxJX6bWqBZvFsutN3Psjn5mcPV37r6D7vQ
  (out)Imported-Openssh-Key (RSA)
  {{< /command >}}
  {{< /nav-item >}}
  {{< nav-item header=""Windows (option 2: MobaXterm)"">}}
  {{< youtube id=""nEQB0ztE4yY"" title=""Windows"" autoplay=""true"" >}}
Notes:
- Remember that you need a licence to use MobaXterm beyond the intial free trial period
- The method shown above does not work with applications outside of MobaXterm (like NoMachine NX or VSCode): you would need to use the Windows OpenSSH client instead to use these with an agent.
  The video above shows the following steps to enable MobAgent and load your key:
  - Go to `Settings > Configuration > SSH`
  - Tick `Use internal SSH agent ""MobAgent""`
  - **Un**-tick `Use external Pageant`
  - Click the `+` symbol to locate your private key file (e.g. `id_rsa_jasmin`)
  - Click OK to save the settings. MobaXterm will now need to restart.
  - When you restart MobaXterm, you will be prompted for the passphrase associated with your private key.
  {{< /nav-item >}}
  {{< nav-item header=""Mac"" >}}
  ## Mac
  Mac users (OS X Leopard onwards) can optionally benefit from linking the SSH
  key to Keychain, which securely stores the passphrase as well. This means that
  even after a reboot, your SSH key is always available in any terminal session
  automatically. You can do this by running `ssh-add` with `--apple-use-keychain`:
  {{<command user=""user"" host=""localhost"">}}
  ssh-add ~/.ssh/id_rsa_jasmin --apple-use-keychain
  {{</command>}}
  And then by adding the corresponding command with `--apple-load-keychain`  to your `.zshrc` file so
  that it loads it for every new terminal session:
  {{<command user=""user"" host=""localhost"">}}
  echo ""ssh-add --apple-load-keychain"" >> ~/.zshrc
  {{</command>}}
  {{< /nav-item >}}
  {{< nav-item header=""Linux"">}}
  ## Linux
  Some linux terminal and desktop environments provide an ssh-agent as a graphical application: consult the 
  documentation for your system.
  A common one is `gnome-keyring-daemon`: check for this first in your list of processes: if it's there and
  running already, skip to the `ssh-add` command, rather than starting up another ssh-agent (which might then
  be ignored by the application you're trying to use).
  In the absence of an already-running process, you can use the following commands in a terminal session:
  - Start the ssh-agent
  {{<command user=""user"" host=""localhost"">}}
  eval $(ssh-agent -s)
  (out)Agent pid 94631
  {{</command>}}
  If the agent starts successfully, a process id (pid) is returned as above.
  - Load the key
  {{< command shell=""bash"" >}}
  ssh-add <path to key>
  (out)Enter passphrase for <path to key>: ## right-click to paste your passphrase, then press return
  (out)2048 SHA256:1WgYUGSqffxJX6bWqBZvFsutN3Psjn5mcPV37r6D7vQ
  (out)Imported-Openssh-Key (RSA)
  {{< /command >}}
  {{< /nav-item >}}
{{< /nav >}}
  ### Check that your key is loaded
  In all cases, you should now check that the key is loaded and ready to use
  {{< command shell=""bash"" >}}
  ssh-add -l
  (out)2048 SHA256:1WgYUGSqffxJX6bWqBZvFsutN3Psjn5mcPV37r6D7vQ (fred.bloggs@example.com)
  {{< /command >}}
  If you see output similar to the above, your key is now ready to be used in an SSH connection.
  Sometimes the last part of this output shows your email address, but it is
  just a comment field, which can be ignored. The fact
  that it's returned something which looks like a key ""fingerprint"",
  shows that your key is loaded successfully into the agent.
  If you don't see your key listed in output similar to the above, please try
  again: perhaps you entered the wrong passphrase? But you will need to succeed
  in loading your key before you can use it to make an SSH connection.
  Once the key is loaded, it can be used in an SSH connection, but whether this persists between different
  sessions may be dependent on your system configuration.
",https://help.jasmin.ac.uk/docs/getting-started/present-ssh-key#methods-to-load-your-key,6265,877
Troubleshooting,,https://help.jasmin.ac.uk/docs/getting-started/present-ssh-key#troubleshooting,0,0
Unprotected private key file,"Sometimes the SSH agent or application will refuse to load the private key from the file if the file's permissions 
are set too loosely: some SSH clients insist that you and only you (no other users or services
on the same machine) can access the file.
To overcome this you have 2 options:
- (quick/easy fix?) **send the contents of the file to the `ssh-add` command another way**
  In your terminal where you give the ssh-add command, try this instead:
  {{<command>}}
  cat ~/.ssh/id_rsa_jasmin | ssh-add -
  {{</command>}}
  (replace `id_rsa_jasmin` with the path to and/or name of your private key file)
  The `cat` command simply ""streams"" the contents of the file to standard output (`stdout`), while the trailing hyphen tells the `ssh-add` command to accept this streamed input (`stdin`) instead of from a file.
  You should be asked for your passphrase in the normal way and can check that the key has loaded correctly with:
  {{<command>}}
  ssh-add -l
  {{</command>}}
  or (perhaps for a more permanent solution), and/or if you are getting similar errors mentioning the `~/.ssh/config` file, might need to change the permissions permanently on these file(s).
- **change the permisisons on the file**
  {{< nav type=""tabs"" id=""tabs-key-perms"">}}
  {{< nav-item header=""Linux/Mac/cygwin/Mobaxterm"" show=""true"">}}
  {{<command>}}
chmod 600 ~/.ssh/id_rsa_jasmin
  {{</command>}}
  (replace `id_rsa_jasmin` with the path to and/or name of your private key file)
  {{</nav-item>}}
  {{< nav-item header=""Windows: PowerShell"">}}
  The equivalent method in Windows PowerShell involves these steps,
  replacing the expression with `id_rsa_jasmin` with the path to your key if different. 
  You may need to open the PowerShell window with ""run as administrator"".
{{< command prompt=""PS C:\Users\User>"" shell=""powershell"" >}}
",https://help.jasmin.ac.uk/docs/getting-started/present-ssh-key#unprotected-private-key-file,1826,264
"Set a variable ""Key"" to hold the key filename:","New-Variable -Name Key -Value ""$env:UserProfile\.ssh\id_rsa_jasmin""
","https://help.jasmin.ac.uk/docs/getting-started/present-ssh-key#set-a-variable-""key""-to-hold-the-key-filename:",68,5
Remove Inheritance:,"Icacls $Key /c /t /Inheritance:d
",https://help.jasmin.ac.uk/docs/getting-started/present-ssh-key#remove-inheritance:,33,5
Set Ownership to Owner:,,https://help.jasmin.ac.uk/docs/getting-started/present-ssh-key#set-ownership-to-owner:,0,0
For a key file located beneath directory $env:UserProfile:,"Icacls $Key /c /t /Grant ${env:UserName}:F
",https://help.jasmin.ac.uk/docs/getting-started/present-ssh-key#for-a-key-file-located-beneath-directory-$env:userprofile:,43,6
For a key file located outside of directory $env:UserProfile:,"TakeOwn /F $Key
Icacls $Key /c /t /Grant:r ${env:UserName}:F
",https://help.jasmin.ac.uk/docs/getting-started/present-ssh-key#for-a-key-file-located-outside-of-directory-$env:userprofile:,61,9
"Remove All Users, except for Owner:","Icacls $Key /c /t /Remove:g Administrator ""Authenticated Users"" BUILTIN\Administrators BUILTIN Everyone System Users
","https://help.jasmin.ac.uk/docs/getting-started/present-ssh-key#remove-all-users,-except-for-owner:",117,13
Verify:,"Icacls $Key
",https://help.jasmin.ac.uk/docs/getting-started/present-ssh-key#verify:,12,2
Remove Variable:,"Remove-Variable -Name Key
{{</command>}}
  {{</nav-item>}}
  {{< nav-item header=""Windows: GUI tools"">}}
(awaiting screenshots)
  {{</nav-item>}}
  {{</nav>}}
",https://help.jasmin.ac.uk/docs/getting-started/present-ssh-key#remove-variable:,159,14
SSH public key authentication,"JASMIN employs SSH public key authentication for login instead of username and
password. This article provides a basic overview of public key
authentication.
",https://help.jasmin.ac.uk/docs/getting-started/ssh-auth,158,23
Public key authentication (for SSH),"SSH stands for ""Secure Shell"", a protocol that allows login to another
computer over the network. This allows the user to execute commands on a
remote machine. SSH uses encryption to keep the connection secure so that it
is more difficult for hackers to passwords or other sensitive information that
may pass through the connection.
Public key authentication is an alternative means of identifying yourself to a
login server, instead of typing a password. It is more secure and flexible,
but can appear more difficult for new users.
",https://help.jasmin.ac.uk/docs/getting-started/ssh-auth#public-key-authentication-(for-ssh),533,88
Why is public key authentication more secure than a password?,"When using conventional username/password authentication you will type your
password when you log on to a server. If the server you are working on has
been compromised then an attacker could learn your password and then use it to
gain access to the remote server you are connecting to.
With public key authentication, the private key is kept only on your own machine
and the passphrase to unlock it is entered there too. An attacker would require both knowledge of the
passphrase used to protect your private key as well as the private key file
itself.
{{<alert type=""danger"">}}
It is imperative that your private key is protected by a strong passphrase so
that only you can use it.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/getting-started/ssh-auth#why-is-public-key-authentication-more-secure-than-a-password?,696,119
Public key authentication setup,"Setting up SSH keys involves the following steps:
  1. Create a pair of SSH keys (public and private **with associated passphrase** ).
  2. Provide the **public** key to remote machines/services that you wish to login to.
See [instructions for setting this up on JASMIN]({{% ref ""generate-ssh-key-pair"" %}}).
",https://help.jasmin.ac.uk/docs/getting-started/ssh-auth#public-key-authentication-setup,309,47
Login with your SSH key pair,"Once you have set up your key pair and provided your public key to the remote
machine the process is as follows:
  1. Load the **private** key into an ""authentication agent"" (such as `ssh-agent`) on your local machine.
  2. Use an SSH client (such as the `ssh` command) to login to the remote server. 
See [instructions for setting this up on JASMIN]({{% ref ""how-to-login"" %}}).
",https://help.jasmin.ac.uk/docs/getting-started/ssh-auth#login-with-your-ssh-key-pair,380,65
Logging in from multiple machines,"If you have a requirement to login to your JASMIN account from multiple
servers/locations then please copy your private key file securely to the
~/.ssh directory on the new machine. Note that you should restrict access the
private key so it is only readable by you.
",https://help.jasmin.ac.uk/docs/getting-started/ssh-auth#logging-in-from-multiple-machines,266,46
Using public key authentication with other applications,"Many other tools use the SSH protocol for their communication with remote
servers or services, for example rsync, scp, git and subversion.
",https://help.jasmin.ac.uk/docs/getting-started/ssh-auth#using-public-key-authentication-with-other-applications,139,22
JASMIN status,"This article lists sources of information about the status of JASMIN services.
",https://help.jasmin.ac.uk/docs/getting-started/jasmin-status,79,12
CEDA Status page,"The {{< link ""ceda_status"" >}}CEDA Status page{{< /link >}} is updated regularly to announce or track current and upcoming incidents which may affect JASMIN and CEDA services. Please check for known incidents or announced shceduled maintenance before contacting the helpdesk.
",https://help.jasmin.ac.uk/docs/getting-started/jasmin-status#ceda-status-page,276,40
JASMIN dashboard,"The {{<link ""https://mon.jasmin.ac.uk"" >}}JASMIN metrics dashboard{{</link>}} has been redeveloped and is now available again. 
Please read the introductory information on that page.
Currrently it provides:
- GWS Dashboard
  - view of all the volumes in a group workspace across different storage types, showing quota and usage
- LOTUS Dashboard
  - showing pending and running jobs per partition (queue) and other useful metrics
- Power Dashboard
  - showing power consumption of the various components of JASMIN
- Tape dashboard
  - showing summary information on tape usage by consortium
This is still under active development and further dashboards may be added in due course.
Please also keep an eye on:
- `JASMIN-USERS` email list (all users should be on this list. If not, please ask)
- {{< link ceda_news >}}CEDA News{{< /link >}} articles on the CEDA website
- {{< link ceda_x >}}@cedanews on X (formerly Twitter){{</link>}}
",https://help.jasmin.ac.uk/docs/getting-started/jasmin-status#jasmin-dashboard,934,145
How to login,"The instructions below cover the process of logging in using a terminal client
only. For a graphical linux desktop, please see alternative instructions using
[NoMachine NX]({{% ref ""graphical-linux-desktop-access-using-nx"" %}}).
",https://help.jasmin.ac.uk/docs/getting-started/how-to-login,229,29
Preparing your credentials: loading your SSH private key,"In order to log in using SSH, you need to present your SSH private key as your
credential (instead of a username and password). Your private key should
reside **only** on your local machine, so this process of loading your key is
something that you do on that local machine. Even if you connect via a
departmental server, there should be no need to upload your private key to
that machine: the process of loading your key and enabling agent forwarding
should ensure that the key is available to subsequent host(s) in the chain of
SSH ""hops"".
The details of how to do this can vary depending on whether your local machine
runs Windows, macOS or Linux.
{{<alert type=""info"">}}
See [presenting your ssh key]({{%ref ""present-ssh-key"" %}}) for recommended methods
to present your SSH key, depending on what type of machine you are using.
{{</alert>}}
Once you have set that up successfully, return here and continue below.
",https://help.jasmin.ac.uk/docs/getting-started/how-to-login#preparing-your-credentials:-loading-your-ssh-private-key,918,156
The JASMIN login servers,"See this article for a [description and listing of the login servers]({{% ref ""login-servers"" %}}).
",https://help.jasmin.ac.uk/docs/getting-started/how-to-login#the-jasmin-login-servers,100,15
Logging in to JASMIN,"Assuming that you have loaded your SSH private key using one of the methods
described above, then you can login to a login server as follows (do this on your own/local machine):
{{<command user=""user"" host=""localhost"">}}
ssh -A <user_id>@<login_server>
{{</command>}}
For example, user `jpax` might login to a JASMIN login server with:
{{<command user=""user"" host=""localhost"">}}
ssh -A jpax@login-01.jasmin.ac.uk
{{</command>}}
The `-A` **argument is important** because it enables ""agent-forwarding"".
This means that your the information about your SSH private key is forwarded
to your remote session on the login server so that you can use it for further
SSH connections.
",https://help.jasmin.ac.uk/docs/getting-started/how-to-login#logging-in-to-jasmin,674,98
Can't login?,"- Check our troubleshooting guide: [login problems]({{% ref ""login-problems"" %}})
",https://help.jasmin.ac.uk/docs/getting-started/how-to-login#can't-login?,82,10
The login message,"When you first login you will see a message that provides some useful
information (see Figure 1).
{{<image src=""img/docs/login/motd-labelled.png"" caption=""The login message shown on login-01.jasmin.ac.uk."">}}
",https://help.jasmin.ac.uk/docs/getting-started/how-to-login#the-login-message,209,25
X-forwarding for graphical applications (within JASMIN only),"Some applications involve displaying graphical output from an application or user interface running on a remote server,
typically to display or interact with data graphically. You can instruct
your SSH connection to enable forwarding of X-server capability by adding the
`-X` argument to the `ssh` command, as follows:
{{<command user=""user"" host=""localhost"">}}
ssh -X <user>@<hostname>
{{</command>}}
Note that the `-X` argument can be used in conjunction with the agent-forwarding
`-A` argument. In some cases the `-Y` option may be needed instead of
`-X`.
Please note that this arrangement sends your graphical output back to your
desktop machine over the network, so **should only be used within JASMIN, not
to your local desktop machine**. A solution has been put in place for a
[graphical linux desktop environment within JASMIN using NoMachine NX]({{% ref ""graphical-linux-desktop-access-using-nx"" %}}),
removing the need to send X11
graphics over the wide-area network. You are strongly advised to use NX for
any situation which involves graphical output on JASMIN. **Using X11 graphics
over the wide-area network outside of JASMIN is not supported: you will not
get good performance** and this makes inefficient use of shared resources
which can impair performance for other users. **Please use NX instead.** Of
course, you may still need to use X11 graphics to send graphical output back
to your JASMIN-side graphical desktop within JASMIN, but this is OK as it is
all within the JASMIN network.
",https://help.jasmin.ac.uk/docs/getting-started/how-to-login#x-forwarding-for-graphical-applications-(within-jasmin-only),1506,232
Where next?,"Having been through all the steps and made an initial connection to JASMIN (well done!) you
will be keen to do some real work. You should try the [sci servers]({{% ref ""sci-servers"" %}}) to get started. Use
the list presented on the login screen to select a `sci` server which is not
under heavy usage.
For example, from the JASMIN login server, you might choose to login to
`sci-vm-01`:
{{<command user=""user"" host=""login-01"">}}
ssh <user>@sci-vm-01.jasmin.ac.uk
{{</command>}}
If you are asked for a password when trying to login to this second machine,
it indicates that your ssh key is not being forwarded. Please check that you
have used the `-A` option in your initial connection to the login server, or
set up agent forwarding permanently in your SSH client configuration on your
local machine.
**There is no point in trying to enter a password (or even the passphrase
associated with your key) as only an ssh key presented in the way described
above is accepted.**
Note that once you are logged into a login server then you can omit the
`<user_id>@` prefix before the server name for the onward connection, since
your username will be the same on both systems. But there is no harm in
including it anyway, to ensure that you connect as the correct user. As shown
above, the `-A` option is not needed for the onward connection, although there
is no harm in including it.
Remember to log out of the login server in addition to the sci server when you
have finished your work, to get back to your own (local) machine:
{{<command user=""user"" host=""sci-vm-01"">}}
exit
(out)logout
{{</command>}}
{{<command user=""user"" host=""login-01"">}}
(out)Connection to sci-vm-01.jasmin.ac.uk closed.
exit
(out)logout
{{</command>}}
{{<command user=""user"" host=""localhost"">}}
",https://help.jasmin.ac.uk/docs/getting-started/how-to-login#where-next?,1765,284
,"(out)Connection to login-01.jasmin.ac.uk closed.
{{</command>}}
You are then back on your own machine.
See also [connecting to a sci server via a login server]({{% ref ""login-servers#connecting-to-a-sci-server-via-a-login-server"" %}})
for some options of different methods for connecting to a sci server.",https://help.jasmin.ac.uk/docs/getting-started/how-to-login#,304,39
Understanding new JASMIN storage,"{{<alert type=""info"">}}This article was originally written in 2018/19 to introduce new forms of storage which were brought into production at that stage. Some of the information and terminology is now out of date, pending further review of JASMIN documentation.{{</alert>}}
",https://help.jasmin.ac.uk/docs/getting-started/understanding-new-jasmin-storage,274,39
Introduction,"JASMIN continues to grow as a unique collaborative analysis environment for an
expanding community of scientists. Some of the big challenges we attempted to
address with Phase 4 were the ever-growing demand for storage space and the
increasing diversity of scientific workflows. However, we’re aware that some
aspects of the changes introduced in Phase 4 have presented some challenges in
themselves. Here, we outline the reasons for the changes and try to summarise
some of the challenges and what can be done to help deal with them.
",https://help.jasmin.ac.uk/docs/getting-started/understanding-new-jasmin-storage#introduction,535,87
Background,"(skip this if you just want to go straight to the advice):
With phase 4 we knew we had to both replace existing storage that had become
uneconomic to maintain, and add significantly more volume! However, we also
knew that most of the data stored on JASMIN disk is not touched for months on
end, but that some data is heavily used. We also knew that the traditional way
of building disk systems was no longer suitable for the scales (volumes of
data) we needed to handle, being supplanted by new technologies, and that at
some point our community would have to get used to these new technologies too.
The solution we chose for JASMIN is the same solution being deployed at most
large HPC sites: deploying tiered storage, that is more types of storage, and
requiring you, the user, to use the right kind of storage in the right place
in your workflow!
",https://help.jasmin.ac.uk/docs/getting-started/understanding-new-jasmin-storage#background,850,154
Understanding the four types of JASMIN disk:,"We have settled on four kinds of disk storage - quite an increase from the one
we had previously! Each is best for one kind of workflow, although each “can
do” most things, although not always well. We will see below that there is one
kind of activity that we now need to be much more careful about, because doing
it not only causes problems for individuals, but also for everyone else. We
could stop allowing this to happen, but it would be at a performance penalty
which would occur all the time: we have gone for “better with occasional
really slow” in preference to “always predictably slow” performance. What we
need you to do is learn how to avoid creating the “occasionally really slow”
times!
The four types are:
- **Solid state fast but not parallel disk (SSD),** really suitable for small files. This is what is used for your `/home/users` directories, so is good for things you really don’t want to lose, because this area is backed up. The same type of storage is also used for the scratch area `/work/scratch-nopw`, although this is NOT for persistent storage and is NOT backed up. SSD is great for compiling and storing millions of small files, but is the most expensive storage, so we don’t have a lot of it.
- **Fast parallel disk (PFS)** , great for jobs that read and write the same file from many different processes. This is what we had before. It’s not so great for lots of small files. This is still pretty expensive, which is one of the reasons why we haven’t simply stayed with it. Some GWS still use this, but most are migrating to the next category - scale-out file storage.
- **Scale Out File Storage (SOF)**. This is what most of our Group Workspaces (GWS) will use. This is great for large volumes of data with regular use (consider near-line tape storage if you don’t need access for a significant period). SOF is not so great for small files, so if you have lots of small files, best to either aggregate them or tar them up. This is *terrible* for parallel *write* access to files, and you must avoid that. More details on that below, but the key point is you might find you need to use the fast parallel scratch (currently /work/scratch) in your workflows as an intermediary between persisting your data and your Lotus batch jobs.
- **High Performance Object Storage (HPOS)**. This is a new type of storage, and it’s best if you are working with the cloud. It’s going to be a bit tricky to get the hang of, so pay attention to the various things we’re going to be saying over the next few months about how to use it. It is the future though...
",https://help.jasmin.ac.uk/docs/getting-started/understanding-new-jasmin-storage#understanding-the-four-types-of-jasmin-disk:,2576,470
What type of storage am I using?,"Storage  |  Type  |  Parallel-write  |  Good for small files?  |  Backed up?  
---|---|---|---|---  
/home/users  |  SSD  |  no  |  yes e.g. Installing Conda  |  yes  
/gws/pw/j07/*  |  PFS  |  yes  |  no  |  no  
/gws/nopw/j04/*  |  SOF  |  no  |  no  |  no  
/gws/smf/j04/*  |  SSD  |  no  |  yes  |  yes  
/work/scratch-pw[23] |  PFS  |  yes  |  no  |  no  
/work/scratch-nopw2  |  SSD  |  no  |  yes  |  no  
/work/xfc/volX  |  SOF  |  no  |  no  |  no
{.table .table-striped}
**Automounted SOF storage:** GWS storage volumes 
`/gws/nopw/j04/*` are automounted. This means that a particular GWS volume is not mounted by a
particular host until the moment it is first accessed. If the volume you
are expecting to see is not listed at the top level ( /gws/nopw/j04/) you
should use the full path of the volume to access it, and after a very short delay, the
volume should appear.
See also [here]({{% ref ""storage"" %}}) to see where these are mounted
throughout JASMIN.
",https://help.jasmin.ac.uk/docs/getting-started/understanding-new-jasmin-storage#what-type-of-storage-am-i-using?,971,165
The BIG issue: why you need to be careful about parallel file access,"(even if you don’t think you are doing it):
Traditional disk systems try and do cunning things when different processes
are writing to the same file; they can lock the file so only one process can
have a turn at a time, or they can try and stack up the updates and do them
one after another (and hope they don’t interfere), but at scale, all those
tricks come with a performance cost. That cost gets paid in many ways: raw I/O
speed, how many extra copies of blocks get written, how long it takes to
rebuild if things go wrong, how big any part of the storage can be… and, how
much kit and software the vendors need to deliver to make it work. All that
cost is worth it if your workflow needs it (and can’t avoid it), but in the
JASMIN environment, not many workflows actually need it.
Our fast parallel disk is fine for those workflows, but none of the others
support it well, and in particular for our scale-out file storage, as used by
most GWSs, the way it works means that if we turn on the support for parallel
write, it will become much slower and write many more copies of some data
blocks, meaning it will do I/O slower, and it will store less! Not what we
want. The parallel read is fine! However, avoiding parallel writes has turned
out to be harder than we anticipated, your workflows have many more ways of
doing it than we thought! Sadly, when you do parallel writes, the file system
can get “stuck” and that’s when everything goes really slow, for everyone on
that host …
One way around this is for us to apply “global write locking” to a GWS volume
(your GWS manager would need to request this). This solves the problem by
preventing parallel writes altogether, but at a significant cost in
performance.
",https://help.jasmin.ac.uk/docs/getting-started/understanding-new-jasmin-storage#the-big-issue:-why-you-need-to-be-careful-about-parallel-file-access,1720,320
"FAQs, issues and solutions","Please read our collection of FAQs and known issues (and solutions!) which
we've put together [HERE]({{% ref ""faqs-storage"" %}}).
","https://help.jasmin.ac.uk/docs/getting-started/understanding-new-jasmin-storage#faqs,-issues-and-solutions",130,19
Working with many Linux groups,"The number-of-groups limitation - and how to work around it
",https://help.jasmin.ac.uk/docs/uncategorized/working-with-many-linux-groups,60,10
Description,"On JASMIN, users are added to Linux groups for access to the group workspaces
and CEDA datasets which they have registered to use, in addition to a couple
of standard groups granted for all users.
Type the `id` command to see which groups you are a member of. The first one
(shown with `gid=`) is your primary group, and the list starting `groups=`
contains your supplementary groups.
Although the Linux operating system allows the group list to contain a large
number of supplementary groups, certain types of filesystem are subject to a
maximum number that is supported. When such filesystems are accessed, a
truncated copy of the group list may be used while deciding whether to grant
read or write access to a given file or directory. This can mean that although
a user is a member of the group which is needed, a permissions error still
occurs, because the relevant group is being ignored. The groups which are
ignored are those with the higher numerical group IDs.
The most significant limitation affecting JASMIN users is for filesystems
which are mounted as type NFS, because this only supports 16 groups. In
particular, this applies to the group workspaces that are optimised for small
files (under path `/gws/smf`). It also applies to the home directories and
`/apps` software directories, and although with these directories it is not
normally necessary to restrict access via Linux groups, the restriction can
affect for example access to the NAG library licence file for NERC users. The
`panfs` filesystem type (Panasas group workspaces under `/group_workspaces`)
is also affected in principle, but it has a limit of 32 groups, which is less
likely to affect users.
",https://help.jasmin.ac.uk/docs/uncategorized/working-with-many-linux-groups#description,1679,279
Workarounds,,https://help.jasmin.ac.uk/docs/uncategorized/working-with-many-linux-groups#workarounds,0,0
newgrp,"The `newgrp` program is available on all machines, and can be used to choose a
particular group. For example, typing:
newgrp ukmo_clim
will start a session (sub-shell) in which the primary group ID of the process
is `ukmo_clim` (provided that you are already a member of that group). This
will ensure that, in that session, you have access to any files which require
that group. Note that it will also mean that any files and directories you
create in that session will be owned by the group which you selected (although
they can subsequently be changed with the `chgrp` command). When you exit from
the sub-shell, you will be returned to the original session with your normal
group list.
The main limitations of `newgrp` are that:
- It only works interactively, so it is not possible to use it in LOTUS jobs.
- It only affects the primary group ID, so cannot be used to guarantee access to more than one group simultaneously, because some groups on the supplementary group list might still be ignored for filesystem accesses.
Note that if `newgrp` prompts for a password, it is because you are trying to
use it to access a group that you are not a member of (and then there is no
password that you can usefully type).
",https://help.jasmin.ac.uk/docs/uncategorized/working-with-many-linux-groups#newgrp,1219,217
withgroups,"On the JASMIN scientific analysis machines running CentOS7, including the
LOTUS nodes in the `centos7` queue, a utility (written locally by CEDA) has
been added, which overcomes the above mentioned limitations of the `newgrp`
program. It is not available on the machines running RHEL6.
To use `withgroups`, you **first need the following command** (in your
interactive session or shell script):
module load jasmin-sci
Once you have done this, you can run any individual command with the syntax
`withgroups <group> <command>` followed by any commands arguments, for
example:
withgroups ukmo_clim ls /badc/ukmo-mslp/
You can also use a comma-separated list of groups if a command requires more
than one group, for example if you wanted to copy a file between group
workspaces:
withgroups gws_foo,gws_bar cp /gws/smf/foo/myfile /gws/smf/bar/
In these cases, the group list consists _only_ of the specified groups. So if
you specify a few additional groups, these should be safe from being ignored
during filesystem access, because they are no longer part of a long list.
Note that the group list in the calling session does not get modified. You
will see this if for example you type:
module load jasmin-sci    # a reminder of the setup command
id                        # ""id"" reports your whole list of groups
withgroups ukmo_clim id   # ""id"" only reports the ""ukmo_clim"" group
id                        # again, ""id"" will show you the whole group list
This means that you should use `withgroups` on every command for which it is
needed. If you prefer to use a subshell in which every command will have this
group(s) list (for similar behaviour to `newgrp`) you could start it by doing
something like:
withgroups ukmo_clim bash
(You might see from the help message that the `withgroups` command includes a
`-a` option to include all the original groups in the group list, just with
the specified ones at the front. However, this option is not recommended for
this purpose, because it turns out that filesystem access will ignore the
ordering and still truncate the list in numerical order.)
",https://help.jasmin.ac.uk/docs/uncategorized/working-with-many-linux-groups#withgroups,2090,335
Use of workarounds with python (and other) programs,"Note that there is no python module equivalent of either `newgrp` or
`withgroups`. In most cases, it is sufficient to run your **whole** python
script either inside a newgrp session or via withgroups, for example:
withgroups ukmo_clim python my_script.py
In the unlikely event that your Python program needs access to a large number
of groups, you will have to lauch external commands (using `os.system` or the
`subprocess` package) that start with the relevant `withgroups` prefix.
Similar considerations apply to code written in other programming languages.
",https://help.jasmin.ac.uk/docs/uncategorized/working-with-many-linux-groups#use-of-workarounds-with-python-(and-other)-programs,560,85
MobaXterm (Windows terminal client),"Windows users are recommended to try the 3rd party application MobaXterm for
connecting to JASMIN from Windows.
Please note its licence conditions for ongoing use, however.
",https://help.jasmin.ac.uk/docs/uncategorized/mobaxterm,173,26
Versions of MobaXterm,"The instructions given below are for version 24 of MobaXterm. For other
versions please check the MobaXterm documentation.
The method of configuring MobaXterm to store your private key has changed with
different versions. The recommended method, and an alternative, are shown
below.
",https://help.jasmin.ac.uk/docs/uncategorized/mobaxterm#versions-of-mobaxterm,283,42
Downloading and installing MobaXterm,"Visit the {{< link ""https://mobaxterm.mobatek.net/download-home-edition.html"" >}}MobaXterm website{{</link>}}
to download the free Home edition.
There are 2 editions available:
- ""Portable"" edition (can be installed as a regular user)
- ""Installer"" edition (may need admin privileges on your machine)
Both editions should be functionally the same once installed, but your choice
may depend on what level of access you have to your Windows machine.
For the ""portable"" edition, the contents of the downloaded zip file should be
extracted to a folder (eg. on your Windows desktop) where you can double-click
the executable file `MobaXterm_Personal_xx.x` (where `xx.x` is the version
number). Note that the `CygUtils.plugin` file should remain in this folder as
this is used for storing settings.
Once opened, MobaXterm presents a screen like this:
{{<image src=""img/docs/mobaxterm/initial-screen.png"" caption=""MobaXterm's initial screen"">}}
You can use the `Start local terminal` button or click the `+` tab to open
multiple tabs with a different terminal session in each tab. However, it's
worth setting up MobaXterm so that your private key is held globally and
is available to any new terminal session that you open.
",https://help.jasmin.ac.uk/docs/uncategorized/mobaxterm#downloading-and-installing-mobaxterm,1217,179
Enabling MobAgent to store your private key,"In order to log in to a remote host (e.g. a JASMIN host) you need to present
your private key which is kept your local machine. MobaXterm provides MobAgent
which can store your key for the time you are running MobaXterm, and can then
present it for you, for any session in any tab, so you don't have to enter your
passphrase for each new tab that you open.
",https://help.jasmin.ac.uk/docs/uncategorized/mobaxterm#enabling-mobagent-to-store-your-private-key,357,69
Video demonstration,"The video below demonstrates this process, or you can follow the screenshots
which follow:
{{< youtube nEQB0ztE4yY >}}
",https://help.jasmin.ac.uk/docs/uncategorized/mobaxterm#video-demonstration,119,18
Steps with screenshots,"You need to enable MobAgent in `Settings > Configuration > SSH`:
{{<image src=""img/docs/mobaxterm/ssh-configuration.png"" caption=""SSH Configuration tab"">}}
To do this:
- Tick `Use internal SSH agent ""MobAgent""`
- **Un**-tick `Use external Pageant`
- Click the `+` symbol to locate your private key file (e.g. `id_rsa_jasmin`)
- Click OK to save the settings. MobaXterm will now need to restart.
- When you restart MobaXterm you will be prompted for the passphrase associated with your private key.
{{<image src=""img/docs/mobaxterm/passphrase-prompt.png"" caption=""Private key passphrase prompt"">}}
Once MobaXterm has started, you can check that your SSH key has been loaded by
clicking on `Start local terminal` and using `ssh-add -l` to list the keys currently loaded.
When you type the following command in the MobaXterm terminal, you should see output similar to below:
{{<alert type=""info"">}}
IMPORTANT: The box below is an example of what your command line prompt
might look like.
The username and computer name on the left indicates which machine you are
currently on. Everything to the right of the dollar symbol '$' is the command
which you are entering into the terminal.
You don't need to type the '$' or anything before it!
The rest of the documentation will use this format to show whether you should
run the command on your local machine (`user@localhost`) or on JASMIN (`user@sci-vm-01`).
{{</alert>}}
{{<command user=""user"" host=""mobaxterm"">}}
ssh-add -l
(out)2048 SHA256:1WgYUGSqffxJX6bWqBZvFsutN3Psjn5mcPV37r6D7vQ
(out)Imported-Openssh-Key (RSA)
{{</command>}}
Sometimes the last part of this output shows your email address, but it is
just a comment field at the end of the key, which can be ignored. The fact
that it's returned something which looks like a key ""fingerprint"", shows that your key is loaded successfully into the agent.
If you don't see your key listed in output similar to the above, please try
again: perhaps you entered the wrong passphrase? But you will need to succeed
in loading your key before you can connect to JASMIN.
",https://help.jasmin.ac.uk/docs/uncategorized/mobaxterm#steps-with-screenshots,2061,315
Logging in to JASMIN using key stored in MobAgent,"As shown in the video above, once you have set up MobAgent you can connect to
JASMIN by creating a new terminal window. Click the `Start local terminal`
button.  
Next, try connecting to the login server:
{{<command user=""user"" host=""mobaxterm"">}}
ssh -A <user_id>@login-01.jasmin.ac.uk
{{</command>}}
Notes:
- replace `<user_id>` with your own JASMIN username)
- check the list of available [login servers]({{% ref ""login-servers"" %}}): you don't have to use the one above!
",https://help.jasmin.ac.uk/docs/uncategorized/mobaxterm#logging-in-to-jasmin-using-key-stored-in-mobagent,475,71
Logging in to JASMIN without storing your key in MobAgent,"MobAgent provides the most convenient way of accessing JASMIN. However, if you
want to login to JASMIN without setting up MobAgent you can do so as follows.
Click on the `Start local terminal` button then enter the following command. The final two lines show the output you should see: you are prompted for your key's passphrase, then if successful, you see confirmation that the key is loaded.
{{<command user=""user"" host=""mobaxterm"">}}
eval $(ssh-agent -s)
ssh-add ~/.ssh/id_rsa_jasmin
(out)Enter passphrase for ~/.ssh/id_rsa_jasmin:
(out)Identity added: ~/.ssh/id_rsa_jasmin
{{</command>}}
If your key is named something different, or stored at a different location
than shown above, you will need to specify its location in the `ssh-add`
command. Note that MobaXterm refers to Windows drives as e.g. `/drives/c/`
(with forward slashes). So if you've put your key on your desktop, then the
path to that **might** be `/drives/c/Users/fred/Desktop/id_rsa_jasmin` (if ""**fred**""
is your local username on Windows).
You will need to do this each time you open a new terminal session. To connect
to JASMIN:
{{<command user=""user"" host=""mobaxterm"">}}
ssh -A <user_id>@login-01.jasmin.ac.uk
{{</command>}}
Again, you must replace `<user_id>` with your JASMIN username.
As you can see, the MobAgent method mentioned previously makes this a bit
easier, because it persists between sessions and you navigate to the location
of your private key using graphical tools instead of having to type the path.
",https://help.jasmin.ac.uk/docs/uncategorized/mobaxterm#logging-in-to-jasmin-without-storing-your-key-in-mobagent,1495,221
Additional MobaXterm features,"MobaXterm is a comprehensive application that enables many useful features
such as:
- Saved session configurations
- X-forwarding / X11 / X server (**Not recommended on JASMIN**, see [NoMachine NX for graphical linux desktop]({{% ref ""graphical-linux-desktop-access-using-nx"" %}}) instead)
- SSH agent forwarding
- SFTP access (with graphical drag-n-drop)
- Split-tab mode
- SSH tunnelling
- Basic Linux commands such as: `cd, ls, pwd, cat`
- Command-line transfer utilities: `scp, rsync, wget`
Please see the {{< link ""https://mobaxterm.mobatek.net/documentation.html"" >}}MobaXterm documentation{{</link>}} for details
on these.
",https://help.jasmin.ac.uk/docs/uncategorized/mobaxterm#additional-mobaxterm-features,630,83
Processing requests for resources,"This article **is for consortium managers** and explains:
- The overall process and role of consortium manager
- How to process a request for new resources for a service, submitted by a project
- How to process a request for additional resources on an existing project's service
",https://help.jasmin.ac.uk/docs/uncategorized/processing-requests-for-resources,279,47
Introduction,"Please make sure you have read the article
[""Requesting resources""]({{% ref ""requesting-resources"" %}}) to understand how the process works from the
requester's point of view.
This article will show you how to process requests for resources, but also
show you what you need to consider when doing so.
",https://help.jasmin.ac.uk/docs/uncategorized/processing-requests-for-resources#introduction,301,48
Overall process,"The overall process for handling requests for resources from projects is
summarised as follows. It all happens via the
{{< link ""jasmin_projects_portal"" >}}JASMIN Projects Portal{{</link>}}.
  * A **project owner** will have created a record representing a **project** for you to review
  * This will come to you for review because they have selected your consortium as the most relevant to their work.
    * But this means that the resources they are requesting need to come out of your consortium allocation.
  * The project record should contain:
    * A project description
    * Requests for 1 or more services. Each proposed service (e.g., a Group Workspace or a Cloud Tenancy) can have
      * 1 or more requirements for **resources**
        * For a Group Workspace service, these could be requirements for disk or tape storage
        * For a Cloud Tenancy service, these could be requirements for disk, CPU and memory resources for the tenancy
    * Once **all the requirements for a service** have been agreed with the project owner, you, as consortium manager can submit the request for provisioning by the JASMIN team.
      * Different services on the same project can be provisioned separately: for example, we can provision the Group Workspace resources while you're still discussing what's needed for a cloud tenancy.
      * But all the requirements for a particular service (for example, disk and tape, for a Group Workspace) need to be agreed before it can be submitted for provisioning.
      * If necessary, certain requirements can simply be rejected and excluded from the submission and can be added later.
      * Requests made by projects directly to the JASMIN team will be referred back to this process, as it's essential that we manage valuable resources in an organised manner, involving you as consortium manager to make decisions for your consortium's allocations. So this does involve active engagement on your part.
  * Once resources have been provisioned, you should be notified so that you can track the progress of the request on behalf of the project.
",https://help.jasmin.ac.uk/docs/uncategorized/processing-requests-for-resources#overall-process,2091,333
Role of the consortium manager,"Your task when reviewing should be to:
  * Check that the project has selected the appropriate consortium: does this activity sit within your domain?
  * **Scrutinise** the project's request for use of your allocation of consortium resources for this project. Questions to consider are:
    * Do the proposed resource requirements sound reasonable?
      * Have they properly considered using shared services like scratch and XFC to minimise the need for their own dedicated resources? For example:
        * Instead of 100TB SOF for their Group Workspace, could they manage with 25TB, knowing that copious scratch space is nearly always available?
        * Could some of the 100TB go on tape first, ""streaming"" their processing so that only some fraction is needed on disk at a time?
        * Have they asked for tape resources alongside their disk space? (this would reassure you that they've considered a sensible workflow like above, but you might want to discuss with them). It's reasonable to ask for an equivalent amount of space on tape as on disk. Historically this has usually been provisioned by default anyway, but we do want to capture what the project actually plans to use.
      * Is the proposed workflow free of unnecessary duplication of data already available elsewhere on JASMIN, either in the CEDA Archive or other Group Workspaces (which can be processed in place rather than needing to be copied)?
      * Some of this information will be in the project description but feel free to contact the project owner to gather more details yourself to form your opinion.
    * Are the start and end dates realistic?
      * You will need to reclaim your consortium's resources once a project has finished (in order to support other projects from your allocation), so it's important that these dates are agreed and regularly reviewed.
      * Does the project actually need all the requested resources initially, or could they manage with some fraction to start with?
        * ""Hogging"" disk space (or other resources), but not using it, wastes expensive resources.
        * Is there a date beyond which the data could be moved off disk onto tape, freeing up disk? The project's requirements can be modified during its lifetime to achieve this, but this should be considered (at the start) as a way to ""sunset"" the disk requirements while keeping some data still available after the active phase of the project.
        * Group Workspace storage is short-term storage for the duration of a project only. Keeping data in Group Workspace disk storage in an open-ended way is bad for a number of reasons: other projects won't have the resources they need, plus there are a number of data-sharing services attached to the CEDA Archive (such as, enabling other projects to discover the data) which make this a much better place to share data from, with all the benefits of professional data curation. The disk requirement for Group Workspaces should only be allocated for the active phase of the project where working space is needed.
      * Longer-term visibility of any data produced should be addressed by involving the CEDA Archive team at the project planning stage.
",https://help.jasmin.ac.uk/docs/uncategorized/processing-requests-for-resources#role-of-the-consortium-manager,3187,517
Processing a request for new resources,"Log in to the {{< link ""jasmin_projects_portal"" >}}JASMIN Projects Portal{{</link>}}
This requires your JASMIN credentials (so you need to have a JASMIN account to
proceed). Two-factor authentication is used: you can opt to send a
verification code to the email address registered with your JASMIN account,
then enter that code to proceed. Check your spam folder if the message doesn't
appear.
{{<image src=""img/docs/processing-requests-for-resources/file-beGPU0FT33.png"" caption="""">}}
Follow the ""Consortia"" link at the top, to show all the consortia. The one(s)
you are responsible for will have a ""Go to consortium"" button.
{{<image src=""img/docs/processing-requests-for-resources/file-Sgzblb0QRe.png"" caption="""">}}
Click ""Go to consortium""
{{<image src=""img/docs/processing-requests-for-resources/file-ywQMwjynoE.png"" caption="""">}}
This screen has 3 tabs: we are looking at the
""overview"" tab first, showing your how much of each type of resource is
committed (provisioned) against the allocated resources for your consortium.
Go to the ""Projects"" tab. If you have any projects with outstanding items for
review, the number of projects will be indicated next to the ""Projects"" tab.
{{<image src=""img/docs/processing-requests-for-resources/file-e2cXo2Xt2L.png"" caption="""">}}
Here we can see all the projects in this consortium:
scroll down to see ""cards"" for all the projects. The tab indicated that there
were 2 to be reviewed and these are labelled here too.
Click ""Go to Project"" for the one you want to review, to see the **project
overview** screen.
{{<image src=""img/docs/processing-requests-for-resources/file-uhXOLmsJmX.png"" caption="""">}}
The ""Overview"" screen for a project gives you the
timeline of what's happened (most recent first) so you can see the description
and any comments.
Now go to the ""Services"" tab, to see what service(s) this project thinks it
needs, and what resources are requested for those services:
{{<image src=""img/docs/processing-requests-for-resources/file-YtZ08rDv8i.png"" caption="""">}}
In this case, they're just asking for a Group
Workspace, and have documented a requirement for 10 TB of SOF disk space,
between the dates shown. For now, we'll just consider how to approve that one
request (but you could encourage them to ask for tape space as well, then
approve both together).
The SOF requirement is in the ""REQUESTED"" state, meaning it's awaiting your
review, so click the ""?"" icon, to review that requirement:
{{<image src=""img/docs/processing-requests-for-resources/file-B2GlK1C4RM.png"" caption="""">}}
Here, you can see that this requirement for 10 TB SOF
of SOF disk is OK in terms of your consortium's overall allocation for SOF
(1.6 PB), and current commitments (770.2 TB or 48%) against that allocation.
But only you know what other projects are in the pipeline in your science
domain:
  * is there some big project on the horizon which will need lots of space in the near future?
  * if you're struggling for space, are there other projects which you could ask to give up some or all of their disk space, if they've finished their work?
  * if they've asked for particular types of disk space (PFS, SSD or HPOS), have they justified why? The default storage type for Group Workspaces should be SOF, but there may be reasons why others are more appropriate, e.g. 
    * HPOS (Object Storage) for visibility inside / outside of JASMIN (to be encouraged!)
    * SSD (Solid State Disk) for small file storage (but very limited amounts available, normally 100GB at a time)
    * PFS (Parallel File System) for workflows which absolutely must be able to write in parallel to the same file from multiple processes (...but there are 2 PB of scratch space available for this purpose, so consider carefully).
If you want to suggest changes to the project (e.g. you think they've picked
the wrong type) you can ""Reject"" with comments, and they can re-submit the
request.
If you're happy to approve, click ""Approve"":
{{<image src=""img/docs/processing-requests-for-resources/file-Wc5Leta8pS.png"" caption="""">}}
The SOF requirement now has status ""APPROVED"" so (as
long as there are no other requirements in ""REQUESTED"" state), you can click
""Submit for provisioning"".
If there are other requirements that have been requested, you either need to
approve them too, or reject them so that you can agree acceptable resource
request with the project owner.
{{<image src=""img/docs/processing-requests-for-resources/file-wbaUpLxykd.png"" caption="""">}}
If submitted, the requirement will then have the
status ""AWAITING PROVISIONING"" and the JASMIN team will pick up the request to
arrange provision of the resources.
{{<image src=""img/docs/processing-requests-for-resources/file-Tsra3DcRkq.png"" caption="""">}}
Once the JASMIN team has completed provisioning the
resources, the status changes to ""PROVISIONED"" and the location is confirmed:
in this case giving the path to the disk space now that's now available.
At this point, the project owner who requested the resources would also be
notified to check the status on the portal, so should be able to pick up the
location, but you may wish to check with them yourself, to track the request
to completion.
",https://help.jasmin.ac.uk/docs/uncategorized/processing-requests-for-resources#processing-a-request-for-new-resources,5184,759
Processing a request for additional resources,"Where a project already exists, a project owner can submit a request for
changes to resources on an existing service, or could request an additional
service (e.g. adding a cloud tenancy where there's already a Group Workspace).
Adding an additional service should work as described above, but reviewing a
request to modify resources on an existing service, is shown below:
We start from the project overview screen (so check above for steps to reach
that):
{{<image src=""img/docs/processing-requests-for-resources/file-Cm6NWP5GdB.png"" caption="""">}}
We can see in the comments that something has been requested: those comments
are attached by the project owner to provide extra context to the request.
Go to the Service tab to review further:
{{<image src=""img/docs/processing-requests-for-resources/file-wexo0BiLCL.png"" caption="""">}}
Here we can see the original 10 TB which is
""PROVISIONED"" but above it is the request for the additional 1 TB, with status
""REQUESTED"".
Click the ""?"" to review:
{{<image src=""img/docs/processing-requests-for-resources/file-Nv3hYTq2de.png"" caption="""">}}
From the consortium manager's point of view, it's the
same process as before, so go through the same steps to scrutinise the
request.
Reject the request (with comments) if you want the project owner to change
things, or approve it if you're happy:
{{<image src=""img/docs/processing-requests-for-resources/file-enxubp0Anh.png"" caption="""">}}
Now the request is marked as ""APPROVED"", and since we
have no other requests in ""REQUESTED"" state for you to review, you can click
""Submit for provisioning"".
{{<image src=""img/docs/processing-requests-for-resources/file-8rA9Xz0Auw.png"" caption="""">}}
Once you have submitted it, the request is marked
""AWAITING PROVISIONING"" for the JASMIN team to pick up.
In this case, we're adding space to the same service, so when it comes to
recording what's provisioned, the 10 TB & 1 TB ""chunks"" will be combined (in
terms of how they're recorded here), reflecting the fact that the JASMIN team
will have simply expanded the size of the (single) disk volume. This makes
sense if the project has asked for more space, but the total space is needed
until the end of the whole project. So the previous 10TB (the record, not the
actual disk) has been marked ""DECOMMISSIONED"":
{{<image src=""img/docs/processing-requests-for-resources/file-MjIM9tdrHr.png"" caption="""">}}
An alternative, which might be applicable in some
cases, is where the extra space ""boost"" is only needed for a shorter period of
time, as shown by the different end dates to the 2 requirements below. They're
both referring to the same physical storage, but the extra 1 TB space
""expires"" first.
{{<image src=""img/docs/processing-requests-for-resources/file-UtPmX7AzE1.png"" caption="""">}}
",https://help.jasmin.ac.uk/docs/uncategorized/processing-requests-for-resources#processing-a-request-for-additional-resources,2768,391
Processing requests for resource types other than storage,"The process for reviewing requests for other resource types, such as those
needed for cloud tenancies, is the same as above. Further examples may follow
here as needed.
",https://help.jasmin.ac.uk/docs/uncategorized/processing-requests-for-resources#processing-requests-for-resource-types-other-than-storage,169,28
How projects and resources are managed,"Resouces on JASMIN, such as storage and compute, are allocated to science
communities separated into ""consortia"". Each consortium has a manager: a
representative of that science community who is in touch with its major
activities and understands the resource requirements for projects in that
domain. Representatives of individual projects should discuss requirements
with their Consortium Manager, who is best placed to make decisions about the
allocation of JASMIN resources within that consortium. Requirements can be
documented using the “JASMIN Resource Management” tool, but need to be
approved by a Consortium Manager before being passed to the JASMIN Team for
provisioning.
[See here for further details](https://jasmin.ac.uk/users/resources-projects),
including a list of current consortia and their managers.
",https://help.jasmin.ac.uk/docs/uncategorized/requesting-resources#how-projects-and-resources-are-managed,819,114
Accessing the JASMIN projects portal,"The {{< link ""https://projects.jasmin.ac.uk"" >}}JASMIN projects portal{{</link>}} provides a place
to:
- document the resources required for a project (new projects, or changes to existing projects)
- submit those requirements for review
- track the provision of those resources
Usually, you would only need to access the projects portal if you are:
- the Principal Investigator (or their delegated project manager) for a project
  - to document requirements for a project
  - to invite other individuals (with a JASMIN account) who may wish to view and/or discuss the requirements
- a consortium manager
  - to review/approve resource requirements
**Note: Please do not make requests yourself unless you are involved in the
management of the project: speak to the project or group workspace (GWS)
manager and ask them to make the request.**
These users need to log in with their JASMIN account credentials, the same as
those used for the JASMIN accounts portal. 2-factor authentication is in use,
with verification codes sent to the email address associated with your JASMIN
account (this is the same process used to access the JASMIN notebook service).
Once you have logged in, you are presented with a view of the projects where
you are named as owner or collaborator (or, for consortium managers, where you
are the relevant consortium manager). A further guide for consortium managers
about how to process requests for resources is available
[here]({{% ref ""processing-requests-for-resources"" %}}).
{{<image src=""img/docs/requesting-resources/file-CFe92vMRQV.png"" caption="""">}}
",https://help.jasmin.ac.uk/docs/uncategorized/requesting-resources#accessing-the-jasmin-projects-portal,1582,237
Create a project to record resource requirements,"To create a new project:
- Go to ""My Projects""
- Click ""Start new project""
{{<image src=""img/docs/requesting-resources/file-zl6tBZHWsW.png"" caption="""">}}
Enter details for the project, as described below.
A project can have several Services, such as:
- a group workspace
- a managed cloud tenancy
- an external cloud tenancy
  - [particular compute requirements*]
  - some services are not yet able to be described/requested via this tool, but will be soon.
  - please contact the helpdesk if you're not able to describe what you need.
To add the services needed for the project:
- in the panel on the right, click ""Add Service""
- select the category of service required: in this case, we're making requirements for a group workspace, but the available options are: 
  - **Group Workspace** (for shared disk storage for a project)
  - **External Tenancy VIO** (for an external cloud tenancy on the VIO cloud platform)
  - **Managed Tenancy VIO** (for a managed cloud tenancy on the VIO cloud platform)
  - (please do not use the ""... Tenancy **MCP** "" options as these will soon be removed)
- provide a short name for the service
- click ""create"".
{{<image src=""img/docs/requesting-resources/file-bo7r6lG1NA.png"" caption="""">}}
{{<image src=""img/docs/requesting-resources/file-GPxShAAROa.png"" caption="""">}}
{{<image src=""img/docs/requesting-resources/file-I0n92JdcKD.png"" caption="""">}}
{{<image src=""img/docs/requesting-resources/file-kbDroJj43z.png"" caption="""">}}
A Service may have several requirements, but, for example, we could request 10
TB of SOF storage for our GWS:
{{<image src=""img/docs/requesting-resources/file-uV9ApenDrm.png"" caption="""">}}
SOF (scale-out filesystem) is the usual type of storage used for GWS volumes,
but you could also request:
- HPOS (high-performance object store available via an S3-like interface)
- PFS (parallel file system, by special request if certain workflows specifically need this)
- SSD (Solid State Disk), used for ""small files"" or ""SMF"" volumes for storing code or virtual environments to share within a GWS.
It is assumed that you've considered carefully how you will do your work on
JASMIN, with some knowledge of its services and components. You may find the
following helpful:
- Article: [Understanding new JASMIN storage]({{% ref ""understanding-new-jasmin-storage"" %}})
- [JASMIN workshop](https://github.com/cedadev/jasmin-workshop) overview talk, explaining the main services offered by JASMIN
- how your request will be [scrutinised]({{% ref ""processing-requests-for-resources"" %}}) by the relevant consortium manager.
Once created, the requirements appear in the list, along with their start and
end dates and status. This one is ""REQUESTED"".
{{<image src=""img/docs/requesting-resources/file-gg37EcPAxa.png"" caption="""">}}
{{<image src=""img/docs/requesting-resources/file-gKtZB9giSE.png"" caption="""">}}
Click ""Submit for review"" and the manager of the relevant consortium will be
notified that they need to review the request, with status updated to
""REQUESTED"".
Only a user with ""OWNER"" status on a project can submit the project for
review. It's best if one person coordinates with the consortium manager once
the outline plan has been agreed with other project contacts (see inviting
another user and joining a project, below)
{{<image src=""img/docs/requesting-resources/file-KwyoJbji2p.png"" caption="""">}}
If there are multiple requirements, make sure these are all documented so that
the consortium manager can consider them all together, in context. Just repeat
the process above for each additional requirement.
The project is marked as ""UNDER REVIEW"" while the requirements are being
agreed.
The consortium manager may approve, reject or request changes to the
requirements before they are agreed.
Once the consortium manager has agreed, the requirements are ready for
provisioning: the JASMIN team will then manage the provisioning of the
requested resources and the project contact will be notified when this is
complete and the new resources available.
",https://help.jasmin.ac.uk/docs/uncategorized/requesting-resources#create-a-project-to-record-resource-requirements,4017,556
Invite another user,"By default, information in the projects portal is only visible by those
nominated ""collaborators"" on the project, the relevant consortium manager, and
the JASMIN Team (or others involved in the provision of JASMIN services). To
share your plans for what's needed on a project with other individuals, you
can invite another user to the project:
- Go to ""My Projects""
- In the panel on the right, click the link with the number of current collaborators
- Enter the email address of the other user you wish to invite, and press ""Invite""
  - Although you are inviting them by email address, they must have a JASMIN account in order to access the projects portal.
",https://help.jasmin.ac.uk/docs/uncategorized/requesting-resources#invite-another-user,659,114
Join an existing project by invitation from another user,"If you have received an invitation code from an existing collaborator on a
project, you can use it to join a project as follows:
- Go to ""My Projects""
- Click ""Join existing project""
- Enter the invitation code which the other user has sent you.
",https://help.jasmin.ac.uk/docs/uncategorized/requesting-resources#join-an-existing-project-by-invitation-from-another-user,246,46
Request additional resources for an existing project,"You can add new requirements to a project once it has been PROVISIONED (but
not while it's already UNDER_REVIEW).
To add new requirements, go to ""My Projects"" and create the new requirement.
For example, if a GWS currently has 10 TB of SOF space provisioned, and the
new and an additional 5 TB of space is needed, then:
- If the GWS as a whole has the same end date, then create a new requirement for 15TB, with that end date, and submit this so that it can be reviewed.
- If it's just a temporary / short-term boost of storage that's needed 
  - consider whether scratch or XFC storage would suffice
  - create a requirement for the additional storage only, confirming the start and end dates of the new storage
  - in some cases, the end dates of the original storage will be out-of-date, so please agree new dates with your consortium manager as part of this process.
Although these examples have concentrated on storage requirements, the same
methods apply to requesting cloud tenancies. More detail on how to request
these, and additional methods for documenting requirements for compute
resources, will follow in due course.
",https://help.jasmin.ac.uk/docs/uncategorized/requesting-resources#request-additional-resources-for-an-existing-project,1131,196
Alternatives,"In some cases, it may not be appropriate to provide dedicated resources to
certain projects. Your consortium manager should be able to help you look at
other options. In some cases, a relevant project may already exist, and by
discussion with the appropriate manager of that project, it may be possible
for you to make use of those existing resources without the need to create a
new one. There is a certain management overhead associated with setting up and
operating each project's dedicated services, which use expensive resources, so
requests do have to be considered carefully.
The following ""generic"" Group Workspaces exist for general use by members of
these communities and often solve the problem of a small GWS needed by an
individual:
- [ncas_generic](https://accounts.jasmin.ac.uk/account/login/?next=/services/group_workspaces/%3Fquery%3Dncas_generic) : (National Centre for Atmospheric Science)
- [nceo_generic](https://accounts.jasmin.ac.uk/account/login/?next=/services/group_workspaces/%3Fquery%3Dnceo_generic) : (National Centre for Earth Observation)
- [ceh_generic](https://accounts.jasmin.ac.uk/account/login/?next=/services/group_workspaces/%3Fquery%3Dceh_generic) : (UK Centre for Ecology and Hydrology)
In these cases, the relevant consortium manager is usually the manager of the
""generic"" workspace so can approve applications for access to these workspaces
themselves.
Please consult the
[list of available group workspaces](https://accounts.jasmin.ac.uk/account/login/?next=/services/group_workspaces/)
for other options.
Another alternative, for easily accessible short-term storage for an
individual user is the [JASMIN Transfer Cache (XFC) service]({{% ref ""xfc"" %}}).
",https://help.jasmin.ac.uk/docs/uncategorized/requesting-resources#alternatives,1700,205
Choice of available Tools/Routes,"See [Data Transfer Tools]({{% ref ""data-transfer-tools"" %}}) for general
details.
Users transferring data between ARCHER2 and JASMIN are often transferring
relatively large sets of data, so it is important to choose the most
appropriate route, method and tools to ensure you get the most efficient and
reliable transfer experience. This can vary depending on system and network
conditions.
The recommended option (as of mid-2024) is now **Globus**.
Common requirements to all of the methods are:
- an account with the [jasmin-login](https://accounts.jasmin.ac.uk/services/login_services/jasmin-login/) access role on JASMIN.
- a login account at ARCHER2
Please note:
- Enquiries about access to or use of ARCHER2 should be directed to ARCHER2 support ([support@archer2.ac.uk](mailto:support@archer2.ac.uk))
- Enquiries about access to or use of JASMIN should be directed to JASMIN support (use beacon, below-right or [support@jasmin.ac.uk](mailto:mailto:support@jasmin.ac.uk))
",https://help.jasmin.ac.uk/docs/data-transfer/transfers-from-archer2#choice-of-available-tools/routes,977,129
Available transfer methods,,https://help.jasmin.ac.uk/docs/data-transfer/transfers-from-archer2#available-transfer-methods,0,0
Basic SSH transfer,"[**scp/rsync/sftp**]({{% ref ""rsync-scp-sftp"" %}}): Simple transfers using easy method, pushing data to general purpose xfer nodes. Convenient, but limited performance. 
_source_ |  _dest_ |  _notes_  
--- | --- | ---
`login.archer2.ac.uk` |  `xfer-vm-0[123].jasmin.ac.uk` |  to virtual machine at JASMIN end  
`login.archer2.ac.uk` |  `hpxfer[34].jasmin.ac.uk` |  to high-performance physical machine at JASMIN end
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/data-transfer/transfers-from-archer2#basic-ssh-transfer,440,53
GridFTP over SSH,"[GridFTP over SSH]({{% ref ""gridftp-ssh-auth"" %}}): GridFTP performance with convenience of SSH. Requires persistent ssh agent
on local machine where you have your JASMIN key. **2nd choice method**
_source_ |  _dest_ |  _notes_  
--- | --- | ---
`login.archer2.ac.uk` |  `hpxfer[34].jasmin.ac.uk` | 
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/data-transfer/transfers-from-archer2#gridftp-over-ssh,324,44
GridFTP using certificate auth,"[GridFTP using certificate auth]({{% ref ""gridftp-cert-based-auth"" %}}): Fully-featured GridFTP. Suitable for person-not-present transfers & long-
running ARCHER2 workflows. **3rd choice method: legacy technology which will be discontinued on JASMIN in 2025**
Additional requirement:
- you need to have registered the subject of your JASMIN-issued short-term credential with ARCHER2 support.
_source_ |  _dest_ |  _notes_  
--- | --- | ---
`login.archer2.ac.uk` |  `gridftp1.jasmin.ac.uk` |  over 10G JANET.<br>Dedicated GridFTP server.<br>**No need for persistent SSH agent at ARCHER2 end**
{.table .table-striped}
Notes:
- We are currently struggling to get the legacy components working on our new operating system, Rocky 9, so the current service
continues on the old (CentOS7) server `gridftp1` for now, but may need to be withdrawn at short notice.
- Even if/when we succeed in redeploying the service on Rocky 9, we plan to discontinue this service now that a better alternative
is available with Globus.
",https://help.jasmin.ac.uk/docs/data-transfer/transfers-from-archer2#gridftp-using-certificate-auth,1012,146
1st choide method: Globus,"This is now the recommended method, because:
- it always ensures the best performance
- it is a managed transfer service, less prone to overloading and system issues
- it is actively maintained
- it is easy to use
Because Globus can do transfers between two third-party locations, you don't necessarily
need to invoke the transfers from a machine on JASMIN, or ARCHER2 (even though it's those 
two locations which will be involved as source and destination for the transfer). This could
be done from your laptop or desktop, but could also be done from within a workflow that's running
somewhere (e.g. ARCHER2 or JASMIN).
So, first think about where you want to control the process from.
In **that** location, follow the steps below:
1\. Set up the Globus Command Line interface
- follow the steps [described here]({{% ref ""globus-command-line-interface/#initial-setup""%}})
2\. Identify the collections that you want to transfer between, for your transfer:
In this case, these are likely to be:
- the [ARCHER2 filesystems collection](https://app.globus.org/file-manager/collections/3e90d018-0d05-461a-bbaf-aab605283d21/overview), with ID `3e90d018-0d05-461a-bbaf-aab605283d21`
- the [JASMIN default collection](https://app.globus.org/file-manager/collections/a2f53b7f-1b4e-4dce-9b7c-349ae760fee0/overview), with ID `a2f53b7f-1b4e-4dce-9b7c-349ae760fee0`
Set an environment variable for each of these, to avoid having to type the ID each time:
{{<command>}}
export a2c=3e90d018-0d05-461a-bbaf-aab605283d21
export jdc=a2f53b7f-1b4e-4dce-9b7c-349ae760fee0
{{</command>}}
3\. Check access to these collections
These collecitons are restricted-access rather than public, so your access to them is via a series of authentication/authorisation/consent steps which the following actions will guide you through:
{{<command>}}
globus ls $a2c:/~/
(out) (ARCHER2 home directory file listing should appear)
{{</command>}}
{{<command>}}
globus ls $jdc:/~/
(out) (JASMIN home directory file listing should appear)
{{</command>}}
The steps above establish your ability to interact with each of the specified collections using Globus. Once you've completed each one,
you should see a directory listing.
Once you've completed the steps for both source and destination collections, you are ready to try a transfer.
4\. Initiate a simple transfer
{{<command>}}
globus transfer $a2c:/~/1M.dat $jdc:/~/1M.dat
(out)Message: The transfer has been accepted and a task has been created and queued for execution
(out)Task ID: aa0597a4-80a7-11ef-b36b-a1206a7ee65f
{{</command>}}
This should complete quite quickly for a small file, but for a larger file you can check the progress using the task ID.
{{<command>}}
globus task show aa0597a4-80a7-11ef-b36b-a1206a7ee65f
(out)Label:                        None
(out)Task ID:                      aa0597a4-80a7-11ef-b36b-a1206a7ee65f
(out)Is Paused:                    False
(out)Type:                         TRANSFER
(out)Directories:                  0
(out)Files:                        1
(out)Status:                       SUCCEEDED
(out)Request Time:                 2024-10-02T10:18:32+00:00
(out)Faults:                       0
(out)Total Subtasks:               2
(out)Subtasks Succeeded:           2
(out)Subtasks Pending:             0
(out)Subtasks Retrying:            0
(out)Subtasks Failed:              0
(out)Subtasks Canceled:            0
(out)Subtasks Expired:             0
(out)Subtasks with Skipped Errors: 0
(out)Completion Time:              2024-10-02T10:18:39+00:00
(out)Source Endpoint:              Archer2 file systems
(out)Source Endpoint ID:           3e90d018-0d05-461a-bbaf-aab605283d21
(out)Destination Endpoint:         JASMIN Default Collection
(out)Destination Endpoint ID:      a2f53b7f-1b4e-4dce-9b7c-349ae760fee0
(out)Bytes Transferred:            1000000
(out)Bytes Per Second:             148452
{{</command>}}
If you wanted to use the above in a script, and block/wait for the transfer task to complete before 
continuing, you can use `globus task wait <taskid>`, for example:
{{<command>}}
globus task wait aa0597a4-80a7-11ef-b36b-a1206a7ee65f
{{</command>}}
will now return control immediately, since the task has completed.
Globus transfer tasks are aysychronous, submitted to **your own** mini-queue,
where you can have as many queued tasks as you like but only 3 in progress at any one time. This ensures good performance for
all users, but your tasks do not linger in long multi-user queues. The best way to reassure yourself of this is to try it out.
For help with any globus command you can do `globus <command> --help`.
Further examples including sync and automation are given in [Globus command line interface]({{% ref ""globus-command-line-interface#examples"" %}}), with further examples in the Globus documentation at [https://](https://docs.globus.org/cli/).
Relevant examples:
- [sync with wait](https://github.com/mjpritchard/my-globus-examples/blob/main/sync_wait_simple.sh) using the CLI.
- [Repeatable transfer](https://github.com/mjpritchard/my-globus-examples/blob/main/repeatableTransferWithRefreshTokenStorage.py) using the PythonSDK (more advanced)
Note that Globus transfers (and other actions) can be managed & monitoried by:
- a web interface
- the command-line interface, and
- a Python library
all of which interact with the same underlying service.
NCAS-CMS users should note that work is currently underway to adopt Globus as a drop-in replacement for certificate-based gridftp
in Rose suites currently in use for automating processing and transferring to JASMIN.
",https://help.jasmin.ac.uk/docs/data-transfer/transfers-from-archer2#1st-choide-method:-globus,5554,664
2nd choice method: gridftp over SSH,"The next-best method for transfers between ARCHER2 and JASMIN is using globus-
url-copy with SSH authentication, as described below:
1\. Load your SSH keys for both JASMIN and ARCHER2 on your local machine, then
log in to ARCHER2.
You will need to have loaded into your SSH agent:
- The SSH key associated with your JASMIN account
- The SSH key associated with your ARCHER2 account, if you have one (it is recommended to use a different one than for JASMIN, if so)
You also need to ensure that you connect with the -A option for agent
forwarding, to enable the JASMIN key to be available for the onward
authentication with the JASMIN server.
Note that you do not (and should not) copy your JASMIN private key to ARCHER2.
It should stay on your local machine. This does mean that you need an ssh-
agent running on your local machine, so this method may not work for long-
running continuous processes that need to spawn transfers.
{{<command user=""user"" host=""login.archer2"">}}
ssh-add <jasmin ssh key> #(path to your JASMIN ssh key file on your local machine)
ssh-add <archer2 ssh key> #(path to your ARCHER2 ssh key if you have one, on on your local machine)
ssh-add -l # check both keys are loaded (are both key signatures listed in the output?)
ssh -A <archer2-username>@login.archer2.ac.uk
",https://help.jasmin.ac.uk/docs/data-transfer/transfers-from-archer2#2nd-choice-method:-gridftp-over-ssh,1294,222
(ARCHER2 now uses multi-factor auth at this stage),"{{</command>}}
2\. Load the `gct` module (to make the current `globus-url-copy` command
available in the path)
{{<command user=""user"" host=""login.archer2"">}}
module load gct
which globus-url-copy
(out)/work/y07/shared/gct/v6.2.20201212/bin/globus-url-copy
{{</command>}}
3\. Transfer a single file to your home directory on JASMIN (limited space,
but to check things work)
{{<command user=""user"" host=""login.archer2"">}}
globus-url-copy -vb <file> sshftp://<jasmin-username>@hpxfer1.jasmin.ac.uk/~/<file>
{{</command>}}
Obviously, replace `<file>` with the path to the file you want to transfer.
From here on, the commands are the same as described above in the ""1st choice
method"" but simply replace
-cred cred.jasmin gsiftp://gridftp1.jasmin.ac.uk
with
sshftp://<jasmin-username>@hpxfer1.jasmin.ac.uk
",https://help.jasmin.ac.uk/docs/data-transfer/transfers-from-archer2#(archer2-now-uses-multi-factor-auth-at-this-stage),802,89
3rd choice method: certificate-based gridftp,"This method for transfers between ARCHER2 and JASMIN uses
globus-url-copy with the concurrency option, as described below, but using
certificate-based authentication rather than SSH. This will work for person-
not-present transfers, so is suitable for long-running processes on ARCHER2
which need to spawn a transfer to JASMIN at intervals up to a month from
initiation.
1\. Load your SSH key for ARCHER2 on your local machine, then log in to
ARCHER2.
This method **does not** require you to use your JASMIN SSH key. It involves:
- obtaining tools to communicate with JASMIN's short-lived credentials service
- using the service to obtain a credential (it should last for 30 days, but a new one can be obtained at any time)
- using the credential to initiate a transfer (this what you would need to repeat for each transfer)
A fuller explanation of the process is given in this document:
- [Data Transfer Tools: GridFTP (certificate-based authentication)]({{% ref ""gridftp-cert-based-auth"" %}})
Once you have done these steps, you should be able to obtain a short-term
credential as follows (do this command at the ARCHER2 end, after having
downloaded the onlineca script as described in the document mentioned above):
{{<command user=""user"" host=""login.archer2"">}}
./onlineca-get-cert-wget.sh -U https://slcs.jasmin.ac.uk/certificate/ -l USERNAME -o ./cred.jasmin
chmod 600 cred.jasmin
{{</command>}}
Note that the path `./` is used for the script `onlineca-get-cert-wget.sh`,
but you should use the path to wherever you saved it. Alternatively, if you
make yourself a `bin` directory and add that to your `PATH`, then you don't
need to specify the path.
2\. Load the `gct` module (to make the current `globus-url-copy` command
available in your path on ARCHER2).
Once loaded, check with `which` to see that you have the `globus-url-copy` command available to you.
{{<command user=""user"" host=""login.archer2"">}}
module load gct
which globus-url-copy
(out)/work/y07/shared/gct/v6.2.20201212/bin/globus-url-copy
{{</command>}}
3\. Transfer a single file to your home directory on JASMIN (limited space,
but to check things work)
{{<command user=""user"" host=""login.archer2"">}}
globus-url-copy -vb -cred cred.jasmin SRC/FILE gsiftp://gridftp1.jasmin.ac.uk/DEST/FILE
{{</command>}}
Note that we specify the credentials file `cred.jasmin` and use the protocol
`gsiftp://` with no need to specify the username in the connection string
(we've used the path `/~/` to signify ""my home directory"" as the destination
path). Note also that the hostname in this case, `gridftp1.jasmin.ac.uk` is a
host that you can ONLY connect to directly using gsiftp: it does not permit
SSH connections.
In all other aspects, the transfer is the same as for the SSH method (see ""2nd
choice method"" below), so the commands below are very similar: we're just
using the gsiftp method instead of sshftp (both are ways of using the gridftp
transfer protocol)
4\. Recursively transfer a directory of files, using the concurrency option
for multiple parallel transfers
{{<command user=""user"" host=""login.archer2"">}}
globus-url-copy -vb -cd -r -cc 4 -cred cred.jasmin SRC/DATA/ gsiftp://gridftp1.jasmin.ac.uk/DEST/DATA/
{{</command>}}
**NOTE:** The `-cc` option initiates the parallel transfer of several files at
a time, which achieves good overall transfer rates for recursive directory
transfers. This is different from using the `-p N -fast` options which use
parallel network streams to parallelism the transfer of each file.
The `-p N -fast` options (for parallel-streamed transfers) are not currently
working to all JASMIN storage locations, so use at your own risk until further
notice. The transfer should work OK out of ARCHER2 (check by writing a single
file to `/dev/null` at the JASMIN end) but currently will not work properly
when writing to the SOF storage (`/gws/nopw/j04` or `/gws/nopw/j07`, or
`/work/xfc/vol[1-3]`, though other paths should work OK). This is a known
issue at the JASMIN end, thought to be related to network configuration, which
is still under investigation. Single-stream transfers (omitting the `-p N
-fast` options) should work fine.
Here, the options used are (see `man globus-url-copy` for full details):
-vb | -verbose-perf 
        During the transfer, display the number of bytes transferred
        and the transfer rate per second.  Show urls being transferred
-concurrency | -cc
      Number of concurrent ftp connections to use for multiple transfers.
-cd | -create-dest
        Create destination directory if needed
-r | -recurse
        Copy files in subdirectories
Experiment with different concurrency options (4 is a good start, more than 16
would start to ""hog"" resources so please consider
5\. Use the sync option to synchronise 2 directories between source and target
file systems:
{{<command user=""user"" host=""login.archer2"">}}
globus-url-copy -vb -cd -r -cc 4 -sync -cred cred.jasmin SRC/DATA/ gsiftp://gridftp1.jasmin.ac.uk/DEST/DATA/
{{</command>}}
where `SRC/DATA/` and `/DEST/DATA/` are source and destination paths,
respectively (include trailing slash).
Options are as before but with:
-sync
        Only transfer files where the destination does not exist or differs
        from the source.  -sync-level controls how to determine if files
        differ
Note that the default sync level is 2, see level descriptions below, which
only compares time stamps. **If you want to include a file integrity check
using checksums, you need to use`-sync-level 3` but there may be a performance
cost.**
-sync-level 
        Choose criteria for determining if files differ when performing a
        sync transfer.  Level 0 will only transfer if the destination does
        not exist.  Level 1 will transfer if the size of the destination
        does not match the size of the source.  Level 2 will transfer if
        the timestamp of the destination is older than the timestamp of the
        source, or the sizes do not match.  Level 3 will perform a checksum of
        the source and destination and transfer if the checksums do not match,
        or the sizes do not match.  The default sync level is 2.
So a full sync including comparison of checksums would be:
{{<command user=""user"" host=""login.archer2"">}}
globus-url-copy -vb -cd -r -cc 4 -sync -sync-level 3 -cred cred.jasmin src/data/ gsiftp://gridftp1.jasmin.ac.uk/path/dest/data/
{{</command>}}",https://help.jasmin.ac.uk/docs/data-transfer/transfers-from-archer2#3rd-choice-method:-certificate-based-gridftp,6379,927
'rclone',"This article provides information about the rclone data transfer tool. In
particular:
- what is rclone?
- installing rclone for yourself on JASMIN.
- configuring rclone
- Dos and Don'ts
",https://help.jasmin.ac.uk/docs/data-transfer/rclone,186,30
What is rclone?,"rclone is a command-line utility which enables access to lots of different
storage systems including object stores and cloud-based storage. It can also
move data between directories act as an SFTP client so could be used to access
files on JASMIN.
It is very well [documented](https://rclone.org/) already, so rather than
repeat that information here, this article highlights aspects relevant to its
use on JASMIN.
Further information will follow in due course as our experience with this tool
develops.
",https://help.jasmin.ac.uk/docs/data-transfer/rclone#what-is-rclone?,504,78
Installing rclone for yourself on JASMIN,"First off, **do not attempt to follow the documented instructions for
installing on Linux**. As a regular user, **you do not have root/administrator
permission and are not permitted to run scripts using sudo**. Normally most
utilities are already installed for you on JASMIN, but in this case you need
to adapt the documented instructions so that you can safely install this in
your OWN home directory. [That may change in the future if this tool proves
useful].
The recommended procedure for installation on JASMIN is as follows:
Fetch and unpack the Linux binary distribution: (in your home directory on an
**xfer** server)
{{<command>}}
curl -O https://downloads.rclone.org/rclone-current-linux-amd64.zip
unzip rclone-current-linux-amd64.zip
cd rclone-*-linux-amd64
{{</command>}}
Next, move the `rclone` executable to your own `bin` directory. You may need
to create that directory if it does not exist already, and add it to your
`PATH` environment variable in your `~/.bash_profile` file.
{{<command shell=""bash"">}}
",https://help.jasmin.ac.uk/docs/data-transfer/rclone#installing-rclone-for-yourself-on-jasmin,1022,148
only need these if these aren't in place already in your own setup:,"mkdir ~/bin # comment here
",https://help.jasmin.ac.uk/docs/data-transfer/rclone#only-need-these-if-these-aren't-in-place-already-in-your-own-setup:,27,5
add this to your PATH (add to your ~/.bash_profile to make this permanent),"export PATH=""~/bin:$PATH""
",https://help.jasmin.ac.uk/docs/data-transfer/rclone#add-this-to-your-path-(add-to-your-~/.bash_profile-to-make-this-permanent),26,2
move the rclone executable,"mv rclone ~/bin/
",https://help.jasmin.ac.uk/docs/data-transfer/rclone#move-the-rclone-executable,17,3
make the permissions on the file executable by you,"chmod 700 ~/bin/rclone
{{</command>}}
Note that you will not have installed the man pages, so these will not be
available: please consult the online documentation instead.
",https://help.jasmin.ac.uk/docs/data-transfer/rclone#make-the-permissions-on-the-file-executable-by-you,172,26
Configuring rclone,"Configuring rclone is covered in the rclone documentation. Essentially you
need to configure a **""remote""** representing each storage system you want to
interact with. You can then use rclone to manage data between those ""remotes"".
",https://help.jasmin.ac.uk/docs/data-transfer/rclone#configuring-rclone,232,35
Dos and Don'ts,"- Please **DO NOT use the following features on JASMIN** (at least until further notice). Some of these features look useful, but more work is needed to understand if/how they can be used safely on JASMIN without causing problems. 
  - `rclone mount` (mounting a remote as a filesystem) - **DO NOT USE**
  - `rclone rcd` (remote control daemon) - **DO NOT USE**
  - `rclone serve`(serve remote over a protocol) - **DO NOT USE**
- You should safely be able to use the following, between remotes that you have configured: 
  - `rclone copy`
  - `rclone sync`
  - `rclone lsd`
  - `rclone ls`
  - `..(other basic commands)`
Help on a particular command is found using
{{<command>}}
rclone <command> --help
{{</command>}}
Further examples of useful ways of using rclone on JASMIN will follow...
",https://help.jasmin.ac.uk/docs/data-transfer/rclone#dos-and-don'ts,791,130
'Globus Command-Line Interface',"{{<alert type=""info"">}}
Updated for new JASMIN Default Collection (replaces previous JASMIN Globus Endpoint)
{{</alert>}}
Please read {{<link ""globus-transfers-with-jasmin""/>}} first for a wider introduction to Globus on JASMIN.
This article describes
- how to transfer data using the Globus Command Line Interface. It covers:
  - how an end-user can set up their host (laptop, desktop or home directory on their departmental server) with the Globus Command-Line Interface (CLI)
  - examples of common tasks using the CLI
It is not necessary to use the Globus CLI on a JASMIN server: it is a tool
that you can use anywhere (for example your own desktop/laptop) to interact
with the Globus service, to orchestrate a transfer between 2 endpoints (collections, in new Globus terminology). The
CLI is not centrally installed on JASMIN, and does not need to be in the same
place as either of the 2 collections involved in the transfer. You could use the CLI on your own
laptop/desktop, even if the 2 collections were 2 institutional Globus collections
on opposite sides of the world. You could of course decide to install the CLI
in your home directory on JASMIN if that were useful as part of your
processing/data transfer workflow.
The [Globus CLI is fully documented here](https://docs.globus.org/cli/) with
[examples](https://docs.globus.org/cli/examples/). It provides a command-line
interface for managed transfers via the Globus cloud-based transfer service,
which usually achieves the best possible transfer rate over a given route
compared to other methods. Typically this will be significantly faster than
can be achieved over scp, rsync or sftp transfers, particularly if the
physical network path is long.
The Globus CLI is designed for use either interactively within an interactive
shell or in scripts. An alternative [Python software development kit
(SDK)](https://globus-sdk-python.readthedocs.io/en/stable/) is also available
and should be considered for more sophisticated workflows.
Alternatively, the Globus web interface at <https://app.globus.org> can be
used as an easy-to-use interface to orchestrate transfers interactively.
Whichever method is used: CLI, SDK or web interface, transfers are invoked as
asynchronous, managed tasks which can then be monitored, and if need be set to
retry automatically until some pre-set deadline.
",https://help.jasmin.ac.uk/docs/data-transfer/globus-command-line-interface,2351,347
Prerequisites,"- Linux environment with normal user privileges, **or**
- Mac environment with ability to install applications, **or**
- Windows environment with ability to install applications
- Python environment for that platform, with ability to create virtual environments (to enable installation of additional packages)
- For use of the JASMIN Default Collection: 
  - An active JASMIN user account, with “jasmin-login” role
- You may also wish to [set up your own Globus endpoint using Globus Connect Personal]({{% ref ""globus-connect-personal"" %}}), though this is not needed for these examples.
",https://help.jasmin.ac.uk/docs/data-transfer/globus-command-line-interface#prerequisites,588,87
Initial Setup,,https://help.jasmin.ac.uk/docs/data-transfer/globus-command-line-interface#initial-setup,0,0
Get a Globus identity,"Go to https://app.globus.org and either:
- choose one of the listed identity providers (e.g. GitHub, Google, ...)
- follow the link at the bottom to ""use Globus ID to sign in""
See also https://docs.globus.org/how-to/get-started/
",https://help.jasmin.ac.uk/docs/data-transfer/globus-command-line-interface#get-a-globus-identity,229,34
Set up the Globus CLI on your machine,"Do the following on your own (local) machine.
Make a Python virtual environment and activate it:
{{<command host=""localhost"" user=""localuser"">}}
python3 -m venv ./venv
source ./venv/bin/activate
{{</command>}}
Download the Globus CLI and install it into the virtual environment ( `venv`).
{{<command host=""localhost"" user=""localuser"">}}
pip install globus-cli
{{</command>}}
Try the `globus login` command. The first time you run this, you will be
prompted to authorise the Globus CLI to carry out operations on behalf of your
Globus ID. The URL will open in your default browser, where you should
authenticate with your Globus ID credentials. If you prefer, you can
copy/paste the URL from the command-line to a browser of your choice. Either
way, you then need to click ""Allow"" in the browser window, then copy/paste the
resulting ""Native App Authorization Code"" back to the terminal window where
you issued the `globus login` command:
{{<command host=""localhost"" user=""localuser"">}}
globus login --no-local-server
(out)Please authenticate with Globus here:
(out)------------------------------------
(out)https://auth.globus.org/v2/oauth2/authorize?client_id=abc1234-9c3c-4ad42-be31-8d6c87101239014&redirect_uri=https%3A%2F%2Fauth.globus.org%2Fv2%2Fweb%2Fauth-code&scope=openid+profile+email+urn%3Aglobus%3Aauth%3Ascope%3Aauth.globus.org%3Aview_identity_set+urn%3Aglobus%3Aauth%3Ascope%3Atransfer.api.globus.org%3Aall+urn%3Aglobus%3Aauth%3Ascope%3Agroups.api.globus.org%3Aall+urn%3Aglobus%3Aauth%3Ascope%3Asearch.api.globus.org%3Aall&state=_default&response_type=code&access_type=offline&prompt=login
(out)------------------------------------ 
(out)Enter the resulting Authorization Code here:
{{</command>}}
You should then see the following:
{{<command>}}
(out)You have successfully logged in to the Globus CLI!
(out)
(out)You can check your primary identity with
(out)  globus whoami
(out)
(out)For information on which of your identities are in session use
(out)  globus session show
(out)Logout of the Globus CLI with
(out)  globus logout
{{</command>}}
You can now use the Globus CLI commands as listed by the following command:
{{<command>}}
globus  --help
(out)  Usage: globus [OPTIONS] COMMAND [ARGS]...
(out)  
(out)    Interact with Globus from the command line
(out)  
(out)    All `globus` subcommands support `--help` documentation.
(out)  
(out)    Use `globus login` to get started!
(out)  
(out)    The documentation is also online at https://docs.globus.org/cli/
(out)  
(out)  Options:
(out)    -v, --verbose                  Control level of output
(out)    -h, --help                     Show this message and exit.
(out)    -F, --format [unix|json|text]  Output format for stdout. Defaults to text
(out)    --jmespath, --jq TEXT          A JMESPath expression to apply to json
(out)                                    output. Takes precedence over any specified '
(out)                                    --format' and forces the format to be json
(out)                                    processed by this expression
(out)    --map-http-status TEXT         Map HTTP statuses to any of these exit codes:
(out)                                    0,1,50-99. e.g. ""404=50,403=51""
(out)  
(out)  Commands:
(out)    bookmark        Manage endpoint bookmarks
(out)    collection      Manage your Collections
(out)    delete          Submit a delete task (asynchronous)
(out)    endpoint        Manage Globus endpoint definitions
(out)    get-identities  Lookup Globus Auth Identities
(out)    group           Manage Globus Groups
(out)    list-commands   List all CLI Commands
(out)    login           Log into Globus to get credentials for the Globus CLI
(out)    logout          Logout of the Globus CLI
(out)    ls              List endpoint directory contents
(out)    mkdir           Create a directory on an endpoint
(out)    rename          Rename a file or directory on an endpoint
(out)    rm              Delete a single path; wait for it to complete
(out)    search          Use Globus Search to store and query for data
(out)    session         Manage your CLI auth session
(out)    task            Manage asynchronous tasks
(out)    transfer        Submit a transfer task (asynchronous)
(out)    update          Update the Globus CLI to its  latest version
(out)    version         Show the version and exit
(out)    whoami          Show the currently logged-in identity
{{</command>}}
",https://help.jasmin.ac.uk/docs/data-transfer/globus-command-line-interface#set-up-the-globus-cli-on-your-machine,4417,503
Examples,"  1. **Find an endpoint (aka collection)**
We will use the `globus endpoint search` subcommand. Find help on the
particular options for that with
{{<command>}}
globus endpoint search --help
(out)Usage: globus endpoint search [OPTIONS] [FILTER_FULLTEXT]
(out)
(out)  Search for Globus endpoints with search filters. If --filter-scope is set to
(out)  the default of 'all', then FILTER_FULLTEXT is required.
(out)
(out)  If FILTER_FULLTEXT is given, endpoints which have attributes (display name,
(out)  legacy name, description, organization, department, keywords) that match the
(out)  search text will be returned. The result size limit is 100 endpoints.
(out)
(out)Options:
(out)  --filter-scope [all|administered-by-me|my-endpoints|my-gcp-endpoints|recently-used|in-use|shared-by-me|shared-with-me]
(out)                                  The set of endpoints to search over.
(out)                                  [default: all]
(out)  --filter-owner-id TEXT          Filter search results to endpoints owned by
(out)                                  a specific identity. Can be the Identity ID,
(out)                                  or the Identity Username, as in
(out)                                  ""go@globusid.org""
(out)  --limit INTEGER RANGE           The maximum number of results to return.
(out)                                  [default: 25; 1<=x<=1000]
(out)  -v, --verbose                   Control level of output
(out)  -h, --help                      Show this message and exit.
(out)  -F, --format [unix|json|text]   Output format for stdout. Defaults to text
(out)  --jmespath, --jq TEXT           A JMESPath expression to apply to json
(out)                                  output. Takes precedence over any specified
(out)                                  '--format' and forces the format to be json
(out)                                  processed by this expression
(out)  --map-http-status TEXT          Map HTTP statuses to any of these exit
(out)                                  codes: 0,1,50-99. e.g. ""404=50,403=51""
{{</command>}}
Search for the collections matching the search term ""tutorial"":
{{<command>}}
globus endpoint search ""tutorial""
(out)ID                                   | Owner                                                        | Display Name                                  
(out)------------------------------------ | ------------------------------------------------------------ | ----------------------------------------------
(out)6c54cade-bde5-45c1-bdea-f4bd71dba2cc | 6df1b656-c953-40a3-91a9-e9e8ad5173ea@clients.auth.globus.org | Globus Tutorial Collection 1                  
(out)31ce9ba0-176d-45a5-add3-f37d233ba47d | 6df1b656-c953-40a3-91a9-e9e8ad5173ea@clients.auth.globus.org | Globus Tutorial Collection 2 
{{</command>}}
The 2 globus tutorial collections actually ""see"" the same filesystem, so we'll
just use the first one.
For convenience, let's set environment variables representing the ID of this collection:
{{<command>}}
export c1=6c54cade-bde5-45c1-bdea-f4bd71dba2cc
echo $c1
(out)6c54cade-bde5-45c1-bdea-f4bd71dba2cc
{{</command>}}
Let's try listing that collection, so that we know we can interact with it. We are prompted to grant consent first:
{{<command>}}
globus ls $c1
(out)The collection you are trying to access data on requires you to grant consent for the Globus CLI to access it.
(out)
(out)Please run:
(out)
  (out)globus session consent 'urn:globus:auth:scope:transfer.api.globus.org:all[*https://auth.globus.org/scopes/6c54cade-bde5-45c1-bdea-f4bd71dba2cc/data_access]'
(out)
(out)to login with the required scopes.
{{</command>}}
Copy & paste the command it gives you (don't copy the one above) and run it, which should open a web browser window. Follow the instructions which should complete the process, then return to your terminal session.
Now let's find another collection, this time a public test collection which can be used for performance testing:
{{<command>}}
globus endpoint search ""star dtn""
(out)ID                                   | Owner              | Display Name                                     
(out)------------------------------------ | ------------------ | -------------------------------------------------
(out)ff2ee779-54fb-4dac-ade2-57568c587ae3 | esnet@globusid.org | ESnet STAR DTN private collection                
(out)ece400da-0182-4777-91d6-27a1808f8371 | esnet@globusid.org | ESnet Starlight DTN (Anonymous read only testing)
(out)e9e0d9f4-c419-44e0-8198-017fd61bf0c4 | esnet@globusid.org | ESnet Starlight DTN (read-write testing)  
{{</command>}}
We'll use the one labelled {{< mark >}}Anonymous read only testing{{</mark>}}.
Set `stardtn` to the ID of this endpoint:
{{<command>}}
export stardtn=ece400da-0182-4777-91d6-27a1808f8371
{{</command>}}
{{<alert type=""info"">}}
None of the endpoints mentioned so far require **authentication** in
order to use them. This makes demonstrating basic functionality simpler, but
we'll look at how to use one that does, later.
{{</alert>}}
  2. **Listing files at a path on an collection**
Use the `endpoint ls` command to list the contents of the `stardtn` endpoint,
at the path `/`
{{<command>}}
globus ls $stardtn:/
(out)500GB-in-large-files/
(out)50GB-in-medium-files/
(out)5GB-in-small-files/
(out)5MB-in-tiny-files/
(out)Climate-Huge/
(out)Climate-Large/
(out)Climate-Medium/
(out)Climate-Small/
(out)bebop/
(out)logs/
(out)write-testing/
(out)100G.dat
(out)100M.dat
(out)10G.dat
(out)10M.dat
(out)1G.dat
(out)1M.dat
(out)500G.dat
(out)50G.dat
(out)50M.dat
{{</command>}}
These are files and directories containing dummy data which can be used for
test purposes.
  3. **Copy a file from one endpoint to another**
Let's transfer the file `1M.dat` from the `stardtn` endpoint to `c1`:
{{<command>}}
globus transfer $stardtn:/1M.dat $c1:/~/1M.dat
(out)Message: The transfer has been accepted and a task has been created and queued for execution
(out)Task ID: 74cb181c-bf63-11ee-a90e-032e06ca0965
{{</command>}}
The transfer task is a separate activity and does not require any connection
from the CLI client to either of the 2 endpoints: the Globus transfer service
manages the transfer for us. We can check on the progress of this transfer
task with:
{{<command>}}
globus task show 74cb181c-bf63-11ee-a90e-032e06ca0965
(out)Label:                        None
(out)Task ID:                      74cb181c-bf63-11ee-a90e-032e06ca0965
(out)Is Paused:                    False
(out)Type:                         TRANSFER
(out)Directories:                  0
(out)Files:                        1
(out)Status:                       SUCCEEDED
(out)Request Time:                 2024-01-30T11:33:58+00:00
(out)Faults:                       0
(out)Total Subtasks:               2
(out)Subtasks Succeeded:           2
(out)Subtasks Pending:             0
(out)Subtasks Retrying:            0
(out)Subtasks Failed:              0
(out)Subtasks Canceled:            0
(out)Subtasks Expired:             0
(out)Subtasks with Skipped Errors: 0
(out)Completion Time:              2024-01-30T11:34:01+00:00
(out)Source Endpoint:              ESnet Starlight DTN (Anonymous read only testing)
(out)Source Endpoint ID:           ece400da-0182-4777-91d6-27a1808f8371
(out)Destination Endpoint:         Globus Tutorial Collection 1
(out)Destination Endpoint ID:      6c54cade-bde5-45c1-bdea-f4bd71dba2cc
(out)Bytes Transferred:            1000000
(out)Bytes Per Second:             421388
{{</command>}}
We can also list the destination collection to check that the file has reached
its destination:
{{<command>}}
globus ls $c1:/~/
(out)  1M.dat
{{</command>}}
We can also make a subdirectory with `mkdir`:
{{<command>}}
globus mkdir $c1:/~/mydata/
(out)    The directory was created successfully
{{</command>}}
We can move our `1M.dat` into that directory with a `globus rename` command
{{<command>}}
globus rename $c1 /~/1M.dat /~/mydata/1M.dat
(out)    File or directory renamed successfully
{{</command>}}
We now have a directory `mydata` containing files `1M.dat`:
{{<command>}}
globus ls $c1:/~/mydata/
(out)    1M.dat
{{</command>}}
  4. **Recursively copy a directory and its contents, from one endpoint to another**
Now Let's copy a directory from the `stardtn` collection which contains some small
files, to our destination endpoint `c1` (The Globus tutorial collections only
provide very limited storage space).
The files we want to copy are at the path `/5MB-in-tiny-files/a/a/` on
the `stardtn` endpoint, and are small, as their names suggest:
{{<command>}}
globus ls $stardtn:/5MB-in-tiny-files/a/a/
(out)a-a-1KB.dat
(out)a-a-2KB.dat
(out)a-a-5KB.dat
{{</command>}}
Copy the parent directory recursively to `ep1`:
{{<command>}}
globus transfer -r $stardtn:/5MB-in-tiny-files/a/a $c1:/~/star-data
(out)Message: The transfer has been accepted and a task has been created and queued for execution
(out)Task ID: 4ae9bab0-7d40-11ec-bef3-a18800fa5978
{{</command>}}
Check destination content:
{{<command>}}
globus ls $c1
(out)mydata1/
(out)star-data/
(out)
globus ls $c1:/~/star-data
(out)a-a-1KB.dat
(out)a-a-2KB.dat
(out)a-a-5KB.dat
{{</command>}}
We could now delete one of the small files using the `globus delete` command:
{{<command>}}
globus delete $c1:/~/star-data/a-a-2KB.dat
(out)Message: The delete has been accepted and a task has been created and queued for execution
(out)Task ID: be4d6934-7d40-11ec-891f-939ceb6dfaf1
{{</command>}}
And list contents again, to verify that it has been deleted:
{{<command>}}
globus ls $c1:/~/star-data
(out)a-a-1KB.dat
(out)a-a-5KB.dat
{{</command>}}
  5. **Sync a source directory to a target (repeatable)**
We could now repeat the copying of the source data, but this time using the
`-s` or `--sync-level exists` command so that we only copy the data that is
now missing from the destination. The full set of sync options is
`[exists|size|mtime|checksum]`.
{{<command>}}
globus transfer -s exists -r $stardtn:/5MB-in-tiny-files/a/a $c1:/~/star-data
(out)Message: The transfer has been accepted and a task has been created and queued for execution
(out)Task ID: 759a3cac-7d41-11ec-bef3-a18800fa5978
{{</command>}}
This should only copy the data that do not already exist at the desination: We
end up with the same set of files at the destination:
{{<command>}}
globus ls $c1:/~/star-data
(out)a-a-1KB.dat
(out)a-a-2KB.dat
(out)a-a-5KB.dat
{{</command>}}
But we can see that only 2000 bytes were transferred (so we know it only
copied that one file, which is what we wanted):
{{<command>}}
globus task show 759a3cac-7d41-11ec-bef3-a18800fa5978
(out)Label:                        None
(out)Task ID:                      759a3cac-7d41-11ec-bef3-a18800fa5978
(out)Is Paused:                    False
(out)Type:                         TRANSFER
(out)Directories:                  1
(out)Files:                        3
(out)Status:                       SUCCEEDED
(out)Request Time:                 2022-01-24T18:14:24+00:00
(out)Faults:                       0
(out)Total Subtasks:               5
(out)Subtasks Succeeded:           5
(out)Subtasks Pending:             0
(out)Subtasks Retrying:            0
(out)Subtasks Failed:              0
(out)Subtasks Canceled:            0
(out)Subtasks Expired:             0
(out)Subtasks with Skipped Errors: 0
(out)Completion Time:              2022-01-24T18:14:58+00:00
(out)Source Endpoint:              ESnet Starlight DTN (Anonymous read only testing)
(out)Source Endpoint ID:           ece400da-0182-4777-91d6-27a1808f8371
(out)Destination Endpoint:         Globus Tutorial Collection 1
(out)Destination Endpoint ID:      6c54cade-bde5-45c1-bdea-f4bd71dba2cc
(out)Bytes Transferred:            2000
(out)Bytes Per Second:             60
{{</command>}}
This task could be repeated in a shell script, cron job or even using the
Globus timer functionality, for either a source or destination directory that
is expected to change.
  6. **Interact with a collection that requires authentication**
Most Globus Connect Server endpoints are configured to require some form of authentication & authorization process. 
In the case of the JASMIN Default Collection, you link your Globus identity to your JASMIN identity.
This may be different for other collections that you use elsewhere.
Let's find, then set up an alias to the JASMIN Default Collection Endpoint. We can search for that name:
{{<command>}}
globus endpoint search ""jasmin default""
(out)ID                                   | Owner                                                        | Display Name
(out)------------------------------------ | ------------------------------------------------------------ | -------------------------
(out)a2f53b7f-1b4e-4dce-9b7c-349ae760fee0 | a77928d3-f601-40bb-b497-2a31092f8878@clients.auth.globus.org | JASMIN Default Collection
{{</command>}}
Set up an alias for this collection:
{{<command>}}
export jdc=a2f53b7f-1b4e-4dce-9b7c-349ae760fee0
{{</command>}}
If you've already interacted with this collection recently, you should find that you can list
it with the CLI already. If not, you will be prompted to authenticate. Follow through all the
steps until you complete the process, then return to the terminal session.
If successful, you can now interact with the JASMIN endpoint, for example
listing your home directory:
{{<command>}}
globus ls $jdc:/~/
(out)...
(out)(file listing of your JASMIN home directory)
(out)...
{{</command>}}
The authentication via your JASMIN account lasts for 30 days, so you can run and re-run transfers during that period without
needing to repeat the process (hence without any human interaction, if you have scheduled/automated transfers, see below).
If this needs to be renewed, then:
the simple way to do this is to either:
- (manually) visit the Globus web interface and access the JASMIN Default Collection again
- (manuall) use the CLI to list the collection again
In either case, if the authentication has timed out, you will be prompted to follow instructions to renew it,
then the action (listing the directory) should complete successfully.
There are ways to do use a ""refresh token"" programatically to renew the authentication. Watch this space for 
details of how to do that (or f)
",https://help.jasmin.ac.uk/docs/data-transfer/globus-command-line-interface#examples,14187,1593
Automation,"The functionality demonstrated above can be combined into scripts which can perform
useful, repeatable tasks such as:
- recursively syncing the contents of directories between 2 endpoints
Globus provide 2 implementations of this here:
[Examples of automation using the Globus CLI](https://github.com/globus/automation-examples), specifically:
- [cli-sync.sh](https://github.com/globus/automation-examples/blob/master/cli-sync.sh) : bash script using the Globus CLI as demonstrated above
- [globus_folder_sync.py](https://github.com/globus/automation-examples/blob/master/globus_folder_sync.py) : Python code using the Globus Python Software Development Kit (SDK)
We have not covered the Python SDK here, but this is a useful example of how you could integrate Globus transfer functionality into your own code and workflows. You would need to install and authorise this SDK first.
Taking the first of these examples, we can adapt it slightly:
1\. Select the JASMIN endpoint at the destination, and set the destination
path. Modify the corresponding variables in the script to these values:
{{<command>}}
DESTINATION_COLLECTION='a2f53b7f-1b4e-4dce-9b7c-349ae760fee0' ##JASMIN Default Collection ID
DESTINATION_PATH='/home/users/<username>/sync-demo/' ##replace <username> with your JASMIN username
{{</command>}}
{{<alert type=""info"">}}
For **STFC users only** where the other collection in the transfer is within the STFC network, an additional collection is provided [""JASMIN STFC Internal Collection""](https://app.globus.org/file-manager/collections/591d44ac-adbb-43db-9931-977708d07450/overview) and has ID `9efc947f-5212-4b5f-8c9d-47b93ae676b7`.
{{</alert>}}
2\. If you haven't already, activate the Python virtual environment where you
have the CLI installed, and login:
{{<command>}}
source ~/.globus-cli-venv/bin/activate
globus login
{{</command>}}
3\. Check that you can interact with the JASMIN collection from the CLI, by trying to list it
Follow any instructions needed, if you need to renew your authentication.
4\. Run the script to sync the data from the Globus Tutorial Endpoint to the
destination directory.
You should see output similar to that shown below.
{{<command>}}
./cli-sync.sh
(out)Checking for a previous transfer
(out)Last transfer f5db7238-8f06-11ec-8fe0-dfc5b31adbac SUCCEEDED, continuing
(out)Verified that source is a directory
(out)Submitted sync from 6c54cade-bde5-45c1-bdea-f4bd71dba2cc:/share/godata/ to a2f53b7f-1b4e-4dce-9b7c-349ae760fee0:/~/sync-demo/
(out)Link:
(out)https://app.globus.org/activity/04e277f4-8f07-11ec-811e-493dd0cf73a1/overview
(out)Saving sync transfer ID to last-transfer-id.txt
{{</command>}}
5\. Check on the status of the task. You could do this by
- following the URL to https://app.globus.org to view the task under ""activities"", or
{{<command>}}
globus task show <taskid>
{{</command>}}
6\. You could then make some change to either source or destination directory,
and simply re-run the script
{{<command>}}
./cli-sync.sh
{{</command>}}
7\. Experiment by changing the `SYNCTYPE`. Other options are:
See [here for descriptions of the available sync levels](https://docs.globus.org/cli/reference/transfer/#sync_levels):
- `EXISTS`
- `SIZE`
- `MTYPE`
- `CHECKSUM`
8\. Automating repeats of the sync operation
You could then consider how to repeat the task automatically. For example:
- **triggering** a re-run of the `cli-sync.sh` command according to some condition that's met in your workflow.
- **scheduling** the running of the `cli-sync.sh` command on your own machine using cron on your own machine.
  - Remember: the invocation of the command does NOT need to be done on JASMIN, it can be done wherever you have the CLI installed, for example your local machine.
- use the web interface (go to ""Transfer & Timer Options"") to configure repeating tasks initiated there.
- Learn about how to [use timers with Globus](https://www.globus.org/blog/globus-now-supports-recurring-and-scheduled-transfers): these can be set up using the web interface or using an additional CLI [globus-timer-cli](https://pypi.org/project/globus-timer-cli/) which can be installed into the same `virtualenv` as the main globus cli.
- Learn about [Globus Flows](https://docs.globus.org/api/flows/) to create fully automated workflows. Globus have created a number of pre-canned workflow actions (e.g. ""make directory"", ""transfer"", ""delete"", ..) which you can chain together in your own workflow, or combine with your own to create custom workflows. A useful example might be:
  - watching a directory for arrival/creation of a certain file
  - triggering a compute/analysis step on files in the directory (using a [Globus Compute](https://www.globus.org/compute) endpoint of your own?)
  - transferring the output of that analysis to elsewhere, and cleaning up
",https://help.jasmin.ac.uk/docs/data-transfer/globus-command-line-interface#automation,4804,602
'hpxfer access role',"{{<alert type=""danger"">}}
This article is deprecated, since the `hpxfer` is no longer needed for
the new Rocky 9 services introduced Autumn 2024.
However, until the older machines (`hpxfer[12]` & `gridftp1`) are taken out of service,
the role is still required for access to those.
{{</alert>}}
This article explains about access to high-performance data transfer services.
",https://help.jasmin.ac.uk/docs/data-transfer/hpxfer-access-role,374,55
Applying for access,"Some data transfer services are hosted in the JASMIN Data Transfer Zone (a special area of JASMIN's network, located optimally for connections to the outside world via {{<link href=""https://www.jisc.ac.uk/janet"" cue=""true"">}}JANET{{</link>}}) for increased performance. However, to
maintain security in this zone, access to some services is controlled via an additional access
role `hpxfer`. If you have a login account already, you can apply here for
this additional role:
{{< button href=""https://accounts.jasmin.ac.uk/account/login/?next=/services/additional_services/hpxfer/"" >}}Apply for hpxfer{{< /button >}}
",https://help.jasmin.ac.uk/docs/data-transfer/hpxfer-access-role#applying-for-access,615,75
Additional information required,"If you will be connecting from your home institution directly via ssh or ssh-
based gridftp to one of the servers in the JASMIN Data Transfer Zone, the
specific IP address of your machine will need to be added to an allow-list.
Please supply this as part of the application process above.
However, this should not be necessary if you are accessing the server from a
remote server which is already allowed.
{{<alert type=""info"">}}If your **only** reason for applying for `hpxfer` is in order to use the **Globus** service on JASMIN, this is no longer required. You only need this role for accessing the hpxfer servers or for using certificate-based gridftp with the `globus-url-copy` command.
{{</alert>}}
**Inward** pulls of data (to JASMIN) are possible by logging in to
`hpxfer[1,2].jasmin.ac.uk` **via the login servers** and pulling data from
external data sources. So if you're not sure what IP address to use when
applying for access above, it is OK to quote the IP address of
`login1.jasmin.ac.uk`, i.e. `130.246.130.28` as a ""dummy"" value if you will
only be accessing them via this route, and not directly from outside.
",https://help.jasmin.ac.uk/docs/data-transfer/hpxfer-access-role#additional-information-required,1129,186
bbcp,"This article provides information about the bbcp data transfer tool.
",https://help.jasmin.ac.uk/docs/data-transfer/bbcp,69,10
What is bbcp?,"{{<link ""http://www.slac.stanford.edu/~abh/bbcp/"">}}bbcp{{</link>}} is a simple command-line tool
which can use your SSH connection to transfer data in and out of JASMIN
efficiently. It works in a similar way to [GridFTP over SSH]({{% ref ""gridftp-ssh-auth"" %}}) in that it connects to the transfer
server using your usual SSH credentials but then can set up parallel data
streams for transferring data. One advantage of `bbcp` that it is provided as
a single binary executable which is easy to download and use.
",https://help.jasmin.ac.uk/docs/data-transfer/bbcp#what-is-bbcp?,513,79
Using bbcp on JASMIN,"Check with your local administrator to see if it is installed centrally on
your own system. If it isn't, you may need to download the correct binary from
the {{<link ""http://www.slac.stanford.edu/~abh/bbcp/bin/"">}}bbcp download site{{</link>}} and
simply place it in your path on your local filesystem: this can be done as a
regular/unprivileged user. At the JASMIN end, you can put the same executable
in your home directory (somewhere in your `$PATH` e.g. in your `~/bin`
directory, and make sure you give the file execute permission). Once you have
the `bbcp` command you can access any file which is readable by you when
logged into JASMIN, or write to a Group Workspace that you have access to.
",https://help.jasmin.ac.uk/docs/data-transfer/bbcp#using-bbcp-on-jasmin,700,115
Configuring bbcp for JASMIN,"When contacting `hpxfer[34].jasmin.ac.uk` you will need to set a couple of
important options for it to work. The exact options depend on whether you are
moving data into or out of JASMIN, and from where the transfer is initiated.
The `bbcp` protocol, in common with most high-bandwidth transfer tools,
requires a set of ports to be open at one or both ends in order to establish
data connections. Due to firewall restrictions this range of ports needs to be
agreed in advance. In the case of `hpxfer[34].jasmin.ac.uk` the range of ports
is **50000:51000**. Therefore all `bbcp` commands must contain the option
`--port 50000:51000`.
Also, `hpxfer[34]` will only allow incoming connections on these ports,
therefore `hpxfer[34]` must be the server which listens for data connections.
By default `bbcp` will listen for data connections at the end receiving data
and connect from the end sending data. Therefore, with the default options,
`bbcp` will succeed when pushing data to `hpxfer[34]` but fail when pulling
data from it. In order to pull data, with the transfer initiated on the remote
server, you must include the `-z` flag. Therefore the recommended commands for
transferring in either direction are:
",https://help.jasmin.ac.uk/docs/data-transfer/bbcp#configuring-bbcp-for-jasmin,1208,193
Initiate on JASMIN: Pull Data from remote server,"{{<command user=""user"" host=""hpxfer3"">}}
bbcp -v -4 -P 5 -F --port 50000:51000 username@remote-server:<PATH-TO-SOURCE-FILE> <PATH-TO-TARGET-FILE>
{{</command>}}
",https://help.jasmin.ac.uk/docs/data-transfer/bbcp#initiate-on-jasmin:-pull-data-from-remote-server,161,14
Initiate on JASMIN: Push Data,"{{<command user=""user"" host=""hpxfer3"">}}
bbcp -v -4 -P 5 --port 50000:51000 <PATH-TO-SOURCE-FILE> username@remote-server:<PATH-TO-TARGET-FILE>
{{</command>}}
",https://help.jasmin.ac.uk/docs/data-transfer/bbcp#initiate-on-jasmin:-push-data,158,13
Initiate on remote server: Pull Data from JASMIN,"{{<command user=""user"" host=""remote"">}}
bbcp -v -z -4 -P 5 --port 50000:51000 username@hpxfer3.jasmin.ac.uk:<PATH-TO-SOURCE-FILE> <PATH-TO-TARGET-FILE>
{{</command>}}
",https://help.jasmin.ac.uk/docs/data-transfer/bbcp#initiate-on-remote-server:-pull-data-from-jasmin,167,14
Initiate on remote server: Push Data to JASMIN,"{{<command user=""user"" host=""remote"">}}
bbcp -v -4 -P 5 -F --port 50000:51000 <PATH-TO-SOURCE-FILE> username@hpxfer3.jasmin.ac.uk:<PATH-TO-TARGET-FILE>
{{</command>}}
In this case the `-v` flag produces verbose output .`-V` can be used for even
more verbose output. The `-4` option forces use of IP version 4 instead of
IPv6 (essential for transfers to and from `hpxfer3` or other JASMIN hosts.
Note: this option may not be available in some older versions of `bbcp`), and
the `-P` <n> option reports the status of the transfer every n seconds. The
default behaviour will print nothing. The `-F` option skips a check on the
target host to check if there is enough disk space. This overcomes occasional
problems where free space is not correctly reported to bbcp by the JASMIN file
system.
For the full set of options, see: <https://www.slac.stanford.edu/~abh/bbcp/>
Note: the `bbcp` command must be in your `$PATH` on both the source and target
machine.
",https://help.jasmin.ac.uk/docs/data-transfer/bbcp#initiate-on-remote-server:-push-data-to-jasmin,954,148
"Initiate on JASMIN: Pull Data from remote server, specifying SSH command to start bbcp","{{<command user=""user"" host=""hpxfer3"">}}
bbcp -v -4 -P 5 -F --port 50000:51000 -S ""/usr/bin/ssh %I -l %U %H /path/to/bbcp"" username@remote-server:<PATH-TO-SOURCE-FILE> <PATH-TO-TARGET-FILE>
{{</command>}}
The path `/path/to/bbcp` can be replaced by `module load bbcp; bbcp` (or
whatever is the appropriate local requirement) in environments where `bbcp` is
a module which needs to be loaded first.
{{<command user=""user"" host=""hpxfer3"">}}
bbcp -v -4 -P 5 -F --port 50000:51000 -S ""/usr/bin/ssh %I -l %U %H module load bbcp; bbcp"" username@remote-server:<PATH-TO-SOURCE-FILE> <PATH-TO-TARGET-FILE>
{{</command>}}
For specifying the SSH command to start bbcp on the TARGET node, use the `-T`
option.
The [bbcp site](http://www.slac.stanford.edu/~abh/bbcp/) has good
documentation on further options, including the `-r` option for recursive
transfers. A number of useful tutorials are also available elsewhere on the
web.
","https://help.jasmin.ac.uk/docs/data-transfer/bbcp#initiate-on-jasmin:-pull-data-from-remote-server,-specifying-ssh-command-to-start-bbcp",919,120
Tuning Recommendations,"We recommend you tune your connection by trying various different options on a
few GBs of data.
1. By default 4 streams are opened. Try 1 stream first, particularly on fast connections, it may be faster. This is achieved with the option `-s 1`.
2. We ask users of JASMIN to tune up to a **maximum of 16 streams** (`-s 16`).
3. Do not tune the window size unless you continue to get very poor bandwidth after adjusting the number of streams. Most modern operating systems will auto-tune this parameter.
4. `bbcp` is not ideal for large directory trees of small files. If you have thousands of small files you may be better off with rsync or possibly GridFTP/Globus, depending on the network. Another simpler option is tarring/zipping the data first before transferring.
",https://help.jasmin.ac.uk/docs/data-transfer/bbcp#tuning-recommendations,769,133
Troubleshooting,"- `bbcp` uses SSH to establish the control connection so you need to set up your SSH key in the same way as you would to SSH into `hpxfer[12].jasmin.ac.uk`. If `bbcp` isn't working you should first check you can SSH to `hpxfer[34].jasmin.ac.uk`. If you can't, please review the steps in the [Getting Started]({{% ref ""get-started-with-jasmin"" %}}) section before contacting the [JASMIN Helpdesk](mailto:support@jasmin.ac.uk).
- Make that you have logged in (via SSH) to both the JASMIN transfer server and the remote server with the `-A` option (agent-forwarding enabled), to ensure that your credentials are used by SSH as it invokes `bbcp` on the other server.
- Try adding the `-F`option to disable `bbcp`'s filesystem checking if you get the following error:
bbcp: Insufficient space to copy all the files from <hostname>.
- If you see the following error message:
Address family not supported by protocol creating inet socket
this is most likely because the `-4` flag was not specified. This may happen with commands that once worked, as a previously installed version of `bbcp` on JASMIN defaulted to IPv4. Currently there is no support for IPv6 on JASMIN. If the version of `bbcp` you have available on your system is old and does not have the `-4` option, consider downloading the appropriate (newer) version from the link above. It is also possible to compile the executable from source.
",https://help.jasmin.ac.uk/docs/data-transfer/bbcp#troubleshooting,1397,227
'ftp and lftp',"This article provides information about FTP (File Transfer Protocol) as a data
transfer tool. In particular:
- what is FTP?
- where and how can I use FTP on JASMIN?
- what are its limitations?
",https://help.jasmin.ac.uk/docs/data-transfer/ftp-and-lftp,193,35
What is FTP?,"{{<link ""https://en.wikipedia.org/wiki/File_Transfer_Protocol"">}}FTP{{</link>}} is a well-established
transfer protocol enabling connections from a client to download
files from, or upload files to, a server, although limited in security. A wide variety of client tools are
available to the user, 2 implementations of which are available on the JASMIN
transfer servers, although no server is provided. `ftp` is also the name of
the basic FTP client program, see below.
",https://help.jasmin.ac.uk/docs/data-transfer/ftp-and-lftp#what-is-ftp?,469,66
Where and how can I use FTP on JASMIN?,"FTP can only be used as a client on JASMIN, to pull data from external FTP
servers to local storage on JASMIN, for example a Group Workspace or your home
directory. **There is no FTP server within JASMIN providing the ability to
upload files to these locations.** Please use an alternative, more secure
method instead. See other [Data Transfer Tools]({{% ref ""data-transfer-tools"" %}}) such as [scp/rsync/sftp]({{% ref ""rsync-scp-sftp"" %}}), [bbcp]({{% ref ""bbcp"" %}}) or GridFTP ([over SSH]({{% ref ""gridftp-ssh-auth"" %}}), [certificate-based]({{% ref ""gridftp-cert-based-auth"" %}}) or using [Globus Online]({{% ref ""globus-transfers-with-jasmin""%}}))
On the [transfer servers]({{% ref ""transfer-servers"" %}}), you can use one of
the installed FTP clients to download data from elsewhere. These are:
- `ftp` basic ftp client. Usage details
- `lftp` parallel-capable ftp client. Usage details
CEDA however runs 2 FTP servers within the JASMIN environment providing
download-only access to the CEDA archive. Access to these is controlled by
your CEDA account and any dataset-specific privileges which are associated
with that account.
",https://help.jasmin.ac.uk/docs/data-transfer/ftp-and-lftp#where-and-how-can-i-use-ftp-on-jasmin?,1134,163
What are its limitations?,"- FTP was never designed as a secure protocol and has {{<link ""https://en.wikipedia.org/wiki/File_Transfer_Protocol#Security"">}}several limitations{{</link>}} affecting how it can be used safely within an environment like JASMIN.
- Some external sites offer anonymous FTP download. In this case, no username or password needs to be exchanged and (as long as the data resources do not need to be protected in any way) this can provide a simple but effective data transfer method.
- Few external sites now provide FTP access to protected data resources, hence many data-intensive institutions are now focussing on more sophisticated data delivery methods which can meet the demands of security and performance in a multi-user environment.
",https://help.jasmin.ac.uk/docs/data-transfer/ftp-and-lftp#what-are-its-limitations?,737,109
Basic client usage: ftp,"The `ftp` client is available on the transfer servers `xfer*.jasmin.ac.uk`
and high-performance transfer servers `hpxfer*.jasmin.ac.uk`.
Example 1: Downloading a file to a location on JASMIN from a remote FTP
server.
This involves setting up an interactive client session. Once logged in (in
this case, using anonymous FTP), you use FTP commands to interact with the
remote server and locate and download the data you require. The session is
terminated with `bye`.
{{<command user=""user"" host=""xfer-vm-01"">}}
ftp someserver.somesite.ac.uk
(out)Trying 123.456.78.123...
(out)Connected to someserver.somesite.ac.uk (123.456.78.123).
(out)220----------------------------------------------------------------------------
(out)220-Welcome message from somesite.ac.uk
(out)220----------------------------------------------------------------------------
(out)220 
(out)Name (123.456.78.123:username): anonymous
(out)331 Please specify the password.
(out)Password:
{{</command>}}
Once connected, the prompt changes to `ftp>`:
{{<command prompt=""ftp>"">}}
(out)230 Login successful.
Remote system type is UNIX.
(out)Using binary mode to transfer files.
cd /sites/pub/testdir/
(out)(out)(out)250-
250-This is the somesite ftp repository.
250-
(out)250 Directory successfully changed.
get md5.sum
(out)local: md5.sum remote: md5.sum
(out)227 Entering Passive Mode.
(out)150 Opening BINARY mode data connection for md5.sum (45 bytes).
(out)226 Transfer complete.
(out)45 bytes received in 0.00267 secs (16.83 Kbytes/sec)
bye
(out)221 Goodbye.
{{</command>}}
Full details of commands available within an interactive session with the
`ftp` client are available via the man page (`man ftp`).
",https://help.jasmin.ac.uk/docs/data-transfer/ftp-and-lftp#basic-client-usage:-ftp,1675,192
Parallel-capable client usage: lftp,"The alternative client `lftp` is less verbose, but the basic workflow is the
same.
{{<command user=""user"" host=""xfer-vm-01"">}}
lftp someserver.somesite.ac.uk
{{</command>}}
Once connected, the prompt changes to `lftp` and the name of the remote server:
{{<command prompt=""lftp someserver.somesite.ac.uk:~>"">}}
cd /sites/pub/testdir
(out)cd ok, cwd=/sites/pub/testdir/
get md5.sum
(out)45 bytes transferred
bye
{{</command>}}
The interactive shell provided by `lftp` also benefits from tab completion and
use of up/down arrows for command history.
In fact, lftp can also be used as an SFTP client, with the added benefit that
it can handle multiple SFTP transfers in parallel.
In the following example, we connect to a remote SFTP server using the `sftp://`
syntax. Once logged in to the remote server, the prompt changes and you can
enter lftp-specific commands like `mirror`, in this case with `-P 4` as the option
to use 4 `sftp` processes in parallel. Try other values but please consider
other users so a suggested limit is 16.
{{<command user=""user"" host=""xfer-vm-01"">}}
lftp sftp://username@someserver.somesite.ac.uk
Password: (enter password when prompted)
{{</command>}}
{{<command prompt=""lftp username@someserver.somesite.ac.uk:~>"">}}
mirror -P 4 sourcedata
bye
{{</command>}}
Note that if you're connecting **_to_** a JASMIN transfer server in this way,
then you would need to make your JASMIN private key available in an ssh agent
locally, and you would not be prompted for the password.
",https://help.jasmin.ac.uk/docs/data-transfer/ftp-and-lftp#parallel-capable-client-usage:-lftp,1500,213
Scheduling/Automating Transfers,"This article explains how to schedule or automate data transfers. It covers:
- Using Globus for transfer automation
- Scheduling download tasks using cron and LOTUS
",https://help.jasmin.ac.uk/docs/data-transfer/scheduling-automating-transfers,165,26
Overview,"In many cases it can be useful to fetch data from an external source for
processing/analysis on JASMIN on a regular basis, for example ""every Monday at
11:00 fetch all last week's data"". It can also be helpful to distribute the
task downloading of some large datasets, or simply to be able rely on data
being pulled in from some external source to an accumulating dataset used for
periodic analysis.
",https://help.jasmin.ac.uk/docs/data-transfer/scheduling-automating-transfers#overview,400,70
Using Globus for transfer automation,"It is easy to automate transfers using [Globus]({{% ref ""globus-transfers-with-jasmin"" %}}). This method has the advantage that you
do not need to remain connected or logged in to any JASMIN server for the automated transfers
to take place on your behalf, and the transfer itself can be more efficient than other methods described below.
Some introductory information about how to do this is available in this article
[Using the Globus command-line interface]({{% ref ""globus-command-line-interface/#automation"" %}})
(with more to follow)
but please also refer to the comprehensive Globus documentation and their
{{< link ""https://github.com/globus/automation-examples"" >}}automation examples{{</link>}}. You can choose whether
to schedule/automate tasks via the {{<link ""https://www.globus.org/blog/scheduled-and-recurring-transfers-now-available-globus-web-app"">}}Globus web interface{{</link>}}, {{<link ""https://docs.globus.org/cli/reference/"">}}command-line interface{{</link>}}, or use their {{<link ""https://globus-sdk-python.readthedocs.io/en/stable/examples/index.html"" >}}Globus Python SDK{{</link>}} to build Python code that uses this functionality.
",https://help.jasmin.ac.uk/docs/data-transfer/scheduling-automating-transfers#using-globus-for-transfer-automation,1162,127
Scheduling download tasks using cron and LOTUS,"While the [cron server]({{% ref ""using-cron"" %}}) `cron-01.jasmin.ac.uk` is provided for scheduling
general tasks, **it should not be used for the work of executing those tasks itself, and not for transfer tasks.**
",https://help.jasmin.ac.uk/docs/data-transfer/scheduling-automating-transfers#scheduling-download-tasks-using-cron-and-lotus,215,32
xfer-vm-03 - transfer machine with cron,"The transfer server `xfer-vm-03.jasmin.ac.uk` is also provided with `cron`, and should be used where
a task is primarily a transfer rather than a processing task and needs the functionality
of a transfer server.
Please refer to the above `cron` guidance for best practice advice.
In particular, you **must** use [crontamer]({{% ref ""using-cron/#crontamer"" %}}) to manage your cron jobs.
",https://help.jasmin.ac.uk/docs/data-transfer/scheduling-automating-transfers#xfer-vm-03---transfer-machine-with-cron,387,58
Invoking LOTUS from cron to carry out multiple download tasks,"Sometimes we need a task to be invoked from `cron` but executed where there
are lots of nodes to parallelise the tasks (i.e. the LOTUS cluster). In this case, we DO need to use the `cron`
server rather than the cron-equipped transfer server `xfer-vm-03`, since we need to be able to submit jobs to LOTUS 
(the transfer server can't do that).
This will only work where the download can happen over HTTP(S), so depends on how the remote data is made available.
We need it to:
- invoke a job submission script at regular intervals
- have that script initiate downloads using LOTUS nodes
In the examples below, we use the `test` queue (or partition, as queues are
known in Slurm). You can use this for testing, but once you know roughly how
long your download(s) should take, you should
[choose an appropriate queue]({{% ref ""slurm-queues"" %}}) so that the jobs can be scheduled in a fair
way alongside other users' jobs.
",https://help.jasmin.ac.uk/docs/data-transfer/scheduling-automating-transfers#invoking-lotus-from-cron-to-carry-out-multiple-download-tasks,918,162
1\. Single download script,"The simple script below is used to download a single file from an external
source via HTTP using `wget`. It initially uses the `test` partition (queue), but
once you have tested it, you should use a more appropriate queue.
#!/bin/bash 
#SBATCH --partition=test
#SBATCH -o %j.out 
#SBATCH -e %j.err
#SBATCH --time=00:30
# executable 
wget -q -O 1MB_${SLURM_JOBID}.zip http://speedtest.tele2.net/1MB.zip
In this example, the file is labelled with the id of the job which downloaded
it, e.g. `1MB_61117380.zip`
The same could be also achieved using `curl`, or using a Python script making
use of (for example) the `requests` library.
**A note about transfer tools:** since we are delegating the actual download
task to a LOTUS node, we are restricted to transfer tools already installed on
those nodes or available in the user's path at a location cross-mounted with
nodes in the LOTUS cluster (see Table 1 in [Access to Storage]({{% ref ""storage"" %}})), such as $HOME or a group workspace. It is not possible for
the JASMIN team to install specialist data transfer tools across the whole
cluster, so you may be limited to downloading via HTTP(S), FTP, or via tools
available via libraries in the Python environment such (which you do have
access to and can easily customise to install additional libraries using
virtual environments).
Due to networking limitations, LOTUS nodes cannot perform downloads using SSH-based methods, i.e. scp/rsync/sftp.
Download tools installed on LOTUS nodes include:
- `wget`
- `curl`
- `ftp` (but not `lftp`)
In our simple example above, we can subit this script to LOTUS from the
command line with
{{<command>}}
sbatch test_download.sh
{{</command>}}
This could be invoked on a regular basis by adding a crontab entry like this
30 * * * * sbatch /home/users/username/test_download.sh
However it would be safer to wrap this in a `crontamer` command like this to
ensure one instance of the task had finished before the next started: (see
[Using cron]({{% ref ""using-cron"" %}}) for details)
30 * * * * crontamer -t 2h 'sbatch /home/users/username/test_download.sh'
",https://help.jasmin.ac.uk/docs/data-transfer/scheduling-automating-transfers#1\.-single-download-script,2093,331
2\. Multi-node downloads,"We could expand this example to download multiple items, perhaps 1 directory
of data for each day of a month, and have 1 element of a job array handle the
downloading of each day's data.
**A few words of warning:** Distributing download tasks as shown below can
cause unintended side-effects. Here, we're submitting an array of 10 download
jobs, each initiating a request for a 1MB file which may well happen
simultaneously. So we need to be confident that the systems and networks at
either end can cope with that. It would be all too easy to submit a task to
download several thousand large data files and cause problems for other users
of JASMIN (and other users on its host institution's network), or indeed at
the other end. Taken to the extreme, this could appear over the network as a
Distributed Denial-of-Service (DDoS) attack. As with all LOTUS tasks: start
small, test, and increase to sensible scales when you are confident it will
not cause a problem. **A limit of 10 jobs would be a sensible maximum, for one
user.**
We'll simulate this here by downloading the same external file to 10 different
output files, but you could adapt this concept for your own purposes depending
on the layout of the source and destination data.
#!/bin/bash 
#SBATCH --partition=test
#SBATCH -o %A_%a.out
#SBATCH -e %A_%a.err
#SBATCH --time=00:30
#SBATCH --array=1-10
#SBATCH --time=00:30
# executable 
wget -q -O 1MB_${SLURM_ARRAY_TASK_ID}.zip http://speedtest.tele2.net/1MB.zip
echo ""script completed""
In this (perhaps contrived) example, we're setting up an array of 10 elements
and using the `SLURM_ARRAY_TASK_ID` environment variable to name the output
files (otherwise they'd all be the same). In a real-world example you could
apply your own logic to divide up files or directories matching certain
patterns to become elements of a job array.
The script could then be scheduled to be invoked at regular intervals as shown
in (1).
Some tools provide functionality for mirroring or synchronising directories,
i.e. only downloading those files in a directory which are new have been added
since the last time a task was run. These can be useful to avoid repeated
downloads of the same data.
",https://help.jasmin.ac.uk/docs/data-transfer/scheduling-automating-transfers#2\.-multi-node-downloads,2188,356
"'rsync, scp, sftp'","This article tells you about some of the basic transfer tools available for
use with JASMIN that work over an SSH connection:
- rsync (over SSH)
- scp
- sftp
",https://help.jasmin.ac.uk/docs/data-transfer/rsync-scp-sftp,158,30
rsync over SSH,"rsync is a file synchronisation and file transfer program for Unix-like
systems that can minimise network data transfer by using a form of delta
encoding such that only the differences between and source and destination
data are actually transmitted. rsync can compress the data transferred further
using zlib compression and SSH or stunnel can be used to encrypt the transfer.
rsync is typically used to synchronise files and directories between two
different systems, one local and one remote. For example, the command:
{{<command user=""localuser"" host=""localhost"">}}
rsync mydata remoteuser@remotehost:/data/
{{</command>}}
will use SSH to connect as `remoteuser` to `remotehost`. Once connected, it will
invoke another copy of rsync on the remote host, and then the two programmes
will talk to each other over the connection, working together to determine
which parts of `mydata` are already on the remote host and don't need to be
transferred over the connection.
The generic syntax is:
{{<command>}}
rsync [OPTION] ... SRC [SRC] ... [USER@]HOST:DEST
rsync [OPTION] ... [USER@]HOST:SRC [DEST]
{{</command>}}
...where `SRC` is the file or directory (or a list of multiple files and
directories) to copy from, and `DEST` represents the file or directory to copy
to. (Square brackets indicate optional parameters.)
For more information visit the
{{< link ""http://rsync.samba.org/"" >}}official rsync website{{</link>}}.
",https://help.jasmin.ac.uk/docs/data-transfer/rsync-scp-sftp#rsync-over-ssh,1421,209
rsync example with JASMIN,"Here is a simple example using rsync over SSH to your home directory copy a file to a Group
Workspace on JASMIN:
{{<command user=""localuser"" host=""localhost"">}}
exec ssh-agent $SHELL
ssh-add ~/.ssh/id_rsa_jasmin  ## local path to your private key file
rsync myfile <username>@xfer1.jasmin.ac.uk:/gws/nopw/j04/myproject/data/
{{</command>}}
NOTE: The first two lines are the standard method for setting up the SSH agent
in order to allow connections without prompting for a passphrase each time.
",https://help.jasmin.ac.uk/docs/data-transfer/rsync-scp-sftp#rsync-example-with-jasmin,495,69
scp,"`scp` is another basic command-line tool for secure copying between two
machines. It is installed as part of most SSH implementations and comes as
standard on the JASMIN transfer servers. `scp` is the secure analogue of the
`rcp` command. Typically, the syntax of `scp` is like the syntax of `cp`
(copy):
To copy a file to a host:
{{<command user=""localuser"" host=""localhost"">}}
scp myfile remoteuser@remotehost:directory/target
{{</command>}}
To copy a file (or directory) from a host to the local system:
{{<command user=""localuser"" host=""localhost"">}}
scp remoteuser@remotehost:directory/source target
scp -r remoteuser@remotehost:directory/source_folder target_folder
{{</command>}}
Note that if the remote host uses a port other than the default of 22, it can
be specified in the command. For example, copying a file from host over a non-
standard SSH port (2222):
{{<command user=""localuser"" host=""localhost"">}}
scp -P 2222 remoteuser@remotehost:directory/source target
{{</command>}}
For more information on scp please visit the following
[website](https://linux.die.net/man/1/scp).
",https://help.jasmin.ac.uk/docs/data-transfer/rsync-scp-sftp#scp,1090,144
sftp,"sftp is a similar command-line tool to scp, but the underlying SFTP protocol
allows for a range of operations on remote files which make it more like a
remote file system protocol. sftp includes extra capabilities such as resuming
interrupted transfers, directory listings, and remote file removal.
For basic transfer of a file on JASMIN to the local machine:
{{<command user=""localuser"" host=""localhost"">}}
sftp remoteuser@xfer1.jasmin.ac.uk:/group_workspaces/jasmin/myproject/data/notes.txt ./
{{</command>}}
For more information see the
{{< link ""https://en.wikipedia.org/wiki/SSH_File_Transfer_Protocol"" >}}Wikipedia page on SFTP{{</link>}}.
There are various 3rd-party tools and clients, for example, WinSCP, FileZilla,
MobaXterm and others, which can do transfers using the SCP and/or SFTP
protocols. Please refer to the documentation for that particular software, but
the basic principles are the same, i.e that you can set up a connection to a
remote host by specifying the same parameters that you would with the command-
line tool, but often through a graphical user interface instead.
",https://help.jasmin.ac.uk/docs/data-transfer/rsync-scp-sftp#sftp,1096,151
Note on performance,"While convenient and familiar to many users, the tools described above do not
make efficient use of available bandwidth for transferring large quantities of
data via high-speed networks over long distances. Please consult [Data
Transfer Tools]({{% ref ""data-transfer-tools"" %}}) to learn more about which
might be the most appropriate tool to use in different contexts.
",https://help.jasmin.ac.uk/docs/data-transfer/rsync-scp-sftp#note-on-performance,370,55
'GridFTP (certificate-based authentication)',"{{<alert type=""danger"">}}
Not yet updated to work with with new Rocky9 servers, so please continue using CentOS7 equivalents for now, as documented below.
{{</alert>}}
This article describes how to transfer data using gridftp with certificate-
based authentication.
{{<alert type=""info"">}}The `globus-url-copy` command used here should not be confused with the Globus online data transfer service. They used to be associated, but no longer. If you are starting out and looking for a reliable, high-performance transfer method, the recommendation now is to learn about [Globus Transfers with JASMIN]({{% ref ""globus-transfers-with-jasmin"" %}}) (using the Globus online data transfer service) instead of command-line gridftp as described in this document.{{</alert>}}
",https://help.jasmin.ac.uk/docs/data-transfer/gridftp-cert-based-auth,766,105
Basics of certificate-based authentication,"Gridftp servers commonly use a network of ""trust"" based on electronic
certificates. In order to make use of a gridftp server at one end of your
proposed transfer, you will need to use a certificate which identifies you as
the user, and which is issued by an identity provider which is ""trusted"" by
the servers at both ends. The trust between the servers is maintained by the
administrators of the service who will ensure that the necessary certificates
are in place.
The presentation of a valid credential which is trusted by the server at the
other end is merely the authentication step (proving who you are).
Authorisation also needs to follow: you, as a user (identified by the
credential you present) need to be authorised to use the resource at the other
end. You should check with the operator of the other gridftp server to see
what additional steps are required before you can actually perform a transfer.
",https://help.jasmin.ac.uk/docs/data-transfer/gridftp-cert-based-auth#basics-of-certificate-based-authentication,914,158
Getting a short-term credential,"In order to access the JASMIN gridftp server, you can now use your JASMIN
portal account to gain a short-term credential which the server will recognise
to authenticate you. This is the same username and password you would use to
log in to <https://accounts.jasmin.ac.uk> to administer your JASMIN account.
**IT IS NOT YOUR SSH PASSPHRASE.**
Here's what to do:
  1. Download tools to interact with JASMIN's Online Certificate Authority (OnlineCA). You can use these to interact with other OnlineCAs too (not just JASMIN's. These replace the ""myproxy-logon"" tool previously mentioned here)
  2. Use these tools to:
    1. ""Bootstrap trust"" i.e. to setup your local certificate store with those needed to interact with the JASMIN server [First time use only]
    2. Obtain a short-term credential using your JASMIN account details [First time, and to renew your short-term credendial as needed]
  3. Use this short-term credential to authenticate with a remote gridftp server which trusts this credential (for example, the JASMIN gridftp server)
",https://help.jasmin.ac.uk/docs/data-transfer/gridftp-cert-based-auth#getting-a-short-term-credential,1044,161
Download OnlineCA tools,"On the machine you intend to use as the transfer client, e.g.
`xfer1.jasmin.ac.uk`, in your JASMIN home directory, download 2 shell scripts
which will interact with the Online CA for you. Make them executable:
{{<command user=""user"" host=""xfer1"">}}
wget https://raw.githubusercontent.com/cedadev/online_ca_client/master/contrail/security/onlineca/client/sh/onlineca-get-cert-wget.sh
wget https://raw.githubusercontent.com/cedadev/online_ca_client/master/contrail/security/onlineca/client/sh/onlineca-get-trustroots-wget.sh
chmod u+x onlineca-get-*.sh
{{</command>}}
View help information for the shell scripts:
{{<command user=""user"" host=""xfer1"">}}
./onlineca-get-trustroots-wget.sh -h
./onlineca-get-cert-wget.sh -h
{{</command>}}
Bootstrap trust between your own machine and the JASMIN gridftp server: (First time only)
{{<command user=""user"" host=""xfer1"">}}
./onlineca-get-trustroots-wget.sh -U https://slcs.jasmin.ac.uk/trustroots/ -b
(out)Bootstrapping Short-Lived Credential Service root of trust.
(out)Trust roots have been installed in /home/users/USERNAME/.globus/certificates.
{{</command>}}
Obtain a credential, to be written to an output file `credfile` using your
JASMIN Accounts Portal username USERNAME:
{{<command user=""user"" host=""xfer1"">}}
./onlineca-get-cert-wget.sh -U https://slcs.jasmin.ac.uk/certificate/ -l USERNAME -o ./cred.jasmin
{{</command>}}
When prompted, enter the password associated with your **JASMIN** account
**(NOT your SSH passphrase)**
Change the permissions on your newly-created `cred.jasmin` file so that it's
only readable by you (client software may insist on this):
{{<command user=""user"" host=""xfer1"">}}
chmod 600 ./cred.jasmin
{{</command>}}
This credential obtained by this method is valid by default for 720 hours (30
days), as you can see by inspecting the certificate using the following
command:
{{<command user=""user"" host=""xfer1"">}}
openssl x509 -in cred.jasmin -noout -startdate -enddate
(out)    notBefore=Mar 11 17:32:59 2022 GMT
(out)    notAfter=Apr 10 17:32:59 2022 GMT
{{</command>}}
After the `notAfter` date, it will no longer be valid, but you can
repeat this process at any time (e.g. before it expires) to update it.
",https://help.jasmin.ac.uk/docs/data-transfer/gridftp-cert-based-auth#download-onlineca-tools,2185,243
Example Gridftp usage,"(General case, or with a JASMIN host as gridftp client)
Once you have obtained a valid short-term credential on the client transfer
server, and assuming that the gridftp server at the remote end of the
transfer recognises and is able to authorize you via this credential, then
you should be able to transfer data between the remote server and local
client with commands such as shown below:
Please consult the documentation for the `globus-url-copy` command for the
full range of options and arguments.
Please note that the examples below use a fictitious client `gridftp-client.localsite.ac.uk` and server `gridftp-server.remotesite.ac.uk` which need to be replaced in your commands with the hostname of the actual gridftp server and client you are actually using.
Check help documentation for the globus-url-copy command:
{{<command user=""user"" host=""gridftp-client"">}}
globus-url-copy -help
{{</command>}}
**NOTE:** On some systems, you have to load a relevant module to get access to the globus-url-copy command, however not on the JASMIN \[hp\]xfer servers.
It is recommended to try things out using the regular xfer servers xfer-vm-0[12] but to perform ""real"" transfers using hpxfer[34] for better performance.
1\. Remote directory listing issued by client on `gridftp-
client.localsite.ac.uk` to server `gridftp-server.remotesite.ac.uk` where you
have a home directory `/home/users/USERNAME`:
{{<command user=""user"" host=""gridftp-client"">}}
globus-url-copy -cred cred.jasmin -vb -list gsiftp://gridftp-server.remotesite.ac.uk/home/users/USERNAME/
{{</command>}}
2\. Download a file from remote directory `/home/users/USERNAME` to
destination on the client machine:
{{<command>}}
globus-url-copy -cred cred.jasmin -vb gsiftp://gridftp-server.remotesite.ac.uk/home/users/USERNAME/myfile file:///path/to/localdir/myfile
{{</command>}}
The `-p N` and `-fast` options can additionally be used in combination to
enable `N` parallel streams at once, as shown below. You can experiment with N
in the range 4 to 16 to obtain the best performance, but please be aware that
many parallel transfers can draw heavily on shared resources and degrade
performance for other users:
{{<command>}}
globus-url-copy -cred cred.jasmin -vb -p 16 -fast gsiftp://gridftp-server.remotesite.ac.uk/home/users/USERNAME/myfile file:///path/to/localdir/myfile
{{</command>}}
3\. Recursively download the contents of a directory on a remote location to a
local destination.
{{<command>}}
globus-url-copy -cred cred.jasmin -vb -p 4 -fast -cc 4 -cd -r gsiftp://gridftp-server.remotesite.ac.uk/home/users/USERNAME/mydir/ file:///path/to/localdir/mydir/
{{</command>}}
Where:
  - `-cc N` requests `N` concurrent transfers (in this case, each with `p=4` parallel streams)
  - `-cd` requests creation of the destination directory if this does not already exist
  - `-r` denotes recursive transfer of directories
  * `-sync` and `-sync-level` options can be used to synchronise data between the two locations, where destination files do not exist or differ - y criteria that can be selected) from corresponding source files. See `-help` option for details.
  - the `file:///` URI is used to specify the destination on the local file system.
",https://help.jasmin.ac.uk/docs/data-transfer/gridftp-cert-based-auth#example-gridftp-usage,3209,423
Uploading data,"The above commands can also be adapted to invoke transfers from a local source
to a remote destination, i.e. uploading data, since the commands all take the
following general form:
{{<command>}}
globus-url-copy [OPTIONS] source-uri desination-uri
{{</command>}}
You can use the above examples by replacing the local machine `gridftp-client.localsite.ac.uk` with one of the jasmin `xfer` or `hpxfer` servers as a client, To do this, you first need to be logged in via SSH to one of these hosts and can initiate a transfer by invoking `globus-url-copy` in one of the ways above.
- For high-performance transfer (large volumes and/or longer distances), use [Globus]({{% ref ""globus-transfers-with-jasmin"" %}}) or the [hpxfer servers]({{% ref ""transfer-servers/#hpxfer-servers"" %}})
- For remote hosts using JASMIN's dedicated network link (Met Office only) use `xfer-vm-0[123].jasmin.ac.uk` as the client (These are virtual machines so have limited performance, but your transfer will be over a dedicated network connection)
",https://help.jasmin.ac.uk/docs/data-transfer/gridftp-cert-based-auth#uploading-data,1022,147
Connecting to the JASMIN GridFTP server,"In order to do a transfer using a JASMIN host as the gridftp server (rather than
client), you would need to interact with the JASMIN GridFTP server
`gridftp1.jasmin.ac.uk`. You cannot log in to this server directly via SSH:
you only initiate GridFTP transfers to and from it from another client.
In the following example, a client is initiated on a fictitious remote host
`client.remotesite.ac.uk` and tests the connection by transferring from
/dev/zero on the local machine (at `remotesite` ) to /dev/null on the JASMIN
gridftp server. Note that you can use the SLCS server at JASMIN to obtain the
short-term credential required ( **but the first time, you will need to
download and use the OnlineCA tools as described above** ). You can renew your
credential and perform the test transfer as follows:
{{<command user=""user"" host=""remoteclient"">}}
./onlineca-get-cert-wget.sh -U https://slcs.jasmin.ac.uk/certificate/ -l USERNAME -o ./cred.jasmin
globus-url-copy -cred cred.jasmin -vb -p 8 -fast /dev/zero gsiftp://gridftp1.jasmin.ac.uk/dev/null
(out)    Source: file:///dev/
(out)    Dest:   gsiftp://gridftp1.jasmin.ac.uk/dev/
(out)      zero  ->  null
(out)
(out)    4153409536 bytes       792.20 MB/sec avg       792.20 MB/sec inst
{{</command>}}
This server is also used as the JASMIN GridFTP Server globus endpoint, see
[GridFTP transfers using Globus Online]({{% ref ""globus-command-line-interface"" %}}) (however you can only currently use your CEDA
SLCSs credential with Globus Online. The JASMIN team is working on a solution
for this).
Please note that the servers `xfer-vm-0[123].jasmin.ac.uk` and
`hpxfer[34].ceda.ac.uk` are not gridftp **servers**. They have the `globus-url-copy` client installed, so can be used as clients to connect to remote
gridftp servers, and also support [gridftp over SSH]({{% ref ""gridftp-ssh-auth"" %}}) (both incoming and outgoing), but do not act as
servers for certificate-based gridftp as shown in these examples. The JASMIN
gridftp server for read-write access to home directories and group workspaces
is `gridftp1.jasmin.ac.uk`. Access to this requires the [hpxfer access role]({{% ref ""hpxfer-access-role"" %}}). See
also [Transfer Servers]({{% ref ""transfer-servers"" %}}).
",https://help.jasmin.ac.uk/docs/data-transfer/gridftp-cert-based-auth#connecting-to-the-jasmin-gridftp-server,2221,307
Third-party transfers,"It should be possible, with the correct configuration at each site, to
initiate on host `A` a transfer of data between two other gridftp servers `B` and
`C` (a third party transfer). Both URIs would use `gsiftp:` as the protocol:
{{<command>}}
globus-url-copy -vb -p 4 gsiftp://B/source gsiftp://C/destination
{{</command>}}
Further information can be found in the documentation for globus-url-copy.
This is the basis of the [Globus Online](https://www.globus.org/app/transfer) 
managed service to orchestrate and monitor transfers between gridftp endpoints
in a more user-friendly way. It has evolved considerably since diverging from 
the ""traditional"" gridftp setup described in this article and is recommended as 
it provides a much easier user experience and better reliability.
See {{<link ""globus-transfers-with-jasmin"" />}}.
",https://help.jasmin.ac.uk/docs/data-transfer/gridftp-cert-based-auth#third-party-transfers,833,114
Future plans,"As [support for the open-source Globus Toolkit (including globus-url-copy) has
now been withdrawn by Globus](https://www.globus.org/blog/support-open-source-
globus-toolkit-ends-january-2018), the future of direct gridftp transfers is
uncertain. It is currently maintained by the Grid Community Forum. 
**We advise users to spend some time understanding and testing
transfer workflows with the {{<link ""globus-transfers-with-jasmin"" >}}Globus Online{{</link>}} transfer service,
including the command-Line, web interfaces and (for advanced users) a Python SDK, as these
are likely to replace direct gridftp on JASMIN in due course.**
",https://help.jasmin.ac.uk/docs/data-transfer/gridftp-cert-based-auth#future-plans,634,78
Introduction,"This article describes how to create your own Globus collection using Globus Connect Personal.
Please read {{<link ""globus-transfers-with-jasmin""/>}} first for a wider introduction to Globus.
Using {{<link ""https://www.globus.org/globus-connect-personal"">}}Globus Connect Personal (GCP){{</link>}} would enable you to transfer files to/from another Globus collection using
any of the Globus Online transfer tools ({{<link ""https://app.globus.org"">}}Web app{{</link>}},
{{<link ""https://docs.globus.org/cli/)"">}}CLI{{</link>}} or {{<link ""https://globus-sdk-python.readthedocs.io/en/stable/"">}}Python SDK{{</link>}}.
The term ""endpoint"" has changed meaning with version 5 of Globus, so users now interact with ""collections"". 
but see {{<link ""https://docs.globus.org/cli/collections_vs_endpoints/#:~:text=The%20Endpoint%20is%20used%20for%20some%20operations%20like%20collection%20listing,management%20capabilities%20and%20data%20transfers."">}}Endpoints vs Collections{{</link>}} for a fuller explanation of these entities.
For example, if you set up GCP on your desktop/laptop, you could transfer data files to/from your home directory or
other storage on JASMIN.
{{<alert type=""info"">}}
- You may not need to do this if your institution already has a Globus endpoint available to you.
- You should NOT install this on a JASMIN server, as a Globus endpoint is already provided for you.
- If you plan to install it in your user area on your departmental server, check with your local IT administrator whether that's an OK thing to do. Point them at the relevant Globus documentation but note that you should be able to do the install with regular/user privileges and that the software does not usually need to be left running: it can be started for the duration of any data transfer tasks, then stopped once they have completed.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/data-transfer/globus-connect-personal#introduction,1840,232
Set up Globus Connect Personal on end-user machine,"Installers are available for Linux, Mac and Windows operating systems:  
- Linux (command-line) <https://docs.globus.org/how-to/globus-connect-personal-linux/#globus-connect-personal-cli>
- Mac <https://docs.globus.org/how-to/globus-connect-personal-mac/>
- Windows <https://docs.globus.org/how-to/globus-connect-personal-windows/>
The instructions below show the process for Linux (command-line):
These commands should be executed on YOUR OWN MACHINE (not JASMIN):
{{<command>}}
wget https://downloads.globus.org/globus-connect-personal/linux/stable/globusconnectpersonal-latest.tgz
tar xzf globusconnectpersonal-latest.tgz
{{</command>}}
This will produce a versioned globusconnectpersonal directory
Replace `x.y.z` in the line below with the version number you see
{{<command>}}
cd globusconnectpersonal-x.y.z
./globusconnectpersonal
{{</command>}}
(see links above for details of how to install without the graphical user interface, if you need to)
Complete the installation using the setup key. If a graphical environment is
detected, a window will appear, to guide you through the steps. If not, text
prompts will appear.
**(Please see the relevant installation guide for your platform, linked above,
for further details)**
",https://help.jasmin.ac.uk/docs/data-transfer/globus-connect-personal#set-up-globus-connect-personal-on-end-user-machine,1230,132
Start Globus Connect Personal (Linux),"{{<command>}}
./globusconnectpersonal -start
{{</command>}}
If you use the web application at <https://app.globus.org>, you should now be
able to see your GCP endpoint listed under ""Collections"" when you filter by
""Administered by you"". You can now try listing the files on it and
perhaps transferring a file to/from one of the Globus Tutorial endpoints using
the web interface.
The setup process will have prompted you for a name for your endpoint. It is
assigned a unique ID, too.
**If you have the Globus Command-Line Interface installed** ([see here]({{%ref ""globus-command-line-interface"" %}})), you can find
the ID of your own endpoint with the CLI command:
{{<command>}}
globus endpoint search <name> --filter-owner-id <your globus id>
{{</command>}}
If successful, you should now be able to interact with your endpoint via any
of the Globus tools (web app, CLI and Python SDK).
For example, you could list the files on the endpoint:
{{<command>}}
globus ls <endpoint_id>:<path>
{{</command>}}
",https://help.jasmin.ac.uk/docs/data-transfer/globus-connect-personal#start-globus-connect-personal-(linux),1001,151
Set directory permissions,"**Don't forget** to configure which directory paths on your system can be accessed by GCP. By default these may NOT be accessible, so you need to allow access.
- Windows: GCP icon in taskbar, then Options / Access
- Mac: menu bar icon / Preferences / Access
- Linux: by editing the config file `~/.globusonline/lta/config-paths`, see {{<link ""https://docs.globus.org/globus-connect-personal/install/linux/#config-paths"">}}here{{</link>}} for syntax.
",https://help.jasmin.ac.uk/docs/data-transfer/globus-connect-personal#set-directory-permissions,450,60
Data transfer overview,"This article introduces the topic of data transfer to/from JASMIN.
",https://help.jasmin.ac.uk/docs/data-transfer/data-transfer-overview,67,10
Introduction to Data Transfer on JASMIN,"As a JASMIN user you are very likely to be involved in data transfer. You
might need to copy data files/directories from JASMIN to remote sites (such as
your own PC, MONSooN or ARCHER2) or bring new data on to JASMIN. These data
transfer articles explain how to use the basic
{{<link ""data-transfer-tools"">}}transfer tools{{</link>}} such as `rsync` and `scp`
as well as more sophisticated services such as Globus. They also cover which
transfer services and servers are available to JASMIN users.
For many users, moving small amounts of data over short distances, the basic
tools will meet their requirements. However, data transfer is a complicated
topic so we also provide articles about how you can improve your transfer
rates to make the most of the available bandwidth. We include details about
transfers over connections to specific sites (such as the Met Office). Advice
is also provided about automating and scheduling data transfers, along with
tips for different transfer workflows.
",https://help.jasmin.ac.uk/docs/data-transfer/data-transfer-overview#introduction-to-data-transfer-on-jasmin,994,159
Transfers to/from JASMIN,,https://help.jasmin.ac.uk/docs/data-transfer/data-transfer-overview#transfers-to/from-jasmin,0,0
1\. Transfers initiated from JASMIN,"When initiating a transfer from a transfer server on JASMIN you would usually
start by logging on to the server (via SSH). Once you are logged in you can
initiate a connection to the outside world in order to push/pull the data you
require.
",https://help.jasmin.ac.uk/docs/data-transfer/data-transfer-overview#1\.-transfers-initiated-from-jasmin,241,44
2\. Transfers initiated from elsewhere,"When initiating a transfer from elsewhere you will transfer data files to/from
a source machine (which may be inside or outside JASMIN) to the transfer
server.
",https://help.jasmin.ac.uk/docs/data-transfer/data-transfer-overview#2\.-transfers-initiated-from-elsewhere,160,26
Transfer directories,"You will typically transfer data to/from a Group Workspace that you have been
granted access to. If you are copying data from JASMIN you might want to copy
data from the CEDA archive (mounted on JASMIN) to a remote site. You might
also wish to copy small volumes of data to/from your $HOME directory. All of
these locations are available on the transfer servers.
",https://help.jasmin.ac.uk/docs/data-transfer/data-transfer-overview#transfer-directories,363,64
JASMIN Transfer servers,"JASMIN provides specific {{<link ""../interactive-computing/transfer-servers"">}}servers{{</link>}}servers for
managing data transfers. Please read about the different servers available for
particular data transfer needs, and about the various
{{<link ""data-transfer-tools"">}}data transfer tools{{</link>}} available.
",https://help.jasmin.ac.uk/docs/data-transfer/data-transfer-overview#jasmin-transfer-servers,316,30
Improving your transfer rates,"To achieve better transfer rates, for large transfers or where speed and reliability are important, you are recommended to:
- use the {{<link ""globus-transfers-with-jasmin"">}}Globus data transfer service{{</link>}} (recommended as the best method), or
- use the high-performance data transfer servers (physical equivalents of the transfer VMs, located in a special network zone)
- use other parallel-capable transfer tools such as bbcp, lftp (parallel-capable ftp client), or gridftp: see {{<link ""data-transfer-tools"">}}Data transfer tools{{</link>}}
Transfer rates depend on many factors, so try to consider all of these:
- **do you really need to transfer some/all of the data?**
  - is the data in the CEDA Archive already (don't copy it, if so, just process it in-place!)
  - can your workflow deal with processing just smaller ""chunks"" at a time (streaming)?
  - do you really need to have/keep all the source data, if it's stored somewhere else?
- **the network path all the way from where the source data resides, to the destination file system**
  - high-performance data transfer tools are great, but is the ""last mile"" over WiFi to your laptop?
  - what is the length of the network path? If it's international or intercontinental, SSH-based methods won't work well. Consider Globus.
- **the host at each end**
  - what sort of host is it (laptop, departmental server, virtual machine, physical machine) and what is its network connectivity?
- **the file systems at each end**
  - not all file systems perform the same, for given types of data or transfer methods
- **the size and number of files involved**
  - large numbers of small files can take a long time to transfer
  - are the data in deep directory trees? These can take a long time to recreate on the destination file system
  - consider creating a tar/zip archive to transfer fewer but larger files, or at least a method that copes well with many files in parallel or ""in flight"" at once.
- **checking data integrity**
  - some methods will verify data integrity at source and destination to ensure integrity. This can be resource-heavy and slow.
- **time of day**
  - would scheduling your transfer to happen at quieter times, mean that it completes more efficiently and/or without impacting others? Consider source and destination time zones!
",https://help.jasmin.ac.uk/docs/data-transfer/data-transfer-overview#improving-your-transfer-rates,2318,375
Data Transfer Tools,"This article lists the data transfer tools available on JASMIN and provides
links to articles that describe them in more detail.
",https://help.jasmin.ac.uk/docs/data-transfer/data-transfer-tools,129,21
Data Transfer tools,"The are many tools that you can use for transferring data to/from JASMIN. See
the articles linked below for more details.
Tool | Info
---|---
{{<link ""globus-transfers-with-jasmin"">}}Globus{{</link>}} | An efficient, secure data transfer service available to all JASMIN users **(recommended)**. Includes capability to schedule, repeat and orchestrate transfers between third-party hosts and receive notifications of job status. Has web and command-line interfaces. Efficient for moving large volumes and/or numbers of files, especially over long distances.
{{<link ""rsync-scp-sftp"">}}scp{{</link>}} |  A basic transfer tool that works over the SSH protocol. Similar to ""cp"" but copies between remote servers.  
{{<link ""rsync-scp-sftp"">}}rsync (over SSH){{</link>}} |  Like scp but more sophisticated. Allows synchronisation between remote directory trees.  
{{<link ""rsync-scp-sftp"">}}sftp{{</link>}} |  SSH FTP - works over SSH.  
{{<link ""bbcp"">}}bbcp{{</link>}} |  A command-line tool that allows the user to specify parallel transfer over multiple streams, using SSH authentication.
{{<link ""gridftp-ssh-auth"">}}GridFTP (over SSH){{</link>}} |  An old but comprehensive data transfer tool. Highly configurable and able to transfer over multiple parallel streams. Used over SSH in this case. Superceded by {{<link ""globus-transfers-with-jasmin"">}}Globus{{</link>}}
{{<link ""gridftp-cert-based-auth"">}}GridFTP (certificate-based){{</link>}} |  Legacy Gridftp using certificate-based authentication instead of SSH credentials. Efficient for moving large volumes and/or numbers of files, especially over long distances.  Superceded by {{<link ""globus-transfers-with-jasmin"">}}Globus{{</link>}}
{{<link ""ftp-and-lftp"">}}FTP{{</link>}} |  File Transfer Protocol. An aged transfer protocol suitable for small file transfers but limited.
{{<link ""ftp-and-lftp"">}}LFTP{{</link>}} |  Parallel-capable FTP client.
wget, curl  |  Download tools for accessing resources over HTTP primarily. (see 3rd party documentation)
{{<link ""../short-term-project-storage/share-gws-data-via-http"">}}sharing GWS data via http{{</link>}} | for exposing part(s) of a Group Workspace via HTTP to users without JASMIN accounts.
Python transfer tools  |  Methods of managing/scripting data transfer tasks using Python. You can use libraries such as `requests` in a Python 3 virtual environment on the transfer servers.  
{{<link ""../mass/external-access-to-mass-faq"">}}MASS client (Met Office){{</link>}}|  A specific command-line tool installed on the mass-cli1.ceda.ac.uk server on JASMIN. Enables extractions from the Met Office MASS Archive.
OPeNDAP  |  A transfer protocol for extracting subsets of files from a remote server (over HTTP)
{{<link ""rclone"">}}rclone{{</link>}} |  A 3rd party, open-source command-line utility which can interface to, and synchronise data between, a wide variety of cloud and other storage backends, such as Google Drive and AWS S3 compatible object stores. It can also sync data over SSH.   This utility is not installed on JASMIN but is well-documented and trivial for users to download and configure themselves on one of the {{<link ""../interactive-computing/transfer-servers"">}}data transfer servers{{</link>}} **Please note {{<link ""rclone#installing-rclone-for-yourself-on-jasmin"">}}install instructions{{</link>}}, and {{<link ""rclone/#dos-and-donts"">}}dos and don'ts{{</link>}}for rclone on JASMIN**.
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/data-transfer/data-transfer-tools#data-transfer-tools,3442,416
'GridFTP (SSH authentication)',"This article describes how to transfer data using GridFTP with SSH
authentication.
{{<alert type=""info"">}}The `globus-url-copy` command used here should not be confused with the Globus online data transfer service. They used to be associated, but no longer. If you are starting out and looking for a reliable, high-performance transfer method, the recommendation now is to learn about [Globus Transfers with JASMIN]({{% ref ""globus-transfers-with-jasmin"" %}}) (using the Globus online data transfer service) instead of command-line gridftp as described in this document.{{</alert>}}
",https://help.jasmin.ac.uk/docs/data-transfer/gridftp-ssh-auth,583,80
Introduction,"GridFTP is the recommended tool for transferring large files or groups of
files across high-speed Wide-Area Networks (WANs). It is commonly used with
certificate-based authentication, but can also take place between suitably
configured* server and client using SSH as the authentication mechanism.
The client may need to have certain ports enabled on the host or institutional firewall if present. Consult your local IT support desk for details and direct them to {{< link ""https://gridcf.org/gct-docs/6.2/gridftp/admin/index.html#gridftp-config-overview"" >}}Configuring GridFTP{{</link>}}.
SSH-based GridFTP does not enable the full feature set provided by
certificate-based GridFTP and in particular does not work with Globus Online,
which provides useful interfaces and APIs for managing large-scale data
transfers, but still provides a major step up in performance by ""filling the
pipe"" more efficiently than scp/rsync/sftp, particularly over longer distances
and can do verification and sync operations as part of the transfer.
See also:
- [Transfer servers]({{% ref ""transfer-servers"" %}}) for details on which servers within JASMIN have GridFTP available.
- [Transfers from ARCHER2]({{% ref ""transfers-from-archer2"" %}}) for details of different routes affecting your choice of server (since this is the one of the most likely places to which JASMIN users will want to transfer data to/from)
",https://help.jasmin.ac.uk/docs/data-transfer/gridftp-ssh-auth#introduction,1399,197
Establishing a connection,"Since you will be using SSH as the authentication mechanism, you should ensure
that your initial connection to the JASIMN transfer server is made with the -A
option enabled, to enable agent forwarding:
{{<command user=""user"" host=""localhost"">}}
ssh -A username1@hpxfer3.jasmin.ac.uk
{{</command>}}
Use the `globus-url-copy` command to list the contents of your home directory
on the remote server (This will only work if you already know that that server
supports GridFTP over SSH). In this case, we are making the connection to a
**fictitious** server `gridftp.remotesite.ac.uk`:
{{<command user=""username"" host=""hpxfer3"">}}
globus-url-copy -vb -list sshftp://username2@gridftp.remotesite.ac.uk/
{{</command>}}
If `username` and `username2` are the same (on the different systems), the
`username@` part of the sshftp URI can be omitted.
Note that the URI of the server, in this case
`sshftp://username2@gridftp.remotesite.ac.uk` must come immediately after (as
it is an argument to) the `-list` option. This is particularly important if
you are combining this command with other options.
",https://help.jasmin.ac.uk/docs/data-transfer/gridftp-ssh-auth#establishing-a-connection,1089,150
Example GridFTP usage,"Once you have successfully established that you can connect to the server (as
above), then you should be able to transfer data between the remote end
(server) and local end (client) with commands such as shown below:
As above, if you have the same username on both local and remote systems, then
the `username@` part of the sshftp URI can be omitted.
Please consult the documentation for the `globus-url-copy` command for the
full range of options and arguments.
{{<command user=""username"" host=""hpxfer3"">}}
globus-url-copy -help
{{</command>}}
See also <http://toolkit.globus.org/toolkit/docs/latest-stable/gridftp/user/#gridftp-user-basic>
2\. Download a file from remote directory `/home/users/USERNAME` to
destination on the local (client) machine, for example a group workspace on
JASMIN:
{{<command user=""username"" host=""hpxfer3"">}}
globus-url-copy -vb sshftp://username@gridftp.remotesite.ac.uk/home/users/USERNAME/myfile /group_workspaces/jasmin/myworkspace/myfile
{{</command>}}
The `-p N` and `-fast` options can additionally be used in combination to
enable `N` parallel streams at once, as shown below. You can experiment with N
in the range 4 to 32 to obtain the best performance, but please be aware that
many parallel transfers can draw heavily on shared resources and degrade
performance for other users:
{{<command user=""username"" host=""hpxfer3"">}}
globus-url-copy -vb -p 16 -fast sshftp://username@gridftp.remotesite.ac.uk/home/users/USERNAME/myfile /group_workspaces/jasmin/myworkspace/myfile
{{</command>}}
3\. Test performance with large files by downloading from /dev/zero on the
remote server to /dev/null locally. This excludes any interaction with either
filesystem and gives an upper limit to the performance that can be achieved at
the time. Repeat with values of N in the range 4 to 32 to compare rates. Note
that the performance takes a while to ""ramp up"", so you will not see the best
rates if transferring small files individually as the process never gets up to
full speed:
{{<command user=""username"" host=""hpxfer3"">}}
globus-url-copy -vb -p 16 -fast sshftp://username@gridftp.remotesite.ac.uk/dev/zero /dev/null
{{</command>}}
Press CTRL-C to interrupt the transfer. Alternatively you can specify that the
transfer should continue for a fixed duration in seconds using the `-t`
option. In this example, data is transferred from the remote host
gridftp.remotesite.ac.uk to jasmin-xfer2.ceda.ac.uk.
{{<command user=""username"" host=""hpxfer3"">}}
globus-url-copy -p 16 -fast -t 10 -vb sshftp://username2@gridftp.remotesite.ac.uk/dev/zero /dev/null
(out)    Source: sshftp://username2@gridftp.remotesite.ac.uk/dev/
(out)    Dest:   file:///dev/
(out)      zero  ->  null
(out)    
(out)       7797473280 bytes       929.52 MB/sec avg      1024.49 MB/sec inst
(out)    Cancelling copy...
{{</command>}}
Note the transfer rate achieved in Megabytes/second (MB/sec), although for
various reasons this is not to be relied upon as an accurate expectation of
speed for real transfers. However, **you are unlikely to achieve even half of
this data rate via`scp`, `rsync` or `sftp` over the same route**.
[Bbcp]({{% ref ""bbcp"" %}}) may achieve similar rates, however, and
this is considered by some as easier to use.
4\. Recursively download the contents of a directory on a remote location to a
local destination.
{{<command user=""username"" host=""hpxfer3"">}}
globus-url-copy -vb -p 4 -fast -cc 4 -cd -r sshftp://username2@gridftp.remotesite.ac.uk/home/users/USERNAME/mydir/ /group_workspaces/jasmin/myworkspace/mydir/
{{</command>}}
Where:
- `-cc N` requests `N` concurrent transfers (in this case, each with `p=4` parallel streams)
- `-cd` requests creation of the destination directory if this does not already exist
- `-r` denotes recursive transfer of directories
- `-sync` and `-sync-level` options can be used to synchronise data between the two locations, where destination files do not exist or differ (by criteria that can be selected) from corresponding source files. See `-help` option for details.
",https://help.jasmin.ac.uk/docs/data-transfer/gridftp-ssh-auth#example-gridftp-usage,4030,529
Upload data (push data from JASMIN to remote server),"The above commands can also be adapted to invoke transfers from a local source
to a remote destination, i.e. uploading data, since the commands all take the
following general form:
{{<command user=""username"" host=""hpxfer3"">}}
globus-url-copy [OPTIONS] source-uri desination-uri
{{</command>}}
Be sure to check your connection with the remote machine via a simple SSH
login and then a directory listing as shown above.
",https://help.jasmin.ac.uk/docs/data-transfer/gridftp-ssh-auth#upload-data-(push-data-from-jasmin-to-remote-server),418,61
JASMIN host as remote server,"So far the examples have used a server within JASMIN as the client in the
GridFTP transfer. The transfer can be reversed so that the client is elsewhere
and the JASMIN host is the server specified in the destination URI. The
following command should work connecting to one of the [xfer]({{% ref ""transfer-servers#xfer-servers"" %}})
or [hpxfer]({{% ref ""transfer-servers#hpxfer-servers"" %}}) servers (consult those pages for current list).
Push data to JASMIN from a remote server:
{{<command user=""username2"" host=""remotehost"">}}
globus-url-copy -vb -p 8 -fast mydir/myfile sshftp://username@hpxfer3.jasmin.ac.uk/group_workspaces/jasmin/myworkspace/mydir/
{{</command>}}
Note that for this to work, you need to be able to authenticate over SSH to the JASMIN host. This should be possible if you can log in interactively, but will NOT work if you are using the command in a cron job or other situation where your ssh-agent (on the host remote to JASMIN) is not running and/or does not have access to your private key. For those situations, consider using either
- [Globus (recommended)]({{% ref ""globus-transfers-with-jasmin"" %}}), or
- [Gridftp using certificate-based authentication]({{% ref ""gridftp-cert-based-auth"" %}})
",https://help.jasmin.ac.uk/docs/data-transfer/gridftp-ssh-auth#jasmin-host-as-remote-server,1224,173
Globus transfers with JASMIN,"This article describes how to do data transfers using JASMIN's **new** Globus
endpoint (now called a **collection** ), based on the most recent version of
[Globus Connect Server](https://www.globus.org/globus-connect-server).
JASMIN's old Globus endpoint, based on
the previous version of the Globus service, ceased operating on 18
December 2023 as support was discontinued by Globus. We have implemented a
new endpoint, based on Globus Connect Server v5.4, with equivalent (but better!)
functionality.
The new collection can be used as a drop-in replacement for the previous
endpoint, aside from a few differences in terminology, and a change to the
authentication process.
",https://help.jasmin.ac.uk/docs/data-transfer/globus-transfers-with-jasmin,675,98
Main differences,"There are some differences to how the new (v5) version of Globus works on JASMIN compared to previously:
- Users now interact with a **collection**
  - **Most users**: please use [""JASMIN Default Collection""](https://app.globus.org/file-manager/collections/a2f53b7f-1b4e-4dce-9b7c-349ae760fee0/overview) with ID `a2f53b7f-1b4e-4dce-9b7c-349ae760fee0`
  - For **STFC users only** where the other collection (either {{<link ""globus-connect-personal"">}}GCP{{</link>}} or {{<link ""https://www.globus.org/globus-connect-server"">}}GCS{{</link>}}) is within STFC, an additional collection is provided [""JASMIN STFC Internal Collection""](https://app.globus.org/file-manager/collections/591d44ac-adbb-43db-9931-977708d07450/overview) and has ID `9efc947f-5212-4b5f-8c9d-47b93ae676b7`.
- You now use the JASMIN Accounts Portal to authenticate (using your JASMIN account credentials) via OpenID Connect (OIDC). 
- During the authentication process, you are redirected to the JASMIN Accounts Portal to link your Globus identity with your JASMIN account.
- Consent needs to be granted at a number of points in the process to allow the Globus service to carry out actions on your behalf.
- The default lifetime of the authentication granted to your JASMIN account is now **30 days**. After this, you may need to refresh the consent for your ""session"".
- This service is now available to **all** users of JASMIN: you no longer need to hold the `hpxfer` access role.
The following examples show you how to authenticate with the new JASMIN
Default Collection and list the contents of your home directory. As before,
however, the following file systems are available via this collection
File system  |  Access  
---|---  
`$HOME` (`/home/users/<username>`)  |  Read-write  
`/gws` (group workspaces)  |  Read-write  
`/work/xfc` (transfer cache)  |  Read-write  
`/badc` (CEDA Archive)  
`/neodc` |  Read-only  |  
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/data-transfer/globus-transfers-with-jasmin#main-differences,1921,245
List your home directory using the web app,"1\. Navigate to <https://app.globus.org>
2\. Log in with your Globus identity (this could be a globusid.org or other
identity)
{{<image src=""img/docs/globus-transfers-with-jasmin/file-qEk9SPARZN.png"" caption=""log in"">}}
3\. In File Manager, use the search tool to search for ""JASMIN Default
Collection"". Select it.
{{<image src=""img/docs/globus-transfers-with-jasmin/file-LtMk6bD3Wz.png"" caption=""Find JASMIN Default Collection"">}}
4\. In the transfer pane, you are told that Authentication/Consent is
required. Click Continue.
{{<image src=""img/docs/globus-transfers-with-jasmin/file-pprxjkRNiw.png"" caption=""Consent"">}}
5\. Click the link to use the JASMIN Accounts Portal OIDC server to link your
JASMIN identity
6\. You are taken to a page on the JASMIN Accounts portal, where you are
invited to ""Authorise"" the external application to authenticate and access
your essential account information.
{{<image src=""img/docs/globus-transfers-with-jasmin/file-LEssDTYdfN.png"" caption=""Authorise application"">}}
7\. If successful, you are taken back to the Globus web app, where you are
invited to ""Allow"" the app to use the listed information and services.
{{<image src=""img/docs/globus-transfers-with-jasmin/file-lYBGlLIk9A.png"" caption=""Allow the app to use the info"">}}
8\. The directory listing of your home directory should now appear in the
transfer pane.
9\. Try navigating to another collection known to you (previously known as
endpoint) in the other pane and transferring some data. If you have Globus
Connect Personal running locally, you should be able to transfer files to/from
that.
",https://help.jasmin.ac.uk/docs/data-transfer/globus-transfers-with-jasmin#list-your-home-directory-using-the-web-app,1594,201
List your home directory using the command-line interface (CLI),"1\. Load the virtual environment where you have the Globus CLI installed:
(in this example, a Python virtual environment named `~/.globus-cli-venv`
already exists. If it doesn't create one with the command `python3 -m venv
~/.globus-cli-venv` on your local machine). Activate this virtual environment
as follows:
{{<command>}}
source ~/.globus-cli-venv/bin/activate
{{</command>}}
2\. It's recommended to update to the latest version of the CLI by doing the
following:
{{<command>}}
pip install -U globus-cli
{{</command>}}
3\. Check that you have an active globus session and follow any instructions
given, e.g.
{{<command>}}
globus login
globus session show
{{</command>}}
4\. Use the ""globus ls"" command to list the collection using its ID, starting
at the path of your home directory (/~/)
{{<command>}}
globus ls a2f53b7f-1b4e-4dce-9b7c-349ae760fee0:/~/
{{</command>}}
TIP: you can set the ID of the collection to be an environment variable like
this, for convenience:
{{<command>}}
export JASMIN_GLOBUS=a2f53b7f-1b4e-4dce-9b7c-349ae760fee0
globus ls $JASMIN_GLOBUS:/~/
{{</command>}}
6\. You will be taken through an equivalent set of steps to those needed for
the web app. First off, you will be asked to copy/paste a URL into your
browser and copy/paste back the resulting authentication code.
7\. Once the authentication/consent process has been completed, you should see
a listing of your home directory.
8\. Use the `globus transfer` command to copy data to/from another
**collection** (previously known as endpoint) to your home directory, within
the JASMIN Default Collection. (see `globus transfer --help` for details)
",https://help.jasmin.ac.uk/docs/data-transfer/globus-transfers-with-jasmin#list-your-home-directory-using-the-command-line-interface-(cli),1633,231
Where to/from?,"Don't forget that to actually transfer data to/from JASMIN (e.g. step 8, above), you'll need another
collection somewhere else. If you're transferring data from ARCHER2, you can use their
{{<link ""https://app.globus.org/file-manager/collections/3e90d018-0d05-461a-bbaf-aab605283d21/overview"">}}ARCHER2 filesystems collection (id: `3e90d018-0d05-461a-bbaf-aab605283d21`){{</link>}}
If not, unless your institution runs a Globus collection, you'll need to 
install a small piece of software called [Globus Connect Personal]({{% ref ""globus-connect-personal"" %}})
on a machine at that end that is able to read/write the data that you want to transfer.
A good idea is to try this on your own desktop/laptop first.
Our help doc guides you through how to do this and some examples of how to use it. Versions available for
Windows, Mac and Linux.
",https://help.jasmin.ac.uk/docs/data-transfer/globus-transfers-with-jasmin#where-to/from?,840,115
Using the JASMIN Object Store,"This article describes how to use the JASMIN high-performance object storage.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store,78,11
What is object storage?,"An {{< link ""https://en.wikipedia.org/wiki/Object_storage"" >}}object store{{</link>}} is a data
storage system that manages data as objects referenced by a globally unique
identifier, with attached metadata. This is a fundamental change from
traditional file systems that you may be used to, as there is no directory
hierarchy - the objects exist in a single flat domain. These semantics allow
the object store to scale out much more easily than a traditional shared file
system.
The other fundamental change is that the data is no longer accessed by
mounting a file system onto a host and referencing a file path (where
authentication is ""can I log in to the host""). Instead, the data is accessed
over HTTP, with authentication using HTTP headers. This has many benefits, the
biggest of which is that we can make the object store available outside of the
JASMIN firewall, for example to the JASMIN External Cloud. Data can be read
**and written** in the same way, using the same tools, from inside and outside
JASMIN. Contrast this with Group Workspaces, where you must be logged in to a
JASMIN host in order to write data using the file system, and data is only
accessible externally in a readonly way using HTTP or OPeNDAP or via data 
transfer methods.
Object stores are seen as the most efficient (and cheapest!) way to store and
access data from the cloud, and all the major cloud providers support some
variant of object store. The JASMIN object store is 
{{< link ""https://www.scality.com/topics/what-is-s3-compatible-storage/"" >}}S3 compatible{{</link>}} \-
S3 is the object store for Amazon Web Services (AWS), and has become a de-
facto standard interface for object stores. This means that all the same tools
that work with AWS S3 will also work with the JASMIN object store.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store#what-is-object-storage?,1788,294
Accessing the object store,"The JASMIN object store is organised into **tenancies**. These are shared
areas of the object store, similar in concept to Group Workspaces, and are
requested by users, usually Group Workspace Managers. Several users can have
access to a tenancy, and so they can be used collaboratively.
To join an existing object store tenancy, navigate to the ""Services"" section
in the JASMIN Accounts Portal and select the ""Object store"" category. Select a
tenancy and submit a request to join. This request will then be considered by
the manager of the tenancy and either accepted or rejected.
{{<image src=""img/docs/using-the-jasmin-object-store/file-kHfcUlqbAf.png"" caption="""">}}
For details on how to request and manage an object
store tenancy, please see the help article ""JASMIN Object Store for Managers""
(forthcoming).
",https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store#accessing-the-object-store,814,121
Default access policies,"As of Jan 2024, we have changed the default access policy for newly created tenancies to provide a more sensible and flexible set of access policies.
The old policy allowed any members of the tenancy access to any bucket created in the tenancy by default. The new policy allow Users (USER only in the JASMIN Accounts Portal) of the tenancy only access to buckets they own by default. This can be effectively changed to the old policy by setting the policy of the bucket to the LDAP group for the tenancy members (<tenancy>-members, e.g. cedadev-o-members) - this can be done using the JASMIN Object Store portal (below) or the Swarm portal. Specific JASMIN users or groups can also be given permission to buckets (group access is controlled by LDAP groups, and only existing LDAP group will work - you may need to ask for one to be created).
The new policy also gives admin access for tenancy MANAGER and DEPUTY roles, who have access to all the buckets in the tenancy.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store#default-access-policies,970,171
Creating an access key and secret,"Authentication with the object store uses an access key and secret that are separate to your JASMIN username and password. You can generate keys and manage bucket permissions through the {{<link ""jasmin_object_store_portal"">}}JASMIN Object Store Portal.{{</link>}}
{{<image src=""img/docs/using-the-jasmin-object-store/file-OkGcJm0Mpo.png"" caption=""JASMIN sbject store portal"">}}
You can log in with your JASMIN username and password. You can then click on the ""Object Stores"" button on the right. This will present you with the list of object store tenancies that you have access to. If you don't see an object store tenancy that you expect to, please check you have access in the {{<link ""jasmin_accounts_portal"">}}JASMIN Accounts Portal{{</link>}}. If you have access in the Accounts Portal, but not in the Object Store Portal then please email the helpdesk.
{{<image src=""img/docs/using-the-jasmin-object-store/file-3kmJDqsPGr.png"" caption=""List of object store tenancies"">}}
The URL for the object store tenancy is also presented here for convenience. You can click on the ""Manage Object Store"" button to manage you keys and buckets. This will ask you to confirm your JASMIN password.
{{<image src=""img/docs/using-the-jasmin-object-store/file-J7sWRXVp9D.png"" caption=""Prompt for password"">}}
You will then be presented with the following page.
{{<image src=""img/docs/using-the-jasmin-object-store/file-gRPOhQpGBG.png"" caption=""existing keys"">}}
From this page you can view your existing keys, and delete them if you require. You can also use the ""Create Key"" tab on the left.
{{<image src=""img/docs/using-the-jasmin-object-store/file-L8tOrtBj7z.png"" caption=""Create access key"">}}
You need to name the key and enter an expiry date for it. This will then present you with a pop-up with details on your access key and secret key. **This is the only time your secret key will visible, so save it immediately in a secure password manager.**
",https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store#creating-an-access-key-and-secret,1941,264
Managing bucket permissions,"You can also manage the permissions on buckets using the ""Buckets"" tab from this page. This allows you to manage the access policies for your buckets without using the S3 API or the Swarm portal.
{{<image src=""img/docs/using-the-jasmin-object-store/file-6m0rwBsm9K.png"" caption=""Bucket permissions"">}}
Click on the ""Manage permissions"" button for a bucket to add or change access policies for that bucket.
{{<image src=""img/docs/using-the-jasmin-object-store/file-CxoXaC08wI.png"" caption=""Granting access"">}}
By default this lets you grant access to specific JASMIN Users and/or groups (these are LDAP groups and you might need to request that one is created for you if you require a subgroup for your tenancy). The advanced tab gives you the same options as available through the Swarm portal - including making buckets publicly accessible. Once done, hit the save to add the policy to the bucket. You can edit or delete permissions from that bucket through the ""View Bucket Policies"" tab.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store#managing-bucket-permissions,991,145
Legacy method for key creation,"This is the old way of creating keys which still works, but the new way above is accessible outside JASMIN on the public internet.
You can generate an access key and secret using the Caringo portal. 
Authentication with the object store uses an access key and secret that are
separate to your JASMIN username and password. You can generate an access key
and secret using the Caringo portal. This portal is not currently available
outside of JASMIN - you will need to use a graphical session on JASMIN to
access a Firefox browser running on a JASMIN system.
**The recommended way to do this is using the** [NX Graphical Desktop
service]({{% ref ""graphical-linux-desktop-access-using-nx"" %}}). You can start Firefox from
the ""Activities"" menu once you have logged in to your graphical desktop on one
of the `nx-login*` servers (so no need to make an onward connection to a `sci`
server).
An alternative option is to using X11 Forwarding on your SSH connection. You 
need to do this on one of the `nx*` servers (not the sci servers as previously)
because this is where firefox is now installed:
ssh -X <user>@nx1.jasmin.ac.uk firefox
(try `-Y` if `-X` does not work for you).
Once you have Firefox open, navigate to
http://my-os-tenancy-o.s3.jc.rl.ac.uk:81/_admin/portal
but replace `my-os-tenancy-o` with your tenancy name.
You will see a login screen where you
should enter your JASMIN username and password:
{{<image src=""img/docs/using-the-jasmin-object-store/file-4dszwvVbge.png"" caption="""">}}
If you receive a ""HTTP ERROR 500 java..."" error, it is likely that you haven't
added the port (81) to the address.  
Upon successfully entering the username and password of a user who belongs to
the tenancy, you will see a dashboard. To create an access key and secret,
click on the cog icon and select ""Tokens"":
{{<image src=""img/docs/using-the-jasmin-object-store/file-sazR2YE8xn.png"" caption="""">}}
On the tokens page, click ""Add"":
{{<image src=""img/docs/using-the-jasmin-object-store/file-tiUnM6oyq6.png"" caption="""">}}
In the dialogue that pops up, enter a description for the token and set an
expiration date. Make sure to click ""S3 Secret Key"" - this will expose an
additional field containing the secret key. **Make sure you copy this and
store it somewhere safe - you will not be able to see it again!** This value
will be used whenever the ""S3 secret key"" is required.
{{<image src=""img/docs/using-the-jasmin-object-store/file-8ezAbSQY56.png"" caption="""">}}
Once the token is created, it will appear in the list. The ""Token"" should be
used whenever the ""S3 access key"" is required:
{{<image src=""img/docs/using-the-jasmin-object-store/file-QSg6RtIAv8.png"" caption="""">}}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store#legacy-method-for-key-creation,2673,397
Accessing data in the object store,,https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store#accessing-data-in-the-object-store,0,0
URLs for internal and external access,"Although the data is exactly the same in both cases, a slightly different URL
must be used depending on whether you are accessing the object store from the
JASMIN managed cloud servers or from the JASMIN External Cloud.
From inside JASMIN, including LOTUS and the Scientific Analysis servers, 
`my-os-tenancy-o.s3.jc.rl.ac.uk` should be used, with the `http://`` prefix.
From the JASMIN External Cloud, and from locations external to JASMIN, `my-os-tenancy-o.s3-ext.jc.rl.ac.uk` should be
used - note the `https://` prefix and additional `-ext`.
(Where `my-os-tenancy-o` needs to be replaced with your tenancy name)
",https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store#urls-for-internal-and-external-access,616,89
Using s3cmd,"`s3cmd` is a command line tool provided by Amazon to work with S3 compatible
Object Storage. It is installed on JASMIN, both on the sci-machines and on
LOTUS. It is a little more complicated to use than the MinIO client, but is
more powerful and flexible. For full details on `s3cmd`, see the 
{{< link ""http://s3tools.org"" >}}s3tools.org website{{</link>}}.
To configure `s3cmd` to use the JASMIN object store, you need to create and
edit a `~/.s3cfg` file. To access the `my-os-tenancy-o` tenancy (where ""my-os-
tenancy-o"" needs to be replaced with your tenancy name), the following should
be in the `~/.s3cfg `file:
access_key = <access key generated above>
host_base = my-os-tenancy-o.s3.jc.rl.ac.uk
host_bucket = my-os-tenancy-o.s3.jc.rl.ac.uk
secret_key = <secret key generated above>
use_https = False
signature_v2 = False
or, from an external tenancy or locations outside of JASMIN:
access_key = <access key generated above>
host_base = my-os-tenancy-o.s3-ext.jc.rl.ac.uk
host_bucket = my-os-tenancy-o.s3-ext.jc.rl.ac.uk
secret_key = <secret key generated above>
use_https = True
signature_v2 = False
To see which commands can be used with s3cmd, type:
s3cmd -h
To list a tenancy's buckets:
s3cmd ls
To list the contents of a bucket:
s3cmd ls s3://<bucket_name>
Make a new bucket:
s3cmd mb s3://<bucket_name>
`s3cmd` uses PUT and GET nomenclature for copying files to and from the object
store.  
To copy a file to a bucket in the object store:
s3cmd put <file name> s3://<bucket_name>
To copy a file from a bucket in the object store to the file system:
s3cmd get s3://<bucket_name>/<object_name> <file_name>
For more commands and ways of using `s3cmd`, see the [s3tools
website](https://s3tools.org/s3cmd).
",https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store#using-s3cmd,1717,256
Using the MinIO client,"The MinIO Client is a command line tool to connect to object stores (among
other types of file storage) and interface with it as you would with a UNIX
filesystem. As such, many of the UNIX file management commands found in
standard installations of the OS are found within this client ( `ls`, `cat`,
`cp`, `rm` for example).
There are a number of ways to install this client as shown in the
{{< link ""https://docs.min.io/docs/minio-client-quickstart-guide.html"" >}}quickstart guide{{</link>}}. Methods
include: docker, Homebrew for macOS, wget for Linux and instructions for
Windows. Follow these steps to get the client installed on the relevant
system.
MinIO Client is not installed on JASMIN, but users can download and install it
to their own user space, following the instructions for ""64-bit Intel"" (linux-
amd64) in the MinIO quickstart guide. Below is an example to install it into
the `bin` directory in your user space
mkdir ~/bin
wget https://dl.min.io/client/mc/release/linux-amd64/mc ~/bin
chmod u+x ~/bin/mc
You can then add the `~/bin` directory to the PATH environment variable in
your `~/.bashrc` file to allow `mc` to be accessed from anywhere on JASMIN.
# User specific aliases and functions
PATH=$PATH:$HOME/bin
To configure the client with the JASMIN object store, create an access key and
secret as documented above and insert them into the command:
mc config host add [ALIAS] [S3-ENDPOINT] [TOKEN] [S3 SECRET KEY]
The ALIAS is the name you'll reference the object store when using the client.
To demonstrate, if the alias was set to ""jasmin-store"", displaying a specific
bucket in the object store would be done in the following way:
mc ls jasmin-store/my-bucket
The commands available in the client are documented in the quickstart guide
(linked above). Copying an object from one place to another is very similar to
a UNIX filesystem:
mc cp jasmin-store/my-bucket/object-1 jasmin-store/different-bucket/
",https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store#using-the-minio-client,1929,298
From Python,"One method of accessing the object store from Python is using
{{< link ""https://s3fs.readthedocs.io/en/latest/index.html"" >}}s3fs{{</link>}}. This library builds
on
{{< link ""https://botocore.amazonaws.com/v1/documentation/api/latest/index.html"" >}}botocore{{</link>}}
but abstracts a lot of the complexities away. There are three main types of
object in this library:
{{< link ""https://s3fs.readthedocs.io/en/latest/api.html#s3fs.core.S3FileSystem"" >}}S3FileSystem{{</link>}},
{{< link ""https://s3fs.readthedocs.io/en/latest/api.html#s3fs.core.S3File"" >}}S3File{{</link>}} and
{{< link ""https://s3fs.readthedocs.io/en/latest/api.html#s3fs.mapping.S3Map"" >}}S3Map{{</link>}}.
The filesystem object is used to configure a connection to the object store.
Note: it's **strongly** recommended to store the endpoint, token and secret
outside of the Python file, either using environment variables or an external
file. This object can be used for lots of the operations which can be done
MinIO:
with open('jasmin_object_store_credentials.json') as f:
    jasmin_store_credentials = json.load(f)
    jasmin_s3 = s3fs.S3FileSystem(
        anon=False, secret=jasmin_store_credentials['secret'],
        key=jasmin_store_credentials['token'],
        client_kwargs={'endpoint_url': jasmin_store_credentials['endpoint_url']}
    )
    my_object_size = jasmin_s3.du('my-bucket/object-1')
Please note in the example above, the `jasmin_object_store_credentials.json`
file would look along the lines of:
{
    ""token"": ""<access key generated above>"",
    ""secret"": ""<secret key generated above>"",
    ""endpoint_url"": ""http://my-os-tenancy-o.s3.jc.rl.ac.uk""
}
or, from an external tenancy or locations outside of JASMIN:
{
    ""token"": ""<access key generated above>"",
    ""secret"": ""<secret key generated above>"",
    ""endpoint_url"": ""https://my-os-tenancy-o.s3-ext.jc.rl.ac.uk""
}
S3File is used for dealing with individual files on the object store within
Python. These objects can read and written to and from the store:
file_object = s3fs.S3File(jasmin_s3, 'my-bucket/object-1', mode='rb')
# refresh can be set to True to disable metadata caching
file_metadata = file_object.metadata(refresh=False)
# Writing data to variable in Python
file_object.write(data)
# Data will only be written to the object store if flush() is used. This can be executed in S3FS source code if the buffer >= the blocksize
file_object.flush()
S3Map is very useful when using {{< link ""http://xarray.pydata.org/en/stable/"" >}}xarray{{</link>}}
to open a number of data files (netCDF4 for example), and turn them into the
zarr format ready to be stored as objects on the store. The function for this
can store a `.zarr` file in a POSIX filesystem, or can be streamed directly to
an object store. These datasets can then be opened back into Python:
xarray.open_mfdataset(filepath_list, engine=netcdf4)
s3_store = s3fs.S3Map('my-bucket/zarr-data', s3=jasmin_s3)
dataset.to_zarr(store=s3_store, mode='w')
# Reopening the dataset from object store using xarray
xarray.open_zarr(s3_store, consolidated=True)
",https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store#from-python,3066,345
"""GWS Alert System""","The Group Workspace (GWS) Alert system is a python app which alerts the managers/deputies of a GWS when their GWS is reaching full capacity.
{{<alert type=""info"">}}
Please note this service is under beta-testing (June 2024) so if you receive any emails in error please let us know
{{</alert>}}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/gws-alert-system,294,48
What is the GWS Alert System?,"A GWS is a collaborative storage made available to a group for a project. Each GWS has a certain quota of storage - more information about how the storage is being used within a GWS can be found using the [GWS Scanner UI](https://gws-scanner.jasmin.ac.uk/). The GWS Alert System is set up to notify the managers/deputies of a GWS when it is reaching full capacity so the managers can make some more space available.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/gws-alert-system#what-is-the-gws-alert-system?,416,72
How the GWS Alert System runs,"The Python app gets a list of all GWSs from the {{<link ""jasmin_projects_portal"">}}JASMIN Projects Portal{{</link>}}, gets the storage information - i.e. how much storage has been used and how much is available, then sends an email alert if the GWS is over a certain percentage full. The managers and deputies are obtained from the {{<link ""jasmin_accounts_portal"">}}JASMIN Projects Portal{{</link>}}.
The GWS Alert System runs on a schedule at 11am daily.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/gws-alert-system#how-the-gws-alert-system-runs,457,69
Threshold value,"The threshold value at which alerts are sent is obtained from a file within the GWS. The file is found at `{GWS_PATH}/.gws_scan/config.ini`. The file should look something like this, for a threshold of **80%**:
[general]
volume_warning_threshold = 80
where the `volume_warning_threshold` (without the `%` symbol) is the threshold value used in the GWS Alert system.
If no file can be found, the default value of `90` is used.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/gws-alert-system#threshold-value,426,68
Issues and questions,"If you receive an email in error, or you think the email may contain incorrect information, please let us know.
If you'd like to change the threshold value, please update the `volume_warning_threshold` value in the `config.ini` file as described above. If the value hasn't been updated the following day, please let us know.
If you make changes to your storage and these aren't reflected in the next alert, please let us know.
For more information about managing a GWS, see https://help.jasmin.ac.uk/docs/short-term-project-storage/managing-a-gws/.
If you have any questions or suggestions, feel free to [get in touch](mailto:nicola.farmer@stfc.ac.uk).
",https://help.jasmin.ac.uk/docs/short-term-project-storage/gws-alert-system#issues-and-questions,653,94
"""GWS Scanner UI""","The [Group Workspace (GWS) Scanner](https://gws-scanner.jasmin.ac.uk/) UI is an interactive web application for JASMIN users to view their GWSs.
**_Please note this service is under beta-testing and should be used with care (July 2023)_**
",https://help.jasmin.ac.uk/docs/short-term-project-storage/gws-scanner-ui,239,33
What is the GWS Scanner UI?,"A GWS is collaborative storage made available to a group for a project. The GWS Scanner UI provides information about the structure of GWSs in the form of a tree graph showing the folders within the GWS. The tiles of the graph can be scaled or coloured by size, count or heat. 
- **Size** refers to the size (in TB or GB) of the folder
- **count** refers to the number of files and folders within the directory
- **heat** refers to the average time since last access (hot = recent).
The tree graph only shows tiles for the folders within the GWS, the files are lumped together as **unindexed children**, but the files are considered in the calculation of the GWS statistics - e.g. the number of **children** is the number of folders and files within the GWS. There are also doughnut graphs with additional breakdowns in terms of users, filetypes and heats which can be scaled by either size or count.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/gws-scanner-ui#what-is-the-gws-scanner-ui?,901,163
Using the GWS Scanner UI,"Navigate to the [GWS Scanner homepage](https://gws-scanner.jasmin.ac.uk/) where you should see a **Log In** button.
{{<image src=""img/docs/gws-scanner-ui/gws-scanner-homepage.png"" caption="""">}}
Clicking the login button will take you to the login page where you can sign in using your JASMIN account. Upon successful sign-in you should be redirected back to the homepage where you will have the option to view [your GWSs](https://gws-scanner.jasmin.ac.uk/my_gws).
{{< alert type=""info"" >}}
 At any time, you can click the JASMIN logo in the navbar to return to the homepage.
{{< /alert >}}
Here you will see a table with the names of your GWSs and their path. Clicking on the path will take you to the tree page for the GWS.
{{<image src=""img/docs/gws-scanner-ui/gws-scanner-my-gws.png"" caption="""">}}
Alternatively, if you know the path to your GWS you can search for it in the url, e.g. `https://gws-scanner.jasmin.ac.uk/tree/<path-here>`. On this page you will see the path to your GWS at the top, then the tree graph with the coloured tiles representing folders within the GWS. *Note that some larger GWSs may take longer to load*. 
{{<image src=""img/docs/gws-scanner-ui/gws-scanner-tree-page.png"" caption="""">}}
There is a **Toggle Patterns** button which can be used to swap the coloured tiles for patterned tiles. Hovering over a tile will reveal more information about that folder. There are buttons on the right hand side to scale and colour the tiles by size, count or heat as described earlier. There is also a key showing the values the colour of the tiles correspond to in this sidebar as well as information showing when the GWS was last scanned. Scans usually take around two weeks to complete so each GWS should be scanned approximately fortnightly. There is a **Go Up One Level** button which will take you back a level, but note that this will only work if you have gone deeper into the file path as it will not show the structure above the root GWS folder. You can click on a tile on the graph to view the tree graph inside that folder. For example, you may have a folder called *user1* in *group-workspace-1*, you could click on this tile to see the structure of the *user1* directory then you could use the **Go Up One Level** button to go back to *group-workspace-1*. Alternatively, you can just use the back button in your browser to go back. You can keep clicking files to go deeper into the GWS structure until you reach an **unindexed children** level where there are no more folders, only files. This is represented by a tile saying **__unindexed children__** which refers to a collection of files (every level will have an unindexed children tile representing the files within that folder).
{{<image src=""img/docs/gws-scanner-ui/gws-scanner-unindexed-children.png"" caption="""">}}
Scrolling down the page, you will find the doughnut graphs. There is one for the **User Breakdown**, **Filetype Breakdown** and **Heat Breakdown**. Each of these have coloured sections corresponding to different users, filetypes or heats and the size of them is proportional to the size or count (there are **Size** and **Count** buttons to change between them). Again, hovering over a section of the graph will give you more information.
{{<image src=""img/docs/gws-scanner-ui/gws-scanner-doughnut-graphs.png"" caption="""">}}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/gws-scanner-ui#using-the-gws-scanner-ui,3329,516
Common issues and questions,"  * If you get stuck on a loading screen try refreshing the page - we are working on fixing this issue. 
  * If you go to the [My GWSs](https://gws-scanner.jasmin.ac.uk/my_gws) page and don't have any GWSs listed, check that your JASMIN account does have access to the GWS you are looking for.
  * Sometimes, if there are lots of tiles they can be hard to read - you can hover over the tile to get the full title or if you want to view the tree graph for that folder, you search for it by URL instead. 
  * If you are ever stuck and requiring help, the help beacon can be accessed from any page.
{{<image src=""img/docs/gws-scanner-ui/gws-scanner-beacon.png"" wrapper=""col-2 mx-auto"" caption="""">}}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/gws-scanner-ui#common-issues-and-questions,696,119
Further information,"Here are some links to learn more about the GWS Scanner:
  * [The GWS Scanner UI](https://gws-scanner.jasmin.ac.uk/)
If you have any questions or suggestions, feel free to [get in touch](mailto:nicola.farmer@stfc.ac.uk).",https://help.jasmin.ac.uk/docs/short-term-project-storage/gws-scanner-ui#further-information,220,29
Introduction,"This article describes the way that storage is usually provided to projects on JASMIN: the **Group Workspace (GWS)**.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/introduction-to-group-workspaces#introduction,118,18
What is a GWS?,"GWSs are portions of disk allocated for particular projects
to manage themselves, enabling collaborating scientists to share network
accessible storage on JASMIN. Users can pull data from external sites to a
common cache, process and analyse their data, and where allowed, exploit data
available from other GWSs and from the CEDA archive.
It is important to understand that these workspaces are not the same as the
CEDA archive. Data in a GWS can be earmarked for ingestion into the CEDA
archive, but this process should be discussed directly with the CEDA Archive team (via the CEDA Helpdesk), it is not automatic and will not happen without prior arrangement.
Data within GWSs are the responsibility of the designated GWS manager and
**are not backed up by the JASMIN team** (see below).
",https://help.jasmin.ac.uk/docs/short-term-project-storage/introduction-to-group-workspaces#what-is-a-gws?,790,130
Accessing a GWS,,https://help.jasmin.ac.uk/docs/short-term-project-storage/introduction-to-group-workspaces#accessing-a-gws,0,0
Where are GWSs available?,"Each GWS volume is found in a directory mounted under a pattern of paths which refers to the capabilities of the storage, ie.
Path | Type | Capability
--- | --- | ---
/gws/nopw/j04/* | SOF | no parallel write (nopw)
/gws/pw/j07/* | PFS | parallel write capable (pw)
/gws/smf/j04/* | SSD | small-files optimised (smf)
{.table .table-striped}
A project may have several GWS volumes, perhaps of different types, for example:
- `/gws/nopw/j04/jules` SOF volume for the `jules` project
GWSs are available on:
- Transfer servers including `xfer*`, `hpxfer*`, `gridftp*` and the JASMIN Default Collection Globus endpoint.
- The general scientific analysis servers `sci*`
- All nodes in the LOTUS and ORCHID clusters
- Some application-specific servers (by arrangement)
They are NOT available on `login` or `nx` servers.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/introduction-to-group-workspaces#where-are-gwss-available?,813,128
Requesting access to a GWS,"There is a Unix group associated with each GWS to provide convenient access
control. Any JASMIN user can apply for access to a given GWS by following the
links provided in the [list of available
GWSs](https://accounts.jasmin.ac.uk/services/group_workspaces/) on the JASMIN Accounts Portal.
**Important:** The GWS
Manager (not the JASMIN team) will need to authorise the request before you are granted access.
Once you have been granted the relevant access role, then the relevant Unix group will be added
to your account. If you are not sure of the group name for your GWS you can
find this by entering the command `groups` to see the names of the groups you
belong to. The group name normally has the prefix “gws_"".
{{<alert type=""info"">}}
All data in a GWS should belong to this group `gws_<name>`, so that group read/write permissions apply to this group rather than the default group, `users`.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/introduction-to-group-workspaces#requesting-access-to-a-gws,911,148
GWS management,"Each GWS has a designated manager. See the article on
[managing a GWS]({{% ref ""managing-a-gws"" %}}).
",https://help.jasmin.ac.uk/docs/short-term-project-storage/introduction-to-group-workspaces#gws-management,102,16
Backup,"Please note that data in GWSs are only backed up if the GWS Manager has put
tasks in place to do this. The 
[Elastic Tape service]({{% ref ""secondary-copy-using-elastic-tape"" %}}) is available to enable to  make a secondary near-line copy of data. Please discuss the details with your GWS Manager.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/introduction-to-group-workspaces#backup,298,49
Recommended directory structure for a GWS,"We recommend that a sensible directory structure is set up within your GWS in
order to conventions are used within your GWS:
<your_gws>/
    users/
        <userid>/   # each user can create their own directory here
    public/         # required if you want to share data via HTTP
    data/
        internal/   # internal/intermediate data
        incoming/   # third-party data brought to the GWS
        output/     # output data generated by project
See the GWS etiquette article for more details about GWSs and the [GWS data
sharing via HTTP]({{% ref ""share-gws-data-via-http"" %}}) article for
information about the use of the `public` directory.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/introduction-to-group-workspaces#recommended-directory-structure-for-a-gws,652,95
Introduction,"This article describes how to configure Cross-Origin Resource Sharing (CORS) on a JASMIN Caringo S3 object store.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/configuring-cors-for-object-storage#introduction,114,17
S3 CORS configuration,"JASMIN's DataCore (previously Caringo) S3 object storage allows domain owners to configure Cross-Origin Resource Sharing (CORS) at bucket level. This article assumes you have read the {{<link ""using-the-jasmin-object-store"">}}this help article{{</link>}} which introduces the object store and the use of the `s3cmd` command line tool.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/configuring-cors-for-object-storage#s3-cors-configuration,335,44
Prerequisites,"You will need a valid S3 Token ID and Secret Key for the domain that you wish to modify.
e.g.
key will not be displayed again!
Token ID: <The Token for your Domain>
S3 Secret Key: <The Secret for your Domain>
Expiration Date: 2024-02-13
Owner: <Your JASMIN ID>
Description: test
See using s3cmd for instructions on generating these.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/configuring-cors-for-object-storage#prerequisites,333,58
CORS XML Configuration File,"CORS configuration is set on the S3 bucket using an XML file format, as shown below:
<CORSConfiguration>
   <CORSRule>
      <AllowedOrigin>http://www.example1.com</AllowedOrigin>
      <AllowedMethod>PUT</AllowedMethod>
      <AllowedMethod>POST</AllowedMethod>
      <AllowedMethod>DELETE</AllowedMethod>
      <AllowedHeader>*</AllowedHeader>
   </CORSRule>
   <CORSRule>
      <AllowedOrigin>http://www.example2.com</AllowedOrigin>
      <AllowedMethod>PUT</AllowedMethod>
      <AllowedMethod>POST</AllowedMethod>
      <AllowedMethod>DELETE</AllowedMethod>
      <AllowedHeader>*</AllowedHeader>
   </CORSRule>
   <CORSRule>
      <AllowedOrigin>*</AllowedOrigin>
      <AllowedMethod>GET</AllowedMethod>
   </CORSRule>
</CORSConfiguration>
The above example shows a configuration which allows CORS access from external web sites www.example1.com and www.example2.com.
You can create a new file on your filesystem to store your CORS configuration using the above example as a reference. In the next step, you'll learn how to apply this file to your bucket.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/configuring-cors-for-object-storage#cors-xml-configuration-file,1063,88
Applying CORS Settings to a Bucket,"To apply the CORS XML file you've created, you can use any S3 compatible client to set the CORS configuration.
The following example uses `s3cmd` on a Linux system.
First confirm that your `s3cmd` settings are correct by showing the `info` of the bucket.
e.g.
{{<command>}}
s3cmd info s3://testbin1
(out)s3://testbin1/ (bucket):
(out)   Location:  objectstore4.jc.rl.ac.uk
(out)   Payer:     none
(out)   Expiration Rule: none
(out)   Policy:    {
(out)                ""Version"":""2008-10-17"",
(out)                ""Id"":""testbin1 Policy"",
(out)                ""Statement"": [
(out)                  {
(out)                    ""Sid"":""1: Full access for Users"",
(out)                    ""Effect"":""Allow"",
(out)                    ""Principal"":{""anonymous"":[""*""]},
(out)                    ""Action"":[""*""],
(out)                    ""Resource"":""*""
(out)                  },
(out)                  {
(out)                    ""Sid"":""2: Read-only access for Everyone"",
(out)                    ""Effect"":""Allow"",
(out)                    ""Principal"":{""anonymous"":[""*""]},
(out)                    ""Action"":[""GetObject"",""GetBucketCORS""],
(out)                    ""Resource"":""*""
(out)                  }
(out)                ]
(out)              }
(out)   CORS:      none
(out)   ACL:       ahuggan: FULL_CONTROL
{{</command>}}
This example shows a bucket which currently doesn't have a CORS policy set. Specifically, this is the section we're interested in:
   CORS:      none
In this example, we'll set a simple ""allow all"" CORS configuration. We've already created a file named `test-cors-file` which we will be uploading to the bucket:
<CORSConfiguration>
  <CORSRule>
    <AllowedOrigin>*</AllowedOrigin>
    <AllowedMethod>GET</AllowedMethod>
    <AllowedMethod>HEAD</AllowedMethod>
    <AllowedHeader>*</AllowedHeader>
  </CORSRule>
</CORSConfiguration>
Using the `s3cmd` command, we apply the CORS XML file to our S3 bucket:
{{<command>}}
s3cmd setcors test-cors-file s3://testbin1
{{</command>}}
(your S3 address will be different to the one shown here)
We can now run the info command to confirm that the CORS configuration from our file has been set on the bucket:
{{<command>}}
s3cmd info s3://testbin1
(out)s3://testbin1/ (bucket):
(out)   Location:  objectstore4.jc.rl.ac.uk
(out)   Payer:     none
(out)   Expiration Rule: none
(out)   Policy:    {
(out)                ""Version"":""2008-10-17"",
(out)                ""Id"":""testbin1 Policy"",
(out)                ""Statement"": [
(out)                  {
(out)                    ""Sid"":""1: Full access for Users"",
(out)                    ""Effect"":""Allow"",
(out)                    ""Principal"":{""anonymous"":[""*""]},
(out)                    ""Action"":[""*""],
(out)                    ""Resource"":""*""
(out)                  },
(out)                  {
(out)                    ""Sid"":""2: Read-only access for Everyone"",
(out)                    ""Effect"":""Allow"",
(out)                    ""Principal"":{""anonymous"":[""*""]},
(out)                    ""Action"":[""GetObject"",""GetBucketCORS""],
(out)                    ""Resource"":""*""
(out)                  }
(out)                ]
(out)              }
(out)   CORS:      <CORSConfiguration>
(out)                <CORSRule>
(out)                  <AllowedOrigin>*</AllowedOrigin>
(out)                  <AllowedMethod>HEAD</AllowedMethod>
(out)                  <AllowedMethod>GET</AllowedMethod>
(out)                  <AllowedHeader>*</AllowedHeader>
(out)                </CORSRule>
(out)              </CORSConfiguration>
(out)   ACL:       ahuggan: FULL_CONTROL
{{</command>}}
To delete the CORS config from the bucket, we can run the following command:
{{<command>}}
s3cmd delcors s3://testbin1
(out)s3://testbin1/: CORS deleted
{{</command>}}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/configuring-cors-for-object-storage#applying-cors-settings-to-a-bucket,3731,341
New storage FAQs and issues,"{{<alert type=""info"">}}This article was originally written in 2018/19 to introduce new forms of storage which were brought into production at that stage. Some of the information and terminology is now out of date, pending further review of JASMIN documentation.{{</alert>}}
Workflows with some of the issues highlighted below will have a knock on
effect for other users, so please take the time to check and change your code
to make appropriate use of new storage system. If used correctly, the new
storage offers us a high performance scalable file system, with the capability
for object storage as tools and interfaces evolve, and we can continue to
serve the growing demand for storage in the most cost effective manner.
We understand these changes may cause you some extra work, but we hope that
you can understand why they were necessary and how to adapt to these changes.
We will continue to add to this page when new issues or solutions are found.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/faqs-storage,955,160
1\. Known cases where parallel write can occur (may be unknowingly to you!):,,https://help.jasmin.ac.uk/docs/short-term-project-storage/faqs-storage#1\.-known-cases-where-parallel-write-can-occur-(may-be-unknowingly-to-you!):,0,0
Use of MPI-IO or OpenMPI,"_Parallel threads can update the same file concurrently on same or from
different servers._
**Suggested solution:** use a `/work/scratch-pw*` volume which is PFS (but not
`/work/scratch-nopw*` !), then move output to SOF storage.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/faqs-storage#use-of-mpi-io-or-openmpi,230,33
Writing all the logs from a LOTUS job or job array to the same output or log file,"**Suggested solution:** see job submission advice 
[here]({{% ref ""how-to-submit-a-job-to-slurm"" %}}) showing how to use SBATCH options to use distinct output and log files for each job, or element of a job array.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/faqs-storage#writing-all-the-logs-from-a-lotus-job-or-job-array-to-the-same-output-or-log-file,214,32
Deleting a file via one host before another host has closed it,"_This is a form of parallel write truncation_
**Suggested solution:** take care to check for completion of 1 process before
another process deletes or modifies a file. Be sure to check a job has
completed before interactively deleting files from any server you are logged
into (eg. sci1.jasmin.ac.uk)
",https://help.jasmin.ac.uk/docs/short-term-project-storage/faqs-storage#deleting-a-file-via-one-host-before-another-host-has-closed-it,301,48
"Attempting to kill a process that was writing/modifying files, but not checking that it has been killed before starting a replacement process which attempts to do the same thing","_This can happen with rsync leading to duplicate copying processes._
**Suggested solution:** check for successful termination of 1 process before
starting another.
","https://help.jasmin.ac.uk/docs/short-term-project-storage/faqs-storage#attempting-to-kill-a-process-that-was-writing/modifying-files,-but-not-checking-that-it-has-been-killed-before-starting-a-replacement-process-which-attempts-to-do-the-same-thing",164,22
Opening the same file for editing in more than one editor on the same or different servers,"Here’s an example of how this shows up using “lsof” and by listing user
processes with “ps”. The same file “ISIMIPnc_to_SDGVMtxt.py” is being edited
in 2 separate “vim” editors. In this case, the system team was unable to kill
the processes on behalf of the user, so the only solution was to reboot sci1.
{{<command user=""user"" host=""sci-vm-01"">}}
lsof /gws/nopw/j04/gwsnnn/
(out)COMMAND   PID     USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
(out)vim     20943 fbloggs  cwd    DIR   0,43        0 2450 /gws/nopw/j04/gwsnnn/fbloggs/sdgvm/ISIMIP
(out)vim     20943 fbloggs    4u   REG   0,43    24576 2896 /gws/nopw/j04/gwsnnn/fbloggs/sdgvm/ISIMIP/.ISIMIPnc_to_SDGVMtxt.py.swp
(out)vim     31843 fbloggs  cwd    DIR   0,43        0 2450 /gws/nopw/j04/gwsnnn/fbloggs/sdgvm/ISIMIP
(out)vim     31843 fbloggs    3r   REG   0,43    12111 2890 /gws/nopw/j04/gwsnnn/fbloggs/sdgvm/ISIMIP/ISIMIPnc_to_SDGVMtxt.py
ps -ef | grep fbloggs
(out)......
(out)fbloggs 20943     1  0 Jan20 ?        00:00:00 vim ISIMIPnc_to_SDGVMtxt.py
(out)fbloggs 31843     1  0 Jan20 ?        00:00:00 vim ISIMIPnc_to_SDGVMtxt.py smc_1D-2D_1979-2012_Asia_NewDelhi.py
{{</command>}}
**Suggested solution:** If you are unable to kill the processes yourself,
contact the helpdesk with sufficient information to ask for it to be done for
you. In some cases, the only solution at present is for the host or hosts to
be rebooted.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/faqs-storage#opening-the-same-file-for-editing-in-more-than-one-editor-on-the-same-or-different-servers,1390,173
2\. Issues with small files,"The larger file systems in operation within JASMIN are suitable for storing
and manipulating large datasets and not currently optimised for handling small
( <64kBytes) files. These systems are not the same as those you would find on
a desktop computer or even large server, and often involve many disks to store
the data itself and metadata servers to store the file system metadata (such
as file size, modification dates, ownership etc). If you are compiling code
from source files, or running code from python virtual environments, these are
examples of activities which can involve accessing large numbers of small
files.
Later versions of our PFS systems handled this by using SSD storage for small
files, transparent to the user. SOF however, can’t do this (until later in
2019), so in Phase 4, we introduced larger home directories based on SSD, as
well as an additional and larger scratch area.
**Suggested solution:** Please consider using your home directory for small-
file storage, or `/work/scratch-nopw2` for situations involving LOTUS
intermediate job storage. It should be possible to share code, scripts and
other small files from your home directory by changing the file and directory
permissions yourself.
We are planning to address this further in Phase 5 by deploying additional SSD
storage which could be made available in small amounts to GWSs as an
additional type of storage. [Now available: please ask about adding an ""SMF"" volume to your workspace]
",https://help.jasmin.ac.uk/docs/short-term-project-storage/faqs-storage#2\.-issues-with-small-files,1475,238
Issues writing netCDF3 classic files to SOF storage type,"Writing netCDF3 classic files to SOF storage e.g. `/gws/nopw/j04/*` should be
avoided. This is due to the fact that operations involving a lot of
repositioning of the file pointer (as happens with netCDF3 writing) has
similar issues from writing large numbers of small files to SOF storage (known
as QB ).
**Suggested solution:** It is more efficient to write netCDF3 classic files to
another filesystem type (e.g. `/work/scratch/pw*` or `/work/scratch-nopw2`) and then move them to a SOF GWS, rather than writing directly to SOF.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/faqs-storage#issues-writing-netcdf3-classic-files-to-sof-storage-type,531,84
"3\. ""Everything's running slowly today""","This can be due to overloading of the scientific analysis servers
(`sci*.jasmin.ac.uk`) which we provide for interactive use. They’re great
for testing a code and developing a workflow, but are not designed for
actually doing the big processing. Please take this heavy-lifting or
long-running work to the LOTUS batch processing cluster, leaving the
interactive compute nodes responsive enough for everyone to use.
**Suggested solution:** When you log in via one of the `login*.jasmin.ac.uk`
nodes, you are shown a 'message of the day': a list of all the `sci*` machines,
along with memory usage and the number of users on each node at that time.
This can help you select a less-used machine (but don’t necessarily expect the
same machine to be the right choice next time!).
","https://help.jasmin.ac.uk/docs/short-term-project-storage/faqs-storage#3\.-""everything's-running-slowly-today""",774,126
Introduction,"This article explains about the process that runs in the background scanning all group workspaces to gather basic information about usage, which are fed into a database to be made available to users via the {{<link ""gws-scanner-ui"">}}GWS Scanner User Interface{{</link>}}.
It is intended for GWS Managers and provides details about how to customise the scan that is done on each GWS.
**It is run centrally, and is a very resource-intensive task, so please don't run similar tasks of your own, as you will be unnecessarily duplicating resource usage.**
There are two different scans of the Group Workspaces (GWSs).
- A daily scan which checks for how full the GWS is, and will email the GWS manager if it is over the default threshold of 83%, or a defined threshold in the GWS config file (see below).
- An approximately fortnightly check of the contents of all GWSs.
As a GWS Manager you will receive e-mails summarising the usage and contents
of the GWS. By default this is a simple volume level summary of the GWS.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/gws-scanner#introduction,1017,175
Customisation,"If you wish for additional directories to be scanned and summarised please add
these to the `{GWS_PATH}/.gws_scan/config.ini`, where {GWS_PATH} is the path to
your group workspace (the directory and the file may need to be created).
Directories can also be excluded from the scan in the same way. This can be
useful for speeding up the run time of a scan. Here is an example of a
config.ini file.
{{<alert type=""info"">}}
Please note that the required directory for this configuration data is `.gws_scan`, not `.gws_scanner` - the
latter is from the **previous** incarnation of the scanner, and will be be removed in due course)
{{</alert>}}
[general]
# GWS fullness threshold for which the daily scan will send a warning email (default 83) (in %)
volume_warning_threshold = 83
# Directories to check for largest sub-dir and filetypes below (comma separated list), these paths must be relative to the group workspace path i.e. path/to/dir, not /group/workspace/path/to/dir
  # Defaults to all top level directories inside volume
dirs = dir1,path/to/dir2
# Directories to exclude from the scan. This can be useful to speed up the scan if there are know directories with a large number of files, for which the scan information is not very useful
excl_dirs = path/to/excl, large_dir
# Filetype extensions to count inside directories above (comma separated list)
  # Defaults to none
exts = html,py,js
`dirs` and `excl_dirs` can have the `*` wildcard anywhere in the directory path.
From the config above for example: `path/to/*`, and `p*h/to/dir2` would pick up
the directory `/path/to/dir2`, and any other directories which matched the
pattern.  
Please don't comment out the arguments. If you don't want to use one just leave
it blank, this will then just use the internal defaults. E.g.
[general]
volume_warning_threshold =
dirs = 
excl_dirs =
exts =
",https://help.jasmin.ac.uk/docs/short-term-project-storage/gws-scanner#customisation,1850,295
Report,"When the fortnightly scan runs it sends the output to the GWS manager as an
email, which looks like this:
{{<alert color=""light"">}}
### Group Workspace Report for: {GWS_PATH}
#### Scan Details
Time most recent scan started: 16/04/2019 - 10:15:57  
Time most recent scan finished: 16/04/2019 - 12:25:27  
Duration of most recent scan (h:m:s): 2:09:29  
Scan Complete: True
#### Usage Details
Filesystem: fuse.quobyte  
Total Storage: 199.2 TiB  
Used Storage: 81.8 TiB  
Free Storage: 117.4 TiB  
Usage: 42%
#### Directory Details
To get information about specific directories within this volume, please add
the relative paths to the volume config file (.gws_scan/config.ini) - see
[https://help.jasmin.ac.uk/article/4499-gws-scanner]({{< ref ""gws-scanner"">}})
**Directory** |  **Total Files** |  **Total Size** |  **Sub-directory with most files** |  **Files in this Sub-directory**  
---|---|---|---|---  
all_dirs  |  238336  |  81.8 TiB  |  {GWS_PATH}/dir1/example/path  |  7929  
.gws_scanner  |  0  |  0.0 B  |  None  |  0  
dir1  |  0  |  0.0 B  |  {GWS_PATH}/dir1/example/path  |  0  
#### Filetype Details
To get information on the quantity and size of specific filetypes under the
directories above, please add the relevant file extensions to the volume
config file (.gws_scan/config.ini) - see
[https://help.jasmin.ac.uk/article/4499-gws-scanner]({{< ref ""gws-scanner""
>}})
_No filetypes requested_
{{</alert>}}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/gws-scanner#report,1422,192
What is HTTP data sharing?,"Specific parts of JASMIN Group Workspaces (GWSs) can be made available via
HTTP, so that:
- data can be shared with users who do not have JASMIN accounts
- common clients such as `wget`, `curl`, client libraries and web browsers can be used to access the data via a commonly-supported protocol.
In this respect, the service should be regarded as another [data transfer
tool]({{% ref ""data-transfer-tools"" %}}). However it must be arranged in
advance between the Group Workspace manager and the JASMIN Helpdesk. It
involves:
1. A member of the workspace creating a `public` directory and placing data inside it
2. The **GWS manager** making a request to the [JASMIN Helpdesk](mailto:support@jasmin.ac.uk) to request that this specific GWS is configured to be shared via HTTP.
Both these steps need to be completed in order for the GWS to be visible via
HTTP. By default, GWSs are not visible by HTTP.
Once a GWS has been made available, it is publicly visible by the entire
internet: please bear that in mind.
The following sections below describe how to set up restricted and
unrestricted access to your ""public"" directory. If you require access control
then see the section on password-protected access below. [Note: this is
currently possible, though not recommended, only because the current webserver
configuration permits this (now deprecated) means of restricting access.
Future revisions of the service may remove or change the way access
restrictions can be imposed].
{{<alert type=""danger"">}}
This facility is **not to be used for hosting project websites**. It is
provided as a simple means for specific data files to be made available to a
wider audience than members of a Group Workspace, using a convenient data
transfer protocol (HTTP). Likewise, it is not recommended to build tools or
front-end applications relying on this service as a dependency if they are to
be operated as a production or (near-)real-time service. The JASMIN team
reserves the right to apply rate-limiting (by project and/or IP address) or to
remove GWSs from the service if they are considered to be causing problems for
the network or other parts of the service infrastructure.
Projects considering web-based solutions for showcasing or disseminating their
data via more complex tools or with specific availability requirements should
consider requesting an external tenancy in the JASMIN Community Cloud, or
indeed external service providers if more appropriate, but should be prepared to
do the necessary design, development, operation and maintenance of those
services themselves.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/share-gws-data-via-http#what-is-http-data-sharing?,2587,407
Public access set up,"In some cases the GWS manager may want to make some files and directories
available over HTTP so that users (perhaps a wider audience than just the
membership of the GWS) can access the data via a web browser or other HTTP-
based tools. This can be done by creating a `public` directory in the top-
level directory of the GWS, for example:
{{<command user=""user"" host=""sci-vm-01"">}}
cd /group_workspace/jasmin/foobaa/
mkdir public 
chmod -Rf 755 public
{{</command>}}
You should then contact [JASMIN Support](mailto:support@jasmin.ac.uk) and ask
for this directory to be made visible via the `gws-access` server. The JASMIN
team will configure this change and your `public` directory will then be
visible from:
https://gws-access.jasmin.ac.uk/public/foobaa/
{{<alert type=""info"">}}
The URL of this service changed in June 2020. A redirect
is in place from the old URL of `https://gws-access.ceda.ac.uk/`, so the change
should be transparent to existing users, but **please use the new URLs**
beginning with `https://gws-access.jasmin.ac.uk/` to avoid any problems for
example with HTTP clients that are unable to handle redirects.
{{</alert>}}
Please see the section below if you wish to control who can access the content
of one or more of the sub-directories within your `public` directory.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/share-gws-data-via-http#public-access-set-up,1293,193
Restricted access set up,"In order to restrict access to your ""public"" directory, and/or any sub-
directories, you will need to create an "".htaccess"" file within it. In turn,
the "".htaccess"" file must point to a "".htpasswd"" file which lists the
usernames and encrypted passwords that have read-access to that directory.
{{<alert type=""danger"">}}
This method of access control is entirely independent of the SSH login
accounts used on JASMIN and would be the responsibility of the GWS Manager to
maintain. It is not secure by modern standards and not particularly
recommended as it adds complication for GWS managers and users, but is an
option for some basic access control if no other options are available.
**Future
revisions of the service may revise or remove this feature.**
{{</alert>}}
In order to create the "".htpasswd"" file, you will need access to the
""htpasswd"" command. This is available on the transfer servers
xfer[12].jasmin.ac.uk
You can then create the "".htpasswd"" file as follows (using the example of a
Group Workspace called ""foobaa""):
{{<alert type=""info"">}}
The `htpasswd` tool is going to be put on the new sci and xfer-vm servers shortly, 
but please use it on the old ones (e.g sci1) for now.
{{</alert>}}
{{<command user=""user"" host=""sci1"">}}
export GWS=/group_workspaces/jasmin/foobaa/
cd $GWS
mkdir -p public 
cd public
htpasswd -b -m -c $GWS/public/.htpasswd i_am_a_user i_am_a_password
{{</command>}}
Before this will work, you also need to create a "".htaccess"" file which you
could do as follows
{{<command user=""user"" host=""sci1"">}}
cat >.htaccess <<EOL
AuthType Basic
AuthName ""Password Required""
AuthUserFile /group_workspaces/jasmin/foobaa/public/.htpasswd
Require valid-user
EOL
{{</command>}}
Finally, change the permissions on these files:
{{<command user=""user"" host=""sci1"">}}
chmod 644 .htaccess .htpasswd
{{</command>}}
Now, you can test that you get prompted for the username and password by
visiting
https://gws-access.jasmin.ac.uk/public/foobaa/
",https://help.jasmin.ac.uk/docs/short-term-project-storage/share-gws-data-via-http#restricted-access-set-up,1964,281
Managing a GWS,"This article explains the responsibilities of the Group Workspace (GWS)
manager.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/managing-a-gws,81,11
Role of the GWS Manager,"When a GWS is created it is important that the designated GWS Manager
understands the responsibilities associated with the role. The GWS Manager has
a duty to:
- Ensure that GWS is being used appropriately: this may include enforcement of limits on particular users.
- Advertise the URL for requesting access to the GWS.
- Respond to e-mail authorisation requests from CEDA.
- Manage disk and tape effectively: specifically the use of the [Elastic Tape system]({{% ref ""secondary-copy-using-elastic-tape"" %}}) to back-up or migrate data.
- Communicate [GWS etiquette]({{% ref ""gws-etiquette"" %}}) to the project scientists.
- Manage additional services such as [sharing of GWS data via HTTP server]({{% ref ""share-gws-data-via-http"" %}}).
- Manage the closing down of the GWS effectively: all GWSs have a termination date and data may be lost if not managed effectively.
- Communicate any issues to the CEDA Helpdesk.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/managing-a-gws#role-of-the-gws-manager,918,143
Managing users,,https://help.jasmin.ac.uk/docs/short-term-project-storage/managing-a-gws#managing-users,0,0
Authorising access to the GWS,"When your GWS has been set up, users can submit requests for access to the GWS
via the JASMIN accounts portal: the new GWS will appear in the list of
{{< link ""https://accounts.jasmin.ac.uk/account/login/?next=/services/group_workspaces/"" >}}JASMIN Services{{</link>}}.
An access request from a user will trigger an e-mail to you that asks you to
approve (or refuse) the request. The e-mail will include details of the user
and their intended use of the resource. As GWS manager, **you** are now
responsible for approving these requests (this is a change from the previous
situation where you confirmed your approval to the CEDA helpdesk who then
actioned the approval). Now the approval is instant.
IMPORTANT: in order for this approval process to work, the GWS manager's own
account needs to have been migrated to the JASMIN accounts portal *if it existed prior to the JASMIN Accounts Portal in 2017*. Very few accounts such now remain in that state, but you can check by going to this page:
{{<button href=""https://accounts.jasmin.ac.uk/account/ceda_claim/confirm/"">}}Migrate your account{{</button>}}
If it says ""You are already logged in with a JASMIN account"" then you need take no further action.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/managing-a-gws#authorising-access-to-the-gws,1204,185
File system permissions and groups,"File system access to a GWS is managed using a Unix group that begins with
`gws_` and normally corresponds with the name of the service in the Accounts Portal. When users are granted the `USER` role for the workspace, they are also made members of the corresponding `gws_<gwsname>` group. As manager, you are normally granted `USER` and `MANAGER` access at the time that the GWS is set up by the JASMIN team, so it should be in place when you first access the storage. This means that:
- you have access yourself to the workspace (via the `USER` role giving you membership of the `gws_<gwsname>` group)
- you take over responsibility (via the `MANAGER` role) for approving other users' applications for the `USER` role.
- you can unilaterally grant `DEPUTY` access to other users if you know their username and want them to help you in the task of approving requests for `USER` access.
Once you yourself have access, a good first task is to set up the [recommended directory structure](introduction-to-group-workspaces/#recommended-directory-structure-for-a-gws).
A list of the user IDs that have access to a given GWS can be found by using
the ""getent group"" command and piping it through ""grep"" to select only your
GWS. For example:
{{<command user=""user"" host=""sci-vm-01"">}}
getent group | grep gws_cedaproc
(out)gws_cedaproc:*:26015:fbloggs,jdoe
{{</command>}}
You can lookup a specific user ID with the following:
{{<command user=""user"" host=""sci-vm-01"">}}
getent passwd | grep fbloggs
(out)fbloggs:*:29775:26030:Fred Bloggs:/home/users/fbloggs:/bin/bash
{{</command>}}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/managing-a-gws#file-system-permissions-and-groups,1575,236
Maintaining group permissions throughout the GWS,"In order to maintain the group permissions throughout the GWS the highest
level directory has the ""sticky bit"" set. This means that the default group
for all files and directories created within the GWS will be the relevant
`gws_*` access group. This is particularly useful to enable data within the
GWS to be shared amongst collaborators. If users have a specific need to
modify the group permissions they can do so using ""chgrp"" command.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/managing-a-gws#maintaining-group-permissions-throughout-the-gws,440,74
Making directory contents writable by other members of the GWS,"If a user wishes to make their files/directories writable by others in the GWS
they can follow the procedure here using the ""umask"" command:
Make a directory (and set it the permission so that the group can read/write
to it):
{{<command user=""user"" host=""sci-vm-01"">}}
mkdir --mode=u+rwx,g+rws,o-rwx testdir
ls -l testdir
(out)drwxrws--- 2 jdoe gws_cedaproc 4096 Jan 26 14:36 testdir
{{</command>}}
Or (as separate steps):
{{<command user=""user"" host=""sci-vm-01"">}}
mkdir testdir
chgrp g+rws testdir
chgrp o-rwx testdir
ls -l testdir
(out)drwxrws--- 2 jdoe gws_cedaproc 4096 Jan 26 14:36 testdir
{{</command>}}
However, please also see {{<link ""#security"" />}}, below (and make sure your users are aware of this).
Check your umask:
{{<command user=""user"" host=""sci-vm-01"">}}
umask
(out)0022
{{</command>}}
Modify your ""umask"" so that any new file or directory that you create will be
writable anyone with the group permission:
{{<command user=""user"" host=""sci-vm-01"">}}
umask 002
touch testdir/newfile
ls -l testdir
(out)-rw-rw-r-- 1 jdoe gws_cedaproc 0 Jan 26 14:39 newfile
{{</command>}}
If you want the `umask` setting to persist, you should set it for yourself in your `~/.bashrc` file, and advise your users to do the same.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/managing-a-gws#making-directory-contents-writable-by-other-members-of-the-gws,1229,180
"Quota, resource allocation and GWS lifetime","The overall usage of a GWS can be determined with the `df` (SOF) or `pan_df`
(PFS) command:
{{<command user=""user"" host=""sci-vm-01"">}}
pan_df -H /gws/pw/j07/workshop/
(out)Filesystem             				Size   Used  Avail Use% Mounted on
(out)panfs://panmanager03.jc.rl.ac.uk/gws/pw/j07     2.6T    16G   2.6T   1% /gws/pw/j07/workshop/
df -H  /gws/nopw/j04/ncas_generic
(out)Filesystem                                      Size   Used  Avail Use% Mounted on
(out)quobyte@sds.jc.rl.ac.uk/gws_ncas_generic        83T    80T   3.4T  96% /gws/nopw/j04/ncas_generic
{{</command>}}
For PFS (paths beginning `/gws/pw/j07/*`), the raw capacity of the GWS is 2.6TB (measured in TB,
defined using powers of 10), but to obtain space available to users this
should be **divided by roughly 1.3**, resulting in around 2TB of free space. Of
this, 16GB is currently in use. The factor of 1.3 can depend on the number of
small files stored in the GWS because lots of small files take up more space
than expected. 
For SOF (paths beginning `/gws/nopw/j04/*`), the value reported by `df -H` is the usable size.
A summary of specific sections of a GWS can be determined using `pan_du -sh \<di\r>` (PFS), and `du -sh --si --apparent-size \<dir\>` (SOF). Set the
`--apparent-size` flag to get an accurate size.
Similarly, you can use the `find` command together with `-atime` or `-mtime` to locate
files accessed or modified more than a certain length of time ago. For
example, to find files which were accessed more than 1 year ago:
find /gws/nopw/j04/upscale/cache -type f -atime +365
**However**, this can
- consume significant system resources in running the `du` command, for a long time, and
- fail due to permission issues (as a regular user, you can't always ""see"" down all the directory branches)
...So we provide some tools to help with this:
1. The {{<link ""gws-scanner"">}}GWS Scanner{{</link>}} runs this for you, centrally, on a roughly 2-week cycle, and stores all the output in a database from which results can be visualised in the {{<link ""gws-scanner-ui"">}}GWS Scanner User Interface{{</link>}}.
1. There is also a live view of GWSs and the available space left on the {{< link ""https://mon.jasmin.ac.uk"" >}}JASMIN Dashboard{{</link>}}. The “JASMIN Storage” tab shows many
JASMIN storage volumes with information about current usage.
**So please don't run large `du` or `find` jobs yourself, as this will be duplicating something already running in the background.**
As a GWS Manager
you will receive e-mails summarising the usage and contents of the GWS. If you
wish for additional directories to be scanned and summarised please add these
to the [GWS scanner configuration]({{% ref ""gws-scanner"" %}}).
The typical lifetime of a GWS is 3 years. All GWS managers are expected to
actively manage the space during its lifetime and plan for the eventual
reclamation of the space by deleting and migrating data to other locations.
Typically, less-frequently-used data might be written to Elastic Tape (see below) and some final
outputs would be curated in the CEDA Archive. In the latter case please note
that you should discuss the requirements with the CEDA Archive team via the
CEDA Helpdesk, ideally before the project starts.
","https://help.jasmin.ac.uk/docs/short-term-project-storage/managing-a-gws#quota,-resource-allocation-and-gws-lifetime",3220,484
Migrating data to tape,"Proactive data management is an important part of providing an effective GWS.
We recommend that the GWS Manager discusses use of the space with the project
team to ensure that the use of disk and tape are being optimised. This may
involve use of the [Elastic Tape system]({{% ref ""secondary-copy-using-elastic-tape"" %}})
for backup or data migration (from disk to tape).
",https://help.jasmin.ac.uk/docs/short-term-project-storage/managing-a-gws#migrating-data-to-tape,371,60
Requesting a change to the GWS size,"Although it is helpful to provide the best estimate of required allocation at
the time of initially requesting the GWS, a GWS Manager may request a change
in size (increase or decrease) of the GWS during its lifetime. We would
positively encourage you to be honest about your requirements so that others
can make use of this expensive resource if you are not using it until later in
your project, or if you no longer require all the space you originally
requested.
Requests for an increase in GWS size will be considered by the Consortium
Manager with responsibility for managing an overall allocation to that
particular scientific community. See 
[Requesting Resources]({{% ref ""requesting-resources"" %}}). Depending on available resources and competing
demand, it may not always be possible to increase the allocation, and you may
be asked to move data to Elastic Tape to free up disk space.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/managing-a-gws#requesting-a-change-to-the-gws-size,894,147
Security,"User account security is very important in a multi-user environment such as
JASMIN. As a GWS Manager you have a responsibility to users of your GWS but
also to all other GWS users in helping to maintain a safe and secure system in
which productive scientific work can be done. There is a strict policy of one-
user-one-key, and on no account must any user make use of the SSH key of
another user to gain access to any part of the JASMIN infrastructure. Private
keys MUST be protected by a strong passphrase. Please encourage adherence to
these rules by users of your GWS. Any infringements may be dealt with swiftly
by removal of user access. No offensive, obscene or otherwise unauthorised
data may be stored in the GWS or anywhere else within JASMIN. Users should not
store any data of a personal or sensitive nature in the GWS.
{{< alert type=""danger"" >}}
Do not set, or allow your users to set, open permissions on files or directories.
By this we mean permissions where data are ""world-writable"" by anyone, for example
`-rw-rw-rw-` for a file, or **<< DON'T USE THESE!!**<br>
`drwxrwxrwx` for a directory. **<< OR THESE!!**
We provide a UNIX a group corresponding to each group workspace, which all members of that GWS belong to: this enables sharing within the group if you set permissions appropriately using that group. If you are unsure about setting permissions, please ask the helpdesk.
{{< /alert >}}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/managing-a-gws#security,1413,244
Keeping informed,"Please maintain contact throughout the life of the GWS via the following
channels:
- Using the {{< link ""https://mon.jasmin.ac.uk"" >}}JASMIN Dashboard{{</link>}} to check on the status of your GWS (used versus available space).
- Email alerts from the system when the GWS reaches >83% full
- Email from the CEDA/JASMIN team
- News articles on the CEDA or JASMIN websites and by monitoring CEDA social media feeds which may be used to post messages regarding system status or security. 
  - {{< link ""ceda_site"" >}}CEDA Website{{</link>}}
  - {{< link ""https://twitter.com/cedanews"" >}}CEDA News on Twitter{{</link>}}
If you are aware that a user who has access to your GWS leaves your project
or, for whatever reason, no longer needs to be a member of the GWS, please let
the [JASMIN Helpdesk](mailto:support@jasmin.ac.uk) know either by email or via
the beacon, lower right on this page. Arrangements may need to be made to
transfer the ownership of files and/or directories to another member of the
GWS to ensure continued access to the data.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/managing-a-gws#keeping-informed,1045,168
What is the XFC?,"The Transfer Cache (XFC) provides a large area of temporary storage for
users of JASMIN to store large files and/or a large volume of files on a
short-term basis.
Users are granted a quota of space in their user area on the temporary
storage. When users exceed their quota some of their files will be deleted
automatically.
Users interact with the XFC in two ways:
  1. to initialise their user area, and to get information about their quota, a [command-line client]({{% ref ""install-xfc-client"" %}}) is used.
  2. to move data in and out of their user area, the standard UNIX command-line tools (cp, mkdir, rm, mv, rsync, etc.) are used.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/xfc#what-is-the-xfc?,639,110
Quotas,"XFC has two different types of quota. The first is the ""hard quota"" (HQ). This
is the absolute maximum volume of data that can be stored in the user area.
This is expressed in TB (terabytes).
The second type of quota is the ""temporal quota"" (TQ). This is expressed in
units of TB day (terabyte days), and has a time component as well as a data
volume component. For an individual file, the TQ for that file is the product
of the size of the file and the number of days the file has been in the user
area. As an example, if the user moves a 2TB file into their area, after 24
hours it will have used 2TB days of the TQ. After 48 hours it will have used
4TB days and after 1 week it will have used 14TB days.
Finally, any file in the XFC can have a maximum persistence of 365 days. i.e.
if a file is in XFC for more than one year then it will be deleted by the
automatic deletion.
{{<image src=""img/docs/xfc/file-7mwfbT5NdE.png"" caption="""">}}
The above figure shows an example of the quota system in use. The red line
shows the temporal quota used (TQ) and the blue line shows the hard quota
(HQ).
- The user initialises their XFC.
- On day **5** , the user copies a 1 TB file into their XFC
- For the next 4 weeks (on days **12** , **19** , **26** and **31** ) the user copies in another 1 TB file
- The TQ steadily grows until on day **79** it has reached its limit of 300TB days, the first 1TB file is deleted
- On day **98** another 1TB file is deleted
- On day **120** , the user copies 10TB into their XFC
- On days **122** , **130** and **140** the 1TB files that were copied in on days **19** , **26** and **31** are deleted.
- On day **151** , the 10TB file is deleted.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/xfc#quotas,1677,334
Default Quota values:,"The default Hard Quota (HQ) is 10 TB
The default Time Quota (TQ) is 300 TB
So you could store 10TB of data for 30 days before risking the deletion of
data.
Additionally, an overall time limit of 365 days is set for ALL data stored by
a given user. You cannot store any data, no matter how small, for longer than
365 days.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/xfc#default-quota-values:,322,64
Automatic deletion,"If users exceed either their temporal quota or hard quota, then files in their
user area will be deleted automatically. The deletion process will delete as
many files as necessary to bring the amount of HQ and TQ below the quotas
allocated to the user. Files will be deleted on an age basis. Those files that
were copied into the user area first will be deleted first, with newer files
deleted after these.
The user can be notified which files will be deleted if they switch the option
to be notified on and supply an email address in the XFC client. Files will be
deleted 24 hours after the notification.
If a file is modified between the notification and the scheduled deletion (24
hours later) then the file will not be deleted. However, the automatic
deletion is relentless and it will choose some other file to delete instead.
XFC is not designed as permanent storage and the automatic deletion process
has been designed to discourage long term storage of files on it.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/xfc#automatic-deletion,974,172
Using XFC,"JASMIN provides access to XFC via a command-line client: `xfc`
Once [installed]({{% ref ""install-xfc-client"" %}}) into your `$HOME`
directory (using one of the `sci` servers), the `xfc` client can be run on
either the `sci` (`sci*.jasmin.ac.uk`) or `xfer` (`xfer*.jasmin.ac.uk`)
servers, but should NOT be run on the high-performance transfer servers
`hpxfer*.jasmin.ac.uk`.
The client is used only for interacting with the service, but is not needed
for accessing the storage it provides. The storage provided is mounted in most
places across JASMIN: the path to your XFC volume is returned by the client in
one of the steps shown below.
Users are expected to use the [xfer servers]({{% ref ""transfer-servers"" %}})
or a high-performance data transfer service to do any data transfers either
within or in/out of JASMIN. This reduces the load on the `sci` servers which
are for general-purpose interactive computing.
The `xfc` client is used to initialise and then query the status (quota,
scheduled deletions etc) of a user's XFC storage volume:
  1. To see all the available options: `xfc -h`
  2. To initialise your user area: `xfc init`
{{<command user=""user"" host=""sci-vm-01"">}}
xfc init
(out)** SUCCESS ** - user initiliazed with:
(out)username: username  
(out)email: user.name@stfc.ac.uk  
(out)quota: 300TB  
(out)path: /work/xfc/vol1/user_cache/username
{{</command>}}
The `path` is the path on the JASMIN system to the user area. Data can be
copied here using standard UNIX command-line tools cp, mv, rsync.
Subdirectories can be created using mkdir. Change read/write permissions on
the directories and files using `chmod`, etc. The user area is just a standard
POSIX directory and so any POSIX commands can be used on it.
  3. To get the user area path again: 
{{<command user=""user"" host=""sci-vm-01"">}}
xfc path
(out)/work/xfc/vol1/user_cache/username
{{</command>}}
  4. To set the user email for notifications: 
{{<command user=""user"" host=""sci-vm-01"">}}
xfc email --email=user.name@stfc.ac.uk
(out)** SUCCESS ** - user email updated to: user.name@stfc.ac.uk
{{</command>}}
  5. To query the email set for the user: `xfc email`
{{<command user=""user"" host=""sci-vm-01"">}}
xfc email
(out)user.name@stfc.ac.uk
{{</command>}}
  6. To switch deletion notifications on / off:
{{<command user=""user"" host=""sci-vm-01"">}}
xfc notify
(out)** SUCCESS ** - user notifcations updated to: on
{{</command>}}
  7. To see remaining quota:
{{<command user=""user"" host=""sci-vm-01"">}}
xfc quota
(out)------------------------
(out)Quota for user: username 
(out)------------------------
(out)Temporal Quota (TQ)
(out)  Used : 1.7 TB  
(out)  Allocated : 300.0 TB
(out)  Remaining : 298.3 TB
(out)------------------------  
(out)Hard Quota (HQ)
(out)  Used      : 444.9 GB  
(out)  Allocated : 40.0 TB  
(out)  Remaining : 39.6 TB
{{</command>}}
  8. To see which files are scheduled for deletion:
{{<command user=""user"" host=""sci-vm-01"">}}
xfc schedule
(out)No files scheduled for deletion
{{</command>}}
  9. To list the files in your user area:
{{<command user=""user"" host=""sci-vm-01"">}}
xfc list
(out)user_cache/username/historical/.ftpaccess 
(out)user_cache/username/historical/00README_catalogue_and_licence.txt 
(out)user_cache/username/historical/day/atmos/day/r1i1p1/COPY_CURRENT_20150326.txt
{{</command>}}
Pattern matching can be used to search for a file. This is just a simple
substring search, e.g. `r1i1p1_19500101-19541231.nc`
{{<command user=""user"" host=""sci-vm-01"">}}
xfc list -m r1i1p1_19500101-19541231.nc
(out)user_cache/username/historical/day/atmos/day/r1i1p1/v20120907/va/va_day_CMCC-CESM_historical_r1i1p1_19500101-19541231.nc
(out)user_cache/username/historical/day/atmos/day/r1i1p1/v20120907/rsds/rsds_day_CMCC-CESM_historical_r1i1p1_19500101-19541231.nc
(out)user_cache/username/historical/day/atmos/day/r1i1p1/v20120907/prc/prc_day_CMCC-CESM_historical_r1i1p1_19500101-19541231.nc
{{</command>}}
File names are given relative to the `user_cache/` directory. To list the
full file path use the `-f` list option:
{{<command user=""user"" host=""sci-vm-01"">}}
xfc list -f
{{</command>}}
  10. To predict when the files will be deleted, if no other files are added to the user area, and none of the current files are removed: `xfc predict`
{{<command user=""user"" host=""sci-vm-01"">}}
xfc predict
(out)Quota is predicted to be exceeded on 21 Aug 2019 14:58 by 252.1 GB
(out)Files predicted to be deleted  
(out)user_cache/username/historical/.ftpaccess
(out)user_cache/username/historical/00README_catalogue_and_licence.txt
(out)user_cache/username/historical/day/atmos/day/r1i1p1/COPY_CURRENT_20150326.txt
{{</command>}}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/xfc#using-xfc,4636,549
Example of initial use,"Below is a list of commands the user might use in their initial session with
XFC.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/xfc#example-of-initial-use,82,16
initial setup,"{{<command user=""user"" host=""sci-vm-01"">}}
xfc init
xfc path
xfc email --email=user.name@email.com
xfc notify
{{</command>}}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/xfc#initial-setup,125,13
query the quota,"{{<command user=""user"" host=""sci-vm-01"">}}
xfc quota
xfc predict
xfc schedule
{{</command>}}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/xfc#query-the-quota,93,10
Elastic Tape command-line interface hints,"{{<alert type=""info"">}}
- Information below relates to the Elastic Tape command-line tools. The [JDMA]({{<ref ""jdma"">}}) system provides a better interface for putting/retrieving data into the Elastic Tape System)
- A new system called [NLDS](https://techblog.ceda.ac.uk/2022/03/09/near-line-data-store-intro.html) is coming very shortly (as of Feb 2023) and will eventually replace both of these.
{{</alert>}}
This article explains the return codes of certain Elastic Tape (ET) commands.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/elastic-tape-command-line-interface-hints,489,62
et_put.py,"Command-line tool to register large numbers of files for upload to Elastic
Tape
",https://help.jasmin.ac.uk/docs/short-term-project-storage/elastic-tape-command-line-interface-hints#et_put.py,80,13
Return codes,"rc 0: normal exit    
rc 1: error in start up parameters    
rc 2: error processing list
{{<alert type=""danger"">}}
When writing data to the ET system, it is very important that data remains in place on disk, in the location where ET expects to find them, until the status of the batch in question has reached `CACHED_SYNCED` or `SYNCED`.  This means that the data have actually been written to tape, but is not the case until that status is shown.
The location where ET expects to find the files will be specified in the `LISTFILE` that the user supplied to the `et_put.py` command, or all files and directories under the `DIR`.  The status of user's batches can be checked by going to the webpage: http://et-monitor.fds.rl.ac.uk/et_user/ET_AlertWatch.php.
You need to be logged into JASMIN to see this webpage, via the
[nx login servers]({{% ref ""graphical-linux-desktop-access-using-nx"" %}}), and use Firefox as the web browser.
Deleting the data from disk prematurely can cause problems for the ET system as a whole (impacting other users) so please be careful with this aspect.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/elastic-tape-command-line-interface-hints#return-codes,1095,177
et_get.py,"Command-line tool to download large numbers of files from Elastic Tape
",https://help.jasmin.ac.uk/docs/short-term-project-storage/elastic-tape-command-line-interface-hints#et_get.py,71,11
Return codes,"0: normal exit
1: error in start up parameters
2: cannot write to target directory
3: error received during download
4: closed by interrupt (^c probably)
5: completed with ""bad files"" (see list via stderr)
",https://help.jasmin.ac.uk/docs/short-term-project-storage/elastic-tape-command-line-interface-hints#return-codes,206,35
et_rm.py,"Command-line tool to delete files from Elastic Tape
",https://help.jasmin.ac.uk/docs/short-term-project-storage/elastic-tape-command-line-interface-hints#et_rm.py,52,8
Return codes,"rc 0: normal exit    
rc 1: error in start up parameters    
rc 2: error processing list
",https://help.jasmin.ac.uk/docs/short-term-project-storage/elastic-tape-command-line-interface-hints#return-codes,89,16
et_ls.py,"Command-line tool for listing holdings in Elastic Tape
",https://help.jasmin.ac.uk/docs/short-term-project-storage/elastic-tape-command-line-interface-hints#et_ls.py,55,8
et_ls.py and grep,"The first character on any data line is the line's type. This allows a script
to know the type of line it is trying to parse
",https://help.jasmin.ac.uk/docs/short-term-project-storage/elastic-tape-command-line-interface-hints#et_ls.py-and-grep,125,26
Return codes,"0: normal exit    
1: error in start up parameters or reading config file    
2: requestor is not authorised for the intended workspace    
4: closed by interrupt (^c probably) or as a result of a head/tail command
",https://help.jasmin.ac.uk/docs/short-term-project-storage/elastic-tape-command-line-interface-hints#return-codes,215,36
et_transfer_mp,,https://help.jasmin.ac.uk/docs/short-term-project-storage/elastic-tape-command-line-interface-hints#et_transfer_mp,0,0
Return codes,"0: OK    
1: config error    
2: log directory error   
3: already running    
4: error creating client
",https://help.jasmin.ac.uk/docs/short-term-project-storage/elastic-tape-command-line-interface-hints#return-codes,104,16
XFC client install,"In order to initiate and manage your usage of the XFC (transfer cache)
service, you need to use the XFC client.
Once set up, you don't need to use this client to read/write data to XFC
storage itself, but you can use it to interrogate the system to find out what
data you have stored and how much of your quota(s) you are using.
The following steps should be used to create a python virtual environment and
pip to install the xfc client:
NOTE: do these steps on one of the `sci` (not `xfer`) servers:
  1. Log into one of the sci machines: e.g. `sci-vm-01.jasmin.ac.uk` (1 - 8 is available)
  2. The `xfc` client can be used with Python 3 using jaspy.  
{{<command user=""user"" host=""sci-vm-01"">}}
module load jaspy
{{</command>}}
Setup a virtual environment in your home directory:
{{<command user=""user"" host=""sci-vm-01"">}}
python -m venv ~/xfc_venv
{{</command>}}
  3. The following steps for Python 2 and Python 3 are now the same.
  4. Activate the virtual environment:
{{<command user=""user"" host=""sci-vm-01"">}}
source ~/xfc_venv/bin/activate
{{</command>}}
  5. Download the client software using git to your home directory:
{{<command user=""user"" host=""sci-vm-01"">}}
git clone https://github.com/cedadev/xfc_client.git
{{</command>}}
  6. pip install the xfc client
{{<command user=""user"" host=""sci-vm-01"">}}
pip install -e ~/xfc_client
{{</command>}}
  7. Use xfc on the command line
{{<command user=""user"" host=""sci-vm-01"">}}
xfc -h
{{</command>}}
Users who already have a Python virtual environment can skip step 2 and
install into an existing virtual environment.
The recommendation for users using NLA, XFC and JDMA is to create a single
virtual environment to install all 3 client applications into.
The `xfc` client can be used without activating the python virtualenv by
adding the path to the xfc client to the `$PATH$` environment variable:
{{<command user=""user"" host=""sci-vm-01"">}}
export PATH=""$PATH:~/xfc_venv/bin"" `
echo 'export PATH=""$PATH:~/xfc_venv/bin""' >> ""$HOME/.bashrc""
{{</command>}}
The `xfc` client can now be used by invoking `xfc` from the command line
without activating the virtualenv. **Python 3** users   load jaspy
using the command `module load jaspy`
For instructions on how to use the xfc client, see the article [JASMIN
Transfer Cache (XFC).]({{% ref ""xfc"" %}})
",https://help.jasmin.ac.uk/docs/short-term-project-storage/install-xfc-client#xfc-client-install,2305,341
Secondary copy using Elastic Tape,"{{<alert type=""info"">}}
- Information below relates to the Elastic Tape command-line tools. The [JDMA]({{% ref ""jdma"" %}}) system provides a better interface for putting/retrieving data into the Elastic Tape System)
- A new system called [NLDS](https://techblog.ceda.ac.uk/2022/03/09/near-line-data-store-intro.html) is coming very shortly (as of Feb 2023) and will eventually replace both of these.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape,413,52
Introduction,"Elastic Tape is a system developed for use with JASMIN Group Workspaces
(GWSs), enabling the Group Workspace Manager to:
- Optimise their use of high-performance online disk by moving data to and from cheaper near-line storage
- Create and manage secondary copies of GWS data
At present, the system is designed only to be used by GWS Managers, rather
than individual members of a GWS. It is the responsibility of a GWS Manager to
create and manage backups or additional copies of data in a GWS.  
The servers used to access Elastic Tape changed in January 2021.
Previous users should note that the server to use now is `et.jasmin.ac.uk`.
**The maximum size for any file put into Elastic Tape is 500GB.  This changed in 2023, when the underlying tape system was upgraded.  Please limit your files to less than 500GB.**
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#introduction,818,140
Who can use ET?,"ET is only for use by the named GWS manager, i.e. the individual responsible for managing the GWS disk space. The high-performance disk space used for a GWS is a valuable commodity and the role of the GWS Manager involves making best use of the online space. This may mean moving data to tape to free up space online, or taking a copy of online data to make a secondary copy. **No undertaking is provided that the secondary copy will exist beyond the lifetime of the Group Workspace itself, hence it is called a secondary copy and not a backup. It is also NOT long-term archive storage:** some data in GWSs may need to be earmarked for longer-term archive storage and wider availability via the CEDA Archive, but this is a **separate process** for which data management plans, ingest processes and metadata need to be put in place. Please contact the helpdesk if this is the case, but ideally this needs to be considered at project design phase (as it may need funding!).
Each GWS has a quota of online disk space (agreed at the time of its creation) and initially the ET quota has been set to the same value. So if you have a 10 Tb workspace, you initially have a 10Tb quota of ET storage to match.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#who-can-use-et?,1200,220
How does it work?,"Putting data into ET storage involves creating a ""batch"" of data which is
transferred to the ET system. Using either a file list or top-level directory
for reference, the system calculates resources needed and creates a batch,
identified by a batch ID. This must be retained by the GWS manager as a
""ticket"" for later retrieval of this batch of data. It is recommended that you
assess the data that you plan to transfer so that you have an idea of the
overall volume to be transferred before initiating any actual transfer jobs.
It is also recommended to test operation with a small set of test data.
Transparent to the user, and asynchronously (so it is not necessary to wait
with a terminal window open), the data are transferred first to online cache
and then to tape storage. It is not an instant process and the task of
migrating data from online cache to tape can take several hours, even days,
depending on factors such as the size of the transfer, contention for the tape
system and network conditions. An RSS feed and a web page provide updates on
the process of data transfer for each batch. Data can later be retrieved, or
removed from ET storage via similar tools.
The transfer of data via a batch involves the ""registration"" of each file in a
database so that its existence is recorded.
Command line tools are provided on a dedicated machine within the JASMIN
infrastructure, to which GWS managers will be given access. A GWS manager has
access to the python tools `et_put.py`, `et_get.py`, `et_rm.py` and
`et_ls.py`. Some initial documentation for these command line tools is
attached.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#how-does-it-work?,1600,277
What should I do next?,"It is recommended to try sending **and retrieving** some small data transfers
(a few Gb) at first using the documentation below, but the system has been
designed to cope with storing entire GWSs. You will need ssh login access to
`et.jasmin.ac.uk` first. This should have been arranged for you as part of the 
GWS setup process. If not, please contact the JASMIN helpdesk. Once
there, you should be able to see your group workspace and try out the commands
on a small set of test data.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#what-should-i-do-next?,486,86
System overview,"Elastic Tape provides the ability to create ""batches"" of files which are then
sent to the storage system, initially to an online disk cache before being
written to near-line tape. Batches can later be retrieved, or removed. An
alert system provides the user with the ability to monitor the progress of
transfer jobs.
The system comprises:
- A command-line interface on a client machine
- A backend system, consisting of 
  - I/O servers connected to an online disk cache and database
  - A near-line tape system
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#system-overview,512,86
Configuration file,"As a GWS manager, you will normally be responsible for one or more GWSs. The
GWS with which you wish to work using ET needs to be specified either in a
configuration file in your home directory, or by specifying the workspace as
an option in the command line interface.
Certain default settings are set in a system-wide config file at
`/etc/et_config`.
If needed, you need to create a small text file in your home directory named
`.et_config`, which contains the following, replacing ""myworkspace"" with the
name of your default workspace:
[Batch]
Workspace = myworkspace
`myworkspace` should just be the short name of the workspace, not the full
path to it.
The workspace specified in any command-line option overrides that specified in
the user's (`~/.et_config`) config file, which in turn overrides that
specified in the system (`/etc/et_config`) config file.
Please **REMOVE** any previous reference to host and port from your individual
`~/.et_config file`. This setting is now set from the system /etc/et_config
file.
Further configuration options are available in the `[DIRECTORY]` section of
the file, see the system-wide file /etc/et_config for examples. The main
parameter for which you may wish to override the default is:
outputLevel = workspace|batch|file
although these can be over-ridden at the command line anyway. See `et_ls.py`
command documentation below for the meaning of these options.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#configuration-file,1408,219
User interface,"Please note that NOT ALL features of the currently-implemented user interface
are described here, however we would recommend that users limit their usage to
those features described below.
The user interface consists of the following components:
- **et_put.py** Put data onto tape
- **et_get.py** Retrieve data from tape
- **et_rm.py** Remove data from tape
- **et_ls.py** List data holdings on tape
- **Alerts** Get information about processes and holdings via web interface
The commands are available on host `et.jasmin.ac.uk`. As a GWS manager you
should have been granted login access to this machine using your JASMIN
account, however if accessing the host from outside the RAL network you will
need to use one of the login gateways `login*.jasmin.ac.uk`. Use the -A option or
equivalent for agent forwarding in ssh. STFC users may use the STFC VPN to
connect to `et.jasmin.ac.uk` as if it were a local connection.
{{<alert type=""danger"">}}
When writing data to the ET system, it is very important that data remains in place on disk, in the location where ET expects to find them, until the status of the batch in question has reached `CACHED_SYNCED` or `SYNCED`.  This means that the data have actually been written to tape, but is not the case until that status is shown.
The location where ET expects to find the files will be specified in the `LISTFILE` that the user supplied to the `et_put.py` command, or all files and directories under the `DIR`.  The status of user's batches can be checked by going to the webpage: http://et-monitor.fds.rl.ac.uk/et_user/ET_AlertWatch.php. You need to be logged into JASMIN to see this webpage, via the [nx-login servers]({{% ref ""graphical-linux-desktop-access-using-nx"" %}}), and use Firefox as the web browser.
Deleting the data from disk prematurely can cause problems for the ET system as a whole (impacting other users) so please be careful with this aspect.
{{</alert>}}
* * *
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#user-interface,1932,310
et_put.py,"Put data onto tape.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#et_put.py,20,4
Synopsis,"et_put.py [-v] [-l LOGFILE] [-w WORKSPACE] [-c] [-t one-word-tag] [ -f LISTFILE | DIR ]
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#synopsis,88,15
Description,"Data files to be stored can be specified either in an input list file (-f) or
by specifying the path to the top of a directory tree containing files to be
stored. All symbolic links are ignored (see note below). In both cases, the
system will analyse the request and create a **batch** , identified by a
`BATCH ID`, which can later be used to retrieve that set of files from
storage. Although the main ""put"" operation is asynchronous (and does not
require you to maintain a terminal connection for its duration), the initial
registration step, which creates the BATCH ID is synchronous, so you should
wait for this step to complete before disconnecting.
Given current resources, all users of Elastic Tape share the current
throughput capacity of about 25 TB/day, which may increase over time. Please
consider this when organising your input batches and expectations of
completion time. Large numbers of small files will degrade performance.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#description,941,157
Options,"option | details
---|---  
-v  |  Verbose output
-l LOGFILE  |  Log file in which to record process output
-f LISTFILE  |  Text file containing ABSOLUTE paths of files to be stored, 1 file per line. NB Files and directory names are case-sensitive. The list should not contain any blank lines or extraneous white space.
-w WORKSPACE  |  Name of the group workspace to use. Overrides default set in config file. Case sensitive.
DIR  |  ABSOLUTE path to top of directory tree containing files to be stored  
-c  |  Continue if errors encountered.
-t tag  |  Tag batch with descriptive label meaningful to user. Should be single one-word string. Appears as ""Batch name"" in ET alert output and ""Tag"" in et_ls output.
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#options,736,124
Example usage,"Simple case, using a file input.list which contains paths of all the files to
be included in the batch:
{{<command user=""user"" host=""et"">}}
et_put.py -v -l et_put.log -f input.list -w myworkspace
{{</command>}}
In the following example, the `-c`option is used to continue on errors. One
error that may be encountered is that a file already exists in the system
(e.g. has already been ""put""). This option causes the system to ignore any
errors and continue with the transfer. Note that this should not be used by
default (we would rather know about errors and fix them!)
{{<command user=""user"" host=""et"">}}
et_put.py -v -l et_put.log -f input.list -w myworkspace -c
{{</command>}}
Alternative usage specifying a directory beneath which all files / directories
will be included. In this case the directory must be the last parameter in the
command:
{{<command user=""user"" host=""et"">}}
et_put.py -v -l et_put.log -w myworkspace /group_workspaces/jasmin/myworkspace/mydir
{{</command>}}
**Symbolic links:** Attempting to include symbolic links in an et_put
operation should cause an error. You can override this with the `-c` option
(although this will ignore ALL errors), but a better solution is to generate a
list file as in the first two examples above. If this list file is generated
with a command like `find <path> -type f > listfile.txt`, then referring to it
in the et_put command will ensure that only those files are included in the
batch. You can then keep the list file (perhaps named as per the resulting
batch ID for your own records.
* * *
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#example-usage,1552,249
et_get.py,"Retrieve data from tape
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#et_get.py,24,4
Synopsis,"et_get.py [-v] [-l LOGFILE] [-b BATCHID | -f FILELIST] [-w WORKSPACE] [-r DIR] [-t MAXPROC]
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#synopsis,92,15
Description,"Data files to be retrieved should be specified by referring to the `batch ID`
of the batch in which they were stored. If files have been stored by
specifying an absolute path e.g. `/group_workspaces/jasmin/myworkspace/mydir`,
the retrieval process will not write the retrieved files to the same location
but a new location specified by `DIR`. The final part of the relative path
needs to correspond with the first part of the absolute path of the stored
files, e.g. `group_workspaces`
Proposed best-practice is to create a temporary directory for retrieved data
within your workspace, e.g. `/group_workspaces/jasmin/myworkspace/ettmp` and
to do the initial retrieval into that directory. Once you are satisfied that
the retrieval has completed correctly, data can be moved back to their
original location in the workspace. NB if you need additional storage space
for this, please see [requesting resources]({{% ref ""requesting-resources"" %}}).
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#description,944,141
Options,"option | details
---|---  
-v  |  Verbose output
-l LOGFILE  |  Log file in which to record process output. Note that the log file location must be capable of accepting multi-threaded input, or this parameter should be omitted and instead the output from the et_get command be piped to the log file from stdout
-b BATCHID  |  ID of batch to be retrieved
-f FILELIST  |  A list of individual files to be retrieved, with one file per line. Note that:<br>- entries in the list must contain the full name of the file, including path, just as it was archived<br>- the list should not contain blank lines or any extraneous white space.
-w WORKSPACE  |  name of the group workspace to use. Overrides default set in config file. Case sensitive.
-r DIR  |  ABSOLUTE path of retrieval location
-t MAXPROC  |  Maximum number of worker processes to use in retrieval. MAXPROC recommended to be between 5 and 10. Please feed back your experience of performance improvement with this feature.
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#options,1002,171
Example usage,"{{<command user=""user"" host=""et"">}}
cd /group_workspaces/jasmin/myworkspace
mkdir ettmp
et_get.py -v -l et_get.log -w myworkspace -b 507 -r /group_workspaces/jasmin/myworkspace/ettmp
{{</command>}}
At this point, data will be transferred into the specified retrieval
directory. Files and directories will be restored with their ABSOLUTE path
below the retrieval directory. NB this is a synchronous process and you will
need to keep your terminal window open to ensure it completes (or run within
the `screen` command if you are familiar with this).
When the retrieval process has finished, you should satisfy yourself that it
is correct (using your preferred method). When this is the case, you can move
the data to the required location as shown below:
{{<command user=""user"" host=""et"">}}
mv /group_workspaces/jasmin/myworkspace/ettmp/group_workspaces/jasmin/myworkspace/* /group_workspaces/jasmin/myworkspace
{{</command>}}
* * *
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#example-usage,932,121
et_rm.py,"Remove data from tape
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#et_rm.py,22,4
Synopsis,"et_rm.py [-v] -b BATCHID [-w WORKSPACE]
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#synopsis,40,6
Description,"Deletes the files in the specified batch from the Elastic Tape system.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#description,71,12
Options,"option | details
---|---
-v  |  Verbose output
-b BATCHID  |  ID of batch to be removed   
-w WORKSPACE  |  name of the group workspace to use. Overrides default set in config file. Case sensitive.
{.table .table-striped}
Example usage:
{{<command user=""user"" host=""et"">}}
et_rm.py -v -b 507
{{</command>}}
* * *
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#options,313,50
et_ls.py,"List holdings on tape
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#et_ls.py,22,4
Synopsis,"et_ls.py [-h] [-X XMLSOURCE] [-H] [-b BATCHID] [-w WORKSPACE] [-L {file,batch,workspace}] [-F {text}]
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#synopsis,102,13
Description,"Lists the holdings of a workspace within Elastic Tape at the file, batch or
workspace level.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#description,93,16
Options,"option | details
---|---
-h, --help  |  show this help message and exit
-x XMLSOURCE --xmlsource XMLSOURCE  |  Base XML source, if not default. Note that this has to be compatible with the current base source currently, so can’t be pointed at files, for example
-H --headerWanted  |  Print headers showing column names for text output
-b BATCHID --batchid BATCHID  |  ID of batch by which to filter results
-w WORKSPACE  |  Name of the group workspace to use. Overrides default set in config file. Case sensitive.
-L {file, batch, workspace} --outputLevel {file, batch, workspace}  |  Level of detail to display for results (default is ""workspace"")
-F {text} --outputFormat {text}  |  Format to use for the display of results
{.table .table-striped}
Example usage:
{{<command user=""user"" host=""et"">}}
et_ls.py -w myworkspace -H -L file -b 504
{{</command>}}
Works with the workspace ""myworkspace"", selects display of headers in output,
results at file level, filter by batchid 504 (i.e. shows the files present in
ET in the given batch.)
{{<command user=""user"" host=""et"">}}
et_ls.py -w myworkspace -H -L batch
{{</command>}}
Works with the workspace ""myworkspace"", selects display of headers in output,
results at batch level (i.e. shows the batches present in ET holdings for this
workspace.)
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#options,1294,201
Alerts,"The system provides real-time status messages on the progress of operations
requested. **These services are now available only inside the RAL firewall** ,
so JASMIN users outside of RAL should use the 
[NX graphical desktop service]({{% ref ""graphical-linux-desktop-access-using-nx"" %}}) to open a
firefox browser on one of the nx-login servers, to access these URLs
**Alerts Dashboard** <http://et-monitor.fds.rl.ac.uk/et_user/ET_AlertWatch.php>
**RSS Feed** <http://et-monitor.fds.rl.ac.uk/et_rss/ET_RSS_AlertWatch_atom.php>
In both cases these can be customised to display only alerts from the
workspace of interest to the GWS manager.
**Alerts Dashboard** <http://et-monitor.fds.rl.ac.uk/et_user/ET_AlertWatch.php?workspace=WORKSPACE>
**RSS Feed** <http://et-monitor.fds.rl.ac.uk/et_rss/ET_RSS_AlertWatch_atom.php?workspace=WORKSPACE>
(replace `WORKSPACE` with your workspace name in the above URLs)
**Further views**
**ET Home** <http://et-monitor.fds.rl.ac.uk/et_user/ET_Home.php?caller=USERNAME>
**Holdings summary**[http://et-monitor.fds.rl.ac.uk/et_user/ET_Holdings_Summary.php?caller=USERNAME&workspace=WORKSPACE](http://et-monitor.fds.rl.ac.uk/et_user/ET_Holdings_Summary.php?caller=USERNAME&workspace=WORKSPACE)
(replace `USERNAME` with your username, `WORKSPACE` with your workspace name
in the above URLs)
",https://help.jasmin.ac.uk/docs/short-term-project-storage/secondary-copy-using-elastic-tape#alerts,1320,117
Joint-storage Data Migration App (JDMA),"{{<alert type=""info"">}}
A new system called [NLDS](https://techblog.ceda.ac.uk/2022/03/09/near-line-data-store-intro.html) is in development and will eventually replace both Elastic Tape and JDMA.
{{</alert>}}
**See the JDMA user documentation
at:[cedadev.github.io/jdma_client](https://cedadev.github.io/jdma_client/docs/build/html/index.html)
for more information about using JDMA.**
The joint-storage data migration app (JDMA) is a multi-tiered storage system
which provides a single API to users to allow the movement of data to a number
of different storage systems, query the data they have stored on those storage
systems and retrieve the data.
These interactions are carried out using a common user interface, which is a
command line tool to be used interactively, a python library or a HTTP API,
both to be used programmatically. The command line tool essentially provides a
wrapper for calls to the python library, which in turn makes calls to the HTTP
API.
JDMA was designed with the following usability criteria in mind:
- The user experience for moving data, regardless of the underlying storage systems, should be identical.
- The user should not be responsible for maintaining the connection to the storage system in the case of asynchronous transfers.
- User and group ownership and permissions should be preserved and restored on downloading the data
- The user should receive notifications when the transfers are complete.
- Users should be able to transfer data from one storage system to another
- JDMA is only a request and query layer. Any cataloguing of data should be carried out by the backend system. So that, if JDMA fails, then the data is still available independently of JDMA, from the storage backend.
**See the JDMA user documentation
at:[cedadev.github.io/jdma_client/](http://cedadev.github.io/jdma_client/docs/build/html/index.html)
for more information about using JDMA.**
JDMA was development under a Horizon 2020 grant from the EU Commission. A
report submitted to the EU Commission can be found in the repository at:
[github.com/cedadev/django-jdma_control/blob/master/doc/ESiWACE-
Milestone-8_final.pdf](https://github.com/cedadev/django-
jdma_control/blob/master/doc/ESiWACE-Milestone-8_final.pdf)
The JDMA client github is at:
[github.com/cedadev/jdma_client](https://github.com/cedadev/jdma_client)
",https://help.jasmin.ac.uk/docs/short-term-project-storage/jdma,2342,307
Quick guide to installing the JDMA client on JASMIN,"If you are working on JASMIN and you wish to use the JDMA client, then you can
install it as follows on a `sci` server:
{{<command user=""user"" host=""sci-vm-01"">}}
module load jaspy
python -m venv ~/venvs/jdma-venv
source ~/venvs/jdma-venv/bin/activate
pip install git+https://github.com/cedadev/jdma_client
{{</command>}}
You should then have the **jdma** command-line tool available in your terminal
session.
{{<alert type=""info"">}}
  In **August 2024** the JDMA server was upgraded to a new operating system.
  This requires an upgraded JDMA client to be installed.
  If you were using JDMA prior to **August 2024** then you will *have* to upgrade your client.
  This is a straightforward process of three steps, shown below:
  1. Activate the virtual environment as above:
  2. Install the upgraded JDMA client:
  3. Check the version of the JDMA client:
  The correct version is ``1.0.1``
  {{<command user=""user"" host=""sci-vm-01"">}}
  source ~/jdma_venv/bin/activate
  pip install --upgrade git+https://github.com/cedadev/jdma_client
  pip list | grep jdma-client
  {{</command>}}
{{</alert>}}",https://help.jasmin.ac.uk/docs/short-term-project-storage/jdma#quick-guide-to-installing-the-jdma-client-on-jasmin,1098,147
GWS etiquette,"This article highlights the essential principles of working with group
workspaces.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/gws-etiquette,83,11
Keeping informed,"Please maintain contact throughout the life of the GWS via the following
methods:
- Using the [JASMIN dashboard](https://mon.jasmin.ac.uk) to check on the status of your GWS (used versus available space).
- Using the {{<link ""gws-scanner-ui"">}}GWS Scanner User Interface{{</link>}} to check on where, for how long and by whom, space is being used.
- GWS Managers should {{<link ""gws-scanner"">}}configure the GWS Scanner{{</link>}} to gather the above information and arrange email alerts.
- Look out for emails from the CEDA/JASMIN team
- News articles on the {{<link ""ceda_site"">}}CEDA{{</link>}} or {{<link ""jasmin_site"">}}JASMIN{{</link>}} websites and by monitoring {{<link name=""ceda_x"" cue=""true"" >}}CEDA social media{{</link>}}feeds which may be used to post messages regarding system status or security.
If you are aware that a user who has access to your GWS leaves your project,
or, for whatever reason, no longer needs to be a member of the GWS, please let
the CEDA helpdesk know, as arrangements may need to be made to transfer the
ownership of files and/or directories to another member of the GWS (e.g. the
manager) to ensure continued access to the data.
",https://help.jasmin.ac.uk/docs/short-term-project-storage/gws-etiquette#keeping-informed,1170,178
Introduction,"This article explains how a GWS Manager can organise for some directories within a GWS to be shared with other users on JASMIN.
**Note:** this only applies to sharing data within the managed environment of the JASMIN platform.
If you need to share data with users outside JASMIN, or to users of external cloud tenancies, please consider
the [HTTP option]({{% ref ""share-gws-data-via-http"" %}}).
",https://help.jasmin.ac.uk/docs/short-term-project-storage/share-gws-data-on-jasmin#introduction,395,63
How to share specific directories,"Sometimes it is useful to share the contents of specific directories within
your GWS with other users on JASMIN (that do not have access to your GWS).
This can be achieved with the following approach:
Suppose you manage the GWS `/group_workspaces/jasmin/superproj` and you want
to share the directory `/group_workspaces/jasmin/superproj/mydata` with other
JASMIN users.
1\. Add read and execute permission for all to the top-level GWS directory.
This requires root access so you might need to request that CEDA make this
change for you:
{{<command>}}
chmod 775 /group_workspaces/jasmin/superproj
{{</command>}}
2\. Alter the permissions of all sub-directories to remove the execute
permission for all users that don't have access to the GWS:
{{<command>}}
find /group_workspaces/jasmin/superproj -type d -exec chmod o-x {} \;
{{</command>}}
3\. Add execute permission on the sub-directory you want to share:
{{<command>}}
chmod o+x /group_workspaces/jasmin/superproj/mydata
{{</command>}}
NOTE: You may need to change permissions on directories and files within the
sub-directory as well. Please consult the `chmod` man pages (by typing `man
chmod`) for details.
NOTE: if you have a `public` directory then it needs `755` access if you want it
to be visible via the webserver via the `gws-access.jasmin.ac.uk` service. So
you may wish to re-add execute permission on that directory, e.g.:
{{<command>}}
chmod o+x /group_workspaces/jasmin/superproj/public
{{</command>}}
{{< alert type=""danger"" >}}
Do not set open permissions on files or directories.
By this we mean permissions where data are ""world-writable"" by anyone, for example
`-rw-rw-rw-` for a file, or **<< DON'T USE THESE!!**
`drwxrwxrwx` for a directory. **<< DON'T USE THESE!!**
We provide a UNIX a group corresponding to each group workspace, usually named `gws_<name>` which all members of that GWS belong to: this enables sharing within the group if you set permissions appropriately using that group. If you are unsure about setting permissions, please ask the helpdesk.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/short-term-project-storage/share-gws-data-on-jasmin#how-to-share-specific-directories,2051,299
Setting up your JASMIN account for access to MASS,"The following notes are written assuming you are using a Linux machine in your
home institution, that you have [applied for a new MASS account]({{% ref ""how-to-apply-for-mass-access"" %}}), and that you have received an email from the
Met Office Storage Team with your new MASS credentials file attached.
",https://help.jasmin.ac.uk/docs/mass/setting-up-your-jasmin-account-for-access-to-mass,304,48
Load your SSH key,"Start an ssh-agent on your home institution machine, load your private key, and enter your passphrase when requested.
{{<command user=""localuser"" host=""localhost"">}}
eval $(ssh-agent -s)
ssh-add ~/.ssh/id_rsa_jasmin
(out)Enter passphrase for ~/.ssh/id_rsa_jasmin:
{{</command>}}
**Note:** it's a good idea to keep your private keys for different systems
separated, so you may want to keep your private key for JASMIN in a separate
file, just move the one created during the process described above to a new
sensible location such as ~/.ssh/jasmin_id_rsa.
",https://help.jasmin.ac.uk/docs/mass/setting-up-your-jasmin-account-for-access-to-mass#load-your-ssh-key,555,78
Test login to the JASMIN login node,"{{< alert type=""info"" >}}
Please read the notes in [login servers]({{% ref ""login-servers/#recent-changes"" %}}) about the need to
keep your SSH client up to date in order to be able to connect securely to JASMIN.
{{< /alert >}}
**Note:** that the `-A` in the first ssh command is mandatory to enable access
to the client VM, the `-X` enables X11 forwarding and is optional.
{{<command user=""localuser"" host=""localhost"">}}
ssh -A -X <userid>@login-01.jasmin.ac.uk
{{</command>}}
",https://help.jasmin.ac.uk/docs/mass/setting-up-your-jasmin-account-for-access-to-mass#test-login-to-the-jasmin-login-node,478,72
3\. Test login to the MASS client host,"From the login machine, you can then login to the MASS client host.
**Note:** If it does not let you log in even when you have agent
forwarding correctly set up and can log into the ""sci"" nodes, then you have
either not requested additional access to the dedicated client machine, or
access hasn't been approved yet, email the Met Office Service Manager
[monsoon@metoffice.gov.uk](mailto:monsoon@metoffice.gov.uk), to verify that
approval has been granted. Allow a couple of days for this process to happen
after submitting your request for access to the VM.
{{<command user=""user"" host=""login-01"">}}
ssh -X <userid>@mass-cli.jasmin.ac.uk
{{</command>}}
{{<command user=""user"" host=""mass-cli"">}}
echo ""Hello World""
(out)Hello World
exit
{{</command>}}
{{<command user=""user"" host=""login-01"">}}
exit
{{</command>}}
{{<command user=""localuser"" host=""localhost"">}}
",https://help.jasmin.ac.uk/docs/mass/setting-up-your-jasmin-account-for-access-to-mass#3\.-test-login-to-the-mass-client-host,862,115
back on your local machine,"{{</command>}}
",https://help.jasmin.ac.uk/docs/mass/setting-up-your-jasmin-account-for-access-to-mass#back-on-your-local-machine,15,1
Install your MOOSE credentials file,"You can `scp` the file via a JASMIN transfer server, make sure the credentials
file is called `moose`, and you must run the `moo install` command on `mass-
cli.jasmin.ac.uk` to set it up for you.
{{< alert type=""info"" >}}
The external moose client has improved security settings, so **you
must use the `moo install` command** to put your moose credentials file in the
correct place in order to get remote access to work. This can only be done on
the client machine mass-cli.jasmin.ac.uk. The credentials file is also changed
by the running of moo install, so this process can be run only once.
{{< /alert >}}
{{<command user=""user"" host=""localhost"">}}
scp moose <userid>@xfer1.jasmin.ac.uk:~/moose
ssh -A -X <userid>@login-01.jasmin.ac.uk
{{</command>}}
{{<command user=""user"" host=""login-01"">}}
ssh -X <userid>@mass-cli.jasmin.ac.uk
{{</command>}}
{{<command user=""user"" host=""mass-cli"">}}
ls -l ~/moose  
(out)-rwx------ 1 <userid> users 511 Jul  3 13:45 /home/users/<userid>/moose
moo install  
(out)### passwd, command-id=148593088         
(out)Your password is due to expire in -1 day(s).    
(out)A new password can be generated using 'moo passwd -r'.          
ls -l ~/.moosedir/moose  
(out)-rw------- 1 <userid> users 511 Jul  3 13:45 /home/users/<userid>/.moosedir/moose
{{</command>}}
Having run these commands on the client machine, the moose file will have
disappeared from your home directory, but a .moosedir directory will have been
created, this will contain a new moose file, an install.log file, and once you
start making MOOSE queries, a moose-external-client.log will be created.
",https://help.jasmin.ac.uk/docs/mass/setting-up-your-jasmin-account-for-access-to-mass#install-your-moose-credentials-file,1602,224
Test use of the locally installed MOOSE client,"{{<command user=""user"" host=""mass-cli"">}}
which moo  
(out)/opt/moose/external-client-version-wrapper/bin/moo   
moo si  
(out)<system information appears here>  
moo help  
(out)<help details appear here>      
moo projlist  
(out)<list of projects appears here>
{{</command>}}
You have now successfully accessed MASS from JASMIN!
If you are new to MOOSE, you might like to read the 
[User Guide]({{% ref ""moose-the-mass-client-user-guide"" %}}) next.
",https://help.jasmin.ac.uk/docs/mass/setting-up-your-jasmin-account-for-access-to-mass#test-use-of-the-locally-installed-moose-client,452,52
Introduction,"To access data held in the Met Office MASS archive, you will need:
- a sponsor 
- access to the mass-cli1 client machine
- a MASS account 
Your sponsor will need to be a **Senior Met Office Scientist** with whom you
are working on a collaborative research project. If you are a Met Office
employee, your sponsor will be your manager.
Note: The following instructions also assume you already have a **JASMIN login
account** and the **jasmin-login** service. If you do not, please follow steps
1-4 [here]({{% ref ""get-started-with-jasmin"" %}}).
",https://help.jasmin.ac.uk/docs/mass/how-to-apply-for-mass-access#introduction,543,90
Step One - Sponsor Form,"**Ask your Met Office sponsor** to complete the sponsor form {{<link ""https://metoffice.service-now.com/sp?id=sc_cat_item&sys_id=f00fb9d4dbd06b404690fe9b0c9619a6"">}} available here only to those in the Met Office{{</link>}}
The following information will be asked for, so please provide your sponsor
with any details they may not have:
- Your full name
- Your official email address
- Your organization's name
- Your department name
- The host country of your organization
- A list of MASS projects and/or data sets that you need access to. A full MOOSE dataset path is required, and your sponsor should help you determine this.
- Your JASMIN username
- Your JASMIN user ID number (UID). You can get this by typing echo $UID at the terminal on any JASMIN machine.
The information you provide to the Met Office will be treated in accordance
with the [Met Office Privacy Policy](https://www.metoffice.gov.uk/about-
us/legal/privacy), and your use of the service will be subject to the [Terms
and Conditions of Use](https://metoffice.github.io/JASMIN-MASS-
access/Terms_and_Conditions.html) for the service.
",https://help.jasmin.ac.uk/docs/mass/how-to-apply-for-mass-access#step-one---sponsor-form,1105,159
Step Two - Access to mass-cli1,"  1. Sign into your JASMIN account
  1. Select ‘JASMIN Services’
  1. Select ‘Additional Services’
  1. Next to ‘mass’ click ‘More information’ 
  1. Select ‘Apply for access’
  1. Within the request, please name your sponsor
The direct link is:
<https://accounts.jasmin.ac.uk/services/additional_services/mass/>
",https://help.jasmin.ac.uk/docs/mass/how-to-apply-for-mass-access#step-two---access-to-mass-cli1,313,39
Step Three - MASS account,"When steps one and two are complete, you will be provided with a specific MASS account to
use on JASMIN and emailed a MOOSE credentials file. Please see [Setting up
your JASMIN account for access to MASS]({{% ref ""setting-up-your-jasmin-account-for-access-to-mass"" %}}) once you have received the credentials file.
Your MASS account will give (read-only) access to the relevant data on MASS based on the information provided by your sponsor;
please note that the access control is handled by the Met Office and not by the JASMIN team.
Note: If you have access to MASS on other systems you cannot copy those MOOSE
credentials file/s onto JASMIN – they will not work! Please also see the
[External Users’ MOOSE Guide]({{% ref ""moose-the-mass-client-user-guide"" %}})
for what MOOSE commands are available on JASMIN.
",https://help.jasmin.ac.uk/docs/mass/how-to-apply-for-mass-access#step-three---mass-account,813,129
External Access to MASS FAQ,"## Introduction
The Managed Archive Storage System (MASS) provides storage and restore services for large volumes of Met Office data. It is a service operated by the UK Met Office.
This article provides answers to MASS frequently asked questions:
Click on the link for each of the FAQs below to expand the answer.
## General
{{< accordion id=""accordion-1"" >}}
  {{< accordion-item header=""Can I use my existing MASS account"" show=""false"" >}}
    No. You need a separate MASS account for use on the Met Office internal
network (CDN), Monsoon, ECMWF HPCs, and JASMIN. With these different account
types, you can have permission to access different datasets specific to these
computing environments.
  {{< /accordion-item >}}
  {{< accordion-item header=""How do I use MOOSE?"" >}}
    Please see the [MOOSE User Guide here]({{< ref ""moose-the-mass-client-user-guide"" >}})
  {{< /accordion-item >}}
  {{< accordion-item header=""Will my account expire?"" >}}
Yes. By default, MASS via JASMIN accounts will expire after 500 days and your
account will be automatically disabled.
Shortly before your account is due to expire you will receive an email, and it
will contain instructions for you and your sponsor about how to extend your
access. If your account has already expired and you are looking to reactive
it, please email: [Monsoon@metoffice.gov.uk](mailto:Monsoon@metoffice.gov.uk)
  {{< /accordion-item >}}
  {{< accordion-item header=""Why am I asked for a password when logging in to mass-cli?"" >}}
    There are two reasons that may result in you being prompted for a password
when attempting to login to the MASS client machine (mass-cli.jasmin.ac.uk).
The first is if you do not have permission to access the machine. A quick
method to check is to verify if you are a member of the `moose` user group. It
should be listed when you use the ‘groups’ command:
[login1]$ groups
moose
If this happens, please contact:
[Monsoon@metoffice.gov.uk](mailto:Monsoon@metoffice.gov.uk)
The second is if you forget the `-A` option for agent forwarind when you ssh to a JASMIN login
node. You can test for this condition by listing loaded identities on the
login node, and finding you have none:
[login1]$ ssh-add -l
Could not open a connection to your authentication agent.
If this happens, please exit back to your local machine and ssh in again using
the `-A` flag or tick the relevant box for ""agent forwarding"".
  {{< /accordion-item >}}
  {{< accordion-item header=""How can I directly login to the MASS client machine?"" >}}
    You can't, but you can edit your ssh configuration so that it automatically enables
you to jump through the intermediary login servers.
Add the following to your home institute ssh config file ($HOME/.ssh/config
file):
Host mass-cli 
    User your_jasmin_userid 
    HostName mass-cli.jasmin.ac.uk
    ProxyCommand ssh -YA -t your_jasmin_userid@login1.jasmin.ac.uk -W %h:%p 2>/dev/null
You should then be able to login directly using:
$ ssh mass-cli
Please note that this only works if you are using **OpenSSH version 5.4** or
greater as earlier versions do not support the `-W` flag. You can check your
version using: `ssh -v`
  {{< /accordion-item >}}
  {{< accordion-item header=""Can I write to MASS from JASMIN?"" >}}
    No, MASS access from JASMIN is strictly read-only. If you need to write to the MASS archive,
    contact monsoon@metoffice.gov.uk and ask to be put in touch with the relevant team.
  {{< /accordion-item >}}
{{< /accordion >}}
## MOOSE messages and what to do
{{< accordion id=""accordion-2"" >}}
  {{< accordion-item header=""Is this process running in the correct environment?"" show=""false"" >}}
    When running 'moo install' you may get an error message similar to:
Cannot read file: /home/user/<userid>/.moosedir/moose     
- is this process running in the correct environment?
This can be the result of the wrong combination of Unix user-id and UID having
been used to encrypt the credentials file. If you encounter this error
message, please type `id` on the command line whilst logged into JASMIN, and
send the `uid=` section of the output to:
[Monsoon@metoffice.gov.uk](mailto:Monsoon@metoffice.gov.uk)
Your credentials file will then be reissued.
    {{</accordion-item>}}
    {{<accordion-item header=""Your password is due to expire in X day(s)."">}}
Occasionally on running a MOOSE command you will be told that your password is
due to expire with a message of the form:
    Your password is due to expire in 6 day(s).   
    A new password can be generated using 'moo passwd -r'.
This refers specifically to your MASS via JASMIN, it does not affect any other
MOOSE accounts you may have.
You need to run the command as advised in order to update your credentials
whilst you are logged into mass-cli. You do not actually need to provide a new
password, as this is generated and hidden from you by the command.
If you have a retrieval in progress, it is safe to run this command as it will
not affect processes already running.
    {{</accordion-item>}}
    {{< accordion-item header=""ERROR_SINGLE_COPY_UNAVAILABLE"" >}}
MOOSE - Single Copy Unavailable error
On occasion, a tape library needs to be taken down for maintenance. If a user is trying to retrieve a single-copy file stored on one of those tapes, the retrieval will temporarily fail with the message `ERROR_SINGLE_COPY_UNAVAILABLE`. As soon as the maintenance is completed, the file will be available again.
Tapes are taken out of MASS for copying to the new MASS system and become unavailable for roughly 14 days. The process is as follows:
- Thursday (week one): Tapes are marked unavailable for indexing by the system.
- Tuesday (week two): Tapes get taken out for copying to the new MASS system.
- Following Thursday (week three): Tapes are returned to Met Office library and should be available again.
So, if you find that data or files are unavailable due to the `ERROR_SINGLE_COPY_UNAVAILABLE` error, try reading the data again on Friday, and if still not available, try the following Friday when the migration should have completed.
    {{< /accordion-item >}}
{{< /accordion >}}
## MOOSE basics
{{< accordion id=""accordion-3"" >}}
  {{< accordion-item header=""What is MOOSE?"" >}}
    The software that allows you to interact with MASS.
  {{< /accordion-item >}}
  {{< accordion-item header=""What is a project?"" >}}
    A collection of access rules.
  {{< /accordion-item >}}
  {{< accordion-item header=""What is an access rule?"" >}}
    Permission to access an area in MASS. For example, project-random might have
an access rule to moose:/crum/random-numbers
Being part of project-random would allow you to access the random-numbers set.
  {{< /accordion-item >}}
    {{< accordion-item header=""How do I see what projects I am a member of?"" >}}
    You can use: `moo prls`
  {{< /accordion-item >}}
    {{< accordion-item header=""How do I see what access rules a project has?"" >}}
    You can use: `moo projinfo -l projectname` (Replace _projectname_ with the name of one of your projects)
  {{< /accordion-item >}}
  {{< accordion-item header=""How do I get access to a project, or add an access rule to one of my projects?"" >}}
Please contact your sponsor. They can then complete this form if they also
agree you require access:
https://metoffice.service-now.com/sp?id=sc_cat_item&sys_id=5653331e1bbaf0d88ffa422ad34bcba0&referrer=recent_items
Please note that the link above is only visible to those in the Met Office.
  {{< /accordion-item >}}
      {{< accordion-item header=""Why can I not access a set that I know is part of a project?"" >}}
    If you are given access to a project but do not have access to all the sets
associated with it, this can be due to the Access Control Lists (ACLs).
The project owner will be able to change the ACLs on sets to make them
readable if it is appropriate.
      {{< /accordion-item >}}
  {{< accordion-item header=""How do I retrieve a file from MASS?"" >}}
Use `moo get` or `moo select`. More information about both commands is in the
[MOOSE User Guide]({{< ref ""moose-the-mass-client-user-guide"" >}}).
  {{< /accordion-item >}}
    {{< accordion-item header=""How do I make sure my directory has all the available data retrieved from MASS?"" >}}
**The problem:** You are running a model over a period of several days or weeks,
and you need to analyse the output of the model as it runs. You have a moo get
or moo select command that you run to fetch the data that is available. You
want to be able to re-run it to fetch the files or fields that have been added
to MASS since you last ran the command, but you do not want it to waste time
re-fetching things you already have.
**The solution:** Use the -i or --fill-gaps option when you run moo get or moo
select. This option tells MOOSE that you only want to fetch files that don't
already exist in the specified local directory. Note that MASS works out where
gaps are by doing checks to see if files of the expected name exist in your
destination directory, so it won't behave correctly if you rename files after
you have retrieved them, or if you use the -C option with moo select which
condenses all the matching fields into a single file.
You might also find the `-g` / `--get-if-available` option to moo get useful. This
tells MOOSE to get every file from your moo get list that is available, but
ignore ones that are not there rather than exit with an error. This could help
if you are expecting files to be archived at some point but are not sure
whether they will be there when your job runs. If you use this option MOOSE
will get as much as it can from your list without bailing out.
  {{< /accordion-item >}}
    {{< accordion-item header=""How can I script my data retrieval from MASS?"" >}}
There are restrictions on how to login to JASMIN and use of Linux utilities
such as ‘cron’ and ‘at’ but it is possible to remotely initiate a retrieval
from MASS on to JASMIN, provided you have your ssh agent running on a machine
local to you.
eval $(ssh-agent -s)
ssh-add ~/.ssh/jasmin_id_rsa 
ssh -A -X sci1.jasmin.ac.uk 'ssh mass-cli my_script.sh'
If you have set up your $HOME/.ssh/config to allow more direct access, then
the following should work:
ssh mass-cli my_script.sh
This will run the script “my_script.sh” on the MASS client VM. You can put the
moose retrieval commands into a script and it should work:
#!/bin/bash 
SRC_URI=moose:/opfc/atm/global/SOMETHING
moo get $SRC_URI jasmin_copy.pp 
exit
If you have access to an appropriate JASMIN workspace, then you can scp data
from the workspace directly through one of the dedicated data transfer VMs.
Again, you need the ssh-agent running locally:
eval $(ssh-agent -s)
ssh-add ~/.ssh/jasmin_id_rsa 
scp userid@xfer1.jasmin.ac.uk:/group_workspaces/cems/<project>/jasmin_file.pp my_local_copy.pp
  {{< /accordion-item >}}
  {{< accordion-item header=""Can I run MASS retrievals on LOTUS or through a workload manager?"" >}}
In addition to the interactive mass-cli server there is also the moose1 server
that is only accessible through the {{<link ""../batch-computing/lotus-overview"">}}LOTUS batch processing cluster{{</link>}}. To submit jobs to moose1 you must use the [Slurm scheduler]({{< ref
""slurm-scheduler-overview"" >}}). You will need to specify the account
mass and partition mass, for example:
 sbatch -A mass -p mass [<options>] <jobscript>
where \<jobscript\> looks something like:
#!/bin/bash 
SRC_URI=moose:/opfc/atm/global/SOMETHING
moo get $SRC_URI jasmin_copy.pp 
exit
It is also easy to configure the [Rose/Cylc workflow manager]({{< ref ""rose-cylc-on-jasmin"" >}}) to submit jobs to moose1 through the Slurm scheduler by
including the following lines in your suite.rc file:
[[[job submission]]]
    method = slurm
[[[directives]]]
    --partition=mass
    --account=mass
[<options>]
  {{< /accordion-item >}}
{{< /accordion >}}
",https://help.jasmin.ac.uk/docs/mass/external-access-to-mass-faq,11815,1826
NAG Library,"This article introduces the Fortran and C library of software under the
Numerical Algorithm Group (NAG) license. NAG Library is a collection of
robust, documented, tested and maintained numerical algorithms.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/nag-library,208,30
Accessing the NAG Library,,https://help.jasmin.ac.uk/docs/software-on-jasmin/nag-library#accessing-the-nag-library,0,0
Requesting access,"If you wish to use the NAG Library on JASMIN you will need to request access
via the JASMIN Accounts Portal at:
<https://accounts.jasmin.ac.uk/services/additional_services/nerctools/>
This will give your JASMIN user account access to the ""nerctools"" Unix Group
that is used to limit access to NAG.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/nag-library#requesting-access,298,44
Loading the NAG Library for use on JASMIN,"The NAG library is made available via `module` command which is only available
once you are on the [scientific analysis servers]({{% ref ""sci-servers"" %}})
and [LOTUS cluster]({{% ref ""lotus-overview"" %}}) on JASMIN. In addition to
loading a module for the library, you will usually need to load a module for
the compiler you are using. For example:
{{<command user=""user"" host=""sci1"">}}
module load contrib/nag/25
module list
(out)Currently Loaded Modulefiles:
(out)    1) intel/fce/15.0.090   2) contrib/nag/25
{{</command>}}
The NAG library is loaded as well as the Intel Fortran compiler. Now you can
compile your code and link to the NAG library, for example:
{{<command user=""user"" host=""sci1"">}}
ifort your_code.f90 -lnag_nag -o your_code.exec
{{</command>}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/nag-library#loading-the-nag-library-for-use-on-jasmin,766,109
How to find a NAG library routine,"Please search the NAG documentation when looking for specific routines:
<https://www.nag.co.uk/numeric/fl/nagdoc_fl25/html/indexes/kwic.html>
",https://help.jasmin.ac.uk/docs/software-on-jasmin/nag-library#how-to-find-a-nag-library-routine,142,11
How to use the NAG library,"The following shows the directory and file organisation of the materials.
/apps/contrib/nag/fll6i25dcl/
                    |- in.html  (Installer's Note - this document)
            |- doc -|- un.html  (Users' Note)
            |       |- lic_agr.txt  (license agreement)
            |
            |       |- libnag_nag.a      (static self-contained library
            |       |                     including NAG BLAS/LAPACK)
            |       |- libnag_nag.so.25  (shareable self-contained library
            |       |                     including NAG BLAS/LAPACK)
            |       |- libnag_nag.so     (symbolic link pointing at
            |- lib -|                     libnag_nag.so.26)
            |       |- libnag_mkl.a      (static library requiring
            |       |                     MKL BLAS/LAPACK)
            |       |- libnag_mkl.so.25  (shareable library requiring
            |       |                     MKL BLAS/LAPACK)
            |       |- libnag_mkl.so     (symbolic link pointing at
            |                             libnag_mkl.so.26)
fll6i25dcl -|
            |- nag_interface_blocks -|- *  (interface blocks for Intel compiler)
            |
            |            |- source --|- ??????e.f90
            |            |
            |- examples -|- data ----|- ??????e.d
            |            |           |- ??????e.opt
            |            |
            |            |- results -|- ??????e.r
            |
            |           |- nag_example*  (scripts to compile and run
            |- scripts -|                  NAG example programs)
            |           |
            |           |- nag_recompile_mods  (script to recompile
            |                                   interface blocks)
            |
            |- c_headers -|- * (C/C++ header file and information)
            |
            |- mkl_intel64_11.2.0 -|- *  (Intel Math Kernel Library)
            |
            |- rtl -|- *  (Intel compiler run-time libraries)
            |
            |           |- bin -|- *  (directories of license management
            |           |              binaries for supported platforms)
            |- license -|- README.txt
                        |
                        |- doc -|- *  (license management documentation)
",https://help.jasmin.ac.uk/docs/software-on-jasmin/nag-library#how-to-use-the-nag-library,2295,211
Further information,"See the full NAG library manual at:
<https://www.nag.co.uk/numeric/fl/nagdoc_fl26/html/frontmatter/manconts.html>
",https://help.jasmin.ac.uk/docs/software-on-jasmin/nag-library#further-information,114,8
Using Matplotlib for visualisation on JASMIN,"This article provides a basic example of using Matplotlib on JASMIN to
generate a plot. It also gives an important tip that may stop your code
failing when run on the LOTUS cluster.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/matplotlib,182,33
Matplotlib - a basic example,"[Matplotlib](https://matplotlib.org/) is a very well documented plotting
library for Python. Here is a brief example of generating a line graph on a
PNG file using matplotlib.
Load the Jaspy Python 3 environment, and start a Python session:
{{<command user=""user"" host=""sci1"">}}
module load jaspy
python
{{</command>}}
In python, set some x-values, y-values, axis labels and a title, and plot:  
import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt
x_values = [1, 5, 3, 9, 14]
y_values = [2000, 2005, 2010, 2015, 2020]
x_label = 'Temperature (degC)'
y_label = 'Year'
title = 'Average temperature of garden shed (2000-2020)'
plt.plot(y_values, x_values, 'g--')
plt.ylabel(y_label)
plt.xlabel(x_label)
plt.title(title)
plt.savefig('output.png')
",https://help.jasmin.ac.uk/docs/software-on-jasmin/matplotlib#matplotlib---a-basic-example,762,101
Plotting with matplotlib on LOTUS,"When using matplotlib on LOTUS hosts please make sure that you are setting the
_rendering_ _backend_ to a setting that will definitely work. This must be
done **before** importing `matplotlib.pyplot`.
On JASMIN it is safe to use:
import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt
or alternatively, the `MPLBACKEND` environment variable can be set in the job
script before invoking python:
export MPLBACKEND=agg
If you do not set this option or you choose an alternative backend then you
may see **failures which include very large dump (error) files being written
(up to 56GB per file!)**. Please remove these files if you accidentally create
them, and switch over to selecting an appropriate rendering backend as
indicated above.
Note that if you see the following error message, this results from attempting
to use the default GTK backend on LOTUS (as GTK is only available in an
interactive X-windows environment). The solution is to use `agg`, as described
above.  
{{<command>}}
(out)ValueError: Namespace Gtk not available for version 3.0
{{</command>}}
For more information please see the [matplotlib back-ends
page](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).
",https://help.jasmin.ac.uk/docs/software-on-jasmin/matplotlib#plotting-with-matplotlib-on-lotus,1208,174
Introduction,"This article introduces {{<link ""https://geocat-viz.readthedocs.io/en/latest/"">}}geocat{{</link>}} as a replacement on JASMIN for NCAR Command Language (NCL) which is now deprecated.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/geocat-replaces-ncl#introduction,183,19
NCL now deprecated,"NCL is now deprecated by NCAR (see [this announcement](https://www.ncl.ucar.edu/Document/Pivot_to_Python/)),
there is no Rocky 9 version available, so we will not be providing it when JASMIN moves from CentOS to using
Rocky 9 in the next few months. The plan is to add to Jaspy the `geocat-viz` package, which is NCAR's Python
replacement for NCL. 
",https://help.jasmin.ac.uk/docs/software-on-jasmin/geocat-replaces-ncl#ncl-now-deprecated,349,54
Installation in a conda environment,"To give you a chance now to familiarise yourself with `geocat-viz`, here are some
instructions for how you could install it in a Conda environment under your own home directory.
{{<alert type=""info"">}}
Note that such an environment cannot be activated at the same time as Jaspy.
{{</alert>}}
Total disk space required is 3.2GB.
Commands marked with `#*` below will be needed again in order to activate the environment in later sessions.
Deactivate `jaspy` in this session:
{{<command user=""user"" host=""sci-vm-01"">}}
module unload jaspy     ##*
{{</command>}}
Download `miniforge` installer:
{{<command user=""user"" host=""sci-vm-01"">}}
wget <https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh>
{{</command>}}
Install base environment:
{{<command user=""user"" host=""sci-vm-01"">}}
bash Miniforge3-Linux-x86_64.sh
{{</command>}}
Accept the default answers to the questions, saying no to the question about `conda init`.
Activate the base environment:
{{<command user=""user"" host=""sci-vm-01"">}}
source ~/miniforge3/bin/activate  ##*
{{</command>}}
Create and activate an environment:
{{<command user=""user"" host=""sci-vm-01"">}}
mamba create -n my-geocat-env
conda activate my-geocat-env    ##*
{{</command>}}
Install the packages:
{{<command user=""user"" host=""sci-vm-01"">}}
mamba install geocat-viz geocat-datafiles
{{</command>}}
Try one of the examples from:\
https://geocat-examples.readthedocs.io/en/latest/gallery/index.html
{{<command user=""user"" host=""sci-vm-01"">}}
wget https://geocat-examples.readthedocs.io/en/latest/_downloads/efafc109e5344e8e33052ad5213ee4be/NCL_box_1.py
python NCL_box_1.py
{{</command>}}
(should display a plot)
Full documentation is at <https://geocat-viz.readthedocs.io/en/latest/>",https://help.jasmin.ac.uk/docs/software-on-jasmin/geocat-replaces-ncl#installation-in-a-conda-environment,1748,180
Creating and using miniforge environments,"{{<alert type=""danger"">}}
Important changes took place in September 2024 affecting what software can be used on JASMIN.
Please read [this announcement](https://www.ceda.ac.uk/news/updates/2024/2024-08-29-important-software-changes-autumn/) carefully.
This supercedes the information below which has yet to be updated in line with this.
{{</alert>}}
On JASMIN, we provide a wide range of packages via the {{<link ""jaspy-envs"">}}jaspy{{</link>}}
environment (which is itself a Conda environment). This page gives
detail on how to create and use your own personal Conda environments via the
`miniforge` installer, as an alternative to the use of Jaspy.
To decide which to use, please see this page: 
{{<link ""conda-environments-and-python-virtual-environments"">}}conda environments and python virtual environments{{</link>}}.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/creating-and-using-miniforge-environments,823,101
Obtaining miniforge,"In order to create your own conda environments, you will first need to
download the {{<link href=""https://github.com/conda-forge/miniforge"">}}miniforge{{</link>}}
installer.  This is a lightweight installer, and will also ensure that
packages are downloaded from the conda-forge channel (important for
licensing reasons).  For this reason, it should be used in preference to
either condaforge or Anaconda.
Miniforge can be downloaded using:
wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh
",https://help.jasmin.ac.uk/docs/software-on-jasmin/creating-and-using-miniforge-environments#obtaining-miniforge,539,60
Deactivating Jaspy,"You cannot have your own conda environment activated at the same time as
Jaspy, so it is recommended that if you have loaded the jaspy module, then you
start by typing:
module unload jaspy
",https://help.jasmin.ac.uk/docs/software-on-jasmin/creating-and-using-miniforge-environments#deactivating-jaspy,189,34
Creating a base environment,"You can run the installer by typing:
bash Miniforge3-Linux-x86_64.sh
You will be asked to confirm the licence agreement, to choose an installation
location, and to decide whether it should run ""conda init"". It is recommended
that you:
- Accept the default location (`~/miniforge3`). If you need to change this, see
the section ""Varying the installation location"" near the end of this page for more info.
- Say **no** to the question about `conda init`, because saying yes will cause it to add
lines to your `~/.bashrc` file causing your base environment to be activated every time
you log in, which may interfere with the use of Jaspy. If you say no, you can still
follow the instructions below when you wish to activate your base environment.  
(Add the `-b` option at the end of the above command to run the installer in
batch mode, which will also skip the ""conda init"". Or add `-h` to see help on
other available command-line options.)
",https://help.jasmin.ac.uk/docs/software-on-jasmin/creating-and-using-miniforge-environments#creating-a-base-environment,940,162
Activating the base environment,"Assuming that you made the choices recommended above when running the
installer, you should type the following in order to activate the base
environment:
source ~/miniforge3/bin/activate
(You may encounter documentation elsewhere which suggests `conda activate`
instead, but the above command is a workaround for the fact that you have not
run `conda init`, the reasons for which are explained above.)
Your command prompt will then change to include `(base)` at the start, in
order to remind you that this environment is activated. You can deactivate the
environment by typing:
conda deactivate
",https://help.jasmin.ac.uk/docs/software-on-jasmin/creating-and-using-miniforge-environments#activating-the-base-environment,595,91
Creating and activating a sub-environment,"Although once you have activated the base miniforge environment, you can in
principle start to install packages immediately, your use of miniforge will
generally be better organised if you do not install packages directly into the
base environment, but instead use a named sub-environment. You can have
multiple sub-environments under a single base environment, and activate the
one that is required at any one time. These sub-environments will work independently.
Miniforge provides two installer programs: `conda` and `mamba`. These use
different algorithms, and mamba is implemented in C++ while conda is
implemented in Python.  Both commands work with the same type of environments,
here referred to as ""conda environments"", but in our experience from preparing the
Jaspy environments, mamba is faster than conda and uses less memory. Therefore,
our recommendation is to use mamba for installing environments.  However, if
you are not running `mamba init`, then you will need to use the `conda`
command when activating or deactivating environments.  The commands shown on this
page reflect this, but if you prefer, you can use `conda` throughout.
Also, for some of the commands not involving package installation, e.g. when
listing environments, the two commands can be used interchangeably, although
the `mamba` command is shown on this help page.
To create a named environment (for example, called `myenv`), ensure that the
base environment is activated (the command prompt should start with `(base)`),
and type:
mamba create -n myenv
It will show the proposed installation location, and once you answer the
prompt to proceed, will do the installation. If you have followed these
instructions, this location should be
`/home/users/<your_username>/miniforge3/envs/myenv`. You can alternatively
give it a different location using the option `-p <path>` instead of `-n
<name>`.
Once you have created your sub-environment, you can activate it using
`conda activate <name>` for example:
conda activate myenv
although you can also activate it using its full path, useful if you used the
`-p` option to specify a non-standard path for the environment, for example:
conda activate /gws/smf/j04/mygws/myenv
The command prompt will then change (e.g. to start with `(myenv) `) to reflect
this. Typing `conda deactivate` once will return you to the base environment;
typing it a second time will deactivate conda completely (as above).
mamba env list
will list your environments.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/creating-and-using-miniforge-environments#creating-and-activating-a-sub-environment,2474,376
Installing conda packages,"Once you have activated a named environment, you can install packages with the
`mamba install` command, for example:
mamba install gcc
You can also force particular versions to be installed. For example:
mamba install gcc=13.2.0  # exact version
mamba install ""gcc>=13.2.0""  # greater than or equal to
mamba install ""gcc>=13.2.0,<14""  # AND
(Here, quotation marks are needed to protect the `<` and `>` symbols from the shell.)
You can also search for available versions, using for example:
mamba search gcc
To list the packages installed in the currently activated environment, you can
type `mamba list`.  This should normally indicate that all packages are from the
`conda-forge` channel.  You can install from a channel other than `conda-forge`
by using the `-c` option, for example:
mamba install -c ncas cf-python   # install cf-python from 'ncas' channel
but for licensing reasons, **do not use the Anaconda defaults channel**.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/creating-and-using-miniforge-environments#installing-conda-packages,933,145
Running packages from your conda environment,"In order to run packages from a conda environment that you installed
previously, you will first need to activate the environment in the session
that you are using. This means repeating some of the commands typed above. Of
course, you will not need to repeat the steps to create the environment or
install the software, but the following may be needed again:
module unload jaspy
source ~/miniforge3/bin/activate
conda activate myenv
",https://help.jasmin.ac.uk/docs/software-on-jasmin/creating-and-using-miniforge-environments#running-packages-from-your-conda-environment,432,70
Installing pip packages,"Many python packages that are available via PyPI are also available as conda
packages in conda-forge, and it is generally best to use these via `mamba install`
as above.
Nonetheless, you can also install pip packages (as opposed to conda packages)
into your conda environment. However, first you should type:
mamba install pip
before typing the desired commands such as
pip install numpy
If you do not install pip into your sub-environment, then either:
- your shell will fail to find the `pip` executable, or
- your shell will find `pip` in some other location, which might lead to pip packages being installed in an unexpected location, possibly resulting in interference between your environments
Explicitly installing pip into your sub-environment will guard against this.
Before running `pip install`, you could check that `pip` is being run from the correct place, by typing:
which pip
and it should report something like:
~/miniforge3/envs/myenv/bin/pip
",https://help.jasmin.ac.uk/docs/software-on-jasmin/creating-and-using-miniforge-environments#installing-pip-packages,961,151
Cloning a conda environment,"There can be occasions when you wish to create a conda environment which is
based on the contents of an existing environment. An example when you might
wish to do this is in order to create an environment of your own which is
based on Jaspy but with certain changes such as the addition or removal of
certain packages. (Recall that your environment cannot be activated at the
same time as Jaspy.)
To do this, you can export a list of packages to a YAML file and use this file
to create the new environment -- as follows:
- first activate the conda environment that you wish to clone (for Jaspy, load the jaspy module)
- export a list of contents to a YAML file (for example `environment.yml`) by typing  
mamba env export > environment.yml
- deactivate this environment (or as the case may be, unload the jaspy module)
- ensure that the relevant base environment is activated
- create the new environment (for example `my_new_env`) by using:  
mamba env create -n my_new_env -f environment.yml
Note the use of `mamba env create`, rather than `mamba create` as above.  
You might also edit the YAML file before using it to create the environment if
you do not want an exact clone -- for example, adding packages, removing
packages that are not of relevance, or removing version requirements in order
to give mamba more flexibility about what versions to install
(for example if you do not require particular legacy versions, or if the
versions in the original environment are no longer available).
",https://help.jasmin.ac.uk/docs/software-on-jasmin/creating-and-using-miniforge-environments#cloning-a-conda-environment,1497,260
Varying the installation location,"(This is the section that is referred to above, where running the Miniforge
installer is discussed.)
The default installation location offered by the installer for your base
environment will be `~/miniforge3` (that is, a `miniforge3` subdirectory of
your home directory). We recommend accepting this default, or using another
location under your home directory. It is possible to change this, but note
that a conda environment can have tens of thousands of files and that group
workspaces on JASMIN will generally perform poorly for this use case. If you
need to make a conda environment which is shared with collaborators, you may
need to request a {{<link ""share-software-envs"">}}small files GWS{{</link>}}{{< ref """" >}} as
these will give better performance.
If you are creating a conda environment for very short-term testing only, you
may find best performance using `/tmp` due to the large number of files.
However, you may need several gigabytes, which is too big for the `/tmp` areas
on most of the `sci` machines at time of writing, although the physical
sci machines (currently `sci-ph-01` and `sci-ph-02` for Rocky 9)
have larger `/tmp` areas.
Choose an appropriate machine, make use of the `df`
command to check available disk space, and ensure that you do not fill up `/tmp`
as this would impact negatively on other users.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/creating-and-using-miniforge-environments#varying-the-installation-location,1336,216
Software Overview,"JASMIN is a large platform where a range of software tools, packages and
environments are available. Many users employ software already installed on
JASMIN whilst some need to install their own tools for a particular purpose.
This page provides an overview of the software on JASMIN. It links to further
information about a range of tools and environments.
To help get you started, these have been split into categories:
- Software available to all on JASMIN analysis/batch servers
- Additional tools for compiling and building software
- Restricted software
- Server-specific software
- Data movement software
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-overview,611,96
Which software should I use?,"There are a lot of different options when you are trying to work out which
tools and/or environments to use on JASMIN. Here are some quick questions to
help you get started:
1\. Do you want to use NAME, JULES, MOOSE or the NAG libraries?
- If yes, see: Restricted software
2\. Do you want a workflow management tool or a graphical Linux desktop?
- If yes, see: Server-specific software
3\. Do you want tools for transferring data or migrating it to/from tape?
- If yes, see: Data movement software
4\. If you need anything else:
- See: Software available to all on JASMIN analysis/batch servers
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-overview#which-software-should-i-use?,595,106
Software available on all sci/batch nodes,,https://help.jasmin.ac.uk/docs/software-on-jasmin/software-overview#software-available-on-all-sci/batch-nodes,0,0
Data analysis and visualisation tools,"If you are looking for software packages and environments that allow you to
analyse, process and visualise data then take a look at these options:
- Jaspy software environments (Python, R and other tools)
- The ""jasmin-sci"" software environment (packages not provided by Jaspy)
- Additional packages (provided under: ""/apps/jasmin"")
- IDL (and MIDL)
- Creating your own software environments
**NOTE** : If you are using Matplotlib to visualise data please refer to the
advice on our [Matplotlib help page]({{% ref ""matplotlib"" %}}).
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-overview#data-analysis-and-visualisation-tools,533,83
"Jaspy Software Environments (Python, R and other tools)","Jaspy is a toolkit for managing and deploying Conda environments that include
both Python and non-Python packages. Jaspy environments, along with the
""jasmin-sci"" environment (see below), provide the main software on the
scientific analysis servers and LOTUS cluster on JASMIN. Details of the Jaspy
environments and packages are available on the [Jaspy page]({{% ref ""jaspy-envs"" %}}).
","https://help.jasmin.ac.uk/docs/software-on-jasmin/software-overview#jaspy-software-environments-(python,-r-and-other-tools)",386,56
"The ""jasmin-sci"" Software Environment","The ""jasmin-sci"" software environment is intended as a supplement to Jaspy
(see above). It contains extra software packages for use with scientific data
analysis which, for various reasons, are not provided as part of Jaspy itself.
Details of this environment are provided on the 
[""jasmin-sci"" software page]({{% ref ""jasmin-sci-software"" %}}).
","https://help.jasmin.ac.uk/docs/software-on-jasmin/software-overview#the-""jasmin-sci""-software-environment",346,50
Additional packages,"A number of additional packages are available under the ""/apps/jasmin/""
directory scientific analysis servers and LOTUS cluster. Details of these
packages are provided on the [additional sofware packages page]({{% ref ""additional-software"" %}}).
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-overview#additional-packages,246,32
IDL,"{{< link ""https://www.nv5geospatialsoftware.com/Products/IDL"" >}}IDL{{</link>}} stands for
Interactive Data Language. It is a licensed data manipulation toolkit made
available on JASMIN. IDL is available on the JASMIN scientific
analysis servers and LOTUS cluster. See [IDL]({{% ref ""idl"" %}}).
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-overview#idl,295,37
Creating your own software environments,"If you intend to create your own software environments then please take a look
at the following pages:
  * [Building virtual environments on top of Jaspy environments]({{% ref ""python-virtual-environments"" %}})
  * [Sharing your JASMIN software environments with other users]({{% ref ""share-software-envs"" %}})
  * Compilers on JASMIN
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-overview#creating-your-own-software-environments,335,46
Restricted software available on specific servers,,https://help.jasmin.ac.uk/docs/software-on-jasmin/software-overview#restricted-software-available-on-specific-servers,0,0
Workflow Management with Rose and Cylc,"Rose and Cylc provide a suite of tools available for managing sophisticated
multi-step workflows. See full details on the 
[Rose and Cylc page]({{% ref ""rose-cylc-on-jasmin"" %}}).
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-overview#workflow-management-with-rose-and-cylc,180,26
Graphical Linux desktop access using NoMachine NX,"NoMachine NX is a tool that allows users to run a virtual graphical Linux
desktop on JASMIN. See details on the [NX page]({{% ref ""graphical-linux-desktop-access-using-nx"" %}}).
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-overview#graphical-linux-desktop-access-using-nomachine-nx,178,26
Data movement software,,https://help.jasmin.ac.uk/docs/software-on-jasmin/software-overview#data-movement-software,0,0
Data transfer,"There are numerous tools for transferring data to/from JASMIN. Please consult
the [Data Transfer Tools page]({{% ref ""data-transfer-tools"" %}}) for details.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-overview#data-transfer,157,21
Data migration disk/tape,"The Joint Data Migration App, or JDMA, is a flexible tool for managing
large migrations of data between a range of storage media. On JASMIN, it can
be used for migrating data to/from tape, disk and object-store. See more
details on the [JDMA page]({{% ref ""jdma"" %}}).
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-overview#data-migration-disk/tape,269,47
Still have a question?,"Please consult the [JASMIN software FAQs]({{% ref ""jasmin-software-faqs"" %}}).
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-overview#still-have-a-question?,79,9
Running python on JASMIN,"On the JASMIN [scientific analysis servers]({{% ref ""sci-servers"" %}}) and on the Lotus batch cluster, we currently support Python version 3.11 with [Jaspy]({{% ref ""jaspy-envs"" %}}).
When you first log in, the default version of Python is that provided by the
operating system. This is different to the one you shoud use for your work,
and we recommend using the [Jaspy environments]({{% ref ""jaspy-envs"" %}}). In this example, we
activate the current Jaspy environment before running Python.
{{<command user=""user"" host=""sci-vm-01"">}}
module load jaspy
{{</command>}}
Check the Python version:
{{<command user=""user"" host=""sci-vm-01"">}}
python -V
(out)Python 3.11.9
{{</command>}}
Run a script:
{{<command user=""user"" host=""sci-vm-01"">}}
python your_script.py
{{</command>}}
If you want to use an executable script (which can be invoked just by name),
then the recommended line to put at the top of the script would be:
#!/usr/bin/env python
after which you should set ""write"" permission, and then you can run it without
the ""python "" prefix:
{{<command user=""user"" host=""sci-vm-01"">}}
chmod 755 your_script.py
./your_script.py
{{</command>}}
You should work with Python on the [scientific analysis servers]({{% ref ""sci-servers"" %}}).
Login servers do not have any software installed, or filesystems mounted other than home directories.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/running-python-on-jasmin,1340,190
Python Virtual Environments,"This article describes how you can use ""virtual environments"" to install
Python packages that are not provided in the
[common software environments on JASMIN]({{% ref ""software-overview"" %}}). You might wish to do this if you
want to use different packages/versions from those installed on the system, or
if you have requested for a package to be installed system-wide but wish to
start using it before this request can be acted upon.
To decide whether you should use a _Python virtual environment_ or a _Conda
environment_ for this purpose, see:
[overview of software environments]({{% ref ""conda-environments-and-python-virtual-environments"" %}}).
",https://help.jasmin.ac.uk/docs/software-on-jasmin/python-virtual-environments,650,96
"What is a ""virtual environment""?","A ""virtual environment"" is a self-contained directory tree that contains a
Python installation for a particular version of Python (such as 2.7, 3.7,
3.8), plus a number of additional packages. It provides a very useful method
for managing multiple environments on a single platform that can be used by
different applications.
","https://help.jasmin.ac.uk/docs/software-on-jasmin/python-virtual-environments#what-is-a-""virtual-environment""?",326,51
Creating a virtual environment,"**As a pre-requisite, when using any modern Python (i.e. Python2.7 onwards),
you should [activate a Jaspy environment]({{% ref ""quickstart-software-envs"" %}})
before following the instructions below.**
Python allows you to create a directory containing a private virtual
environment, into which you can install your packages of choice. This is done
differently for python2 and python3, as follows:
{{<command user=""user"" host=""sci1"">}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/python-virtual-environments#creating-a-virtual-environment,436,59
Python 3 onwards:,"python -m venv /path/to/my_virtual_env
(out)
",https://help.jasmin.ac.uk/docs/software-on-jasmin/python-virtual-environments#python-3-onwards:,45,5
Python 2.7 (deprecated),"virtualenv /path/to/my_virtual_env
{{</command>}}
The path can be an absolute or relative path, but it should not already exist.
Note: `/path/to/my_virtual_env` here (and also in the commands shown below)
should be replaced by the actual path where you choose to create your virtual
environment.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/python-virtual-environments#python-2.7-(deprecated),296,43
"Using the system ""site-packages"" with your virtual environment","Note that if you create a virtual environment using the above syntax, the
packages initially installed in it will **only be those in the standard Python
library**. This means, for example, that the `numpy` package (not in the
standard library, but installed as part of Jaspy) will be unavailable unless
you install it yourself. If you would prefer as a starting point to have all
the add-on packages which have already been installed in Jaspy, then use
instead:
{{<command user=""user"" host=""sci1"">}}
python -m venv --system-site-packages /path/to/my_virtual_env
{{</command>}}
This will work for most packages in Jaspy. We have seen situations where one
or two packages from Jaspy do not work in private virtual environments, and if
you are affected by this then please see the ""package-specific fixes"" section
below.
","https://help.jasmin.ac.uk/docs/software-on-jasmin/python-virtual-environments#using-the-system-""site-packages""-with-your-virtual-environment",818,128
Activating a virtual environment,"Before the virtual environment can be used, it needs to be "" **activated** "".
This is done by running the `activate` script using the `source` command:
{{<command user=""user"" host=""sci1"">}}
source /path/to/my_virtual_env/bin/activate
{{</command>}}
(If you prefer, you can use `.` instead of `source`.)
After you run the activate script, some environment variables will be set so
that the `python` (or `python2.7` (deprecated), `python3`) command will point to the one in the
virtual environment, allowing installation and use of packages in that
environment.
You can see that `python` points to the python executable in the virtual
environment, with:
{{<command user=""user"" host=""sci1"">}}
which python
(out)/home/users/my_username/my_virtual_env/bin/python
{{</command>}}
Note that you have to source the `activate` script in **every** shell (login
session) in which you intend to use the virtual environment. If there is a
particular virtual environment which you want to use consistently, you might
consider putting the command to source the `activate` script in your
`$HOME/.bashrc` file.
If you wish to deactivate the currently active virtual environment in a
particular shell, just type `deactivate`. The environment variable changes
will be undone, and you will again be using the system default set of
packages. This is also reflected in the shell prompt.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/python-virtual-environments#activating-a-virtual-environment,1364,198
Installing packages into a virtual environment,"Once you have activated a virtual environment, the `pip` utility will be
available. This allows package installation into the environment using the
command:
{{<command user=""user"" host=""sci1"">}}
pip install your_package
{{</command>}}
`pip` is quite flexible what you can use for `your_package`. It can include:
- a package name in the {{< link ""https://pypi.python.org/pypi"" >}}Python Package Index (PyPI){{</link>}}
- a URL pointing to a package repository
- the local path of a `.tar.gz` or `.zip` file containing the package source
- the local path of a directory containing the extracted package source
- the download URL of a `.tar.gz` or `.zip` file
If the package requires other packages that are not already installed into the
virtual environment, then `pip` will use the package's requirements file to
install them automatically from PyPI.
One thing to consider when doing this, is that some temporary space is needed
by the install process. The location of this temporary space may be set by default
to `/tmp`, which is restricted on the `sci` machines.
You might see this error, despite having ample free space in your own home directory:
ERROR: Could not install packages due to an OSError: \
[Errno 122] Disk quota exceeded
In order to avoid encountering this, 
please [follow this advice]({{% ref ""storage#avoid-inadvertently-writing-to-tmp"" %}}) to over-ride the `TMPDIR` environment variable, setting it to the location of somewhere you know have free space. Don't forget to clean up afterwards!
To upgrade an existing package, use:
{{<command user=""user"" host=""sci1"">}}
pip install --upgrade your_package
{{</command>}}
If your Python package cannot be installed with `pip` for any reason, it can
also be installed directly from the `setup.py` file after activating the
virtual environment.
{{<command user=""user"" host=""sci1"">}}
python setup.py install
{{</command>}}
To install a specific version of a package, this can be specified with:
{{<command user=""user"" host=""sci1"">}}
pip install your_package==1.2.3
{{</command>}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/python-virtual-environments#installing-packages-into-a-virtual-environment,2043,302
Inspecting the virtual environment,"To list the packages installed into the virtual environment, with their
version numbers, type:
{{<command user=""user"" host=""sci1"">}}
 pip freeze
{{</command>}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/python-virtual-environments#inspecting-the-virtual-environment,160,20
Using the virtual environment,,https://help.jasmin.ac.uk/docs/software-on-jasmin/python-virtual-environments#using-the-virtual-environment,0,0
Interactive use,"After you have activated the virtual environment in your shell, any packages
that you have installed into it can be imported into an interactive python
session.
{{<command user=""user"" host=""sci1"">}}
python # automatically uses python in your virtualenv
{{</command>}}
The prompt changes to `>>>`:
{{<command user=""user"" prompt="">>>"">}}
import my_package
{{</command>}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/python-virtual-environments#interactive-use,369,49
Scripts,"If a script is run using the `python` command on the command-line in a similar
way to when starting an interactive Python session, this will use any virtual
environment that has been activated in the calling shell.
{{<command user=""user"" host=""sci1"">}}
python my_script.py
{{</command>}}
If an executable script is run using the `#!` mechanism, and the first line of
the script has the hard-coded path to the executable in the virtual
environment, then it is not necessary to activate the virtual environment in
the calling shell.
{{<command user=""user"" host=""sci1"">}}
head -n 1 myscript.py  # show the first line
(out)#!/path/to/my_virtual_env/bin/python3.7
(out)
chmod u+x myscript.py  # ensure that it is executable
(out)
./myscript.py  # run it
{{</command>}}
As an alternative to hard-coding the path of the virtual environment, it is
possible to use the `/usr/bin/env` approach to ensure that the script is run
using whichever python executable is found via `$PATH`. The script will then
run using any virtual environment that has been activated in the calling
shell. This makes the script more portable, although at the expense of having
to source the activate script.
{{<command user=""user"" host=""sci1"">}}
head -n 1 myscript.py
(out)#!/usr/bin/env python3.7
(out)
chmod u+x myscript.py
(out)
./myscript.py
{{</command>}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/python-virtual-environments#scripts,1329,198
Package-specific fixes,"When using the `--system-site-packages` option in combination with Jaspy, it
has been found that some packages provided by Jaspy (and which work correctly
in the Jaspy environment itself) require fixes in order to use them in virtual
environments that are created on top of Jaspy. In particular:
- if you use `shapely`, we suggest to reinstall this into your virtual environment using `pip install --ignore-installed shapely` after activating the environment
- if you use `geopandas`, you will need to reinstall shapely as above, and also when running python you will need to set an environment variable to enable it to find the `spatialindex` library. After loading Jaspy and activating the virtual environment, you could use either one of the following: 
  - `export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH`
  - `export LD_PRELOAD=$CONDA_PREFIX/lib/libspatialindex.so:$CONDA_PREFIX/lib/libspatialindex_c.so`
Note that these environment variables could potentially also affect the
behaviour of other Linux commands, although unlikely, so you might prefer to
set them only for the python session (using a command of the form `env
variable_name=value python`) rather than using `export`.
- if you use `cartopy` (also used by `iris`), you may need to create a symbolic link into your virtual environment to allow the correct loading of `libgeos_c.so` during `import cartopy` or `import iris`. To do this: 
{{<command user=""user"" host=""sci1"">}}
ln -s $CONDA_PREFIX/lib/libgeos_c.so /path/to/my_virtual_env/lib/
{{</command>}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/python-virtual-environments#package-specific-fixes,1534,213
'JASMIN software changes,"{{<alert type=""danger"">}}
This article is now out of date, but a similar exercise is about to get underway
for the migration of the JASMIN platform from CentOS7 operating system (end of life June 2024) to Rocky Linux 9. Look out for futher details in due course.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-migration-2020,276,47
Introduction,"This article explains the changes in the provision of common software on
JASMIN (""sci"" servers and LOTUS) when upgrading to main operating system from
RedHat Enterprise Linux 6 (RHEL6) to CentOS7. It answers for the following
questions:
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-migration-2020#introduction,237,37
What is changing?,"The upgrade from RHEL6 to introduces a new way of working with software
environments on JASMIN.
The previous system involved installations via RPM using a collection of
packages known as the ""JASMIN Analysis Platform"" (JAP).
The current system uses two approaches to providing software environments and
packages:
  1. Jaspy environments – a collection of software environments that you can ""activate"" for use.
  2. ""jasmin-sci"" environment – a single set of software packages installed separately because they were difficult to include in Jaspy.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-migration-2020#what-is-changing?,546,82
Which systems are affected?,"This change applies to all generally available JASMIN servers and the LOTUS
cluster. These include:
- jasmin-sci*
- cems-sci*
- jasmin-cylc
- cron
- All LOTUS nodes (accessed via LSF)
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-migration-2020#which-systems-are-affected?,184,30
Why is it changing?,"The reason for moving away from the JAP was to provide a system that could
support multiple versions of software packages, and environments, on a single
platform. From the perspective of reproducible science, the [Jaspy]({{% ref ""jaspy-envs"" %}}) approach is more useful because it:
- keeps previous environments on the system when a new version of an environment is launched
- includes a listing of all packages (and their versions) that are provided in each date-stamped software environment
From a management point-of-view, Jaspy builds on the packaging tool
""[conda](https://docs.conda.io/en/latest/)"" and the community repositories
known as ""[conda-forge](https://conda-forge.org/)"". These tools are widely
used in the scientific community and provide many components that are re-used
in Jaspy.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-migration-2020#why-is-it-changing?,800,114
"Comparison of packages in old JASMIN Analysis Platform and new Jaspy/""jasmin-sci"" environments","Most packages that were previously provided as part of the JASMIN Analysis
Platform (JAP) are now included in Jaspy environments or the ""jasmin-sci""
environment. Some packages have however been dropped. The treatment of certain
packages is documented on the ""[extra-sci-
packages](https://github.com/cedadev/extra-sci-packages)"" GitHub repository. A
full table of packages that were in JAP and their mappings to Jaspy/""jasmin-
sci"" is provided below.
","https://help.jasmin.ac.uk/docs/software-on-jasmin/software-migration-2020#comparison-of-packages-in-old-jasmin-analysis-platform-and-new-jaspy/""jasmin-sci""-environments",451,61
How do I get started (quickly)?,"If you want to access most of the packages you can find them in the most
recent Jaspy environment. This can be activated using:
module load jaspy
If you need packages that are provided in the ""jasmin-sci"" environment then
you can activate it using:
module load jasmin-sci
",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-migration-2020#how-do-i-get-started-(quickly)?,272,47
Full table of packages from JAP,"Here is a full table of packages that were provided via JAP, and how they are
provided for use on CentOS7 machines (as of initial release).
In the summary column:
- **C** \- package is provided via Conda environment (i.e. Jaspy)
- **R** \- package is provided via an RPM (as part of the base OS or via jasmin-sci)
- **CR** \- package is provided both via Jaspy and via an RPM. The version in Jaspy is recommended for use with user code; the RPM version is also installed in order to satisfy an RPM dependency only, and may be subject to change.
- **N** \- not provided (mostly these have been deprecated by the third-party sources)
**NAME (in JAP)** | **Version in Jaspy 3.7 (conda)** | **Version in Jaspy 2.7 (conda)** | **Version in RPMs (base or jasmin-sci SCL)** | **Summary:** **(C)onda/** **(R)PM /** **(N)one** | **Comments**  
---|---|---|---|---|---  
arpack  |  3.6.3  |  3.6.3  |  |  C  |  
atlas  |  |  3.8.4  |  3.10.1  |  R  |  
atlas-devel  |  |  3.8.4  |  3.10.1  |  R  |  
bbcp  |  |  |  |  N  |  deprecated  
bbcp-config  |  |  |  |  N  |  deprecated  
blas  |  1.1  |  1.0  |  |  C  |  
boost-devel  |  |  1.70.0  |  |  C  |  
cdo  |  1.9.5  |  |  |  C  |  
cmip6-cmor-tables  |  |  |  |  N  |  groups advised to maintain own CMOR  
cmor-libs  |  |  |  |  N  |  groups advised to maintain own CMOR  
coda  |  |  0.19_1  |  |  C  |  
ddd  |  |  |  3.3.12  |  R  |  
diffuse  |  |  |  0.4.8  |  R  |  
dvipng  |  |  |  1.14  |  R  |  package texlive-dvipng-svn26689  
eccodes  |  2.9.2  |  |  |  C  |  
eccodes-devel  |  2.9.2  |  |  |  C  |  
eccodes-fortran  |  2.9.2  |  |  |  C  |  
eccodes-python27  |  2.9.2  |  |  |  C  |  
emacs  |  |  26.1  |  24.3  |  R  |  _emacs is broken in jaspy 2.7_ \-
/usr/bin/emacs may be required to override  
emacs-common-ess  |  |  |  |  N  |  might provide later  
emacs-ess  |  |  |  |  N  |  might provide later  
emacs-ess-el  |  |  |  |  N  |  might provide later  
emacs-gnuplot  |  |  |  4.6.2  |  R  |  
emos  |  |  |  |  N  |  deprecated  
esmf  |  7.1.0r  |  7.1.0r  |  |  C  |  
esmf-doc  |  |  |  |  N  | See [here](http://www.earthsystemmodeling.org/esmf_releases/public/last/ESMF_usrdoc/) and ESMF_refdoc  
esmf-python27  |  7.1.0r  |  7.1.0r  |  |  C  |  
ferret  |  |  |  7.5.0  |  R  |  
fftw  |  3.3.8  |  3.3.8  |  |  C  |  
fftw-devel  |  3.3.8  |  3.3.8  |  |  C  |  
firefox  |  |  |  |  N  |  
flex-devel  |  |  2.6.4  |  |  C  |  
gcc-gfortran  |  |  |  4.8.5  |  R  |  
gdal  |  2.2.4  |  2.0.0  |  |  C  |  
gdal-devel  |  |  |  |  N  |  gdal provided as ""gdal""  
gdal-doc  |  |  |  |  N  |  gdal provided as ""gdal""  
gdal-java  |  |  |  |  N  |  
gdal-javadoc  |  |  |  |  N  |  
gdal-libs  |  |  |  |  N  |  
gdal-perl  |  |  |  |  N  |  
gdal-python27  |  2.2.4  |  2.0.0  |  |  C  |  
geany  |  |  |  1.31  |  R  |  
geos  |  3.6.2  |  3.7.1  |  |  C  |  
geos-devel  |  3.6.2  |  3.7.1  |  3.4.2  |  R  |  
git  |  2.20.1  |  2.20.1  |  |  C  |  
gitk  |  |  |  1.8.3.1  |  R  |  
glibc-static  |  |  |  2.17  |  R  |  
gnuplot  |  |  |  4.6.2  |  R  |  
grads  |  |  |  2.0.2  |  R  |  
GraphicsMagick-c++  |  |  |  1.3.32  |  R  |  
graphviz  |  2.38.0  |  2.38.0  |  |  C  |  
graphviz-gd  |  |  |  |  N  |  
graphviz-python27  |  2.38.0  |  2.38.0  |  |  C  |  
grass  |  |  |  6.4.4  |  R  |  
grass-devel  |  |  |  6.4.4  |  R  |  
grass-libs  |  |  |  6.4.4  |  R  |  
grib_api  |  |  |  |  N  |  deprecated - use eccodes  
grib_api-devel  |  |  |  |  N  |  deprecated - use eccodes  
grib_api-fortran  |  |  |  |  N  |  deprecated - use eccodes  
grib_api-python27  |  |  |  |  N  |  deprecated - use eccodes  
gsl  |  2.2.1  |  2.5  |  |  C  |  
gsl-devel  |  2.2.1  |  2.5  |  |  C  |  
gsl-static  |  2.2.1  |  2.5  |  |  C  |  
gtk2  |  2.24.31  |  |  |  C  |  
gtk2-devel  |  2.24.31  |  |  |  C  |  
gv  |  |  |  3.7.4  |  R  |  
hdf  |  4.2.13  |  4.2.13  |  |  C  |  
hdf-devel  |  4.2.13  |  4.2.13  |  |  C  |  
hdf5  |  1.10.3  |  1.10.1  |  |  C  |  
hdf5-devel  |  1.10.3  |  1.10.1  |  |  C  |  
hdfeos2  |  2.2  |  2.20  |  20.1.00  |  CR  |  
hdfeos5  |  5.1.16  |  5.1.16  |  |  C  |  
ImageMagick  |  7.0.8_16  |  7.0.8_10  |  |  C  |  
JAGS  |  |  |  |  N  |  available as ""rjags""  
jasper-devel  |  1.900.1  |  2.0.14  |  |  C  |  
ksh  |  |  |  20120801  |  R  |  
lapack  |  3.6.1  |  3.6.1  |  |  C  |  
lapack-devel  |  3.6.1  |  3.6.1  |  |  C  |  
leafpad  |  |  |  0.8.18  |  R  |  
libcdms  |  3.1.0  |  3.0.1  |  |  C  |  
libcrayutil  |  |  |  20121128  |  R  |  
libcurl-devel  |  |  |  7.29.0  |  R  |  
libdrs  |  3.1.0  |  3.1.0  |  3.1.2  |  CR  |  
libuuid-devel  |  2.32.1  |  2.32.1  |  2.23.2  |  CR  |  
llvm-devel  |  |  3.3  |  |  C  |  
lxterminal  |  |  |  0.3.2  |  R  |  
mercurial  |  |  4.9.1  |  2.6.2  |  R  |  
mo_unpack  |  |  |  2.0.1  |  R  |  
mtk  |  |  |  1.4.5  |  R  |  
mtk-devel  |  |  |  1.4.5  |  R  |  
mtk-python27  |  |  |  |  N  |  see build instructions for user  
ncBrowse  |  |  |  |  N  |  deprecated  
nccmp  |  |  |  1.8.3.1  |  R  |  
ncl  |  6.5.0  |  |  |  C  |  
nco  |  4.7.8  |  |  |  C  |  
nco-devel  |  4.7.8  |  |  |  C  |  
ncview  |  |  |  2.1.2  |  R  |  
nedit  |  |  |  5.7  |  R  |  
netcdf  |  4.6.1  |  4.6.1  |  4.3.3.1  |  CR  |  called libnetcdf in conda  
netcdf-c++  |  4.2.1  |  4.3.0  |  |  C  |  called netcdf-cxx4 in conda  
netcdf-c++-devel  |  4.2.1  |  4.3.0  |  |  C  |  called netcdf-cxx4 in conda  
netcdf-devel  |  4.6.1  |  4.6.1  |  4.3.3.1  |  CR  |  called libnetcdf in
conda  
netcdf-fortran  |  4.4.4  |  4.4.4  |  4.2  |  CR  |  
netcdf-fortran-devel  |  4.4.4  |  4.4.4  |  4.2  |  CR  |  
octave  |  |  |  3.8.2  |  R  |  
octave-devel  |  |  |  3.8.2  |  R  |  
octave-doc  |  |  |  |  N  |  docs are on <https://octave.org/doc/>  
octave-netcdf  |  |  |  1.0.6  |  R  |  
octave-octcdf  |  |  |  |  N  |  deprecated  
p7zip  |  |  |  16.02  |  R  |  
parallel  |  |  20190522  |  |  C  |  
pdftk  |  |  |  |  N  |  discontinued  
perl-core  |  |  |  5.16.3  |  R  |  
perl-devel  |  |  |  5.16.3  |  R  |  
perl-Image-ExifTool  |  |  |  11.7  |  R  |  
perl-XML-Parser  |  |  2.44_01  |  |  C  |  
postgresql-devel  |  |  |  9.2.24  |  R  |  
proj  |  4.9.3  |  5.2.0  |  4.8.0  |  CR  |  called proj4 in conda  
proj-devel  |  4.9.3  |  5.2.0  |  4.8.0  |  CR  |  called proj4 in conda  
proj-epsg  |  4.9.3  |  5.2.0  |  4.8.0  |  CR  |  called proj4 in conda  
proj-nad  |  4.9.3  |  5.2.0  |  4.8.0  |  CR  |  called proj4 in conda  
proj-static  |  4.9.3  |  5.2.0  |  4.8.0  |  CR  |  called proj4 in conda  
python27  |  3.7.1  |  2.7.15  |  |  C  |  
python27-alabaster  |  0.7.12  |  0.7.12  |  |  C  |  
python27-astral  |  |  1.9.2  |  |  C  |  
python27-Babel  |  2.6.0  |  2.7.0  |  |  C  |  
python27-backports-common  |  |  1.0  |  |  C  |  
python27-backports-functools_lru_cache  |  |  1.5  |  |  C  |  
python27-backports-ssl_match_hostname  |  |  1.0  |  |  C  |  
python27-basemap  |  1.2.0  |  1.2.0  |  |  C  |  
python27-biggus  |  |  0.15.0  |  |  C  |  
python27-boto3  |  1.9.67  |  1.9.188  |  |  C  |  
python27-botocore  |  1.12.68  |  1.12.188  |  |  C  |  
python27-cartopy  |  0.17.0  |  0.16.0  |  |  C  |  
python27-ccplot  |  |  |  |  P  |  
python27-cdat_lite  |  |  |  |  N  |  deprecated  
python27-cerbere  |  |  |  |  N  |  
python27-certifi  |  2018.11.29  |  |  |  C  |  
python27-cf  |  |  2.3.6  |  |  C  |  called cf-python in conda  
python27-cf-checker  |  |  3.1.1  |  |  C  |  
python27-cf-plot  |  |  2.4.10  |  |  C  |  
python27-cf-units  |  2.0.2  |  2.1.1  |  |  C  |  
python27-cf-view  |  |  |  |  N  |  no longer supported  
python27-cftime  |  1.0.3.4  |  1.0.1  |  |  C  |  
python27-chardet  |  3.0.4  |  3.0.4  |  |  C  |  
python27-cis  |  |  1.6.0  |  |  C  |  
python27-cloudpickle  |  0.6.1  |  1.2.1  |  |  C  |  
python27-cmor  |  |  |  |  N  |  groups advised to maintain own CMOR  
python27-cycler  |  0.10.0  |  0.10.0  |  |  C  |  
python27-Cython  |  0.29.2  |  0.29.12  |  |  C  |  
python27-dask  |  1.0.0  |  1.2.2  |  |  C  |  
python27-dateutil  |  |  |  |  C  |  As ""dateutil""  
python27-descartes  |  1.1.0  |  1.1.0  |  |  C  |  
python27-docutils  |  0.14  |  0.14  |  |  C  |  
python27-ecmwf-api-client  |  |  |  |  N  |  To be added soon.  
python27-emcee  |  2.2.1  |  2.2.1  |  |  C  |  
python27-enum34  |  |  1.1.6  |  |  C  |  
python27-eofs  |  1.3.1  |  1.4.0  |  |  C  |  
python27-esgf-pyclient  |  |  0.1.8  |  |  C  |  
python27-filelock  |  3.0.10  |  3.0.10  |  |  C  |  
python27-Fiona  |  1.7.13  |  |  |  C  |  
python27-geopandas  |  0.4.0  |  |  |  C  |  
python27-h5py  |  2.8.0  |  2.8.0  |  |  C  |  
python27-httplib2  |  |  0.13.0  |  |  C  |  
python27-idna  |  2.8  |  2.8  |  |  C  |  
python27-ilamb  |  |  2.3.1  |  |  C  |  
python27-ImageHash  |  4.0  |  4.0  |  |  C  |  
python27-imagesize  |  1.1.0  |  1.1.0  |  |  C  |  
python27-ipython  |  7.2.0  |  5.8.0  |  |  C  |  
python27-iris_sample_data  |  2.1.0  |  2.1.0  |  |  C  |  
python27-iris-grib  |  |  0.12.0  |  |  C  |  
python27-jinja2  |  2.1  |  2.10.1  |  |  C  |  
python27-jmespath  |  0.9.3  |  0.9.4  |  |  C  |  
python27-joblib  |  0.13.0  |  0.13.2  |  |  C  |  
python27-Jug  |  1.6.7  |  1.6.8  |  |  C  |  
python27-kiwisolver  |  1.0.1  |  1.1.0  |  |  C  |  
python27-latexcodec  |  1.0.5  |  1.0.7  |  |  C  |  
python27-locket  |  0.2.0  |  0.2.0  |  |  C  |  
python27-MarkupSafe  |  1.1.0  |  1.1.1  |  |  C  |  
python27-matplotlib  |  3.0.2  |  2.2.2  |  |  C  |  
python27-mo_pack  |  0.2.0  |  0.2.0  |  |  C  |  
python27-mock  |  2.0.0  |  3.0.5  |  |  C  |  
python27-mpi4py-mpich  |  |  3.0.1  |  |  C  |  
python27-mpmath  |  1.1.0  |  1.1.0  |  |  C  |  
python27-nappy  |  1.1.4  |  1.2.1  |  |  C  |  
python27-nc-time-axis  |  1.1.0  |  1.1.0  |  |  C  |  
python27-netCDF4  |  1.4.2  |  1.3.1  |  |  C  |  
python27-nose  |  1.3.7  |  1.3.7  |  |  C  |  
python27-numpy  |  1.15.4  |  1.15.4  |  |  C  |  
python27-packaging  |  18.0  |  19.0  |  |  C  |  
python27-pandas  |  0.23.4  |  0.24.2  |  |  C  |  
python27-partd  |  0.3.9  |  1.0.0  |  |  C  |  
python27-patsy  |  0.5.1  |  0.5.1  |  |  C  |  
python27-pep8  |  1.7.1  |  1.7.1  |  |  C  |  
python27-Pillow  |  5.3.0  |  5.2.0  |  |  C  |  
python27-psutil  |  5.4.8  |  5.6.3  |  |  C  |  
python27-psycopg2  |  2.7.6.1  |  2.7.7  |  |  C  |  
python27-pybtex  |  0.22.0  |  0.22.2  |  |  C  |  
python27-pycairo  |  1.18.0  |  1.16.3  |  |  C  |  
python27-pycodestyle  |  2.4.0  |  2.5.0  |  |  C  |  
python27-Pydap  |  3.2.2  |  3.2.2  |  |  C  |  
python27-pygeode  |  |  1.2.2  |  |  C  |  
python27-Pygments  |  2.3.1  |  2.4.2  |  |  C  |  
python27-pygobject2  |  |  3.28.3  |  |  C  |  pygobject in conda  
python27-pygrib  |  2.0.3  |  2.0.2  |  |  C  |  
python27-pygtk2  |  |  |  |  C  |  
python27-pygtk2-libglade  |  |  |  |  N  |  
python27-pyhdf  |  0.9.10  |  0.10.1  |  |  C  |  
python27-pyke  |  1.1.1  |  1.1.1  |  |  C  |  
python27-pyparsing  |  2.3.0  |  2.4.0  |  |  C  |  
python27-pyproj  |  1.9.5.1  |  1.9.6  |  |  C  |  
python27-pyshp  |  2.0.0  |  2.1.0  |  |  C  |  
python27-pyside  |  |  5.6.0a1  |  |  C  |  
python27-pyspharm  |  1.0.9  |  1.0.9  |  |  C  |  
python27-pystan  |  2.17.1.0  |  2.17.1.0  |  |  C  |  
python27-pytz  |  2018.7  |  2019.1  |  |  C  |  
python27-pyugrid  |  0.3.1  |  0.3.1  |  |  C  |  
python27-PyYAML  |  3.13  |  5.1.1  |  |  C  |  
python27-pyzmq  |  17.1.2  |  18.0.2  |  |  C  |  
python27-requests  |  2.21.0  |  2.22.0  |  |  C  |  
python27-rpy2  |  2.9.4  |  2.8.5  |  |  C  |  
python27-s3transfer  |  0.1.13  |  0.2.1  |  |  C  |  
python27-ScientificPython  |  |  |  |  N  |  only available in Python2.7  
python27-scikit-image  |  |  0.14.2  |  |  C  |  
python27-scikit-learn  |  0.20.1  |  0.20.3  |  |  C  |  
python27-scipy  |  1.1.0  |  1.1.0  |  |  C  |  
python27-scitools-iris  |  2.2.0  |  1.13.0  |  |  C  |  
python27-seaborn  |  |  0.9.0  |  |  C  |  
python27-setuptools  |  40.6.3  |  41.0.1  |  |  C  |  
python27-Shapely  |  1.6.4  |  1.6.4  |  |  C  |  
python27-singledispatch  |  |  3.4.0.3  |  |  C  |  
python27-six  |  1.12.0  |  1.12.0  |  |  C  |  
python27-snowballstemmer  |  1.2.1  |  1.9.0  |  |  C  |  
python27-Sphinx  |  1.8.2  |  1.8.5  |  |  C  |  
python27-sphinxcontrib-websupport  |  1.1.0  |  1.1.2  |  |  C  |  
python27-statsmodels  |  0.9.0  |  0.10.0  |  |  C  |  
python27-sympy  |  1.3  |  1.4  |  |  C  |  
python27-Theano  |  1.0.3  |  1.0.4  |  |  C  |  
python27-toolz  |  0.9.0  |  0.10.0  |  |  C  |  
python27-tornado  |  5.1.1  |  5.1.1  |  |  C  |  
python27-tqdm  |  4.28.1  |  4.32.2  |  |  C  |  
python27-typing  |  |  3.7.4  |  |  C  |  
python27-urllib3  |  1.24.1  |  1.25.3  |  |  C  |  
python27-virtualenv  |  16.0.0  |  16.0.0  |  |  C  |  
python27-WebOb  |  1.8.4  |  1.8.5  |  |  C  |  
python27-windspharm  |  1.7.0  |  1.7.0  |  |  C  |  
python27-wxPython  |  4.0.3  |  4.0.3  |  |  C  |  
python27-xarray  |  0.11.0  |  0.11.3  |  |  C  |  
qt-devel  |  |  |  4.8.7  |  R  |  
R  |  3.5.1  |  3.4.1  |  |  C  |  
R-devel  |  3.5.1  |  3.4.1  |  |  C  |  
R-ncdf4  |  1.16  |  1.16  |  |  C  |  
redhat-lsb  |  |  |  4.1  |  R  |  
rjags  |  |  4_6  |  |  C  |  
sqlite-devel  |  |  |  3.7.17  |  R  |  
subversion  |  |  |  1.8.17  |  R  |  
subversion-devel  |  |  |  1.8.17  |  R  |  
subversion-tools  |  |  |  1.8.17  |  R  |  
tcl-devel  |  |  |  8.5  |  R  |  
tcl-devel  |  |  |  8.5.13  |  R  |  
tcsh  |  |  |  6.18.01  |  R  |  
thea  |  |  |  |  N  |  discontinued  
tk  |  8.6.9  |  8.6.9  |  8.5.13  |  CR  |  
tk-devel  |  8.6.9  |  8.6.9  |  8.5.13  |  CR  |  
tkdiff  |  |  |  4.3.5  |  R  |  
tmux  |  2.7  |  2.7  |  |  C  |  
tree  |  |  |  1.6.0  |  R  |  
udunits-devel  |  2.2.27.6  |  2.2.27.6  |  2.2.20  |  R  |  called
udunits2-devel  
umutil  |  |  |  20130102  |  R  |  
umutil-lib  |  |  |  20130102  |  R  |  
uuid  |  |  |  1.6.2  |  R  |  
uuid-devel  |  |  |  1.6.2  |  R  |  
valgrind  |  |  3.15.0  |  |  C  |  
vim-enhanced  |  |  |  7.4.629  |  R  |  
wxGTK-devel  |  |  |  2.8.12  |  R  |  
xconv  |  |  |  1.94  |  R  |  
xemacs  |  |  |  21.5.34  |  R  |  
xorg-x11-util-macros  |  |  |  1.19.0  |  R  |  
xpdf  |  |  |  3.04  |  R  |
{.table .table-striped}",https://help.jasmin.ac.uk/docs/software-on-jasmin/software-migration-2020#full-table-of-packages-from-jap,14239,2651
'Additional software',"This article provides details of additional packages that exist under the
`/apps/jasmin/` directory which is available on all scientific analysis
servers and on the LOTUS batch cluster on JASMIN.
The `/apps/jasmin/` directory has been provided as a home for additional
software packages that are not installed within either [Jaspy]({{% ref ""jaspy-envs"" %}})
or [""jasmin-sci""]({{% ref ""jasmin-sci-software"" %}}) environments.
This page details which packages are available along with details of how they
are managed and accessed.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/additional-software,529,75
Community packages under: /apps/jasmin/,"Software installed as _community packages_ are provided, and maintained, by
developers outside the CEDA/JASMIN Team. If you have queries about using
community packages on JASMIN then please contact the JASMIN Helpdesk and we
will forward them to the team that supports that specific package on JASMIN.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/additional-software#community-packages-under:-/apps/jasmin/,302,46
ESMValTool,"The Earth System Model Evaluation Tool (ESMValTool) is a community diagnostics
and performance metrics tool for the evaluation of Earth System Models (ESMs)
that allows for routine comparison of single or multiple models, either
against predecessor versions or against observations. See the
[ESMValTool on JASMIN page]({{% ref ""community-software-esmvaltool"" %}}) for more info.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/additional-software#esmvaltool,379,52
Introduction,"This article explains that users may request access to a PostgreSQL (known as
Postgres) database on JASMIN for use with scientific workflows.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/postgres-databases-on-request#introduction,142,22
Overview of the user database service,"There are a large number of different workflows on JASMIN and it is common for
users to require access to _persistent storage_ of information about the progress of a workflow.
In some cases, it is
appropriate for users to save information in a relational database. Examples
might be to create/update/delete data records (when working with _small_
results) or to store the success/failure of batch tasks (running on LOTUS).
To meet this need, a _user database service_ is available on request. The
service provides secured access to a [Postgres](https://www.postgresql.org/)
database that is accessible from the interactive and batch compute nodes on
JASMIN. An individual user, or group of users, can be issued with login
credentials to enable read and/or write access to the database.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/postgres-databases-on-request#overview-of-the-user-database-service,786,123
How to request a Postgres database for use on JASMIN,"In order to request a Postgres database from the user database service, please
ensure you have a JASMIN Login account and then send a message to the JASMIN
Helpdesk. You should include the following information in your message:
- state that you would like a user database set up
- your JASMIN user ID
- Postgres details: database name, database user account name
- the expected size of the database (fully populated)
- machines (inside JASMIN) from which you would like to contact the database
- If you require backups of the database to be made.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/postgres-databases-on-request#how-to-request-a-postgres-database-for-use-on-jasmin,547,96
Postgres extensions,"The Postgres installation includes the following extension:
- {{<link ""https://postgis.net/"">}}PostGIS{{</link>}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/postgres-databases-on-request#postgres-extensions,114,10
Backups,"By default we make daily backups of the databases, which are kept for a week.
We keep a weekly backup for a month and a monthly backup for 6 months. These
intervals may be subject to change, so if you have particular concerns about
backups please let us know. If your database is particularly large we may
either make less frequent backups or not make backups at all. We will discuss
any particular backup requirements you may have when we set up your database.
You should contact the JASMIN Helpdesk to request access
to previous backups.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/postgres-databases-on-request#backups,540,96
Restrictions and limitations,"Please note that the following restrictions and limitations apply to the user
database service:
- We cannot provide training in SQL or the use of relational databases: please ensure that you have the appropriate experience before requesting access.
- There is a size limit on the server and the disk that the user databases are housed on: if you expect to store many GBs of data then please discuss this with the JASMIN Team.
- The database cannot be made accessible outside the JASMIN firewall: it is intended to support applications and workflows that take place inside JASMIN.
- The database should not be considered as a long-term data store: you should migrate any content elsewhere if you intend to keep it for the medium or long term.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/postgres-databases-on-request#restrictions-and-limitations,742,128
Overview,"`checksit` is a tool that checks the structure and content of a file against a range of available checks. Checks can be made using either ""spec"" files defining rules that objects within a file must meet, or comparison against a template file.
Whilst initial development focussed around the standards developed for NCAS data, checksit can be adapted to check files against any desired requirements.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/community-software-checksit#overview,398,64
Features,"Currently, checksit can:
- use spec files and define rules against which to check files
- check a file against a given template file
- check for compliance against `NCAS-GENERAL-2.0.0` and `NCAS-IMAGE-1.0` standards
- output in either ""standard"" mode or a one line ""compact"" mode
- summarise output from multiple ""compact"" mode file checks
Work in progress includes:
- check for compliance against other NCAS standards (e.g. `NCAS-RADAR`)
- check against future versions of standards (`NCAS-GENERAL-2.1.0` and `NCAS-IMAGE-1.1`)
- allow user defined specs and rules
- Visit the GitHub repository linked at the bottom of this page for further information on what is being worked on.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/community-software-checksit#features,681,106
Use on JASMIN,"`checksit` is available on all `sci` machines. To check a file in your current directory:
{{<command>}}
/apps/jasmin/community/checksit/checksit check name-of-file.ext
{{</command>}}
For a complete guide on how to use checksit, please visit the documentation site linked at the bottom of this page.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/community-software-checksit#use-on-jasmin,299,41
Further information,"Documentation: https://checksit.readthedocs.io/en/latest/
GitHub: https://github.com/cedadev/checksit",https://help.jasmin.ac.uk/docs/software-on-jasmin/community-software-checksit#further-information,101,4
Introduction,"This article describes the `jasmin-sci` software environment on JASMIN. It
covers the following topics:
- Overview of the `jasmin-sci` software environment
- Activating and deactivating the `jasmin-sci` environment
- Using the `jasmin-sci` environment with Jaspy
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jasmin-sci-software-environment#introduction,263,35
"Overview of the ""jasmin-sci"" software environment","The `jasmin-sci` software environment is intended as a supplement to 
[Jaspy]({{% ref ""jaspy-envs"" %}}) and contains extra software packages for use with
scientific data analysis which, for various reasons, are not provided as part
of Jaspy itself. These packages are generally installed on the same
machines where Jaspy is available, for example, the `sci` machines
(e.g. `sci*.jasmin.ac.uk`) and LOTUS nodes, but not
the login machines. It is not intended for the `jasmin-sci` environment 
itself to provide a complete suite of analysis software.
The packages included in `jasmin-sci` are provided via RPMs. A list of
explicitly included packages can be seen using the command `rpm -qR jasmin-sci`,
although some additional packages may be installed to satisfy
dependencies.
The packages fall into two categories:
- Packages provided by standard RPM repositories (example ""gnuplot"") - these are installed into ordinary system paths (such as `/usr/bin/gnuplot` for the gnuplot program) and require no special setup in order to run. So if you are on a relevant machine, you should just be able to type `gnuplot`.
- Packages which we have built locally for use on JASMIN (although most are third-party software). To avoid any potential later conflicts with standard packages, they are installed under the path `/opt/rh/jasmin-sci/` rather than in system paths. 
Also, the RPM package names, as can be seen in the above `rpm -qR jasmin-sci` command,
are prefixed with `jasmin-sci-`. So for example, the local build of `nccmp` (a program to compare netCDF files) is a package called `jasmin-sci-nccmp`, and the executable is at `/opt/rh/jasmin-sci/root/usr/bin/nccmp`. Before these packages can be conveniently used, it is necessary to ""activate"" the environment as described below, so that when you type e.g. ""nccmp"" the relevant files can be found.
Unlike the Jaspy environments, `jasmin-sci` can only provide one version of each
package at a time, so the versions are subject to change when updates are
done. If we anticipate any important changes, then we will notify JASMIN users
by email.
In a few cases, software packages are provided in `jasmin-sci` which are also
provided in Jaspy. This is only done where other RPM packages depend on it.
For example, the netcdf package is provided in Jaspy, but is also installed as
an RPM because the nccmp package requires it. This means that copies of the
netCDF libraries exist both in Jaspy (for the full path, type `nc-config --libs` after
activating Jaspy) and also under `/usr/lib64` (from the RPM). When
linking code to the netCDF library, it is recommended to use the one in Jaspy
because this will be version controlled. Also, although the `jasmin-sci`
software might provide some software that happens to be implemented in Python,
any such Python modules are not intended to be imported into your own code.
For Python development, packages from Jaspy should be used.
The development for `jasmin-sci` takes place via the {{< link ""https://github.com/cedadev/extra-sci-packages"" >}}extra-sci-packages{{</link>}} GitHub repository,
and an associated {{< link ""https://github.com/cedadev/extra-sci-packages/issues"" >}}issues{{</link>}} page. The readme file on the repository has some
package-specific documentation (including how to build the python bindings for Misr toolkit).
","https://help.jasmin.ac.uk/docs/software-on-jasmin/jasmin-sci-software-environment#overview-of-the-""jasmin-sci""-software-environment",3329,505
"Activating and deactivating the ""jasmin-sci"" environment","To **activate** the jasmin-sci environment, use the command:
{{<command user=""user"" host=""sci-vm-01"">}}
module load jasmin-sci
{{</command>}}  
and to **deactivate** it, use the command:
{{<command user=""user"" host=""sci-vm-01"">}}
module unload jasmin-sci
{{</command>}}
(""add"" and ""purge"" can also be used).
The `module load` command must be done in each session, or added to your `$HOME/.bashrc` file.
As mentioned above, this is only required for a subset of packages in jasmin-
sci. The majority of packages do not require it, but for example, those which
do include ferret, and the leafpad (notepad-like) editor.
Ferret users should note that it is still necessary to do ""source
ferret_paths.sh"" after activating jasmin-sci, in order for ferret to find all
its additional resource files.
","https://help.jasmin.ac.uk/docs/software-on-jasmin/jasmin-sci-software-environment#activating-and-deactivating-the-""jasmin-sci""-environment",792,115
"Using the ""jasmin-sci"" environment with Jaspy","To activate both jasmin-sci and Jaspy, it is best to activate them in the
following order:
{{<command user=""user"" host=""sci-vm-01"">}}
module load jasmin-sci
module load jaspy
{{</command>}}
This will ensure that in the unlikely event of an executable in Jaspy also
existing under `/opt/rh/jasmin-sci/`, then one in Jaspy will take priority.
The corresponding `module unload` commands can be done in either order.
","https://help.jasmin.ac.uk/docs/software-on-jasmin/jasmin-sci-software-environment#using-the-""jasmin-sci""-environment-with-jaspy",413,61
Migration to Rocky Linux 9 2024,"{{< alert color=""success"" icon=""fas circle-exclamation"" >}}
Lots of updated information below about the new Rocky Linux 9 environment on JASMIN: please read, and keep checking back here regularly for now.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/rocky9-migration-2024,218,31
Introduction,"As with a previous migration completed in 2020, the change of operating system version is needed to make sure that the version in use is current and fully supported, i.e. that package updates are available and important security updates can be obtained and applied to keep the platform secure.
The current operating system, CentOS7 is officially end-of-life as of the end of June 2024. We will be moving from CentOS7 to Rocky Linux 9, which is supported until May 2032. Rocky 9 should provide a very similar user experience to that provided by CentOS7, but with more recent software packages. Some software may have been removed or replaced during this transition.
This change affects JASMIN and CEDA services in several ways, including but not limited to the following:
- Components of all CEDA Archive and JASMIN web-based services need to be redeployed
- User-facing service hosts (e.g. `login`/`sci`/`xfer` and LOTUS nodes) all need to be redeployed
- All of these hosts need appropriate versions of drivers for various hardware and infrastructure components (e.g. storage, network, …) to be configured.
- The Slurm scheduler used for the LOTUS and ORCHID clusters needs to be adapted to work under Rocky 9, in terms of its own management functions and the worker nodes which it controls. A separate announcement will cover the expansion of LOTUS with new processing nodes: these will be introduced as a new cluster under Slurm, with existing nodes moved from old to new as part of the transition. There will be a limited window in which the 2 clusters will co-exist, during which time the old cluster will shrink in size: the current estimate for this is between July to September 2024, but we will provide updates on this as the new hardware is installed and timescales become clearer. We will endeavour to provide sufficient overlap and temporary arrangements to help users to migrate their workflows.
- Software made available centrally via the `module` system and under `/apps` needs to be made available in versions compatible with Rocky 9. Some software may need to be recompiled.
- Other software (e.g. run by users or groups, without being centrally managed) may need to be tested and in some cases recompiled in order to work correctly under Rocky 9.
- Management and monitoring systems need to be updated to operate in the new environment
- For tenants of the JASMIN Cloud, you should already be aware of our plans to move to use the STFC Cloud as the base platform for the JASMIN Cloud Service. Images are currently in preparation so that new (empty) tenancies will soon be available for tenants to manage the migration of their own virtual machines over to new instances using Rocky 9 images. It is anticipated at this stage that managed tenancies (with tenancy sci machines) will be discontinued as part of this move, so users of those VMs will be advised to use the new Rocky 9 general-use sci servers instead.
Much of this work is already underway by teams in CEDA and STFC’s Scientific Computing Department. As a result of extensive work by these teams in recent years to improve the way services are deployed and managed, we are now in a much better position to undertake this kind of migration with as little disruption to users as possible. Some disruption and adaptation by users will be inevitable, however.
Some services have already been migrated and are already running under Rocky 9, but there is still much work to be done over the coming weeks so please watch this space as we do our best to keep you informed of the progress we’re making, and of any actions you may need to take to minimise disruption to your work on JASMIN.
{{<alert type=""info"">}}
Please find below details of the new Rocky 9 environment on JASMIN. We will update other documentation to match this in due course, but the information below will be the most up-to-date source until further notice.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/rocky9-migration-2024#introduction,3911,666
Details of the new Rocky Linux 9 environment,,https://help.jasmin.ac.uk/docs/software-on-jasmin/rocky9-migration-2024#details-of-the-new-rocky-linux-9-environment,0,0
General,"The move to Rocky Linux 9 (abbreviated to ""Rocky 9"" or ""R9"" from here on) involves many changes at
lower levels transparent to users, so we will focus here on those most relevant to how services on 
JASMIN are accessed and used. The reasons for the choice of Rocky 9 itself, and for some of the
associated changes to software, machines and services provided, will not be covered in detail,
but have been influenced by a number of factors including:
- organisational security and maintenance policies
- availability of packages and dependencies for the chosen operating system
- user feedback
",https://help.jasmin.ac.uk/docs/software-on-jasmin/rocky9-migration-2024#general,592,100
Login nodes,"The list of new login nodes is as follows:
name | status
--- | ---
`login-01.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} ready to use
`login-02.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} ready to use
`login-03.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} ready to use
`login-04.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} ready to use
{.table .table-striped .w-auto}
Notes:
- There is no longer any requirement for forward/reverse DNS lookup or any restriction by 
institutional domain. You no longer need to register non-`*.ac.uk` domains with the JASMIN 
team (exception: {{<link ""#hpxfer-servers"">}}`hpxfer`{{</link>}})
- This means all users can access all login servers (previously some users could only use
 `login2`)
- As before, no filesystems other than the home directory are mounted.
- Use only as a ""hop"" to reach other servers within JASMIN.
- **Make sure your SSH client is up to date**. Check the version with `ssh -V`. If
it's significantly older than `OpenSSH_8.7p1, OpenSSL 3.0.7`, speak to your local
admin team as it may need to be updated before you can connect securely to JASMIN.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/rocky9-migration-2024#login-nodes,1163,180
NX login nodes,"name | status
--- | ---
`nx1.jasmin.ac.uk` | {{< icon fas triangle-exclamation text-warning >}} [Ready, but new setup steps recommended]({{% ref ""graphical-linux-desktop-access-using-nx#setting-up-your-connection"" %}})
`nx2.jasmin.ac.uk` | {{< icon fas triangle-exclamation text-warning >}} [Ready, but new setup steps recommended]({{% ref ""graphical-linux-desktop-access-using-nx#setting-up-your-connection"" %}})
`nx3.jasmin.ac.uk` | {{< icon fas triangle-exclamation text-warning >}} [Ready, but new setup steps recommended]({{% ref ""graphical-linux-desktop-access-using-nx#setting-up-your-connection"" %}})
`nx4.jasmin.ac.uk` | {{< icon fas triangle-exclamation text-warning >}} Not yet moved to Rocky 9 (works as previously for now)
{.table .table-striped .w-auto}
Notes:
- [New steps are recommended]({{% ref ""graphical-linux-desktop-access-using-nx#setting-up-your-connection"" %}}) for setting up your connection, including a small edit to a config file.
- New nodes have identical configuration so are accessible from all network locations (no further need for some users use only certain nodes).
- By keeping the host names as short as possible, we mitigate the issue some users (with long
usernames created before the 8-character rule) had with agent forwarding: all should behave
the same as the old `nx4` in this respect.
- As before, no filesystems other than the home directory are mounted.
- Use only with the NoMachine Enterprise Client to get a graphical Linux desktop, from where you can
  - use the Firefox browser on the linux desktop to access web resources only accessible within JASMIN
  - make onward connections to a `sci` server for using graphics-intensive applications
- Make sure you are using the most up-to-date version of 
{{<link ""https://downloads.nomachine.com/download-enterprise/#NoMachine-Enterprise-Client"">}}NoMachine Enterprise Client{{</link>}}.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/rocky9-migration-2024#nx-login-nodes,1886,238
`sci` servers,"We have introduced a new naming convention which helps identify virtual and physical/high-memory `sci` servers.
The new list is as follows:
name | status | specs
--- | --- | ---
Virtual servers | | 
`sci-vm-01.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} Ready to use | 8 CPU / 32 GB RAM / 80 GB (virtual disk)
`sci-vm-02.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} Ready to use | 8 CPU / 32 GB RAM / 80 GB (virtual disk)
`sci-vm-03.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} Ready to use | 8 CPU / 32 GB RAM / 80 GB (virtual disk)
`sci-vm-04.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} Ready to use | 8 CPU / 32 GB RAM / 80 GB (virtual disk)
`sci-vm-05.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} Ready to use | 8 CPU / 32 GB RAM / 80 GB (virtual disk)
`sci-vm-06.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} Ready to use | 8 CPU / 32 GB RAM / 80 GB (virtual disk)
Physical servers | |
`sci-ph-01.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} Ready to use | 48 CPU AMD EPYC 74F3 / 2 TB RAM / 2 x 446 GB SATA SSD
`sci-ph-02.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} Ready to use | 48 CPU AMD EPYC 74F3 / 2 TB RAM / 2 x 446 GB SATA SSD
{.table .table-striped .w-auto}
Notes:
- For users within the STFC network, there is no longer any reverse DNS restriction, so all
should be accessible directly within that network without need to go via a login node.
- Replacements for common tools:
  - `lxterminal` has been replaced with {{<link href=""https://docs.xfce.org/apps/terminal/start"">}}xfce-terminal{{</link>}}
  - for a more richly-featured editor or Integrated Development Environment (IDE), users should consider using
   the remote editing features of {{<link href=""https://code.visualstudio.com/docs/remote/ssh"">}}VSCode{{</link>}} or 
   {{<link ""https://www.jetbrains.com/pycharm/"">}}PyCharm{{</link>}}, since these can be installed and customised locally
   by the user to their taste rather than needing central installation and management on JASMIN. Watch this space for
   further advice about how to configure and use VSCode in this way.
- See {{<link ""#jaspy"">}}jaspy{{</link>}}, {{<link ""#jasr"">}}jasr{{</link>}} and {{<link ""#jasmin-sci"">}}jasmin-sci{{</link>}}
sections below for further information on software.
- For graphical applications, use the {{<link ""#nx-login-nodes"">}}NoMachine NX service{{</link>}} rather than
sending X11 graphics over the network back to your laptop/desktop, to ensure performance.
  - X11 graphics functionality is still to be added to these machines (coming shortly), but currently this will fail with an error like:
    ```
    xterm: Xt error: Can't open display: 
    xterm: DISPLAY is not set
    ```
- As before, physical servers are actually re-configured nodes within the LOTUS cluster and as such have different a network
configuration from the virtual `sci` servers, with limited outward connectivity.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/rocky9-migration-2024#`sci`-servers,2978,453
`xfer` servers,"name | status | notes
--- | --- | ---
`xfer-vm-01.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} ready to use | Virtual server
`xfer-vm-02.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} ready to use | Virtual server
`xfer-vm-03.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} ready to use | Virtual server, has `cron`.
{.table .table-striped .w-auto}
Notes:
- Similar config on all 3 (no domain or reverse DNS restrictions now)
- Same applies re. **SSH client version**, see [login nodes]({{% ref ""#login-nodes"" %}})
- If using cron on `xfer-vm-03`, you must use [crontamer]({{% ref ""using-cron/#crontamer"" %}})
- Throttle any automated transfers to avoid many SSH connections in quick succession, otherwise you may get blocked.
- Consider using [Globus]({{% ref ""#globus-data-transfer-service"" %}}) for any data transfer in or out of JASMIN
- A new software collection `jasmin-xfer` has now been added to these servers, providing these tools:
emacs-nox
ftp
lftp
parallel
python3-requests
python3.11
python3.11-requests
rclone
rsync
s3cmd
screen
xterm
",https://help.jasmin.ac.uk/docs/software-on-jasmin/rocky9-migration-2024#`xfer`-servers,1087,159
`hpxfer` servers,"name | status | notes
--- | --- | ---
`hpxfer3.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} ready to use | Physical server
`hpxfer4.jasmin.ac.uk` | {{< icon fas circle-check text-success >}} ready to use  | Physical server
{.table .table-striped .w-auto}
Notes:
- Tested with `sshftp` (GridFTP over SSH) from ARCHER2
- Same applies re. **SSH client version**, see [login nodes]({{% ref ""#login-nodes"" %}})
- The software collection `jasmin-xfer` available as per [xfer servers, above]({{% ref ""#xfer-servers"" %}})
- `hpxfer` access role **no longer required for these new servers** (role will be retired along with the old servers in due course, so no need to renew if you move to the new servers)
",https://help.jasmin.ac.uk/docs/software-on-jasmin/rocky9-migration-2024#`hpxfer`-servers,712,113
GridFTP server,"For users of certificate-based GridFTP only (specifically, `gsiftp://` using the `globus-url-copy` client), there is a new server:
name | status
--- | ---
`gridftp2.jasmin.ac.uk` | {{< icon fas square-xmark text-danger >}} Not yet ready
{.table .table-striped .w-auto}
Notes:
- Make sure you are using `slcs.jasmin.ac.uk` as the short-lived credentials server, with your JASMIN
account credentials. CEDA identities can no longer be used for authentication with this server.
- We now encourage users of this service to migrate to use the {{<link ""#globus-data-transfer-service"">}}Globus service{{</link>}}
for data transfers. This older GridFTP service will likely be decommissioned over the next 12 months.
- Use of `globus-url-copy` is nothing to do with the {{<link ""#globus-data-transfer-service"">}}Globus service{{</link>}}: they are now very separate things.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/rocky9-migration-2024#gridftp-server,864,118
Globus data transfer service,"Where possible you should now use the Globus data transfer service for any data transfer in or out of JASMIN: this is now the recommended method,
which will get you the best performance and has a number of advantages over logging into a server and doing transfers manually.
As introduced earlier this year, the following Globus collections are available to all users of JASMIN, with no special access roles required:
name | uuid | status | notes
--- | --- | --- | ---
JASMIN Default Collection | `a2f53b7f-1b4e-4dce-9b7c-349ae760fee0` | {{< icon fas circle-check text-success >}} Ready to use | Best performance, currently has 2 physical Data Transfer Nodes (DTNs).
JASMIN STFC Internal Collection | `9efc947f-5212-4b5f-8c9d-47b93ae676b7` | {{< icon fas circle-check text-success >}} Ready to use | For transfers involving other collections inside the STFC network. 2 DTNs, 1 physical, 1 virtual. Can be used by any user in case of issues with the above collection.
{.table .table-striped .w-auto}
Notes:
- These collections can be used with the Globus {{<link ""https://app.globus.org"">}}web interface{{</link>}},
{{<link href=""https://docs.globus.org/cli/"">}}command-line interface (CLI){{</link>}}, or its {{<link href=""https://globus-sdk-python.readthedocs.io/en/stable/"">}}Python software development kit (SDK){{</link>}}, and use the JASMIN accounts portal for authentication
",https://help.jasmin.ac.uk/docs/software-on-jasmin/rocky9-migration-2024#globus-data-transfer-service,1381,192
Software,"Please see the table below and accompanying notes which together summarise the upcoming changes to aspects of software on JASMIN:
Software | CentOS7 | Rocky 9
--- | --- | ---
IDL versions<br>IDL licence server<br>**see Note 1** | 8.2, 8.5 (D), 8.5, 8.6<br>Flexnet | 8.9, 9.0. (8.6?)<br>Next generation
Cylc<br>Cylc UI visualisation<br>**see Note 2**  | 7.8.14 and 8.3.3-1<br>UI functionality integrated | 8.3.3-1<br>UI via browser: discussion ongoing
Jaspy<br>Jasr<br>jasmin-sci | 2.7, 3.7*, 3.10* (*: all variants)<br>3.6, 4.0 (all variants), 4.2<br>URL page of the packages | 3.11<br>4.3<br>rpm/Glibc compatibility tba?
Intel compilers | 12.1.5-20.0.0 (11 variants) | Intel oneAPI
MPI library/ OpenMPI<br>versions/compiler<br>**see Note 3**  | 3.1.1/Intel,GNU, 4.0.0<br>4.1.[0-1,4-5]/Intel<br>4.1.2, 5.0.1, 5.1.2 | 4.1.5/Intel/gcc &  5.0.4 /intel/gcc<br><br>Possibility to support mpich or IntelMPI
NetCDF C library<br>NetCDF Fortran binding lib. | netcdf/gnu/4.4..7, netcdf/intel/14.0/<br>netcdff/gnu/4.4.7/*, netcdff/intel/4.4.7<br>parallel-netcdf/gnu/201411/22<br>parallel-netcdf/intel/20141122 | A new module env for serial and parallel version GNU and Intel oneAPI build of NetCDF against either OpenMPI and/or Intel MPI
GNU compilers | 7.2.0 ,8.1.0,  8.2.0<br>13.2.0 conda-forge (12.1.0 from legacy JASPY) | 11.4.1 (OS)<br>13.2.0 conda-forge via JASPY
JULES <br>**see Note 4**| | Information to follow
{.table .table-striped .w-auto}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/rocky9-migration-2024#software,1442,175
Notes,"1. **IDL:** We will not support IDL 8.5 & older versions on Rocky 9 but we might continue to support IDL 8.6 if there is a need from the user community: we are still assessing that. The present version of IDL 8.6 must be migrated from the current ""Flexnet"" to the new ""Next Generation"" licensing system.
We obtained IDL 8.9 and IDL 9 from NV5 and are in the process to setup “Next Generation"" licensing to activate the licence. Once this is done on server and client machines and testing is completed, a new module environment will be created users for IDL 8.9 and 9.0 on the new sci machines and a subset of the new LOTUS Rocky 9 nodes. The default `module add idl` will then load IDL 8.9 instead of IDL 8.6.
2. **Cylc:** Note that Cylc 8 differs from Cylc 7 in many ways: architecture, scheduling algorithm, security, UIs, working practices and more. The Cylc 8 web UI requires the use of a browser (e.g. Firefox in the NoMachine desktop service)
3. **MPI:** (further details to follow)
4. **JULES:** (further details to follow)
",https://help.jasmin.ac.uk/docs/software-on-jasmin/rocky9-migration-2024#notes,1031,186
Upgraded LOTUS cluster,"Preliminary node specification: further info to follow.
type | selector | status | specs
--- | --- | --- | ---
standard | tbc | {{< icon fas square-xmark text-danger >}} Not yet available | 190 CPU / 1.5 TB RAM / 480 GB SATA SSD + 800 GB NvMe SSD
high-mem | tbc | {{< icon fas square-xmark text-danger >}} Not yet available | 190 CPU / 6 TB RAM / 480 GB SATA SSD + 800 GB NvMe SSD
{.table .table-striped .w-auto}
Notes:
- Overall ~55,000 cores: ~triples current capacity
- New nodes will form a new cluster, managed separately to the ""old"" LOTUS
- Submission to the new cluster will **only** be via the new `sci` machines: `sci-vm-[01-06]` and `sci-ph-[01-02]`
  - and from **one** additional physical node (details TBC) with extended CentOS7 support, with restricted access to enable use of Cylc 7 for limited period.
- Submission to ""old"" LOTUS will **only** be from current CentOS7 `sci` machines `sci[1-8]`
  - and from **one** additional physical node (details TBC) with extended CentOS7 support, with restricted access to enable use of Cylc 7 for limited period.
- Nodes will gradually be removed from the ""old"" cluster and retired, timetable TBC once new cluster is up & running.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/rocky9-migration-2024#upgraded-lotus-cluster,1187,208
Other services,"Further information to follow.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/rocky9-migration-2024#other-services,31,4
IDL,"{{<alert type=""danger"">}}
There are currently some licensing issues which are affecting how IDL can be used on new
Rocky 9 servers. Please check back to our [Rocky 9 migration page]({{% ref ""rocky9-migration-2024"" %}}) for the latest updates as they become available.
{{</alert>}}
This article explains how to:
- use the IDL software on JASMIN 
- run these tools on the scientific analysis servers and LOTUS
- make efficient use of the IDL licences
",https://help.jasmin.ac.uk/docs/software-on-jasmin/idl,449,73
What is IDL?,"{{<link ""https://www.nv5geospatialsoftware.com/Products/IDL"">}}IDL{{</link>}} stands for
Interactive Data Language. It is a licensed data manipulation toolkit made
available on JASMIN.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/idl#what-is-idl?,185,18
Availability of IDL on JASMIN,"IDL is available on all [scientific analysis servers]({{% ref ""sci-servers"" %}}) and [LOTUS]({{% ref ""lotus-overview"" %}}).
To get started with **IDL**, login to one of scientific analysis servers and
do as follows:
Check which versions are available:
{{<command user=""user"" host=""sci-vm-01"">}}
module avail idl
(out)
(out)-------------------------------------------- /apps/jasmin/modulefiles -----------------------------------------------
(out)  idl/8.2   idl/8.5 (D)   idl/8.6   idl/8.9
(out)
(out)  Where:
(out)   D:  Default Module
{{</command>}}
The current default version is labelled with `(D)` and can be loaded using just `module load idl`. Alternatively, load a specific version by
adding its version string to the command:
{{<command user=""user"" host=""sci-vm-01"">}}
module load idl ## or idl/8.5 to specify the version
idl
(out)IDL Version 8.5 (linux x86_64 m64). (c) 2015, Exelis Visual Information Solutions, Inc., a subsidiary of Harris Corporation.
(out)Installation number: 406672.
(out)Licensed for use by: Science & Technology Facilitie
{{</command>}}
You can then type commands at the `IDL` prompt
{{<command prompt=""IDL>"">}}
print,1+4
(out)  5
exit
{{</command>}}
For help on the `idl` module you can type the following :
{{<command user=""user"" host=""sci-vm-01"">}}
module help idl
(out)----------- Module Specific Help for 'idl/8.5' --------------------
(out)         Adds IDL 8.5 to your environment variables,  
{{</command>}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/idl#availability-of-idl-on-jasmin,1450,185
Making efficient use of IDL development licences,"We have a large pool of **run-time** licences but a much more limited pool of
**development** licences. In each case, these consist of floating licences shared
between JASMIN sci machines and the LOTUS cluster.
{{<alert type=""info"">}}
**6 September 2024: IDL v8.9** This version is available but without the full set of run-time
licences. This may affect usage, particularly on the LOTUS cluster. This will be
resolved in due course. Also please ignore the error message on startup re. GL graphics device.
{{</alert>}}
Users are welcome to run multiple instances of IDL code, but for that purpose
please make use of the run-time licences by compiling your code using a **single**
development session and then running the pre-compiled code using the `-rt`
flag. An example of this is shown in the next section (below).
Please try not to run more than one or two simultaneous IDL development
sessions. However, for licence purposes, each unique combination of username,
hostname, and `$DISPLAY` variable counts as a single session. So for example,
if you run idl (development mode) in one window, then suspend it with {{<kbd ""CTRL-Z"">}} and
start another development session in the same window, this still is only
counted as one session by the licence server because the username, hostname,
and $DISPLAY are all identical between the two processes. But if you ""ssh"" in
on two different windows, probably the `$DISPLAY` will differ between the two
windows (e.g. `localhost:10` and `localhost:11`), so if you start idl
development sessions in each window they will require separate licences.
To see what licences you and others are using, you can use the following
sequence of commands:
{{<command>}}
module add idl/8.5
lmstat -a
{{</command>}}
When interpreting the numbers, note that a single session is counted as 6
licences.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/idl#making-efficient-use-of-idl-development-licences,1825,292
Using IDL on LOTUS (via the run-time Licences),"IDL run-time licences are available for use on the LOTUS cluster. In order to
specify use of the run-time licences please follow the instructions here. You
need to compile your IDL code in order to run in run-time mode.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/idl#using-idl-on-lotus-(via-the-run-time-licences),220,39
Example program,"The example program, ""foo"", depends on some other functions.
======== foo.pro =======
    pro foo  
    print, doubleit(10) end 
========================
===== doubleit.pro ===== 
function doubleit, n   
return, two() * n 
end 
========================
======= two.pro ======== 
function two   
return, 2 
end 
========================
You must save a compiled version of the code in order to run it.
1\. Compile the program:
Compiles top-level routine only
{{<command prompt=""IDL>"">}}
.compile foo
(out)% Compiled module: FOO.
{{</command>}}
2\. Use resolve_all to compile routines it depends on:
Recursively search for and compile modules called
{{<command prompt=""IDL>"">}}
resolve_all
(out)% Compiled module: DOUBLEIT.
(out)% Compiled module: TWO.
{{</command>}}
3\. Save all compiled routines to a file:
{{<command prompt=""IDL>"">}}
save, /routines, file='foo.sav'
{{</command>}}
4\. To run the program, using a run-time licence only:
{{<command user=""user"" host=""sci-vm-01"">}}
idl -rt=foo.sav 
(out)IDL Version 8.5 (linux x86_64 m64). (c) 2015, Exelis Visual Information Solutions, Inc., a subsidiary of Harris Corporation.
(out)Installation number: 406672.
(out)Licensed for use by: Science & Technology Facilitie
(out)  20
{{</command>}}
{{<alert type=""info"">}}Using` -vm=` instead of `-rt=` opens the save file in the IDL
virtual machine. No run-time licence is required, but a splash screen must
be dismissed interactively, so it is not suitable for queues on the
cluster.
{{</alert>}}
To see what routines are present in the save file:
{{<command prompt=""IDL>"">}} 
.reset_session     <=== removes any existing compiled modules  
help               <=== show compiled modules (and variables); there shouldn't be any 
(out)% At $MAIN$           
(out)Compiled Procedures:
        (out)$MAIN$  
(out)Compiled Functions:  
restore,'foo.sav'   <=== load contents of save file  
help 
(out)% At $MAIN$          
(out)Compiled Procedures:
        (out)$MAIN$  FOO                     <=== this was loaded from foo.sav  
(out)Compiled Functions: 
        (out)DOUBLEIT    TWO            <=== so were these
{{</command>}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/idl#example-program,2122,266
Passing arguments,"You can also pass arguments in to your code as follows:
In your code, use function `command_line_args`, for example:
argsarray = command_line_args(count = nparams)
Call the code with -args flag:
{{<command user=""user"" host=""sci-vm-01"">}}
idl -rt=foo.sav -args 10 20 30
{{</command>}}
`command_line_args` returns a string array, so convert type as required, e.g. `n = fix(argsarray[0]) `
",https://help.jasmin.ac.uk/docs/software-on-jasmin/idl#passing-arguments,387,55
Further reading,"- Vendor documentation: {{<link ""https://www.nv5geospatialsoftware.com/docs/using_idl_home.html"">}}Using IDL{{</link>}} (although may be for a newer version than on JASMIN)
",https://help.jasmin.ac.uk/docs/software-on-jasmin/idl#further-reading,173,16
Related software,"- The related software `MIDL` is no longer available on JASMIN.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/idl#related-software,64,11
"Jaspy Software Environments (Python 3, R and other tools)","{{<alert type=""danger"">}}
Important changes took place in September 2024 affecting what software can be used on JASMIN.
Please read [this announcement](https://www.ceda.ac.uk/news/updates/2024/2024-08-29-important-software-changes-autumn/) carefully.
The information below **has** been updated in line with this announcement.
{{</alert>}}
This page provides details of the ""Jaspy"" software environments that provide
access to Python 3, R and a range of other tools on JASMIN.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs,476,58
Overview,"Jaspy is a toolkit for managing and deploying Conda environments that include
Python, R and other packages. Jaspy is used to provide software environments
of common packages on the scientific analysis servers and LOTUS cluster on
JASMIN.
One advantage of Jaspy is that multiple environments can co-exist on the same
platform. This allows us to retain previous environments and provide new ones
simultaneously. This may be particularly useful for scientists undertaking
long-running studies that require a consistent software environment to ensure
reproducibility and continuity.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#overview,579,84
Working with Jaspy environments,,https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#working-with-jaspy-environments,0,0
Quickstart for Python 3 environment,"If you want to get on, you can select a Jaspy environment to ""activate"". This
means that once you have run these commands then the various tools and
libraries will be available in your current session.
{{<command user=""user"" host=""sci-vm-01"">}}
module load jaspy
{{</command>}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#quickstart-for-python-3-environment,278,43
Activating the environment in scripts,"If you want a particular script to activate a Jaspy environment then add the
""module"" command to it, e.g.:
#!/bin/bash
module load jaspy
python do-something.py
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#activating-the-environment-in-scripts,160,25
Setting your profile to always use a Jaspy environment,"If you want all your JASMIN sessions to use a particular Jaspy environment
then you can add the `module load jaspy` command to your `$HOME/.bashrc` file.
In order to avoid issues with using ""module load"" on
unsupported servers, please wrap the call in an ""if"" clause, such as:
if [[ $(hostname) =~ (sci[0-9]|host[0-9]|cylc) ]] ; then
    module load jaspy
fi
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#setting-your-profile-to-always-use-a-jaspy-environment,359,60
Discover which environments are available,"You can list the currently available Jaspy environments using:
{{<command user=""user"" host=""sci-vm-01"">}}
module avail jaspy
-------------------------------- /apps/jasmin/modulefiles ----------------------------------
   jaspy/3.10/v20230718    jaspy/3.11/v20240508        snappy/8.0/jaspy-3.7-r20210320
   jaspy/3.11/v20240302    jaspy/3.11/v20240815 (D)
{{</command>}}
This lists all jaspy modules (i.e. environments) that can be loaded.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#discover-which-environments-are-available,440,36
Jaspy Python (plus other tools) environments,"The packages available in the Jaspy environments can be found by searching the
GitHub repository where the Conda environment files are defined. This table
lists all the Jaspy Python 3.7+ environments provided on JASMIN and specifies
the current (default) version.
Jaspy Python Environment |  Versioned list of software packages |  Default? |  Comments / Issues
---|---|---|---
jaspy/3.10/v20230718 | [List of packages including versions](https://github.com/cedadev/ceda-jaspy-envs/blob/main/environments/py3.10/mf-22.11.1-4/jaspy3.10-mf-22.11.1-4-v20230718/final_spec.yml) | No | 
jaspy/3.11/v20240302 | [List of packages including versions](https://github.com/cedadev/ceda-jaspy-envs/blob/main/environments/py3.11/mf3-23.11.0-0/jaspy3.11-mf3-23.11.0-0-v20240302/final_spec.yml) | No  | 
jaspy/3.11/v20240508 | [List of packages including versions](https://github.com/cedadev/ceda-jaspy-envs/blob/main/environments/py3.11/mf3-23.11.0-0/jaspy3.11-mf3-23.11.0-0-v20240508/final_spec.yml) | No  | 
jaspy/3.11/v20240815 | [List of packages including versions](https://github.com/cedadev/ceda-jaspy-envs/blob/main/environments/py3.11/mf3-23.11.0-0/jaspy3.11-mf3-23.11.0-0-v20240815/final-spec.yml) | Yes  | [Release notes](https://github.com/cedadev/ceda-jaspy-envs/releases/tag/jaspy3.11_v20240815)
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#jaspy-python-(plus-other-tools)-environments,1319,100
Jaspy Python 2.7 (plus other tools) environments,"Python 2.7 environments are no longer supported.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#jaspy-python-2.7-(plus-other-tools)-environments,49,7
Jasr R environments,"Environments for the ""R"" programming language are packaged into separate
software environments, known as ""Jasr"". This table lists all the Jaspy R
environments provided on JASMIN and specifies the current (default) version.
{{<alert type=""danger"" >}}
We are aware of a newly discovered vulnerability in the R Language (CVE-2024-27322) which allows arbitrary code execution from maliciously built RDS (R Data Serialisation) files.
We will be updating to the latest version of R as soon as possible to remove this vulnerability, but we do not plan to remove access to R beforehand.
Our advice, as always, is to not open data from untrusted sources and not to install untrusted packages from CRAN.
Please note that this position may change at short notice as more information becomes available- this notice was last updated on Friday 10th May 2024.
{{< /alert >}}
Jaspy R Environment (""Jasr"") |  Versioned list of software packages|  Default?
---|---|---
jasr/4.2/v20230718  |  [List of packages including versions](https://github.com/cedadev/ceda-jaspy-envs/blob/main/environments/r4.2/mf-22.11.1-4/jasr4.2-mf-22.11.1-4-v20230718/final_spec.yml) | No | 
jasr/4.3/v20240320  |  [List of packages including versions](https://github.com/cedadev/ceda-jaspy-envs/blob/main/environments/r4.3/mf3-23.11.0-0/jasr4.3-mf3-23.11.0-0-v20240320/final_spec.yml)  | No | 
jasr/4.3/v20240815  |  [List of packages including versions](https://github.com/cedadev/ceda-jaspy-envs/blob/main/environments/r4.3/mf3-23.11.0-0/jasr4.3-mf3-23.11.0-0-v20240815/final-spec.yml)| Yes | [Release notes](https://github.com/cedadev/ceda-jaspy-envs/releases/tag/jaspy3.11_v20240815)
{.table .table-striped}
The available R environments can be listed with:
{{<command user=""user"" host=""sci-vm-01"">}}
module avail jasr
{{</command>}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#jasr-r-environments,1797,198
Understanding versioning with Jaspy/Jasr,"Jaspy environments are labelled as ""jaspy/<python_version>/<release>"". The
environment is selected and activated using the ""module load"" command:
{{<command user=""user"" host=""sci-vm-01"">}}
module load jaspy/3.10/v20230718
{{</command>}}
However, if you wish to get the latest environment for a given Python version
you can omit the ""<release>"", as follows:
{{<command user=""user"" host=""sci-vm-01"">}}
module load jaspy/3.10
{{</command>}}
And if you just want the most up-to-date Python you can even omit the
`<python_version>`, as follows:
{{<command user=""user"" host=""sci-vm-01"">}}
module load jaspy
{{</command>}}
{{<alert type=""info"" >}}
If you choose to omit the `<release>` and `<python_version>`
components then it is important to be aware that the resulting environment may
differ over time. For continuity, you ay wish to use the full
environment specification.
{{< /alert >}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#understanding-versioning-with-jaspy/jasr,885,118
How Jaspy works: managing Python and non-Python packages using conda,"[Jaspy](https://github.com/cedadev/jaspy-manager) is a framework for managing
multiple Python (and other) environments simultaneously on a single platform.
It was created in order to meet the requirements tabulated below.
**Requirement** |  **Details** |  **Jaspy solution** |  **Further info**
---|---|---|---
Reproducibility  | 1. Generate a specific set of packages and versions from a generic set of requirements. | 1. Conda has a powerful package-management workflow:<br>a. Begin with a minimal set of package/version requirements.<br>b. Generate a consistent environment.<br>c. Provide a detailed description of all exact packages/versions in the environment.|  Conda: [https://docs.conda.io](https://docs.conda.io/)  jaspy-manager: <https://github.com/cedadev/jaspy-manager/blob/master/README.md>  CEDA jaspy environments: <https://github.com/cedadev/ceda-jaspy-envs>
Documentation  |  Provide an appropriate level of documentation detailing which software packages exist in each release.  |  We use Conda ""environment files"" to build the environments. These list the packages and versions and are stored in public GitHub repositories, so each environment is documented as a collection of packages/versions.  |  See: <https://github.com/cedadev/jaspy-manager/blob/master/README.md>  Example package list: <https://github.com/cedadev/ceda-jaspy-envs/blob/master/environments/py3.7/m3-4.5.11/jaspy3.7-m3-4.5.11-r20181219/packages.txt>
Multiple simultaneous environments  |  Allow multiple, but separate, software environments to co-exist on a single operating system.  |  Conda is designed to allow multiple environments to co-exist. Within jaspy it is possible to document each environment. Therefore, multiple environments can be deployed on one system. Key advantages are:<br>- Supporting multiple versions of Python and side-by-side.<br>- Releasing an update to an environment as a ""pre-release"" so that users can adapt their code and test it whilst still having access to the ""current"" (production) environment.|
Manageability  |  Provide tools to easily construct, test, deploy, document and reproduce software environments.  |  Jaspy builds upon a set of excellent Conda command-line tools that simplify the package management process. Jaspy wraps the Conda functionality so that command-line tools can be used to build, test, deploy and distribute Conda environments for use by our community.  |
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#how-jaspy-works:-managing-python-and-non-python-packages-using-conda,2433,291
Updates and tracking of Jaspy/Jasr environments,,https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#updates-and-tracking-of-jaspy/jasr-environments,0,0
History of environments on JASMIN,"Please see the [Jaspy Python (and other tools) environments](#jaspy-python-plus-other-tools-environments) section
above for information about releases on JASMIN.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#history-of-environments-on-jasmin,162,17
"Which environment is ""current""?","Please refer to the [Jaspy Python (and other tools) environments](#jaspy-python-plus-other-tools-environments) section
above for information about the current release on JASMIN.
","https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#which-environment-is-""current""?",178,20
Citing Jaspy environments,,https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#citing-jaspy-environments,0,0
Can I cite a jaspy (conda) environment?,"We do not yet have an agreed approach for citing a Jaspy environment. However,
you can refer to the environment description URLs given in the table above.
These provide a definitive list of the software packages, their versions and
other information.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#can-i-cite-a-jaspy-(conda)-environment?,251,41
Requesting updates to a Jaspy environment,"If you would like us to add a new package, or an updated version, to the Jaspy
environments on JASMIN then please use one of the following approaches:
  1. Contact the JASMIN helpdesk with the subject: ""Request for Jaspy update: <package name>""
  2. Get a GitHub account and add an issue to the `ceda-jaspy-envs` repository at:
    1. <https://github.com/cedadev/ceda-jaspy-envs/issues/new>
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#requesting-updates-to-a-jaspy-environment,391,58
"Conda method of ""activating"" Jaspy environments","Jaspy environments can also be activated in a more traditional way using
standard the standard conda approach, for example:
List the available environments:
{{<command>}}
conda info --envs
(out)# conda environments:
(out)#
(out)base                  *  /apps/jasmin/jaspy/miniforge_envs/jaspy3.11/mf3-23.11.0-0
(out)jaspy3.11-mf3-23.11.0-0-v20240302     /apps/jasmin/jaspy/miniforge_envs/jaspy3.11/mf3-23.11.0-0/envs/jaspy3.11-mf3-23.11.0-0-v20240302
(out)jaspy3.11-mf3-23.11.0-0-v20240508     /apps/jasmin/jaspy/miniforge_envs/jaspy3.11/mf3-23.11.0-0/envs/jaspy3.11-mf3-23.11.0-0-v20240508
(out)jaspy3.11-mf3-23.11.0-0-v20240815     /apps/jasmin/jaspy/miniforge_envs/jaspy3.11/mf3-23.11.0-0/envs/jaspy3.11-mf3-23.11.0-0-v20240815
{{</command>}}
Select one of them, e.g. `jaspy3.11-mf3-23.11.0-0-v20240302` and set up to activate it:
{{<command>}}
export PATH=/apps/jasmin/jaspy/miniforge_envs/jaspy3.11/mf3-23.11.0-0/envs/jaspy3.11-mf3-23.11.0-0-v20240302:$PATH
source activate
conda activate jaspy3.11-mf3-23.11.0-0-v20240302
{{</command>}}
Prompt changes to:
{{<command prompt=""(jaspy3.11-mf3-23.11.0-0-v20240302) user@host:~$"">}}
","https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#conda-method-of-""activating""-jaspy-environments",1134,68
,"{{</command>}}
This has the same result as the `module load` approach. The naming of the
environment identifiers includes the ""miniforge"" version used to generate the
environment. The `module load` approach is recommended as the standard method
for activating Jaspy environments.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#,280,40
Using Jaspy beyond JASMIN,"Jaspy is a versatile and generic tool for managing multiple conda
environments. The code is open source, and more information is available at:
<https://github.com/cedadev/jaspy-manager>
",https://help.jasmin.ac.uk/docs/software-on-jasmin/jaspy-envs#using-jaspy-beyond-jasmin,186,24
Met Office NAME Model,"This article provides links to information about running the Met Office
atmospheric dispersion model, NAME, on JASMIN.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/name-dispersion-model,119,17
The NAME atmospheric dispersion model,"The Met Office's Numerical Atmospheric-dispersion Modelling Environment (NAME)
<https://www.metoffice.gov.uk/research/modelling-systems/dispersion-model> can
be run on JASMIN using the LOTUS cluster. A NAME Group Workspace is
provided to store both the Numerical Weather Prediction (NWP) met data needed
to drive NAME as well as the user outputs derived from NAME itself.
This service has been developed through a collaboration between STFC CEDA, the
Met Office Atmospheric Dispersion and Air Quality team and the University of
Leicester.
If you would like to know more about running NAME on JASMIN, please contact us
at [atmospheric.dispersion@metoffice.gov.uk](mailto:atmospheric.dispersion@metoffice.gov.uk).
",https://help.jasmin.ac.uk/docs/software-on-jasmin/name-dispersion-model#the-name-atmospheric-dispersion-model,712,90
Sharing software environments,"This article explains how you can share software environments with other users
on JASMIN.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/share-software-envs,90,14
What is a software environment?,"A software environment is typically a collection of files and directories
associated with a set of environment variables that allows a given session to
access them. In the context of JASMIN, there are a number of Python/other
environments already available on the system. See the 
[Jaspy page]({{% ref ""jaspy-envs"" %}}) for examples of these.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/share-software-envs#what-is-a-software-environment?,343,54
Creating software environments on JASMIN,"In some cases, JASMIN users will need to create their own software
environments because the existing environments do not include all the packages
that they require. See the [virtual environments page]({{% ref ""python-virtual-environments"" %}}) for details on creating your own Python
environments.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/share-software-envs#creating-software-environments-on-jasmin,298,42
Tips on sharing software environments on JASMIN,"If you need to create your own environment it is important to be aware of
which file system you are working on:
- SOF (e.g. `/gws/nopw/j04/*`): does not perform well with small files.
- SSD (e.g. `$HOME` and `/gws/smf/j04/*`): performs much better with small files, but is expensive so only limited quotas are available.
If you are building an environment for your use only then it makes sense to
create it under your $HOME directory.
If you need to share an environment with other JASMIN users you can:
- Request a ""small files"" (**smf**) Group Workspace volume.
- Install the software environment within the ""small files"" volume.
- Then all users with access to that GWS will be able to access the environment.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/share-software-envs#tips-on-sharing-software-environments-on-jasmin,713,123
Quickstart for activating/deactivating software environments,"This article provides a minimum quick-start guide for working with software environments on JASMIN.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/quickstart-software-envs,100,14
Activate (load) an environment,"To activate an environment containing the ""current"" common software packages
(including a modern Python):
{{<command user=""user"" host=""sci1"">}}
module load jaspy
{{</command>}}
or for a specific version, see [how to discover what versions are available]({{% ref ""jaspy-envs/#discover-which-environments-are-available"" %}})
To activate additional packages (known as ""jasmin-sci""):
{{<command user=""user"" host=""sci1"">}}
module load jasmin-sci
{{</command>}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/quickstart-software-envs#activate-(load)-an-environment,456,51
Deactivate (unload) an environment,"If you want to deactivate an environment that you have previously activated,
do:
{{<command user=""user"" host=""sci1"">}}
module unload <env-id>
{{</command>}}
Where ""<env-id>"" is the name used when you activated the environment.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/quickstart-software-envs#deactivate-(unload)-an-environment,227,31
Which environment(s) should you use?,"If you are not sure which environment(s) to use please see details on the
[overview]({{% ref ""software-overview"" %}}) of Jaspy and ""jasmin-sci""
environments page.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/quickstart-software-envs#which-environment(s)-should-you-use?,163,24
'Community Software,"ESMValTool is installed on JASMIN as a _community package_. This article
provides:
- a brief overview of the ESMValTool software
- a description of the main features of the tool
- a quick-start for using ESMValTool on JASMIN
- links to further information
",https://help.jasmin.ac.uk/docs/software-on-jasmin/community-software-esmvaltool,256,43
Overview of ESMValTool,"The Earth System Model Evaluation Tool (ESMValTool) is a community diagnostics
and performance metrics tool for the evaluation of Earth System Models (ESMs)
that allows for routine comparison of single or multiple models, either
against predecessor versions or against observations. The priority of the
effort so far has been to target specific scientific themes focusing on
selected Essential Climate Variables, a range of known systematic biases
common to ESMs, such as coupled tropical climate variability, monsoons,
Southern Ocean processes, continental dry biases and soil hydrology-climate
interactions, as well as atmospheric CO2 budgets, tropospheric and
stratospheric ozone, and tropospheric aerosols.
The tool is being developed in such a way that additional analyses can easily
be added. A set of standard recipes for each scientific topic reproduces
specific sets of diagnostics or performance metrics that have demonstrated
their importance in ESM evaluation in the peer-reviewed literature. The
ESMValTool is a community effort open to both users and developers encouraging
open exchange of diagnostic source code and evaluation results from the CMIP
ensemble. This will facilitate and improve ESM evaluation beyond the state-of-
the-art and aims at supporting such activities within the Coupled Model
Intercomparison Project (CMIP) and at individual modelling centres.
Ultimately, we envisage running the ESMValTool alongside the Earth System Grid
Federation (ESGF) as part of a more routine evaluation of CMIP model
simulations while utilizing observations available in standard formats
(obs4MIPs) or provided by the user.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/community-software-esmvaltool#overview-of-esmvaltool,1639,236
"Installation as a ""community package"" on JASMIN","ESMValTool is installed on JASMIN as a _community package_. This means it is
provided, and maintained, by developers outside the CEDA/JASMIN Team. If you
have queries about using ESMValTool _on JASMIN_ then please contact the JASMIN
Helpdesk and we will forward them to the team that supports this package on
JASMIN.
","https://help.jasmin.ac.uk/docs/software-on-jasmin/community-software-esmvaltool#installation-as-a-""community-package""-on-jasmin",317,51
Main Features,"ESMValTool has the following features:
- Facilitates the complex evaluation of ESMs and their simulations submitted to international Model Intercomparison Projects (e.g., CMIP).
- Standardized model evaluation can be performed against observations, against other models or to compare different versions of the same model.
- Wide scope: includes many diagnostics and performance metrics covering different aspects of the Earth System (dynamics, radiation, clouds, carbon cycle, chemistry, aerosol, sea-ice, etc.) and their interactions.
- Well-established analysis: standard namelists reproduce specific sets of diagnostics or performance metrics that have demonstrated their importance in ESM evaluation in the peer-reviewed literature.
- road documentation: a user guide (Eyring et al., 2015); SPHINX; a log-file is written containing all the information of a specific call of the main script: creation date of running the script, version number, analyzed data (models and observations), applied diagnostics and variables, and corresponding references. This helps to increase the traceability and reproducibility of the results.
- High flexibility: new diagnostics and more observational data can be easily added.
- Multi-language support: Python, NCL, R... other open-source languages are possible.
- CF/CMOR compliant: data from many different projects can be handled (CMIP, obs4mips, ana4mips, CCMI, CCMVal, AEROCOM, etc.). Routines are provided to CMOR-ize non-compliant data.
- Integration in modelling workflows: for EMAC, NOAA-GFDL and NEMO, can be easily extended.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/community-software-esmvaltool#main-features,1574,218
Quick User Guide on JASMIN,"The latest version of ESMValTool is available for users on JASMIN and can be
accessed via a standard module:
{{<command user=""user"" host=""sci1"">}}
module load esmvaltool
{{</command>}}
To run a yaml-formatted recipe:
{{<command user=""user"" host=""sci1"">}}
esmvaltool run recipe.yml
{{</command>}}
For a complete guide on how to configure the tool, set up and run available
diagnostic recipes please consult the documentation at:
<https://docs.esmvaltool.org/en/latest/>
",https://help.jasmin.ac.uk/docs/software-on-jasmin/community-software-esmvaltool#quick-user-guide-on-jasmin,469,61
Further information,"- Documentation: <https://docs.esmvaltool.org/en/latest/>
- Website: <https://www.esmvaltool.org/index.html>
- GitHub: <https://github.com/ESMValGroup/ESMValTool>
[](https://github.com/ESMValGroup/ESMValTool)
",https://help.jasmin.ac.uk/docs/software-on-jasmin/community-software-esmvaltool#further-information,209,10
Conda environments and Python virtual environments,"{{<alert type=""danger"">}}
Important changes took place in September 2024 affecting what software can be used on JASMIN.
Please read [this announcement](https://www.ceda.ac.uk/news/updates/2024/2024-08-29-important-software-changes-autumn/) carefully.
This supercedes the information below which has yet to be updated in line with this.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/software-on-jasmin/conda-environments-and-python-virtual-environments,349,38
Introduction,"This article describes two types of software environments that you can create
in order to install packages for your own use on JASMIN. Typical examples why
you may wish to do this is if you have asked us to add packages to
{{<link ""jaspy-envs"">}}Jaspy{{</link>}} but wish to make use of them before the next release, or
if they are not likely to be relevant to other users.
Separate pages explain the details of how to create and install {{<link ""python-virtual-environments"">}}Python
virtual environments{{</link>}} and {{<link ""creating-and-using-miniforge-environments"">}}Conda
environments{{</link>}}. This
page gives an overview of what they are, and how to choose which one is most
suitable for your needs.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/conda-environments-and-python-virtual-environments#introduction,713,107
Description of Python virtual environments and Conda environments,"- A Python virtual environment is a relatively lightweight environment, which is used for running Python packages, typically installed using the ""pip"" installer from the Python Package Index {{<link ""https://pypi.org/"">}}pypi{{</link>}} or locally from Python source containing a `setup.py` file. This enables you to install packages in your home directory without writing to the underlying Python installation itself (for example when you do not have write permission), and you can have any number of separate virtual environments and ""activate"" the relevant one when needed. When you run `pip` to install a Python package, additional Python packages may be installed automatically in order to satisfy dependencies. Depending on the package being installed, if it requires compiled libraries to accompany it, it may try to compile these locally, but depending what development libraries are available, occasionally this might not succeed.
- By contrast, a Conda environment is a much bulkier, more fully featured environment, using the `mamba` or `conda` package managers. This enables the installation of packages from conda channels, usually {{<link ""https://conda-forge.org/"">}}conda-forge{{</link>}}, which are not restricted to being Python packages. Where packages contain compiled libraries, these are generally available as pre-compiled binaries. As with python virtual environments, you can have any number of these environments and activate the required one. When you run the mamba or conda installer, similarly it will install whatever additional packages are required in order to satisfy dependencies. (It is also possible to use the pip installer when working with conda environments.) Various versions of the conda installer are available, but for use on JASMIN, we now ask users to use Miniforge, for licensing reasons.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/conda-environments-and-python-virtual-environments#description-of-python-virtual-environments-and-conda-environments,1836,267
Environment size,"To take an example of the size, a new Python virtual environment without
additional packages occupies about 10MB and contains under 1000 files (maybe
approximately twice this if using the `--system-site-packages` option
explained in more detail elsewhere), whereas as a new conda base environment
occupies about 400MB and contains over 20,000 files.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/conda-environments-and-python-virtual-environments#environment-size,350,52
Installation examples,"To give an example of installing a Python package, the `numexpr` library is a
numerical expression evaluator for NumPy. It is available as a pip package
called `numexpr`, and also as a conda package called `numexpr`. (For some
packages, the two may have slightly different names.) It can be installed
successfully into either a Python virtual environment using
`pip install numexpr` or a conda environment using `mamba install numexpr`
(or `conda install numexpr`). In either case, the `numpy` package on which
it depends, amongst other things, will be installed automatically if required.
To give an example of installing a non-python package, `zsh` is a Unix shell
which combines various features of bash and csh. You cannot install this using
`pip install` because it is not a python package, but it is available on
conda-forge, and can be installed into a conda environment using `mamba
install zsh`.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/conda-environments-and-python-virtual-environments#installation-examples,905,146
Combining environments,"We already provide a wide range of packages via {{<link ""jaspy-envs"">}}Jaspy{{</link>}}.
This is in itself a conda environment, and it is important to note that
although you can use a Python virtual environment to install additional
packages when using a conda environment, you cannot have more than one conda
environment activated at the same time.
- When using Python virtual environments, you are advised to start by activating a Jaspy environment, as this will ensure that you are using a version of Python that we support, as well as significantly increasing the range of compiled libraries available during the `pip install` process. You can also use the `--system-site-packages` option to access the Python packages provided by Jaspy itself, as described in more detail on the {{<link ""python-virtual-environments"">}}virtual environments{{</link>}} help page.
- However, if you choose to use your own conda environment, then you will activate it **instead** of Jaspy, and you will need to install into it **everything** that you will need to accompany the package which you wish to use.
Note that because you can install `pip` packages into conda environment, it is
not generally useful to create a virtual environment in order to extend your
own private conda environment. The reason for creating one to extend Jaspy is
that users do not have write permission to add packages to Jaspy itself.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/conda-environments-and-python-virtual-environments#combining-environments,1401,223
Choosing between a Python virtual environment and a Conda environment,"Because python virtual environments are much more lightweight that conda
environments, and also give you access to packages that we provide via Jaspy,
we would generally recommend that you start by trying a Python virtual
environment. However, this might prove not to be possible, for example
because:
- The package that you wish to install is not available via PyPI (perhaps it is not a Python package).
- The package that you wish to install depends on compiled libraries that do not build successfully during a `pip install`. Often pre-compiled versions will be available from conda-forge.
If you do decide to install a conda environment, then remember to install
additional packages that you might otherwise have used via Jaspy.
",https://help.jasmin.ac.uk/docs/software-on-jasmin/conda-environments-and-python-virtual-environments#choosing-between-a-python-virtual-environment-and-a-conda-environment,733,119
How to submit an MPI parallel job,"This article explains the submission of an MPI parallel job to Slurm/ LOTUS.
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-submit-an-mpi-parallel-job,77,13
What is an MPI parallel job?,"An MPI parallel job runs on more than one core and more than one host using
the Message Passing Interface (MPI) library for communication between all
cores. A simple script, such as the one given below ""my_script_name.sbatch  ""
#!/bin/bash
#SBATCH -p par-multi
#SBATCH -n 36
#SBATCH -t 30
#SBATCH -o %j.log
#SBATCH -e %j.err
# Load a module for the gcc OpenMPI library  (needed for mpi_myname.exe)
module load eb/OpenMPI/gcc/4.0.0
# Start the job running using OpenMPI's ""mpirun"" job launcher
mpirun ./mpi_myname.exe
`-n` refers to the number of processors or cores you wish to run on. The rest
of  the `#SBATCH` input  options, and many more besides, can be found in  the
`sbatch` manual  page or in the related articles. You must only use the par-
multi queue for parallel jobs using MPI.
For those familiar with LOTUS running Platform MPI and Platform LSF, please
note that the job is started using the OpenMPI native ""mpirun"" command not the
""mpirun.lotus"" wrapper script that was previously required. We have provided
an mpirun.lotus script for backward compatiblity but it just runs the native
mpirun
To submit the job, do not run the script, but rather use it as the standard
input to sbatch, like so:
sbatch --exclusive my_script_name.sbatch
The `--exclusive` flag  is used to group the parallel jobs onto hosts which
are allocated only to run this job. This ensures the best MPI communication
consistency and bandwidth/latency for the job and ensures no interference from
other users' jobs that might otherwise be running on those hosts.
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-submit-an-mpi-parallel-job#what-is-an-mpi-parallel-job?,1547,253
MPI implementation and Slurm,"The OpenMPI library is the only supported MPI library on the cluster. OpenMPI
v3.1.1 and v4.0.0 are provided which are fully MPI3 compliant. MPI I/O
features are fully supported *only* on the LOTUS /work/scratch-pw* volumes as
these use a Panasas fully parallel file system. The MPI implementation on
CentOS7 LOTUS/Slurm is available via the module environment for each compiler
as listed below:
eb/OpenMPI/gcc/3.1.1 
eb/OpenMPI/gcc/4.0.0       
eb/OpenMPI/intel/3.1.1
**Note:** OPenMPI Intel compiler is due shortly as is 4.0.3  
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-submit-an-mpi-parallel-job#mpi-implementation-and-slurm,531,75
Parallel MPI compiler with OpenMPI,"Compile and link to OpenMPI libraries using the mpif90, mpif77, mpicc, mpiCC
wrapper scripts which are in the default unix path. The scripts will detect
which compiler you are using (Gnu, Intel) by the compiler environment loaded
and add the relevant compiler library switches. For example:
module load intel/20.0.0
module load eb/OpenMPI/intel/3.1.1
mpif90
will use the Intel Fortran compiler `ifort` and OpenMPI/3.1.1.  Whereas
module load eb/OpenMPI/gcc/3.1.1
mpicc
will call the GNU C compiler `gcc` and  OpenMPI/3.1.1.
The OpenMPI User Guides can be found at <https://www.open-mpi.org/doc/>.
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-submit-an-mpi-parallel-job#parallel-mpi-compiler-with-openmpi,597,85
LOTUS overview,"This article gives an overview of the LOTUS batch computing cluster which is
part of JASMIN. It covers:
- What LOTUS is and what it can be used for
- Where LOTUS can be accessed from
",https://help.jasmin.ac.uk/docs/batch-computing/lotus-overview,183,36
What is LOTUS,"LOTUS is not, in itself, a High-Performance Computing (HPC) facility, but
provides the batch and parallel processing component of the JASMIN data-
intensive scientific analysis environment. LOTUS is a cluster of physical
machines, running the [Slurm workload manager]({{% ref ""slurm-scheduler-overview"" %}}), enabling efficient scheduling of larger data analysis tasks
across nodes in the cluster as a single unit -see Figure 1.
Each node in the
cluster is connected by 10Gbit/s Ethernet to JASMIN's high-performance
40Gbit/s core network. Although not its primary function, LOTUS also
facilitates MPI-based parallel processing.
JASMIN provides both interactive and batch computing environments,
recognising  that scientists often need to develop and test workflows
interactively before running those workflows efficiently at scale. Nodes
within LOTUS run the same stack of software and can access the same high-
performance storage as the JASMIN Scientific Analysis servers, ensuring a
consistent working environment for all phases of users' workflows.
See [LOTUS Hardware]({{% ref ""lotus-cluster-specification"" %}}) for details of
the current LOTUS environment summarised in this schematic presentation
**Figure 1** shows a schematic presentation of the LOTUS cluster and its
environment
{{< image src=""img/docs/lotus-overview/file-QPxolXD1Tu.png"" caption=""LOTUS schematic"" >}}
",https://help.jasmin.ac.uk/docs/batch-computing/lotus-overview#what-is-lotus,1380,185
When to use LOTUS,"LOTUS is ideally suited to workflows which need to process or compare entire
datasets, stored either in Group Workspaces or in the CEDA archives. The
latter are directly accessible read-only so can be processed in-place without
the need to copy files. Intermediate working files (within batch jobs) should
be stored temporarily in `/work/scratch-pw*` and ` /work/scratch-nopw*`
volumes which are shared across the cluster, while persistent outputs can be
written efficiently to Group Workspaces and shared with collaborators for the
duration of a project.
See [Access to Storage]({{% ref path=""/docs/getting-started/storage"" %}}) for details about which file
systems are appropriate to use and how to access them.
LOTUS currently has around 19,000 cores, but is heavily used and implements a
fair-share scheduling system between users. It is not intended as a substitute
for dedicated HPC facilities, rather as a complementary environment in which
model outputs can be analyzed and compared with observational data. Users with
large-scale compute-heavy requirements (in particular those requiring large-
scale parallel processing) should look to access other parts of the national
HPC infrastructure such as [ARCHER2](http://www.archer2.ac.uk/) or
[MONSooN](http://collab.metoffice.gov.uk/twiki/bin/view/Support/MONSooN).
In order to maintain a safe and reliable working environment for all within
LOTUS and more widely within JASMIN, users are expected to follow [the best
practice outlined in this documentation]({{% ref ""tips-for-new-users"" %}}).
",https://help.jasmin.ac.uk/docs/batch-computing/lotus-overview#when-to-use-lotus,1550,214
How to gain access to LOTUS,"LOTUS is accessible via the Slurm batch scheduler that is available to use from
all JASMIN [scientific analysis servers]({{% ref ""sci-servers"" %}}).
From the above servers, it is possible to submit, monitor, and control batch
jobs using the Slurm commands.
Please note that if you have only recently requested access to [JASMIN login
services]({{% ref ""get-login-account"" %}}) and had this approved, there can
sometimes be a delay (typically up to a day, but in rare cases can be longer)
before the necessary configuration is created for you on LOTUS. You will not
be able to submit jobs to LOTUS queues until this has been completed.
Typically you would see an error message such as this, in this case after an
unsuccessful attempt to submit to the `short-serial` queue:
sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified
If this occurs, please try again in 24 hours before contacting the JASMIN helpdesk.
",https://help.jasmin.ac.uk/docs/batch-computing/lotus-overview#how-to-gain-access-to-lotus,959,154
What is a Job Scheduler?,"A job or batch scheduler, is a tool that manages how user jobs
are queued and run on a set of compute resources. In the case of LOTUS the
compute resources are the set of compute nodes that make up the [LOTUS hardware]({{% ref ""lotus-cluster-specification"" %}}). Each user can submit
jobs to the scheduler which then decides which jobs to run and where to
execute them. The scheduler manages the jobs to ensure that the compute
resources are being used efficiently and that users get appropriate access to
those resources.
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-scheduler-overview#what-is-a-job-scheduler?,523,90
The Slurm Scheduler,"{{< link ""https://slurm.schedmd.com"" >}}Slurm{{</link>}} is the job
scheduler deployed on JASMIN. It allows users to submit, monitor, and control
jobs on the LOTUS cluster.
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-scheduler-overview#the-slurm-scheduler,173,24
General principles for working with Slurm,"Before learning how to use Slurm, it is worthwhile becoming familiar with the
basic principles of scheduler operation in order to get the best use out of
the LOTUS cluster. Scheduler software exists simply because the amount of jobs
that users wish to run on a cluster at any given time is usually greatly in
excess of the amount of resources available. This means that the scheduler
must queue jobs and work out how to run them efficiently.
Several factors are taken into account during scheduling, such as job duration
and size, but the basic principles remain the same throughout - every user
gets a fair share of the cluster based on the jobs that they have submitted.
This leads to a small number of important principles:
- Do not try to second guess the scheduler! Submit all of your jobs when you want to run them and let the scheduler figure it out for you. You will get a fair share, and if you don't then we need to adjust the scheduler (so get in touch and let us know).
- Give the scheduler as much information as possible. There are a number of optional parameters (see [How to submit jobs]({{% ref ""how-to-submit-a-job-to-slurm"" %}})) such as job duration, and if you put these in then you have an even better chance of getting your jobs to run.
- It is very difficult for one user to monopolise the cluster, even if they submit thousands of jobs. The scheduler will still aim to give everyone else a fair share, so long as there are other jobs waiting to be run.
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-scheduler-overview#general-principles-for-working-with-slurm,1478,270
Fair share for all users,"{{<image src=""img/docs/slurm-scheduler-overview/Screenshot-2023-02-20-at-21.32.28.png"" caption=""Example of scheduling"">}}
In the example above, three users (left column) have jobs in the queue (middle column)
which are waiting to run on the cluster (right column). As the blue user's job
finishes (middle row), all three users could potentially use the two job slots
that become available. However, the orange and purple users already have jobs
running, whereas the blue user does not, and as such it is the blue user's
jobs that are run (bottom row).
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-scheduler-overview#fair-share-for-all-users,552,81
LOTUS queues,"There are five standard Slurm queues (also known as ""partitions"" in Slurm terminology) for batch job submissions to the LOTUS
cluster: `short-serial`, `long-serial`, `par-single`, `par-multi` and `high-mem`.
The default queue is `short-serial`. For testing new workflows, the
additional queue `test`is recommended. The specification of each queue is
described in detail in this article: [Slurm queues on LOTUS]({{% ref ""slurm-queues"" %}})
Queues other than the five standard queues with the test queue should be
ignored unless you have been specifically instructed to use them.
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-scheduler-overview#lotus-queues,578,83
The typical workflow for LOTUS jobs,"One of the great advantages of using JASMIN is the ability to create batch
jobs that run simultaneously on multiple LOTUS nodes. However, users familiar
with running interactively on a single machine often take time to adapt to
this new way of working. The change involves moving from a ""watching your job
run"" approach to ""submitting your job and coming back later"".
The typical workflow for setting up and running LOTUS jobs is as follows:
  1. Login to one of the [scientific analysis servers]({{% ref ""sci-servers"" %}}).
  2. Install/write/configure your processing code.
  3. Test your code interactively: run it locally in a single-process test case.
  4. Create a wrapper script for your code that allows multiple versions to run independently: e.g. running for different dates or processing different spatial regions/variables.
  5. [Submit your jobs]({{% ref ""how-to-submit-a-job-to-slurm"" %}}) via the batch script.
  6. [Monitor your jobs]({{% ref ""how-to-monitor-slurm-jobs"" %}}).
  7. Gather/analyse/review the outputs as required.
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-scheduler-overview#the-typical-workflow-for-lotus-jobs,1045,154
Project-specific LOTUS queues,"Occasionally a project has a specific requirement for a collection of compute
nodes that involve the provision of a project-specific queue. If you are
working on such a project your project lead will provide guidance on which
queue to use. Please contact the helpdesk if
you are interested in setting up a project-specific queue.
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-scheduler-overview#project-specific-lotus-queues,330,54
Slurm queues,"This article introduces the Slurm scheduler queues/partitions for batch job
submissions to the LOTUS and ORCHID clusters.
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-queues,122,17
Queue names,"The Slurm queues in the LOTUS cluster are:
- `test`
- `short-serial`
- `long-serial`
- `par-single`
- `par-multi`
- `high-mem`
- `short-serial-4hr`
Each queue is has attributes of run-length limits (e.g. short, long) and
resources. A full breakdown of each queue and its associated resources is
shown below in Table 1.
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-queues#queue-names,319,51
Queue details,"Queues represent a set of pending jobs, lined up in a defined order, and
waiting for their opportunity to use resources. The queue is specified in the
job script file using Slurm scheduler directive like this:
#SBATCH -p <partition=queue_name>`
where `<queue_name>` is the name of the queue/partition (Table 1. column 1)
Table 1 summarises important specifications for each queue such as run time
limits and the number of CPU core limits. If the queue is not specified, Slurm
will schedule the job to the queue `short-serial` by default.
Table 1. LOTUS/Slurm queues and their specifications
Queue name  |  Max run time  |  Default run time  |  Max CPU cores per job  |  MaxCpuPerUserLimit  |  Priority  
---|---|---|---|---|---  
`test` |  4 hrs  |  1hr  |  8  |  8  |  30  
`short-serial` |  24 hrs  |  1hr  |  1  |  2000  |  30  
`par-single` |  48 hrs  |  1hr  |  16  |  300  |  25  
`par-multi` |  48 hrs  |  1hr  |  256  |  300  |  20  
`long-serial` |  168 hrs  |  1hr  |  1  |  300  |  10  
`high-mem` |  48 hrs  |  1hr  |  1  |  75  |  30  
`short-serial-4hr`<br>(**Note 3**)  |  4 hrs  |  1hr  |  1  |  1000  |  30
{.table .table-striped}
**Note 1** : Resources requested by a job must be within the resource
allocation limits of the selected queue.
**Note 2:** The default value for `--time=[hh:mm:ss]` (predicted maximum wall
time) is 1 hour for the all queues. If you do not specify this option
and/or your job exceeds the default maximum run time limit then it will be
terminated by the Slurm scheduler.
**Note 3** : A user must specify the Slurm job account `--account=short4hr`
when submitting a batch job to the `short-serial-4hr` queue.
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-queues#queue-details,1654,287
State of queues,"The Slurm command `sinfo` reports the state of queues and nodes
managed by Slurm. It has a wide variety of filtering, sorting, and formatting
options.
{{<command shell=""bash"">}}
sinfo   
(out)PARTITION     AVAIL  TIMELIMIT  NODES  STATE NODELIST
(out)test             up    4:00:00     48   idle host[146-193]
(out)short-serial*    up 1-00:00:00     48   idle host[146-193]
(out)long-serial      up 7-00:00:00     48   idle host[146-193]
(out)par-single       up 2-00:00:00     48   idle host[146-193]
(out)par-multi        up 2-00:00:00     48   idle host[146-193]
(out)high-mem         up 2-00:00:00     48   idle host[146-193]
(out)lotus_gpu        up 7-00:00:00     48   idle host[146-193]
(out)copy             up 7-00:00:00     48   idle host[146-193]
(out)cpom-comet       up 7-00:00:00     48   idle host[146-193]
(out)...
{{</command>}}
{{< alert type=""info"" >}}
Queues other than the standard queues `test` , `short-serial` ,
`long-serial`, `par-single`, `par-multi` and `high-mem` should be ignored
as they implement different job scheduling and control policies.
{{< /alert >}}
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-queues#state-of-queues,1090,124
'sinfo' Output field description:,"By default, the Slurm command 'sinfo' displays the following information:
- **PARTITION** : Partition name followed by **\*** for the default queue/partition
- **AVAIL** : State/availability of a queue/partition. Partition state: up or down.
- **TIMELIMIT** : The maximum run time limit per job in each queue/partition is shown in TIMELIMIT in days- hours:minutes  :seconds . e.g. 2-00:00:00 is two days maximum runtime limit 
- **NODES** : Count of nodes with this particular configuration e.g. 48 nodes
- **STATE** : State of the nodes. Possible states include: allocated, down, drained, and idle. For example, the state ""idle"" means that the node is not allocated to any jobs and is available for use.
- **NODELIST** List of node names associated with this queue/partition
The `sinfo` example below, reports more complete information about the
partition/queue short-serial
{{<command>}}
sinfo --long --partition=short-serial
(out)Tue May 12 18:04:54 2020
(out)PARTITION    AVAIL TIMELIMIT JOB_SIZE  ROOT  OVERSUBS  GROUPS NODES    STATE NODELIST
(out)short-serial* up  1-00:00:00  1-infinite  no  NO    all     48  idle host[146-193]
{{</command>}}
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-queues#'sinfo'-output-field-description:,1152,164
How to choose a Slurm queue/partition,,https://help.jasmin.ac.uk/docs/batch-computing/slurm-queues#how-to-choose-a-slurm-queue/partition,0,0
Test queue,"The `test` queue can be used to test new workflows and also to help new
users to familiarise themselves with the Slurm batch system. Both serial and
parallel code can be tested on the `test`queue. The maximum runtime is 4 hrs
and the maximum number of jobs per user is 8 job slots. The maximum number of
cores for a parallel job e.g. MPI, OpenMP, or multi-threads is limited to 8
cores. The `test`queue should be used when unsure of the job resource
requirements and behavior at runtime because it has a confined set of LOTUS
nodes (Intel node type) not shared with the other standard LOTUS queues.
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-queues#test-queue,599,108
Serial queues,"Serial and array jobs with a single CPU core should be submitted to one of the
following serial queues depending on the job duration and the memory
requirement. The default queue is `short-serial`
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-queues#serial-queues,197,33
short-serial,"Serial and/or array jobs with a single CPU core each and run time less than 24
hrs should be submitted to the `short-serial` queue . This queue has the
highest priority of 30. The maximum number of jobs that can be scheduled to
start running from  the `short-serial` is 2000 jobs whilst  both job's
resources are available and user' priority is high
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-queues#short-serial,350,62
long-serial,"Serial or array jobs with a single CPU core and run time greater than 24 hrs
and less than 168 hrs (7 days) should be submitted to the queue `long-serial`
.  This queue has the lowest priority of 10 and hence jobs might take longer
to be scheduled to run relatively to other jobs in higher priority queues.
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-queues#long-serial,307,58
high-mem,"Serial or array jobs with a single CPU core and high memory requirement (> 64
GB) should be submitted to the `high-mem` queue and the required memory must
be specified `--mem=XXX` (XXX is in MB units). The job should not exceed the
maximum run time limit of 48hrs. This queue is not configured to accept
exclusive jobs.
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-queues#high-mem,320,57
Parallel queues,"Jobs requiring more than one CPU core should be submitted to one of the
following parallel queues depending on the type of parallelisms such as shared
memory or distributed memory jobs.
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-queues#parallel-queues,186,31
par-single,"Shared memory multi-threaded jobs with  a maximum  of 16 threads should be
submitted to  the `par-single` queue . Each thread should be allocated one CPU
core. Oversubscribing the number of threads to the CPU cores will cause the
job to run very slow. The number of CPU cores should be specified via the
submission command line `sbatch -n <number of CPU cores>` or  by adding the
Slurm directive `#SBATCH -n <number of CPU cores>`in the job script file. An
example is shown below:
{{<command>}}
sbatch --ntasks=4 --partition=par-single < myjobscript
{{</command>}}
Note: Jobs submitted with a number of CPU cores greater than 16 will be
terminated (killed) by the Slurm scheduler with the following statement in the
job output file:
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-queues#par-single,733,119
par-multi,"Distributed memory jobs with inter-node communication using the MPI library
should be submitted to  the `par-multi` queue . A single MPI process (rank)
should be allocated  a  single CPU core. The number of CPU cores should be
specified via the Slurm submission command  flag `sbatch -n <number of CPU
cores>` or  by adding the Slurm directive `#SBATCH -n <number of CPU cores>`
to  the job script file. An example is shown below:
{{<command>}}
sbatch --ntasks=4 --partition=par-multi < myjobscript
{{</command>}}
Note 1: The number of CPU cores gets passed from Slurm submission  flag `-n` .
Do not add  the `-np` flag  to `mpirun` command  .
Note 2: Slurm will reject a job that requires a number of CPU cores greater
than the limit of 256.
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-queues#par-multi,743,124
Slurm Scheduler,"{{< link ""https://slurm.schedmd.com/"" >}}Slurm{{</link>}} is the job scheduler deployed on JASMIN. It
allows users to submit, monitor, and control jobs on the [LOTUS]({{% ref ""lotus-overview"" %}}) (CPU)
and [ORCHID]({{% ref ""orchid-gpu-cluster"" %}}) (GPU) clusters.
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#slurm-scheduler,266,34
Essential Slurm commands,"| **Slurm command**                  | **Description**                         |
| ---------------------------------- | --------------------------------------- |
| sbatch _script_file_               | Submit a job script to the scheduler    |
| sinfo                              | Show available scheduling queues        |
| squeue -u _\<username\>_           | List user's pending and running jobs    |
| srun -n 1 -p test \--pty /bin/bash | Request an interactive session on LOTUS |
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#essential-slurm-commands,510,61
Job specification,"Long and short argument names are separated by a comma.
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#job-specification,56,10
`#SBATCH`,"- Scheduler directive - goes in front of the arguments below in a job script file
- An example Slurm job script file is available [here]({{% ref ""how-to-submit-a-job-to-slurm/#method-1-submit-via-a-slurm-job-script"" %}})
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#`#sbatch`,221,29
"`--partition=QUEUE_NAME, -p QUEUE_NAME`","- Specify the scheduling queue/partition by replacing `QUEUE_NAME`
- A list of queues/partitions that you can use are available [here]({{% ref ""slurm-queues"" %}})
","https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#`--partition=queue_name,--p-queue_name`",163,23
"`--time=hh:mm:ss, -t hh:mm:ss`","- Set the maximum runtime limit by replacing `hh:mm:ss`
","https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#`--time=hh:mm:ss,--t-hh:mm:ss`",56,9
`--time-min=hh:mm:ss`,"- Set an estimated runtime by replacing `hh:mm:ss`
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#`--time-min=hh:mm:ss`,51,8
`--job-name=JOB_NAME`,"- Specify a name for the job by replacing `JOB_NAME`
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#`--job-name=job_name`,53,10
"`--output=FILE_NAME, -o FILE_NAME`","- Standard job output - where your program prints to normally (`stdout`)
- Defaults: appends to the file and file name is `slurm-%j.out`, where `%j` is replaced by the job ID
","https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#`--output=file_name,--o-file_name`",175,31
"`--error=FILE_NAME, -e FILE_NAME`","- Standard error output - where your program prints to if an error occurs (`stderr`)
- Defaults: appends to the file and file name is `slurm-%j.err`, where `%j` is replaced by the job ID
","https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#`--error=file_name,--e-file_name`",187,34
`--open-mode=append|truncate`,"- Write mode for error/output files
- Pick either `append` or `truncate`
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#`--open-mode=append|truncate`,73,12
`--mem=XXX`,"- Specify that `XXX` memory is required for the job. Default units are megabytes (e.g. `--mem=250` means 250MB)
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#`--mem=xxx`,112,18
`--array=INDEX`,"- Specify a job array, e.g. `--array=1-10` - for an example submission script, see [this page]({{% ref ""how-to-submit-a-job-to-slurm/#job-array-submission"" %}})
- The default standard output file name is `slurm-%A_%a.out`, where `%A` is replaced by the job ID and `%a` with the array index
- To change this, use `--output` and `--error` as above with `%A` and `%a` instead of `%j`
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#`--array=index`,381,59
`--array=INDEX%ArrayTaskThrottle`,"- A maximum number of simultaneously running tasks from the job array may be specified using a `%` separator
- For example, `--array=1-15%4` will limit the number of simultaneously running tasks from this job array to 4
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#`--array=index%arraytaskthrottle`,220,37
"`--chdir=DIRECTORY, -D DIRECTORY`","- Set the working directory of the batch script to `DIRECTORY` before it is executed
","https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#`--chdir=directory,--d-directory`",85,15
`--exclusive`,"- Exclusive execution mode
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#`--exclusive`,27,4
`--dependency=<dependency_list>`,"- Defer the start of this job until the specified dependencies have been satisfied as completed
- See the {{< link ""https://slurm.schedmd.com/sbatch.html#OPT_dependency"" >}}Slurm documentation{{</link>}} for examples
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#`--dependency=<dependency_list>`,217,26
"`--ntasks=NUMBER_OF_CORES, -n NUMBER_OF_CORES`","- Number of CPU cores
","https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#`--ntasks=number_of_cores,--n-number_of_cores`",22,5
`--constraint=HOST_GROUP_NAME`,"- To select a node with a specific processor model
- A list of host groups that you can use are available [here]({{% ref ""lotus-cluster-specification"" %}})
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#`--constraint=host_group_name`,156,26
Job control commands,"| **Slurm command**               | **Description**               |
| ------------------------------- | ----------------------------- |
| scancel _\<jobid\>_             | Kill a job                    |
| scontrol show job _\<jobid\>_   | Show details job information  |
| scontrol update job _\<jobid\>_ | Modify a pending job          |
| scancel \--user=_\<username\>_  | Kill all jobs owned by a user |
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#job-control-commands,432,55
Job environment variables,"| **Slurm variable**    | **Description**                      |
| --------------------- | ------------------------------------ |
| $SLURM_JOBID          | Job identifier number                |
| $SLURM_ARRAY_JOB_ID   | Job array                            |
| $SLURM_ARRAY_TASK_ID  | Job array index                      |
| $SLURM_ARRAY_TASK_MAX | Last index number within a job array |
| $SLURM_NTASKS         | Number of processors allocated       |
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/batch-computing/slurm-quick-reference#job-environment-variables,479,52
Job information,"Information on all running and pending batch jobs managed by Slurm can be
obtained from the Slurm command `squeue`. Note that information on completed
jobs is only retained for a limited period. Information on jobs that ran in
the past is via `sacct`. An example of the output `squeue` is shown below.
{{<command user=""user"" host=""sci-vm-01"">}}
squeue
(out)JOBID PARTITION     NAME   USER ST       TIME  NODES NODELIST(REASON)
(out)18957 short-ser     mean   user1  R       0:01      1 host147
(out)18956 short-ser     calc   user2  R      48:38      1 host146
(out)18967      test     wrap   user1  R      14:25      1 host146
{{</command>}}
where the field `ST` is the job state and the `TIME` is the time used by the
job.
A batch job evolves in several states in the course of its execution. The
typical job states are defined in Table 1
Table 1: Job states
Symbol | Job state  |  Description  
---|---|---
PD  |  Pending  |  The job is waiting in a queue for allocation of resources
R  |  Running  |  The job currently is allocated to a node and is running
CG  |  Completing  |  The job is finishing but some processes are still active
CD  |  Completed  |  The job has completed successfully
F  |  Failed  |  Failed with non-zero exit value
TO  |  Terminated  |  Job terminated by Slurm after reaching its runtime limit
S  |  Suspended  |  A running job has been stopped with its resources released to other jobs
ST  |  Stopped  |  A running job has been stopped with its resources retained
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-monitor-slurm-jobs#job-information,1519,248
Slurm commands for monitoring jobs,"A list of the most commonly used commands and their options for monitoring
batch jobs are listed in Table 2, below:
Table 2. List of important Slurm commands and their options for monitoring
jobs
Slurm Command  |  Description  
---|---  
`squeue` |  To view information for all jobs running and pending on the cluster  
`squeue --user=username` |  Displays running and pending jobs per individual user  
`squeue --states=PD` |  Displays information for pending jobs (PD state) and their reasons  
`squeues --states=all` |  Shows a summary of the number of jobs in different states  
`scontrol show job JOBID` |  Shows detailed information about your job (JOBID = job number) by searching the current event log file  
`sacct -b` |  Shows a brief listing of past jobs
`sacct -l -j JOBID` |  Shows detailed historical job information of a past job with jobID  
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-monitor-slurm-jobs#slurm-commands-for-monitoring-jobs,882,141
Inspection of job output files,"An example of the job output file from a simple job submitted to Slurm:
{{<command shell=""bash"">}}
sbatch -p test  --wrap=""sleep 2m""
(out)Submitted batch job 18973  
{{</command>}}
{{<command shell=""bash"">}}
scontrol show job 18973
(out)JobId=18973 JobName=wrap
(out)   UserId=fchami(26458) GroupId=users(26030) MCS_label=N/A
(out)   Priority=1 Nice=0 Account=jasmin QOS=normal
(out)   JobState=RUNNING Reason=None Dependency=(null)
(out)   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
(out)   RunTime=00:00:08 TimeLimit=01:00:00 TimeMin=N/A
(out)  SubmitTime=2020-05-20T14:10:28 EligibleTime=2020-05-20T14:10:28
(out)   AccrueTime=2020-05-20T14:10:28
(out)   StartTime=2020-05-20T14:10:32 EndTime=2020-05-20T15:10:32 Deadline=N/A
(out)   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2020-05-20T14:10:32
(out)   Partition=test AllocNode:Sid=sci2-test:18286
(out)   ReqNodeList=(null) ExcNodeList=(null)
(out)   NodeList=host147
(out)   BatchHost=host147
(out)   NumNodes=1 NumCPUs=1 NumTasks=1 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
(out)   TRES=cpu=1,mem=128890M,node=1,billing=1
(out)   Socks/Node=*NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
(out)   MinCPUsNode=1 MinMemoryNode=128890M MinTmpDiskNode=0
(out)   Features=(null) DelayBoot=00:00:00
(out)   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
(out)   Command=(null)
(out)   WorkDir=/home/users/fchami
(out)   StdErr=/home/users/fchami/slurm-18973.out
(out)   StdIn=/dev/null
(out)   StdOut=/home/users/fchami/slurm-18973.out
(out)   Power=
{{</command>}}
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-monitor-slurm-jobs#inspection-of-job-output-files,1527,116
History of jobs,"{{<command shell=""bash"">}}
sacct
(out)        JobID    JobName  Partition    Account  AllocCPUS      State ExitCode
(out)------------ ---------- ---------- ---------- ---------- ---------- --------
(out)18963              wrap par-single     jasmin          1  COMPLETED      0:0
(out)18964              wrap short-ser+     jasmin          1  COMPLETED      0:0
(out)18965              wrap par-single     jasmin          1  COMPLETED      0:0
(out)18966              wrap short-ser+     jasmin          1  COMPLETED      0:0
{{</command>}}
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-monitor-slurm-jobs#history-of-jobs,541,47
How to submit a job,"This article explains how to submit a batch job to the new scheduler Slurm.
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-submit-a-job,76,14
What is a batch job?,"A batch job is controlled by a script written by the user who submits the job
to the batch system Slurm. The batch system then selects the resources for the
job and decides when to run the job. Note: the term ""job"" is used throughout
this documentation to mean a ""batch job"".
There are two ways of submitting a job to Slurm:
  1. Submit via a Slurm job script - create a bash script that includes directives to the Slurm scheduler
  2. Submit via command-line options - provide directives to Slurm via command-line arguments
Both options are described below.
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-submit-a-job#what-is-a-batch-job?,559,99
Which servers can you submit jobs from?,"Jobs can be submitted to Slurm from any of the [sci servers]({{% ref ""sci-servers"" %}}).
Check the current list of machines on that page.
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-submit-a-job#which-servers-can-you-submit-jobs-from?,138,24
Method 1: Submit via a Slurm job script,"The Slurm job submission command is:
{{<command user=""user"" host=""sci-vm-01"">}}
sbatch myjobscript
{{</command>}}
The job script is a Bash script of user's application and includes a list of
Slurm directives, prefixed with `#SBATCH` as shown in this example:
#!/bin/bash 
#SBATCH --partition=short-serial 
#SBATCH -o %j.out 
#SBATCH -e %j.err
#SBATCH --time=05:00
# executable 
sleep 5m
For job specification of resources please refer to Table 2 of the help article
[Slurm quick reference]({{% ref ""slurm-quick-reference"" %}})
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-submit-a-job#method-1:-submit-via-a-slurm-job-script,527,72
Method 2: Submit via command-line options,"If you have an existing script, written in any language, that you wish to
submit to LOTUS then you can do so by providing Slurm directives as command-
line arguments. For example, if you have a script ""my-script.py"" that takes a
single argument ""-f <filepath>"", you can submit it using ""sbatch"" as follows:
sbatch -p short-serial -t 03:00 -o job01.out -e job01.err my-script.py -f myfile.txt
This approach allows you to submit jobs without writing additional job scripts
to wrap your existing code.
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-submit-a-job#method-2:-submit-via-command-line-options,499,82
Method 3: Submit an interactive session via salloc,"Testing a job on LOTUS can be carried out in an interactive manner by
obtaining a Slurm job allocation or resources (a set of nodes) via the Slurm
command `salloc` . The code/application is executed and the allocation are
released after a specific time -default 1 hour - when the testing is finished.
There are two ways:
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-submit-a-job#method-3:-submit-an-interactive-session-via-salloc,321,57
Interactive execution with pseudo-shell terminal on the compute LOTUS node,"The job is executed on the LOTUS compute node by invoking the Slurm command
srun after allocating resources with `salloc`. See example below.
salloc -p par-single --ntasks-per-node=2
salloc: Pending job allocation 23506
salloc: job 23506 queued and waiting for resources
salloc: job 23506 has been allocated resources
salloc: Granted job allocation 23506
The job allocation ID 23506 has 2 CPUs on the compute node host580 as shown
below:
squeue -u train001-o""%.18i %.9P %.11j %.8u %.2t %.10M %.6D %.6C %R""
JOBID PARTITION        NAME        USER   ST       TIME  NODES   CPUS NODELIST(REASON)
23506 par-singl    interactive   usertest  R       1:32      1      2 host580
To launch an interactive shell session on the compute node host580, use the
following srun command (from a sci server).
srun --pty /bin/bash
@host580 ~]$
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-submit-a-job#interactive-execution-with-pseudo-shell-terminal-on-the-compute-lotus-node,825,122
Interactive execution with no shell,"A code/application can be executed on the LOTUS compute node without a shell
session on the node itself. For example the command 'hostname' is executed
twice as there are 2 CPUs and this outputs the name of the node
srun hostname
host580.jc.rl.ac.uk
host580.jc.rl.ac.uk
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-submit-a-job#interactive-execution-with-no-shell,270,43
Job array submission,"Job arrays are groups of jobs with the same executable and resource
requirements, but different input files. Job arrays can be submitted,
controlled, and monitored as a single unit or as individual jobs or groups of
jobs. Each job submitted from a job array shares the same job ID as the job
array and is uniquely referenced using an array index. This approach is useful
for ‘high throughput' tasks, for example where you want to run your simulation
with different driving data or run the same processing task on multiple data
files.
Important note: The maximum job array size that Slurm is configured for is
MaxArraySize = 10000. If a Job array of size is greater than 10000 is
submitted, Slurm will reject the job submission with the following error
message: ""Job array index too large. Job not submitted.""
Taking a simple R submission script as an example:
#!/bin/bash 
#SBATCH --partition=short-serial 
#SBATCH --job-name=myRtest
#SBATCH -o %j.out 
#SBATCH -e %j.err 
#SBATCH --time=30:00
module add jasr
Rscript TestRFile.R dataset1.csv
If you want to run the same script `TestRFile.R ` with input file `dataset2.csv` through  to `dataset10.csv`, you could create and submit a job script for each dataset. However, by setting up an array job, you could create and submit a single job script.
The corresponding job array script to process 10 input files in a single job
submission would look something like this:
#!/bin/bash 
#SBATCH --partition=short-serial 
#SBATCH --job-name=myRarray
#SBATCH -o %A_%a.out
#SBATCH -e %A_%a.err
#SBATCH --time=30:00
#SBATCH --array=1-10
module add jasr
Rscript TestRFile.R datset${SLURM_ARRAY_TASK_ID}.csv
Here the important differences are :
- The array is created by Slurm directive `--array=1-10` by including elements numbered `[1-10]`to represent our 10 variations
- The error and output file have the array  index `%a` included  in the name and `%A` is the job ID.
- The environment variable `$SLURM_ARRAY_TASK_ID` in the `Rscript` command is expanded to give the job index
When the job is submitted, Slurm will create 10 tasks under the single job
ID. The job array script is submitted in the usual way:
sbatch myRarray.sbatch
If you use  the `squeue -u <username>` command  to list your active jobs, you
will see 10 tasks with the same Job ID. The tasks can be distinguished by  the
`[index] ` e.g. jobID_index. Note that individual tasks may be allocated to a
range of different hosts on LOTUS.
",https://help.jasmin.ac.uk/docs/batch-computing/how-to-submit-a-job#job-array-submission,2443,389
'Example Job 2,"This page records some early CEDA usage of the LOTUS cluster for various
relatively simple tasks. Others may wish to use these examples as a starting
point for developing their own workflows on LOTUS.
## Case 1: Calculating MD5 Checksums on many files
This is a simple case because:
1. the archive only needs to be read by the code and
2. the code that we need to run involves only the basic Linux commands so there are no issues with picking up dependencies from elsewhere.
### Case Description
- we want to calculate the MD5 checksums of about 220,000 files. It will take a day or two to run them all in series.
- we have a text file that contains 220,000 lines - one file per line.
### Solution under LOTUS
- Split the 220,000 lines into 22 files of 10,000 lines.
- Write a template script to:
  - Read a text file full of file paths
  - Run the `md5sum` command on each file and log the result.
- Write a script to create 22 new scripts (based on the template script), each of which takes one of the input files and works through it.
### Workflow steps
Log in to the `sci` server (use any of `sci-vm-0[1-6]`, access from a `login` server):
{{<command user=""user"" host=""login-01"">}}
ssh -A <username>@sci-vm-01.jasmin.ac.uk
{{</command>}}
Split the big file:
{{<command user=""user"" host=""sci-vm-01"">}}
split -l 10000 -d file_list.txt # Produces 22 files called ""x00""...""x21""
{{</command>}}
Create the template file: `scan_files_template.sh`
#!/bin/bash
#SBATCH -e %J.e
infile=/home/users/astephen/sst_cci/to_scan/__INSERT_FILE__  
while read f ; do         
    /usr/bin/md5sum $f >> /home/users/astephen/sst_cci/output/scanned___INSERT_FILE__.log
done < $infile
Run a script to generate all the script files:
for i in `ls /home/users/astephen/sst_cci/to_scan/` ; do
    cp scan_files_template.txt bin/scan_files_${i}.sh 
    perl -p -i -w -e 's/__INSERT_FILE__/'${i}'/g;' bin/scan_files_${i}.sh 
done
Submit all 22 jobs to LOTUS:
for i in `ls /home/users/astephen/sst_cci/to_scan/` ; do      
    echo $i    
    cat /home/users/astephen/sst_cci/bin/scan_files_${i}.sh | sbatch -p short-serial -o /home/users/astephen/sst_cci/output/$i   
done
Monitor the jobs by running:
{{<command user=""user"" host=""sci-vm-01"">}}
squeue -u <username>
{{</command>}}
All jobs ran within about an hour.
## Case 2: Checksumming CMIP5 Data
A variation on Case 2 has been used for checksumming datasets in the CMIP5
archive. The Python code below will find all NetCDF files in a DRS dataset and
generate a checksums file and error log. Each dataset is submitted as a
separate Slurm job.
"""""" 
Checksum a CMIP5 dataset
usage: checksum_dataset.py dataset_id ...
    where dataset_id is a full drs id including version 
    e.g. cmip5.output1.MOHC.HadGEM2-ES.historical.6hr.atmos.6hrLev.r1i1p1.v20110921
""""""
import os
import os.path as op
import sys
import optparse
DRS_ROOT = '/badc/cmip5/data'
def submit_job(dataset):
    # Assume version is in the dataset-id for now
    parts = dataset.split('.')
    path = op.join(DRS_ROOT, '/'.join(parts))
    if not op.exists(path):
        raise Exception('%s does not exist' % path)
    job_name = dataset
    cmd = (""echo -e '#!/bin/bash\n""
            ""srun /usr/bin/md5sum {path}/*/*.nc' ""
            ""| sbatch -p short-serial -J {job_name} ""
            ""-o {job_name}.checksums -e {job_name}.err""
        ).format(job_name=job_name, path=path)
    print(cmd)
    os.system(cmd)
def main():
    parser = optparse.OptionParser(description='Checksum DRS datasets')
    (options, args) = parser.parse_args()
    datasets = args
    for dataset in datasets:
        submit_job(dataset)
if __name__ == '__main__':
    main()
If you have a file containing a list of dataset ids you can submit each as a
separate job by invoking the above script as follows:
{{<command user=""user"" host=""sci-vm-01"">}}
./checksum_dataset.py $(cat datasets_to_checksum.dat)
(out)echo -e '#!/bin/bash
(out)srun /usr/bin/md5sum /badc/cmip5/data/cmip5/output1/MOHC/HadGEM2-ES/rcp85/day/seaIce/day/r1i1p1/v20111128/*/*.nc' | sbatch -p short-serial -J cmip5.output1.MOHC.HadGEM2-ES.rcp85.day.seaIce.day.r1i1p1.v20111128 -o cmip5.output1.MOHC.HadGEM2-ES.rcp85.day.seaIce.day.r1i1p1.v20111128.checksums -e cmip5.output1.MOHC.HadGEM2-ES.rcp85.day.seaIce.day.r1i1p1.v20111128.err
(out)Submitted batch job 40898728
(out)...
{{</command>}}
",https://help.jasmin.ac.uk/docs/batch-computing/example-job-2-calc-md5s,4334,552
Current cluster specification,"LOTUS is a cluster of over 300 nodes/hosts and 19000 CPU cores. A node/host is
an individual computer in the cluster with more than 1 processor. Each
node/host belongs to a specific host group. The number of processors (CPUs or
cores) per host is listed in Table 1 with the corresponding processor model
and the size of the physical memory RAM available per node/host.
**Table 1**. LOTUS cluster specification
**Current** host groups
Host group name |  Number of nodes/hosts  |  Processor model |  CPUs per host |  RAM 
---|---|---|---|---  
broadwell256G  |  37  |  Intel Xeon E5-2640-v4 ""Broadwell""  |  20  |  256 GB  
skylake348G  |  151  |  Intel Xeon Gold-5118 ""Skylake""  |  24  |  348 GB  
epyctwo1024G  | 200  |  AMD  |  48  |  1024 GB | 
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/batch-computing/lotus-cluster-specification#current-cluster-specification,770,128
Selection of specific processor model,"To select a node/host with a specific processor model and memory, add the
following Slurm directive to your job script 
#SBATCH --constraint=""<host-group-name>""
For example 
#SBATCH --constraint=""skylake348G""
{{< alert type=""info"" >}}
Further notes
`intel` and `amd` node types are defined in the Slurm configuration as a feature:
- For any Intel node type use `#SBATCH --constraint=""intel""`
- For a specific Intel CPU model use the host group name (see Table 1)
  - e.g. `#SBATCH --constraint=""skylake348G""`
- For AMD use ` #SBATCH --constraint=""amd""`
- There are 10 nodes of node type `skylake348G` with SSD disk mounted on /tmp 
- LOTUS nodes of node type `epyctwo1024` are not available yet on the `par-multi` queue
{{< /alert >}}
{{< alert type=""danger"" >}}
If you choose to compile code for specific architectures, do not expect it to run elsewhere in the system.
{{< /alert >}}
",https://help.jasmin.ac.uk/docs/batch-computing/lotus-cluster-specification#selection-of-specific-processor-model,885,140
Retired host groups no longer in use,"(For reference only)
Host group name |  Number of nodes/hosts |  Processor model |  CPUs per host |  RAM  
---|---|---|---|---  
~~haswell256G~~ |  ~~7~~ retired |  ~~Intel Xeon E5-2650-v3 ""Haswell""~~  |  ~~20~~  | ~~256 GB~~
~~ivybridge2000G~~  |  ~~3~~  -retired |  ~~Intel Xeon E7-4860-v2 ""Ivy Bridge""~~  |  ~~48~~  | ~~2048 GB~~
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/batch-computing/lotus-cluster-specification#retired-host-groups-no-longer-in-use,357,51
Orchid GPU cluster,"This article provides details on JASMIN's GPU
cluster, named **ORCHID**.
",https://help.jasmin.ac.uk/docs/batch-computing/orchid-gpu-cluster,73,10
GPU cluster spec,"The JASMIN GPU cluster is composed of 16 GPU nodes:
- 14 x standard GPU nodes with 4 GPU Nvidia A100 GPU cards each
- 2 x large GPU nodes with 8 Nvidia A100 GPU cards
{{< image src=""img/docs/gpu-cluster-orchid/file-NZmhCFPJx9.png"" caption=""ORCHID GPU cluster"" >}}
",https://help.jasmin.ac.uk/docs/batch-computing/orchid-gpu-cluster#gpu-cluster-spec,264,43
Request access to ORCHID,"Before using ORCHID on JASMIN, you will need: 
1. An existing JASMIN account and valid `jasmin-login` access role: {{<button size=""sm"" href=""https://accounts.jasmin.ac.uk/services/login_services/jasmin-login/"">}}Apply here{{</button>}}
2. **Subsequently** (once `jasmin-login` has been approved and completed), the `orchid` access role: {{<button size=""sm"" href=""https://accounts.jasmin.ac.uk/services/additional_services/orchid/"">}}Apply here{{</button>}}
The `jasmin-login` access role ensures that your account is set up with access to the LOTUS batch processing cluster, while the `orchid` role grants access to the special LOTUS partition used by ORCHID.
Holding the `orchid` role also gives access to the GPU interactive node.
**Note:** In the supporting info on the `orchid` request form, please provide details
on the software and the workflow that you will use/run on ORCHID.
",https://help.jasmin.ac.uk/docs/batch-computing/orchid-gpu-cluster#request-access-to-orchid,885,109
Test a GPU job,"Testing a job on the JASMIN ORCHID GPU cluster can be carried out in an
interactive mode by launching a pseudo-shell terminal Slurm job from a JASMIN
scientific server e.g. `sci-vm-01`:
{{<command user=""user"" host=""sci-vm-01"">}}
srun --gres=gpu:1 --partition=orchid --account=orchid --pty /bin/bash
(out)srun: job 24096593 queued and waiting for resources
(out)srun: job 24096593 has been allocated resources
{{</command>}}
{{<command user=""user"" host=""gpuhost16"">}}
",https://help.jasmin.ac.uk/docs/batch-computing/orchid-gpu-cluster#test-a-gpu-job,467,59
you are now on gpuhost16,"{{</command>}}
The GPU node gpuhost016 is allocated for this interactive session on LOTUS
Note that for batch mode, a GPU job is submitted using the Slurm command
'sbatch':
{{<command user=""user"" host=""sci-vm-01"">}}
sbatch --gres=gpu:1 --partition=orchid --account=orchid gpujobscript.sbatch
{{</command>}}
or by adding the following preamble in the job script file
#SBATCH --partition=orchid
#SBATCH --account=orchid
#SBATCH --gres=gpu:1
Note 1: `gpuhost015` and `gpuhost016` are the two largest nodes with 64 CPUs and
8 GPUs.
Note 2: **CUDA Version: 11.6**
Note 3: The Slurm batch partition/queue `orchid` has a maximum runtime of 24 hours and
the default runtime is 1 hour. The maximum number of CPU cores per user is
limited to 8 cores. If the limit is exceeded then the job is expected to be in
a pending state with the reason being {{<mark>}}QOSGrpCpuLimit{{</mark>}}
",https://help.jasmin.ac.uk/docs/batch-computing/orchid-gpu-cluster#you-are-now-on-gpuhost16,874,130
GPU interactive node,"There is an interactive GPU node `gpuhost001.jc.rl.ac.uk`, with the same spec as
other Orchid nodes, that you can access via a login server to prototype and
test your GPU code prior to running as a batch job.
{{<command user=""user"" host=""login-01"">}}
ssh -A gpuhost001.jc.rl.ac.uk
{{</command>}}
{{<command user=""user"" host=""gpuhost001"">}}
",https://help.jasmin.ac.uk/docs/batch-computing/orchid-gpu-cluster#gpu-interactive-node,340,47
you are now on gpuhost001,"{{</command>}}
",https://help.jasmin.ac.uk/docs/batch-computing/orchid-gpu-cluster#you-are-now-on-gpuhost001,15,1
Software Installed on the GPU cluster (to be updated),"- CUDA drivers 10.1, and CUDA libraries 10.0, 10.1 and 11.4  
- CUDA DNN (Deep Neural Network Library)  
- NVIDIA container runtime (see notes below)  
- NGC client (GPU software hub for NVIDIA)  
- Singularity 3.7.0 - which supports NVIDIA/GPU containers
- SCL Python 3.6
",https://help.jasmin.ac.uk/docs/batch-computing/orchid-gpu-cluster#software-installed-on-the-gpu-cluster-(to-be-updated),273,45
CEDA Archive,"This article describes accessing the {{<link ""ceda_archive"">}}CEDA Archive{{</link>}} from JASMIN:
",https://help.jasmin.ac.uk/docs/long-term-archive-storage/ceda-archive,99,10
Overview,"The CEDA Archive provides direct access to thousands of atmospheric, climate
change and earth observation datasets. The Archive is directly accessible as a
file system from the shared science machines on JASMIN.
It is a separate service run by the CEDA team - it is not a JASMIN service.
Therefore, many of the links in this document will take you to the [CEDA
Archive help documentation site](https://help.ceda.ac.uk) (as the information
relates to CEDA Archive services). This is separate from the JASMIN help
documentation site (which is specifically about JASMIN services).
",https://help.jasmin.ac.uk/docs/long-term-archive-storage/ceda-archive#overview,578,90
Register for a CEDA Account,"First, you need a CEDA Archive account. If you do not have a CEDA account,
please [follow the steps in this CEDA help
document](https://help.ceda.ac.uk/article/39-ceda-account) to register as a
new CEDA user. It also explains how you can reset your password if you have
forgotten it. When you have made a CEDA account, you will then need to use the CEDA portal to [link
it to your JASMIN account](https://help.ceda.ac.uk/article/5105-linking-your-jasmin-account).
The JASMIN Account Portal deals with the management of access to JASMIN
resources (e.g. compute and storage), whereas
[MyCEDA](https://services.ceda.ac.uk/cedasite/myceda/user/) (the CEDA Accounts
Portal) deals with access to CEDA resources (e.g. access to datasets in the
archives). You will need both accounts linked in order to access CEDA Archive
data from JASMIN - you can check whether your accounts are linked from within
the [CEDA Accounts Portal](https://services-beta.ceda.ac.uk/account/jasmin/).
",https://help.jasmin.ac.uk/docs/long-term-archive-storage/ceda-archive#register-for-a-ceda-account,971,134
Accessing the CEDA Archive on JASMIN servers,"Once you have linked your CEDA and JASMIN accounts, you will have access to
large parts of the archive straightaway.
The contents of the CEDA Archive are available on the file system under `/badc`
and `/neodc`. Note: do not access data via any symlinks that point to
`/datacentre/archvol*` - these are not permanent links and may change when data
are migrated to new storage. Please use the archive path names under `/badc` and
`/neodc`. Search the [CEDA data
catalogue](https://help.ceda.ac.uk/article/137-ceda-data-catalogue) for further
details about data held in the archive.
Note: `/badc` is for atmospheric & climate model data, `/neodc` is for earth observation data - they
are named after CEDA's previous archive names (British Atmospheric Data
Centre, and the NERC Earth Observation Data Centre).
Most data on the Archive is open access - however, some datasets are
restricted. You can work this out by looking at the UNIX access groups the
data are within (see below). If your required datasets are restricted, access
to these can be obtained by applying for specific access via the data centre
(see [this article ](https://help.ceda.ac.uk/article/98-accessing-data)for more
details). If direct access is not possible the data can be obtained via
[standard FTP and web-based](https://help.ceda.ac.uk/article/99-download-data-from-ceda-archives)
access methods to the CEDA Archive and transferred to a
suitable group workspace on JASMIN. As the data centres use the same JASMIN
infrastructure the transfer rates are high.
The [CEDA Data Catalogue](https://help.ceda.ac.uk/article/137-ceda-data-catalogue) is a useful tool to find and apply for access to datasets.
",https://help.jasmin.ac.uk/docs/long-term-archive-storage/ceda-archive#accessing-the-ceda-archive-on-jasmin-servers,1673,243
Archive access groups,"The UNIX access groups used within the CEDA Archive are listed below with
links to example datasets in the CEDA data catalogue for those wishing to use
them:
- `open` Available to any logged in JASMIN user with a linked CEDA user account. See a full list of available datasets [here](https://catalogue.ceda.ac.uk/?q=&results_per_page=20&sort_by=relevance&permission=restricted).
- `cmip5_research` restricted [CMIP3](https://catalogue.ceda.ac.uk/uuid/72afa18db5988d1be0066a26e09422df) and [CMIP5 ](https://catalogue.ceda.ac.uk/?q=wcrp+cmip5&record_types=Observation&sort_by=relevance)datasets
- `esacat1` Satellite data including [MERIS](https://catalogue.ceda.ac.uk/uuid/f26559a9daeae9e6740811d3b3113716), [MIPAS](https://catalogue.ceda.ac.uk/uuid/4a9da084adf4252752e5fe77a5cfd0a9) and [SCIAMACHY](https://catalogue.ceda.ac.uk/uuid/6877f4f100d22f750b44f4c3b7ada498).
- `ecmwf` Access to the [ECMWF Operational Datasets](https://catalogue.ceda.ac.uk/uuid/c46248046f6ce34fc7660a36d9b10a71).
- `eurosat` Satellite data including [IASI](https://catalogue.ceda.ac.uk/?q=iasi&record_types=ObservationCollection&sort_by=relevance), [AVHRR-3](https://catalogue.ceda.ac.uk/?q=avhrr+3&record_types=ObservationCollection&sort_by=relevance) and [GOME-2](https://catalogue.ceda.ac.uk/?q=gome+2&record_types=ObservationCollection&sort_by=relevance).
- `ukmo_wx `  Met Office observational dataset collections including [LIDARNET](https://catalogue.ceda.ac.uk/uuid/38a6e76871fca4c58d0f831e532bff41), [MIDAS](https://catalogue.ceda.ac.uk/uuid/220a65615218d5c9cc9e4785a3234bd0), [MetDB](https://catalogue.ceda.ac.uk/uuid/8ee156b6ed41b153e85dbf02a4134513) and [NIMROD](https://catalogue.ceda.ac.uk/uuid/82adec1f896af6169112d09cc1174499)
- `ukmo_clim` Climatology datasets from the Met Office, including [Central England Temperature](https://catalogue.ceda.ac.uk/uuid/a946415f9345f6da9bf4c475c19477b6) dataset collection, [HadISST](https://catalogue.ceda.ac.uk/?q=hadisst).
- `byacl` These data have specific restrictions on them meaning that they can't be accessed directly from JASMIN, but can be obtained via FTP and web access.
",https://help.jasmin.ac.uk/docs/long-term-archive-storage/ceda-archive#archive-access-groups,2114,140
Data Licensing,"All use of data accessed directly from the CEDA Archive must be used in line
with the relevant data licence in place for the relevant dataset for the
purposes stated in the access application. Data licence information can be
found on the relevant CEDA Data Catalogue page, a link to which can be found
in the `00README_catalogue_and_licence.txt` files found in the archive. For
specific data licences granted for restricted datasets, users should log into
their MyCEDA page to view their granted licence and the associated usage
purpose under which access was granted. **Any required alternative use of the
data beyond the original purpose stated in the original licence application
can only be made with a freshly granted new licence application.**
",https://help.jasmin.ac.uk/docs/long-term-archive-storage/ceda-archive#data-licensing,750,120
Accessing data in the archive,"In the example below, the logged-in user is listing the contents of the CRU
data sets within the BADC archive. These are ""open"" so all logged-in users can
access them:
{{<command user=""user"" host=""sci-vm-01"">}}
ls -l /badc/cru/data
(out)total 320
(out)-rw-r-----  1 badc open  396 Feb 18  2015 00README
(out)drwxr-x---  8 badc open 4096 Mar 22 10:32 cru_cy
(out)drwxr-x---  4 badc open 4096 Dec  6  2014 crutem
(out)drwxr-x--- 12 badc open 4096 May  9 14:11 cru_ts
(out)drwxr-x---  3 badc open 4096 Feb 18  2015 PDSI
{{</command>}}
The {{<link ""ceda_data"">}}CEDA Archive Data Browser{{</link>}} is a good place to start as it gives a web-based view of the data with additional metadata but enables copying & pasting the directory path for use within the JASMIN environment:
{{<image src=""img/docs/ceda-archive/ceda-archive-cru.png"" caption=""ceda archive data browser"">}}
",https://help.jasmin.ac.uk/docs/long-term-archive-storage/ceda-archive#accessing-data-in-the-archive,871,128
Workflow Management with rose/cylc,"This page provides is an overview of Met Office tools (Rose, Cylc and FCM)
that are installed on JASMIN for running and managing suites on the LOTUS
cluster.
 **IMPORTANT** : Users of the **JULES** land surface model are advised to log
into the Cylc server in order to launch the JULES suite.
",https://help.jasmin.ac.uk/docs/workflow-management/rose-cylc-on-jasmin,293,52
About rose/cylc,"Rose and Cylc are very useful workflow management tools. You can:
- Configure a Rose suite to work with the LOTUS batch cluster on JASMIN.
- Run a Rose suite and monitor its progress using the Cylc GUI.
Find out more
- Rose: <https://metomi.github.io/rose/doc/html/>
- Cylc: <https://cylc.github.io/doc/built-sphinx/>
Getting started with rose/cylc
The tools are installed under the following common directory which is visible
on all LOTUS nodes and the dedicated cylc server:
",https://help.jasmin.ac.uk/docs/workflow-management/rose-cylc-on-jasmin#about-rose/cylc,477,72
Add the location of the rose/cylc executables to $PATH,"export PATH=/apps/jasmin/metomi/bin:$PATH
Jobs must be scheduled from the JASMIN server: `cylc.jasmin.ac.uk`
All users with a JASMIN login account can log in to this server.
Access to the Met Office Science Repository Service has also been set up. For
information about authentication to the repositories see:
<https://code.metoffice.gov.uk/trac/home/wiki/AuthenticationCaching#JASMIN>
Following these instructions will give you access to FCM.
If you would like to run the cylc GUI then please login through the 
[NoMachine servers]({{% ref ""graphical-linux-desktop-access-using-nx"" %}}).
",https://help.jasmin.ac.uk/docs/workflow-management/rose-cylc-on-jasmin#add-the-location-of-the-rose/cylc-executables-to-$path,589,74
Example rose/cylc suite,"Please see the JASMIN Workshop tutorial for a {{<link ""https://github.com/cedadev/jasmin-workshop/tree/master/tutorials/tut02"">}}worked example{{</link>}} of setting up a rose/cylc suite and run it on LOTUS.
",https://help.jasmin.ac.uk/docs/workflow-management/rose-cylc-on-jasmin#example-rose/cylc-suite,208,22
"Setting up the ""suite.rc"" file for use with LOTUS","For use on JASMIN, you will need to configure the `suite.rc` to run on LOTUS
(using the Slurm scheduling tool):
[[job submission]]
    method = slurm
    execution time limit = PT15M
[[directives]]
    -q = short-serial
    -W = 05:00
    -n = 1
","https://help.jasmin.ac.uk/docs/workflow-management/rose-cylc-on-jasmin#setting-up-the-""suite.rc""-file-for-use-with-lotus",246,40
Using Cron,"Cron is a very common job scheduler for linux. It allows users to run the same
command or shell script periodically. Typically it is used to automate tasks,
for example, every Monday run my script to plot last week's data. There are
many guides to using [cron and
crontab](https://www.google.co.uk/?q=cron%20crontab) (the command for loading
the cron job table).
",https://help.jasmin.ac.uk/docs/workflow-management/using-cron,363,57
Cron on JASMIN,"Generally cron is disabled on the JASMIN general access machines such as
`sci-vm-01.jasmin.ac.uk`. This is to avoid people killing the machine by setting up
lots of processing jobs when better alternatives, e.g. Lotus, are available.
However, there are times when it is appropriate to use cron and so a generic
cron service machine is provided. `cron-01.jasmin.ac.uk` is configured like
`sci*.jasmin.ac.uk`, except cron is enabled. Anyone who can log into `sci*`
should also be able to login to `cron-01.jasmin.ac.uk`.
An additional transfer server `xfer-vm-03.jasmin.ac.uk` is equipped with `cron` for
scheduling transfers only (no processing), although other methods for
{{<link ""../data-transfer/scheduling-automating-transfers"">}}scheduling/automating transfers{{</link>}} are available. See also [transfer servers]({{% ref ""transfer-servers/#xfer-servers"" %}}).
There are a few rules of the road to using this service:
  1. **Avoid process pile up** : If a job has not finished before the cron starts the next instance of the same job then competition for resources probably means that job will also not finish. Eventually a mass of unfinished jobs will overwhelm the whole machine and it will crash. To avoid this jobs should test to see if the previous job is still running by using a lock file, or making sure the jobs timeout. The crontamer wrapper script, documented below, is available to help you implement this.
  2. **Expect it to break occasionally** : Regardless of any measure introduced by users to stop process pile up, you can expect it to go rouge at some point. We will reboot the machine when this happens, probably without warning. **_We may remove offending jobs from the cron table but persistent offenders may be barred from using the service._**
  3. **Don't do heavy processing or data transfers on `cron-01`**. You can submit jobs to lotus  to offload the processing resource: Slurm submission tools like `sbatch` are installed on `cron-01.jasmin.ac.uk`.
  4. Use `xfer-vm-03` for cron-based transfers (not `cron-01`).
",https://help.jasmin.ac.uk/docs/workflow-management/using-cron#cron-on-jasmin,2049,307
Common Cron Gotchas,"The bash shell environment when cron launches scripts is not identical to the
one when working interactively. Annoyingly, the path to common tools, like
`sbatch`, may not have been setup so that you get an error message from `cron`
when it works perfectly well interactively. A way to get round this is to
source the .bash_profile in the `crontab` file so that the interactive
environment is used by `cron`.
24 * * * * . $HOME/.bash_profile; sbatch -W 12:0 mycmd.sh
",https://help.jasmin.ac.uk/docs/workflow-management/using-cron#common-cron-gotchas,466,80
Crontamer,"Crontamer is a wrapper script to implement lock file and time out checking for
cron jobs to avoid issues of ""runaway"" jobs causing problems for everyone
(e.g. where new jobs start before old ones finish, which can cause a snowball
effect).
You can download the crontamer script from
<https://github.com/cedadev/crontamer>. It is already installed on
`cron-01` and `xfer-vm-03`.
Use `cron` as normal, but include the `crontamer` command before the users own
script and arguments.  
{{<command shell=""bash"">}}
crontab -l
(out)## e.g. cron file entry to run job every day at 4am:
(out)0 4 * * * crontamer -t 2h '/home/users/jblogs/bin/my_repeat_script.sh -opt1 arg1 arg2'
{{</command>}}
Note: `crontamer` has a number of options which can be conflated with the
options for the wrapped script. Use single quotes to make sure the script runs
with the right options.
The flow of the crontamer script is like this:
- Check for existing lock file to indicate if the script is already running. If the lockfile is there and it can see the matching process still running then it exits silently.
- If the lock file is not there or it can't see the matching process on the system then it starts the wrapped script.
- Periodically check the wrapped script is running. If the script fails, with a non-zero exit return code, then it can email you.
- If the script has been running longer than specified timeout (default 12hr) then it will be killed.
Unless a named lock file is given the lock files are created in the `/tmp` directory as
/tmp/crontamer.{unique_id}
These files should be cleaned up automatically, but users may need occasional
checks or find them helpful for identifying problems running their scripts.
The `{unique_id}` is based on a combination of the username, passed script and
arguments, enabling multiple calls of a script with different arguments to be
handled separately.
All the principles above apply whether using cron on:
- the cron server `cron-01.jasmin.ac.uk` (for initiating processing workflows)
- the transfer server `xfer-vm-03.jasmin.ac.uk` (for initiating automated transfers)",https://help.jasmin.ac.uk/docs/workflow-management/using-cron#crontamer,2097,334
Managing storage,"When provisioned, a virtual machine gets allocated a small hard disk (the
exact size of the disk depends on the selected machine size). This disk is
intended to run the operating system only. If you require additional storage
for data, it is possible to add extra volumes to a virtual machine.
First, create a new volume by navigating to the volumes tab and clicking on
""New Volume"":
{{<image src=""img/docs/sysadmin-guidance-external-cloud/file-BsmkG3EXIw.png"" caption=""create volume dialogue"">}}
This will launch a dialog that allows you to specify a name and size for the
volume:
{{<image src=""img/docs/sysadmin-guidance-external-cloud/file-HRTEvPf0f6.png"" caption=""specify name and size for volume"">}}
Once the volume becomes available, you can attach it to a VM. First, click on
the ""Actions"" button and select ""Attach volume to machine"":
{{<image src=""img/docs/sysadmin-guidance-external-cloud/file-Y8uws7yYHi.png"" caption=""menu options"">}}
This will open a dialog allowing you to select the VM that you want to attach
the volume to:
{{<image src=""img/docs/sysadmin-guidance-external-cloud/file-tMKNp6gxCt.gif"" caption=""attach volume to VM"">}}
Once the volume has attached to the VM, the new disk will be visible to the
machine but will not be usable. This can be verified using the `lsblk`
command:
{{<command>}}
lsblk
(out)NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
(out)sda      8:0    0   4G  0 disk 
(out)└─sda1   8:1    0   4G  0 part /
(out)sdb      8:16   0  50G  0 disk
{{</command>}}
Here, we can see that the operating system is recognising the new disk - `sdb`
\- but there are no partitions or file systems associated with it. To make the
disk usable, it must be formatted with a filesystem and mounted somewhere,
e.g. `/data`:
{{<command>}}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/sysadmin-guidance-external-cloud#managing-storage,1759,254
Create a single partition spanning the whole disk,"fdisk /dev/sdb
(out)Device contains neither a valid DOS partition table, nor Sun, SGI or OSF disklabel
(out)Building a new DOS disklabel with disk identifier 0x598d636f.
(out)Changes will remain in memory only, until you decide to write them.
(out)After that, of course, the previous content won't be recoverable.
(out)
(out)Warning: invalid flag 0x0000 of partition table 4 will be corrected by w(rite)
(out)
(out)Command (m for help): n
(out)Partition type:
(out)    p   primary (0 primary, 0 extended, 4 free)
(out)    e   extended
(out)Select (default p):
(out)Using default response p
(out)Partition number (1-4, default 1):
(out)Using default value 1
(out)First sector (2048-33554431, default 2048):
(out)Using default value 2048
(out)Last sector, +sectors or +size{K,M,G} (2048-33554431, default 33554431):
(out)Using default value 33554431
(out)
(out)Command (m for help): w
(out)The partition table has been altered!
(out)
(out)Calling ioctl() to re-read partition table.
(out)Syncing disks.
(out)
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/sysadmin-guidance-external-cloud#create-a-single-partition-spanning-the-whole-disk,1007,140
Verify that the partition was created,"lsblk /dev/sdb
(out)NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
(out)sdb      8:16   0  16G  0 disk
(out)└─sdb1   8:17   0  16G  0 part
(out)
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/sysadmin-guidance-external-cloud#verify-that-the-partition-was-created,140,22
Create a filesystem on the partition,"mkfs.ext4 /dev/sdb1
(out)mke2fs 1.42.9 (4-Feb-2014)
(out)Filesystem label=
(out)OS type: Linux
(out)Block size=4096 (log=2)
(out)Fragment size=4096 (log=2)
(out)Stride=0 blocks, Stripe width=0 blocks
(out)1048576 inodes, 4194048 blocks
(out)209702 blocks (5.00%) reserved for the super user
(out)First data block=0
(out)Maximum filesystem blocks=4294967296
(out)128 block groups
(out)32768 blocks per group, 32768 fragments per group
(out)8192 inodes per group
(out)Superblock backups stored on blocks:
(out)    32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208,
(out)    4096000
(out)
(out)Allocating group tables: done
(out)Writing inode tables: done
(out)Creating journal (32768 blocks): done
(out)Writing superblocks and filesystem accounting information: done
(out)
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/sysadmin-guidance-external-cloud#create-a-filesystem-on-the-partition,792,93
Mount the filesystem,"mkdir /data
mount /dev/sdb1 /data
(out)
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/sysadmin-guidance-external-cloud#mount-the-filesystem,40,6
Verify that the filesystem is now available,"lsblk
(out)NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
(out)sda      8:0    0   4G  0 disk 
(out)└─sda1   8:1    0   4G  0 part /
(out)sdb      8:16   0  50G  0 disk 
(out)└─sdb1   8:17   0  50G  0 part /data
df -h
(out)Filesystem      Size  Used Avail Use% Mounted on
(out)/dev/sda1       4.0G  1.4G  2.7G  34% /
(out)devtmpfs        222M     0  222M   0% /dev
(out)tmpfs           245M     0  245M   0% /dev/shm
(out)tmpfs           245M  8.8M  236M   4% /run
(out)tmpfs           245M     0  245M   0% /sys/fs/cgroup
(out)tmpfs            49M     0   49M   0% /run/user/0
(out)/dev/sdb1        50G   53M   47G   1% /data
(out)
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/sysadmin-guidance-external-cloud#verify-that-the-filesystem-is-now-available,628,86
Add a line to /etc/fstab to make the mount persistent (i.e. automatic mount on boot),"echo ""/dev/sdb1  /data  ext4  defaults  0 0"" >> /etc/fstab
{{</command>}}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/sysadmin-guidance-external-cloud#add-a-line-to-/etc/fstab-to-make-the-mount-persistent-(i.e.-automatic-mount-on-boot),74,10
Cluster-as-a-Service - Slurm,"This article describes how to deploy and use a Slurm cluster using JASMIN
Cluster-as-a-Service (CaaS).
{{< alert type=""danger"" >}}
CaaS Slurm clusters are currently disabled because of a security problem
with the images that were being used. We are working on a new system which
will provide slurm clusters.
{{< /alert >}}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-slurm,323,52
Introduction,"The [Slurm Workload Manager](https://slurm.schedmd.com/) is a popular open-
source job scheduler. It provides facilities for executing and monitoring
workloads across a set of nodes and managing contention for those nodes by
maintaining a queue of pending jobs.
Slurm is a powerful scheduling system, and a full discussion of the available
commands and options is beyond the discussion of this article - please consult
the Slurm documentation. This article focuses on the specifics of how to
deploy and access a Slurm cluster in CaaS.
In CaaS, a Slurm cluster consists of a single login node and several worker
nodes. The Linux users and groups on the cluster are managed by the [Identity
Manager]({{% ref ""cluster-as-a-service-identity-manager"" %}}) for the tenancy,
meaning that SSH access to the nodes can be controlled using FreeIPA groups.
User home directories are mounted on all nodes using a [shared storage
cluster]({{% ref ""cluster-as-a-service-shared-storage"" %}}). Slurm is
configured with a single queue, to which all the compute hosts are added.
The login node can optionally be assigned an external IP, however external IPs
are a scarce resource in the JASMIN Cloud - if you want to preserve your
external IPs for other clusters, you can use the Identity Manager gateway host
as a jump host.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-slurm#introduction,1307,207
Cluster configuration,"The following variables are available to configure a Slurm cluster:
| Variable |  Description  |  Required?  |  Can be updated? |
|---|---|---|---|
| Identity manager  |  The CaaS Identity Manager that is used to control access to the cluster.  |  Yes  |  No  |
| Shared storage  |  The shared storage cluster to use for user home directories.  |  Yes  |  No  |
| Worker nodes  |  The number of worker nodes in the cluster. This can be scaled up or down after deployment. When scaling down, there is currently no effort made to drain the hosts in order to remove them gracefully: jobs executing on the removed hosts will fail. This may change in the future. |  Yes  |  Yes  
| Login node size  |  The size to use for the login node.  |  Yes  |  No  
Compute node size  |  The size to use for the compute nodes.  |  Yes  |  No  
| External IP  |  The external IP to attach to the login node. This is optional - if not given, the cluster can still be accessed by using the Identity Manager's gateway host as a jump host for SSH.  |  No  |  No
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-slurm#cluster-configuration,1065,199
Accessing the cluster,"The Slurm hosts are configured to use the users and groups from FreeIPA using
{{<link ""https://docs.pagure.org/SSSD.sssd/"">}}SSSD{{</link>}}. They are also configured to use
SSH keys from FreeIPA for SSH authentication (password-based SSH is disabled).
For every Slurm cluster that is deployed, CaaS automatically creates a group
in FreeIPA called `<clustername>_users`. This group, along with the `admins`
group, are permitted SSH access to the hosts in the cluster. To permit a user
SSH access to a Slurm cluster, they just need to be [added to one of these
groups]({{% ref ""cluster-as-a-service-identity-manager"" %}}) (depending on
whether you also want them to be an admin on other clusters).
Once they have been added to one of these groups, the Slurm cluster can be
accessed via SSH. The following is an example of accessing a Slurm cluster
without an external IP using the Identity Manager's gateway as a jump host:
{{<command user=""user"" host=""localhost"">}}
    ## Add SSH key to the session
    ssh-add /path/to/ssh/key
    ## SSH to the identity manager gateway with agent forwarding enabled
    ssh -A jbloggs@192.171.139.83
{{</command>}}
{{<command user=""jbloggs"" host=""identity-gateway-0"">}}
    ## SSH to the Slurm login node
    ssh 192.168.3.16
{{</command>}}
{{<command user=""jbloggs"" host=""slurm-login-0"">}}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-slurm#accessing-the-cluster,1327,188
Check that we are in our home directory,"pwd
(out)/home/users/jbloggs
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-slurm#check-that-we-are-in-our-home-directory,29,2
Check the Slurm status,"sinfo
(out)PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
(out)compute*     up 1-00:00:00      3   idle slurm-compute-[0-2]
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-slurm#check-the-slurm-status,127,13
Run a simple job,"srun -N3 -l /bin/hostname
(out)0: slurm-compute-0.novalocal
(out)1: slurm-compute-1.novalocal
(out)2: slurm-compute-2.novalocal
{{</command>}}
A more in-depth discussion of the capabilities of Slurm is beyond the scope of
this document - please refer to the Slurm documentation.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-slurm#run-a-simple-job,279,34
Introduction,"JASMIN Cluster-as-a-Service (CaaS) is a service on the JASMIN Cloud that aims
to make it easy to provision and maintain clusters of various types by
providing a simple, intuitive interface via the JASMIN Cloud Portal.
CaaS is only available in the External Cloud, and machines provisioned by the
CaaS system are subject to the usual constraints:
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service#introduction,346,56
Root access,"- The provisioning user gets root access to the hosts.
- Clusters can be customised, for example to add new packages.
- But be careful not to break the configuration of the clustering software!
  - **Note:** If a tenants makes a change which breaks the cluster patching, the cluster will have to be rebuilt.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service#root-access,308,54
Patching,"- Users are responsible for applying patches.
- However, patching a cluster is a simple task triggered in the JASMIN Cloud Portal.
- Cluster admins to decide when to trigger a patch.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service#patching,183,32
Access to CEDA archive and JASMIN Group Workspaces,"- No POSIX access to the CEDA archive or JASMIN Group Workspaces.
- Read-only access via HTTP/OPeNDAP is possible.
- Read-write access to the JASMIN Object Store is also possible.
- The CaaS system has cluster types that provide shared storage between clusters.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service#access-to-ceda-archive-and-jasmin-group-workspaces,262,43
User management,"- Tenancies must manage their own users/groups.
- Users of services in a tenancy do not need a JASMIN account.
- However a JASMIN account **is** required to use the JASMIN Cloud Portal.
- Encourages a structure where admins provision and maintain clusters on behalf of their users.
- The CaaS system has an Identity Manager which provides identity services for a tenancy, i.e. users have a single identity across all clusters within in a single tenancy.
- However this identity is not linked to a JASMIN account.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service#user-management,513,88
Available cluster types,"Cluster type | Details
---|--- 
[Identity Manager]({{% ref ""cluster-as-a-service-identity-manager"" %}})  |  Manages identity and permissions for other clusters using a combination of [FreeIPA](https://www.freeipa.org/page/Main_Page) and [Keycloak](https://www.keycloak.org/).  
[NFS]({{% ref ""cluster-as-a-service-shared-storage"" %}}) |  Shared storage for other clusters using a simple NFS server.  
[Kubernetes]({{% ref ""cluster-as-a-service-kubernetes"" %}}) |  A Kubernetes cluster deployed using [Rancher Kubernetes Engine](https://rancher.com/docs/rke/latest/en/).
[Pangeo]({{% ref ""cluster-as-a-service-pangeo"" %}}) |  The [Pangeo](https://pangeo.io/) stack deployed on Kubernetes.  
[Slurm]({{% ref ""cluster-as-a-service-slurm"" %}}) (currently disabled) |  A batch cluster running the [Slurm workload manager](https://slurm.schedmd.com/).
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service#available-cluster-types,870,81
Creating a cluster,"Clusters are created via the JASMIN Cloud Portal using a new **Clusters** tab
alongside **Overview** , **Machines** , and **Volumes**. If you do not see
this tab, then clusters are not enabled for your tenancy.
{{< image src=""img/docs/cluster-as-a-service/file-N77Jt6iuMA.png"" caption=""Select clusters tab if available"" wrapper=""col-6 mx-auto"" >}}
Click on the tab and you will see a list of your existing clusters. To create
a new cluster, click on the **New cluster** button - this will launch a
dialogue where you can select a cluster type:
{{<image src=""img/docs/cluster-as-a-service/file-m8MJKBGWbg.png"" caption=""Select a cluster type"">}}
Clicking on a cluster type will show a form collecting parameters for the
cluster, which will be different for each cluster type (the options for each
cluster type are discussed in more detail in other articles):
{{<image src=""img/docs/cluster-as-a-service/file-6zCKxYATJd.png"" caption=""Specify parameters for new cluster"">}}
Click **Create cluster** to start the cluster creation. The cluster may take
several minutes to configure (especially as the initial configuration includes
a full patch of operating system packages):
{{<image src=""img/docs/cluster-as-a-service/file-sBQzvCEIP0.png"" caption=""Create the cluster"">}}
Once configuration is complete, the cluster status will become **READY**. The
cluster is then ready to use:
{{<image src=""img/docs/cluster-as-a-service/file-FysROPzFxf.png"" caption=""Cluster in READY status"">}}
More details of how to use each cluster type are given in other help articles
on this site, linked in the table of available cluster types above.
Visit the **Machines** tab to see the machines that were created as part of
the cluster:
{{<image src=""img/docs/cluster-as-a-service/file-uPRA6pYBcQ.png"" caption=""List machines created as part of the cluster"">}}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service#creating-a-cluster,1835,243
Updating a cluster,"Some cluster options, such the number of workers in a Kubernetes cluster, can
be updated after a cluster has been created. To do this, select **Update
cluster options** from the **Actions...** dropdown for the cluster:
{{<image src=""img/docs/cluster-as-a-service/file-hjbidWNoWg.png"" caption=""Select update cluster options"">}}
This will launch a dialogue similar to the one for creating a cluster, except
some of the options will be greyed out as they cannot be changed:
{{<image src=""img/docs/cluster-as-a-service/file-gov0vLALmy.png"" caption=""Next dialogue"">}}
After updating the options, click **Update cluster** to re-configure the
cluster. As with cluster creation the cluster status will change to
**CONFIGURING** , becoming **READY** once the re-configuration is complete.
Where possible, the CaaS system makes an effort to re-configure the cluster
with as little downtime as possible.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service#updating-a-cluster,893,120
Patching a cluster,"""Patching"" refers to the specific operation of updating the operating system
packages on a machine. It is expected that tenants in the External Cloud will
ensure that their machines are regularly patched as a security measure, as
package updates often contain fixes for known vulnerabilities that can be
exploited if left unpatched.
The CaaS system makes patching clusters easy - just select **Patch cluster**
from the **Actions...** dropdown for the cluster and confirm the operation in
the dialogue that appears:
{{<image src=""img/docs/cluster-as-a-service/file-8UgmxTXHq4.png"" caption=""Select patch cluster"">}}
{{<image src=""img/docs/cluster-as-a-service/file-dUZ0anUR7C.png"" caption=""Patch cluster - confirmation"">}}
As with creating and updating, the cluster status will first become
**CONFIGURING** , becoming **READY** once the patching is complete. Where
possible, the CaaS system will patch the cluster with as little downtime as
possible.
Clusters that have not been patched recently will be flagged in the Cloud
Portal:
{{<image src=""img/docs/cluster-as-a-service/file-O49bJXoZzQ.png"" caption=""Unpatched clusters"">}}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service#patching-a-cluster,1128,144
Deleting a cluster,"To delete a cluster, just select **Delete** from the **Actions...** dropdown
for the cluster and confirm the operation in the dialogue that appears:
{{<image src=""img/docs/cluster-as-a-service/file-YbBzoEzWVV.png"" caption=""Select delete cluster"">}}
{{<image src=""img/docs/cluster-as-a-service/file-90OF1EFAXx.png"" caption=""Delete confirmation"">}}
The cluster status will become **DELETING** :
{{<image src=""img/docs/cluster-as-a-service/file-qxaWljk6Op.png"" caption=""Deleting"">}}
This will delete the machines associated with the cluster. Once the machines
have been deleted, the cluster will be removed. A deleted cluster cannot be
restored.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service#deleting-a-cluster,643,68
Introduction to the JASMIN Cloud,"In addition to the traditional batch computing (LOTUS) and storage (Group
Workspaces) services, JASMIN also provides a cloud computing service.
Many users will already be familiar with cloud services through the use of one
of the large public providers (e.g. Amazon AWS or Microsoft Azure). The JASMIN
Cloud is similar in that it allows an institution or project to consume
compute resources as a utility, with no need to provision and maintain the
associated physical infrastructure. Users can provision their own virtual
machines (VMs) within the JASMIN infrastructure, allowing for greater
flexibility. The JASMIN Cloud also allows users to provision clusters for
Identity Management, Kubernetes, and Slurm clusters amongst others (see
Cluster-as-a-Service).
The thing that makes the JASMIN Cloud unique is its colocation with the CEDA
Archive and Group Workspaces. The JASMIN Cloud is ideally suited to projects
that work with such data, and can enable novel solutions for the manipulation
and presentation of data to end-users.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/introduction-to-the-jasmin-cloud,1033,157
Cloud terminology,"Different cloud providers have different terms for the users within their
cloud and the chunks of resource they have been allocated. In the JASMIN Cloud
documentation, we will use the following terminology:
- **Tenancy:** An allocation of resources, i.e. virtual CPUs, RAM and block storage, within the cloud.
- **Tenant:** A group (institution or project) that has been allocated a tenancy in the cloud.
- **Tenancy Admin(istrator):** The person designated as the administrator of a tenancy. There would usually also be a deputy administrator.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/introduction-to-the-jasmin-cloud#cloud-terminology,545,84
JASMIN Cloud Architecture,"In order to provide as much flexibility as possible for tenants while
preserving the security of the system, the JASMIN Cloud is split into two
parts (see the schematic below). Both parts of the JASMIN Cloud are
administered through the same self-service portal, allowing tenancy admins to
provision VMs as required, within the quota of their tenancy.
{{<image src=""img/docs/introduction-to-the-jasmin-cloud/file-rJTVn4CXil.png"" caption=""jasmin cloud achitecture"">}}
The **JASMIN External Cloud** is an Infrastructure-as-a-Service (IaaS)
offering, and sits outside of the main JASMIN firewall. Tenants are allowed
root access and have complete responsibility for all system administration
tasks. This means that tenants are able to provision their own infrastructure
(e.g. web portals, remote desktop services), but it also means that tenants
are responsible for the security of their machines (e.g. patching, firewall
configuration) and for managing their own users. Because it is outside of the
JASMIN firewall, tenancies in the External Cloud cannot directly access the
JASMIN storage (including PFS, and SOF), and so there is no filesystem level
access to the CEDA Archive or Group Workspaces - all access to these data is
via the usual external interfaces (i.e. the Object Store, FTP, OpenDAP, HTTP).
We also have our Cluster-as-a-Service available to external cloud tenants
which is a Platform-as-a-Service offering that tenants can use to deploy
clusters including, an identity cluster, storage cluster (NFS), and a
Kubernetes cluster.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/introduction-to-the-jasmin-cloud#jasmin-cloud-architecture,1543,224
External Cloud patching policy,"{{<alert type=""info"">}}
We expect tenants to react in a timely manner to any security vulnerabilities.
This means critical vulnerabilities are patched within 7 days, and high
vulnerabilities are patched within 14 days. This is following UKRI security
policy. Failure to comply may result in tenancy access being revoked and
machines powered down.
{{</alert>}}
By contrast, the **JASMIN Managed Cloud** is a Platform-as-a-Service (PaaS)
offering, sitting inside the main JASMIN firewall, meaning it can reach the
JASMIN storage. In order to preserve security, this means that tenants are
_not_ allowed root access, and can only deploy VMs from a limited set of pre-
approved templates. However, tenants are not responsible for the security of
these machines, and users on VMs within the tenancy are JASMIN users.
Currently, only two templates are available - an SSH bastion, or login
machine, and a Scientific Analysis server with a similar configuration to the
shared JASMIN Scientific Analysis servers. The Scientific Analysis servers
have the CEDA Archive and Group Workspaces mounted.
Both offerings have a similar network structure. Each tenancy has its own
local network, where machines have addresses in the `192.168.3.0/24` range -
all machines in the tenancy can talk to each other on this network. In
addition, each tenancy has an ""edge device"", which is effectively a virtual
router. Similarly to your home broadband router, this allows machines within
the tenancy to talk to machines outside the tenancy, and ensures packets
coming back into the tenancy are forwarded to the correct machine. These ""edge
devices"" also provide a [Network Address Translation
(NAT)](https://en.wikipedia.org/wiki/Network_address_translation) facility,
which allows machines to be allocated an IP address that is visible outside of
the tenancy. In the Managed Cloud, this translates to an IP address that is
visible on the JASMIN network. In the External Cloud, it translates to an IP
address that is visible on the _public internet_.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/introduction-to-the-jasmin-cloud#external-cloud-patching-policy,2026,310
External vs. Managed - pros and cons,"Attribute |  Managed Cloud  |  External Cloud  
---|---|---  
Self-service provisioning  |  Yes  |  Yes  
Filesystem level access to JASMIN Storage (PFS, SOF)  |  Yes  |  No  
Root access  |  No  |  Yes  
Provision custom infrastructure  |  SSH bastion or Scientific Analysis server only  |  Build from generic Ubuntu or CentOS templates  
Security and patching  |  Handled by infrastructure team  |  Tenant's responsibility  
User management  |  JASMIN users  |  Tenant's responsibility  
Visibility to public internet  |  No  |  Yes (limited number of external IPs)   
Ability to provision Cluster-as-a-Service  |  No  |  Yes
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/introduction-to-the-jasmin-cloud#external-vs.-managed---pros-and-cons,652,93
Getting a JASMIN Cloud Tenancy,"To start a conversation with us about getting a JASMIN Cloud Tenancy for your
project, please contact JASMIN Support.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/introduction-to-the-jasmin-cloud#getting-a-jasmin-cloud-tenancy,118,19
Cluster-as-a-Service - Pangeo,"This article describes how to deploy and use a Pangeo cluster using JASMIN
Cluster-as-a-Service (CaaS).
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-pangeo,104,15
Introduction,"{{<link ""https://pangeo.io/"">}}Pangeo{{</link>}} is a community that promotes a philosophy of
open, scalable and reproducible science, focusing primarily on the
geosciences. As part of that effort, the Pangeo community provides a curated
{{<link ""https://www.python.org/"">}}Python{{</link>}} ecosystem based on popular open-source
packages like {{<link ""http://xarray.pydata.org"">}}xarray{{</link>}},
{{<link ""https://scitools.org.uk/iris"">}}Iris{{</link>}}, {{<link ""http://dask.readthedocs.io"">}}Dask{{</link>}} and
{{<link ""http://jupyter.org/"">}}Jupyter notebooks{{</link>}}, along with documentation and recipes
for deployment on various infrastructures.
The Pangeo cluster type in CaaS is a multi-user implementation of the Pangeo
ecosystem using {{<link ""https://jupyter.org/hub"">}}JupyterHub{{</link>}} deployed on
{{<link ""https://kubernetes.io/"">}}Kubernetes{{</link>}}, giving users a scalable and fault-
tolerant infrastructure to use for doing science, all through a web-browser
interface. Authentication is handled by the
[Identity Manager]({{% ref ""cluster-as-a-service-identity-manager"" %}}) for the tenancy
via JupyterHub's LDAP integration. Each authenticated user gets their own Jupyter notebook
environment running in its own container, isolated from other users. The
automatic spawning of containers for authenticated users is handled by
JupyterHub, which also provides an interface for admins to manage the running
containers. This is achieved by using the
{{<link ""https://github.com/pangeo-data/helm-chart"">}}Pangeo Helm chart{{</link>}} to deploy the Pangeo
ecosystem on a [CaaS Kubernetes cluster]({{% ref ""cluster-as-a-service-kubernetes"" %}}).
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-pangeo#introduction,1672,181
Cluster configuration,"The Pangeo ecosystem is deployed on top of CaaS Kubernetes, so all the
[configuration variables for Kubernetes]({{% ref ""cluster-as-a-service-kubernetes"" %}}) also apply to Pangeo clusters.
In addition, the following variables are available to configure the Pangeo
installation:
Variable |  Description  |  Required?  |  Can be updated?  
---|---|---|---  
Notebook CPUs  |  The number of CPUs to allocate to each user notebook environment.  |  Yes  |  Yes  
Notebook RAM  |  The amount of RAM, in GB, to allocate to each user notebook environment.  |  Yes  |  Yes  
Notebook storage  |  The amount of persistent storage, in GB, to allocate to each user notebook environment. This is where users will store their notebooks and any other files they import. The storage is persistent across notebook restarts - a user can shut down their notebook server and start a new server later without losing their data. Backups are not provided - if required, they are the responsibility of the user or cluster admins.  |  Yes  |  Yes  
Pangeo domain  |  The domain to use for the Pangeo notebook web interface.<br><br> If left empty, `pangeo.<dashed-external-ip>.sslip.io` is used. For example, if the selected external IP is `192.171.139.83`, the domain will be `pangeo.192-171-139-83.sslip.io`.<br><br>If given, the domain must already be configured to point to the selected **External IP** (see Kubernetes configuration), otherwise configuration will fail. Only use this option if you have control over your own DNS entries - the CaaS system or Kubernetes will not create a DNS entry for you.  |  No  |  No  
{.table .table-striped}
These variables define the amount of resource available to each user for
processing - in order to appropriately configure (a) the size of your cluster
worker nodes and (b) the resources for each notebook environment, you will
need to consider how many users you are expecting and what workloads they
might want to run.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-pangeo#cluster-configuration,1944,304
Accessing the cluster,"Access to the underlying Kubernetes cluster is achieved in the [same way as
any other CaaS Kubernetes cluster]({{% ref ""cluster-as-a-service-kubernetes"" %}}).
The Pangeo web interface will be available at `https://<pangeo domain>`.
Access to the Pangeo interface is managed through FreeIPA, and users sign in
with the same username and password as for other clusters. As part of the
cluster configuration, CaaS will create a [FreeIPA group]({{% ref ""cluster-as-a-service-identity-manager"" %}}) called `<clustername>_notebook_users`.
Granting access to the Pangeo interface is as simple as adding a user to this
group.
Before adding user to group:
{{<image src=""img/docs/cluster-as-a-service-pangeo/file-2pkaT0W3qj.png"" caption=""Adding user to group: before"">}}
And after:
{{<image src=""img/docs/cluster-as-a-service-pangeo/file-BEYTxt4Ed9.png"" caption=""Adding user to group: after"">}}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-pangeo#accessing-the-cluster,885,108
Using the Pangeo environment,"A full discussion of the capabilities of Jupyter notebooks, the JupyterLab
environment and the many libraries included in the Pangeo ecosystem is beyond
the scope of this documentation. There are plenty of examples on the web, and
Pangeo provide some {{<link ""https://github.com/pangeo-data/pangeo-example-notebooks"">}}example notebooks{{</link>}}.
As a very brief example of the power and simplicity of Jupyter notebooks,
especially on Kubernetes, this short video (no sound) shows a user signing
into a CaaS Pangeo cluster and uploading a notebook. When run, the notebook
spawns a Dask cluster inside Kubernetes and uses it to perform a noddy but
relatively time-consuming calculation. You can see the Dask cluster scale up
to meet demand in the Dask dashboard:
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-pangeo#using-the-pangeo-environment,764,113
Administering the hub,"JupyterHub allows admin users to manage the notebooks running in the system,
and even to impersonate other users. Unfortunately, the JupyterHub LDAP
integration does not currently allow for an entire group to be designated as
admins, so admin access in JupyterHub is granted specifically to the FreeIPA
`admin` user.
To access the admin section, first click **Hub > Control Panel** in the
JupyterLab interface:
{{<image src=""img/docs/cluster-as-a-service-pangeo/file-vB8S8UoNaw.png"" caption=""Administering the hub (1)"">}}
This will open the JupyterHub control panel - any user can use this to start
and stop their notebook server, but admins see an extra **Admin** tab:
{{<image src=""img/docs/cluster-as-a-service-pangeo/file-NUxDCoKB3d.png"" caption=""Administering the hub (2)"">}}
Clicking on this tab will show a list of the users in the hub, along with
buttons to start and stop the servers for those users:
{{<image src=""img/docs/cluster-as-a-service-pangeo/file-c8yR4Lveso.png"" caption=""Administering the hub (3)"">}}
Additional admins can be added by clicking the **edit user** button for that
user. This will pop up a dialogue:
{{<image src=""img/docs/cluster-as-a-service-pangeo/file-DBOc03v93N.png"" caption=""Administering the hub (4)"">}}
Check the **Admin** checkbox and click **Edit User** to save. The user is now
an admin for the hub.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-pangeo#administering-the-hub,1344,180
Cluster-as-a-Service - Identity Manager,"This article describes how to deploy and use the JASMIN Cluster-as-a-Service
(CaaS) Identity Manager.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-identity-manager,102,14
Introduction,"The Identity Manager consists of a
[FreeIPA](https://www.freeipa.org/page/Main_Page) server, a
[Keycloak](https://www.keycloak.org/) server and a gateway/proxy server that
work together to provide a single identity across all cluster types, whether
via a web-browser, SSH or custom CLI tools like `kubectl`.
[FreeIPA](https://www.freeipa.org/page/Main_Page) is an open-source identity
management system specifically designed to manage Linux hosts and the user
accounts on those hosts. To do this, It integrates
[LDAP](https://en.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol),
[Kerberos](https://en.wikipedia.org/wiki/Kerberos_\(protocol\)),
[NTP](https://en.wikipedia.org/wiki/Network_Time_Protocol),
[DNS](https://en.wikipedia.org/wiki/Domain_Name_System) and a [certificate
authority](https://en.wikipedia.org/wiki/Certificate_authority) into a single
unit that is easy to install and configure.
[Keycloak](https://www.keycloak.org/) is an open-source product that provides
single sign-on (SSO) using [OpenID Connect](https://openid.net/connect/) and
[SAML](https://en.wikipedia.org/wiki/Security_Assertion_Markup_Language),
primarily aimed at web-based services.
FreeIPA and Keycloak are powerful systems, and a full discussion of their
capabilities is beyond the scope of this article. This article focuses on
their use within the CaaS system, and will be sufficient for the vast majority
of users. **Any usage that deviates from that described in the JASMIN CaaS
documentation is not explicitly supported, should something go wrong.**
All hosts deployed using CaaS are registered with the FreeIPA instance for
your tenancy, and FreeIPA provides DNS, user/group management and access
control policies for those hosts. FreeIPA is also the single source of truth
for users and groups on your clusters. It is **not** possible to link with
other accounts, including JASMIN accounts. Keycloak is used to provide OpenID
Connect support for web applications, and for Kubernetes authentication.
Although Keycloak can manage its own users and groups, in the Identity Manager
setup it consumes the users and groups from FreeIPA via the LDAP integration
in order to provide a single user account across all clusters.
The web interfaces for FreeIPA and Keycloak are exposed through a single
gateway/proxy host. This host is also configured to allow SSH access for all
active users, which means it can be used with [SSH agent
forwarding](https://www.ssh.com/ssh/agent#sec-SSH-Agent-Forwarding) as a [jump
host](https://en.wikipedia.org/wiki/Jump_server) for SSH access to clusters
without an external IP (similar to the way that the  MISSING LINK work.)
The Identity Manager does not have self-service user registration or password
reset - these operations must be performed by an admin on behalf of the user.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-identity-manager#introduction,2813,353
Cluster configuration,"The following variables are available when creating an Identity Manager:
Variable |  Description  |  Required?  |  Can be updated?  
---|---|---|---  
External IP  |  The external IP that will be attached to the gateway host. This is the the IP that can be used as a jump host for SSH access.  |  Yes  | No  
Admin password  |  The password for the `admin` account. When the Identity Manager is created, this is the only user that exists. Please make sure you choose a secure password.  **WARNING: This password cannot be changed.** Changing the admin password in the FreeIPA web interface will break cluster configuration for all clusters. |  Yes  | No 
Admin IP ranges  |  One or more IP ranges from which admins will access the FreeIPA and Keycloak web interfaces, in [CIDR notation](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#CIDR_notation). Any attempt to access the admin interfaces froman IP address that is not in these ranges will be blocked. FreeIPA and Keycloak allow the creation and modification of users and permissions for all your clusters, so it is recommended that this range be as small as possible. If you are not sure what value to use here, contact your local network administrator to find out the appropriate value for your network.  |  Yes  |  Yes  
FreeIPA size  |  The machine size to use for the FreeIPA server.  |  Yes  | No  
Keycloak size  |  The machine size to use for the Keycloak server.  |  Yes  | No  
Gateway size  |  The machine size to use for the gateway server.  |  Yes  | No  
Gateway domain  |  The domain to use for the gateway server.<br>If left empty, `<dashed-gateway-ip>.sslip.io` is used (this uses the [sslip.io](https://sslip.io/) service). For example, if the selected gateway IP is `192.171.139.83`, the domain will be `192-171-139-83.sslip.io`.<br>If given, the domain must already be configured to point to the **External IP** , otherwise configuration will fail. Only use this option if you have control over your own DNS entries - the CaaS system will **not** create a DNS entry for you.  |  No  |  No  
{.table .table-striped}
Once configuration is complete, the FreeIPA web interface will be available at
`https://<gateway domain>`. You should be able to authenticate with the
username `admin` and the password that was given at deployment time:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-8Czq4TDXQY.png"" caption=""FreeIPA web interface"">}}
The Keycloak web interface is available at `https://<gateway domain>/auth/`.
You should be able to authenticate with the same username and password as
FreeIPA.
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-18g9kpcLBU.png"" caption=""Keycloak web interface"">}}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-identity-manager#cluster-configuration,2718,406
Managing users,"The users of your clusters are not related in any way to JASMIN users - in
fact, there is no requirement that the users of your clusters have a JASMIN
account. The pattern we encourage is that one or more admins with JASMIN
accounts and access to the JASMIN Cloud Portal deploy and maintain clusters on
behalf of their users. Those admins can then create user accounts and grant
access to clusters for their own users without those users even needing to be
aware of JASMIN.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-identity-manager#managing-users,474,86
Creating a user,"To add a new user, first log in to the FreeIPA interface. **Do not add users
via the Keycloak interface.** You will be taken to the users panel, where you
click the **Add** button:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-YE3RJbqLI7.png"" caption=""FreeIPA interface: adding a new user"">}}
This will pop up a dialogue for you to populate some basic information about
the user. The **User login** , **First name** , **Last name** and **New/Verify password** fields are the ones that need to be populated. Pick a strong
password for the user - they can change this later via the FreeIPA interface
if they wish:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-ca8mivu4Fu.png"" caption=""User information dialogue"">}}
Click **Add** to create the user. You must then securely distribute this
password to the user - if possible, write it down and give it to them in
person, otherwise use an encrypted email.
The first time they log in, they will be asked to set a new password. Make
sure they do this as soon as possible:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-1focFj2OE5.png"" caption=""Update password dialogue"">}}
The newly added user cannot do anything except view the users and modify some
of their own information. They can see, but not edit, their group memberships.
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-1LpgGS9J5u.png"" caption=""View of user info"">}}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-identity-manager#creating-a-user,1445,200
Adding an SSH public key,"Adding an SSH public key can be done either by the user themselves or by an
admin. First, navigate to the details page for the user. In the **Account
Settings** section, there is an item called **SSH public keys**. Click the
**Add** button next to it:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-rf1CHPSFBa.png"" caption=""Adding an SSH key (1)"">}}
This will open a dialogue where the SSH public key can be entered:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-tkqBXjmgg6.png"" caption=""Adding an SSH key (2)"">}}
After clicking **Set** , the user interface will show **New: key set** under
the **SSH public keys** item. However, the key is not preserved until the user
is saved by clicking the **Save** button:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-83tahCNg1o.png"" caption=""Adding an SSH key (3)"">}}
Once saved, the content of the **SSH public keys** item will change to a
fingerprint, which means the key was saved correctly. The key can be updated
or deleted at any point in the future if the associated private key is
compromised or lost:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-o8B0Efprrr.png"" caption=""Adding an SSH key (4)"">}}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-identity-manager#adding-an-ssh-public-key,1227,165
Changing a user's password,"FreeIPA has no facility for self-service password reset, however users can
change their own password or an admin can reset it on their behalf. The
procedure is the same in both cases, except that when changing their own
password the user is required to provide their current password as well as the
new one.
To change a user's password, first navigate to the user details page then
select **Reset password** from the **Actions** dropdown:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-AN9KYOPlgV.png"" caption=""Reset password (1))"">}}
This will open a dialogue where a new password can be entered. An admin
changing the password on behalf of another user will only see the **New/Verify
Password** fields:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-so4iODYIoC.png"" caption=""Reset password (2)"">}}
A user resetting their own password will also see **Current Password** and
**OTP** fields. The current password must be provided. OTP can be ignored.
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-mxaFQCEo4q.png"" caption=""User resetting own password"">}}
After clicking **Reset Password** , the password is changed.
If a user's password is reset by an admin, the user will be asked to change
their password the first time they log in, like when a new user is created.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-identity-manager#changing-a-user's-password,1324,183
Deleting a user,"To delete a user, navigate to the **Identity > Users > Active users**
page. On this page, check the box next the user you want to disable, then
click the **Delete** button:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-6veMwQytmd.png"" caption=""Deleting a user (1)"">}}
In the confirmation dialogue that pops up, make sure to select **preserve** as
the **Delete mode** \- it is not recommended to permanently delete users:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-a7ogWueenK.png"" caption=""Deleting a user (2)"">}}
Upon clicking the **Delete** button, the user will be moved to the **Preserved
users** section:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-jm0VaaEtgX.png"" caption=""Deleting a user (3)"">}}
They will no longer show up as a user on any CaaS hosts or in Keycloak. They
can be easily restored by selecting the user and clicking the **Restore**
button.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-identity-manager#deleting-a-user,926,120
Managing groups,"When you deploy a cluster through CaaS, it may create one or more access
control groups in FreeIPA as part of its configuration. Some clusters can
also consume additional groups created in FreeIPA. This is discussed in more
detail in the documentation for each cluster type, but the way you manage
group membership is the same in all cases.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-identity-manager#managing-groups,341,59
Creating a new group,"To create a new group, navigate to the **Identity > Groups > User groups**
section and click the **Add** button:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-aNov4uaQ2d.png"" caption=""Creating a new group (1)"">}}
In the resulting dialogue, set the **Group name** and, if you wish, a
**Description** (recommended!). The **Group Type** can be left as **POSIX** ,
even if the group is only to be used for OpenID Connect. By leaving **GID**
empty, a free GID will be allocated:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-KXQDw9RwGW.png"" caption=""Creating a new group (2)"">}}
After clicking the **Add** button, the new group will be available for adding
users.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-identity-manager#creating-a-new-group,699,94
Adding and removing users,"First, navigate to the **Identity > Groups > User groups** section:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-hnj3sXUyRM.png"" caption=""Adding/removing users (1)"">}}
Click on the group that you want to add/remove users for to get to the details
page for that group. To add users, click the **Add** button:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-KEUDEZkOGO.png"" caption=""Adding/removing users (2)"">}}
In the dialogue that pops up, select the users you want to add and click the
**>** button to move them from **Available** to **Prospective** :
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-5D7lycQSQg.png"" caption=""Adding/removing users (3)"">}}
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-eEIqs8FLW0.png"" caption=""Adding/removing users (4)"">}}
Click **Add** to add the users to the group.
To remove users from a group, select them in the user list for the group and
click **Delete** :
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-krx6gFGfbe.png"" caption=""Adding/removing users (5)"">}}
Upon confirmation, the users will be removed from the group.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-identity-manager#adding-and-removing-users,1158,127
The admins group,"There is one special group that is created in FreeIPA by default, called
`admins`. This group is respected by all cluster types and members are granted
permissions across all clusters deployed using CaaS, including (but not
limited to):
- Full admin access to the FreeIPA and Keycloak web interfaces
- SSH access to all hosts deployed using CaaS
- `cluster-admin` access to all Kubernetes clusters
- Access to all Pangeo clusters
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-identity-manager#the-admins-group,430,71
Managing OpenID Connect clients,"For an application to use OpenID Connect to authenticate users, it must first
be registered as a client with Keycloak. Clients are issued with an ID and
secret so that Keycloak knows which application is making an authorisation
request.
To manage your OpenID Connect clients, go to Keycloak at `https://<gateway
domain>/auth/` and click **Administration Console**. Upon signing in with
valid admin credentials (see The admins group above) you will be redirected to
the Keycloak admin console. Click on **Clients** in the menu to see the list
of clients:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-xZnwdQShpx.png"" caption=""Keycloak admin console"">}}
Keycloak itself uses OpenID Connect to handle authentication for its web and
command-line interfaces, so there are several clients related to Keycloak
operations. CaaS will also automatically create new OpenID Connect clients for
clusters that need them - most notably Kubernetes clusters - in which case the
client will be named after the cluster. The client with **Client ID**
`kubernetes` in the list above is an example of a client created by CaaS.
In order to configure an OpenID Connect client to talk to Keycloak, you also
need the client secret. To find out the secret for a client, click on the
client and then click on the **Credentials** tab:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-9ydKRPcO9C.png"" caption=""Keycloak: credentials tab"">}}
The client secret is then shown in a disabled text box, where it can be copied
from:
{{<image src=""img/docs/cluster-as-a-service-identity-manager/file-3uxEBxY9QM.png"" caption=""Keycloak: client secret"">}}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-identity-manager#managing-openid-connect-clients,1651,230
Provisioning a Sci VM in a Managed Cloud Tenancy,"This article is for admins and managers of managed-cloud tenancies
and shows how to provision a sci VM within one. It involves the following:
- Becoming a member of a managed cloud tenancy
- Provisioning a VM
A ""sci vm"" is essentially the same as the general-access scientific analysis
servers, but created within a specific tenancy aimed at a certain group of users. 
The manager/deputy then has the responsibility to stop/start/restart or redeploy the VM, and to
control who can access it.
The managed cloud tenancy has four access roles:
- `MANAGER` role: can approve `DEPUTY`, `ADMIN`, and `USER` role access requests
- `DEPUTY` manager role: can approve `ADMIN` and `USER` role access requests
- `ADMIN` role: can access the cloud portal and can restart or provision the Sci VM
- `USER` role: can log in into the sci VM from a JASMIN login node
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/provisioning-tenancy-sci-vm-managed-cloud,850,145
Apply for access to the Sci tenancy,"A managed cloud tenancy is accessible via the {{<link ""jasmin_cloud_portal"">}}JASMIN cloud portal{{</link>}}. 
Access is controlled by a service corresponding to the name of the tenancy: these services are listed under {{<link ""https://accounts.jasmin.ac.uk/services/analysis_vms/"">}}Sci Analysis VMs{{</link>}} category of ""My Services"".
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/provisioning-tenancy-sci-vm-managed-cloud#apply-for-access-to-the-sci-tenancy,339,38
Access the tenancy,"With an ADMIN role, you can log in to the [JASMIN cloud portal](https://cloud.jasmin.ac.uk/) using the same credentials for signing into the JASMIN accounts portal.
You will be presented with the ""Dashboard"" page -below- showing the tenancies
you have access to. On the dashboard, select the organization representing the
tenancy to find out the VM provisioned within a given tenancy, e.g ncas-sci-M
{{<image src=""img/docs/tenancy-based-sci-vm-managed-cloud/file-jdw37TM5c1.png"" caption=""dashboard showing tenancies available to this user"">}}
Note: The ""ncas-sci-M"" tenancy shown below has 0 machines as this is a new tenancy.
We will proceed next to the provisioning of a virtual machine.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/provisioning-tenancy-sci-vm-managed-cloud#access-the-tenancy,690,96
Provision a virtual machine,"**Step 1:** Select ""Machines"" from the top menu, then click ""New machine"" to
create a new VM. Choose a name for the new VM. Then select a size from the
drop-down menu which shows the catalog of VM template size. For example,
select ""j4.large"" which allocates 8 CPUs and 32GB of RAM resources for the new
VM
**IMPORTANT:** A Sci machine should be deployed with **a minimum of 2 GB RAM**
**IMPORTANT** : A Sci machine name should not exceed 8 characters long. The
preferred naming format is sci<number> e.g. sci1
{{<image src=""img/docs/tenancy-based-sci-vm-managed-cloud/file-9B3PGwdTJf.png"" caption=""dialogue for creating a new VM"">}}
The VM with the chosen name ncas-sci1 is created and it is running as shown
below.
{{<image src=""img/docs/tenancy-based-sci-vm-managed-cloud/file-2BhBYPt9hA.png"" caption=""vm now shown in dashboard"">}}
**Step 2:** Attach an external IP to the new VM by clicking ""Actions"" and
selecting ""Attach external IP"". Note that you can restart the VM from the
""Actions"" menu.
{{<image src=""img/docs/tenancy-based-sci-vm-managed-cloud/file-jTpZDG70NA.png"" caption=""attach external IP (1)"">}}
**Step 3:** From the box that pops up -see image below- click on the ""+""
(green button) to add an IP. Then click on the down arrow next to ""Select an
external IP"" you will see the IP address to assign to the machine, select the
IP and click attach IP
{{<image src=""img/docs/tenancy-based-sci-vm-managed-cloud/file-6VxGf1Kgz4.png"" caption=""attach external IP (2)"">}}
**Step 4:** Click ""Attach IP""
**Important note:** As `ADMIN` and `MANAGER` of a Sci tenancy, you should note the
""External IP"" as this is the IP address you will need to provide to your users
in order for them to connect to the machine via SSH using a JASMIN login
server.
{{<image src=""img/docs/tenancy-based-sci-vm-managed-cloud/file-PDx3Ze7Nub.png"" caption=""summary dashboard showing IP allocated."">}}
**Step 5:** An overview of the resources used by the VM is shown below
{{<image src=""img/docs/tenancy-based-sci-vm-managed-cloud/file-7IJvElS5yE.png"" caption=""resources dashboard"">}}
**Note 1:** Only ADMIN and MANAGER roles have access to the JASMIN cloud
portal and can provision VMs. ADMIN and MANAGER roles of a Sci tenancy will
not be granted root access.
**Note 2:** ADMIN and MANAGER roles will not allow you to SSH into the Sci VM.
It is necessary to have a USER role to do so.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/provisioning-tenancy-sci-vm-managed-cloud#provision-a-virtual-machine,2370,349
Connect to the VM,"From a JASMIN login server, login to the machine using the External IP address.
In the same way, as you login to a JASMIN scientific server via login1. Your
initial connection to JASMIN from your local machine needs to have your SSH
key loaded in your SSH authentication agent, and you must have SSH Agent
Forwarding enabled ""-A"", see also how  MISSING LINK.
{{<image src=""img/docs/tenancy-based-sci-vm-managed-cloud/file-OCr9XwHyWz.png"" caption=""terminal session showing connection to new VM"">}}
**Note** that although the new provisioned Sci VM has a local hostname (in
this example, `sci1-202012041148.ncas-sci-m.jasmin.ac.uk` ), this is NOT
registered in any Domain Name Service (DNS) by default, and we are not able to
arrange this for you, so you need to connect
to the machine using its External IP address, not the name.
**Note** : Users should report issues to the `ADMIN` and/or `MANAGER` of the
tenancy based SCI VM initially, rather than the JASMIN team. If the issue cannot be
resolved by the `ADMIN` and/or `MANAGER`, **they** should contact the JASMIN helpdesk.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/provisioning-tenancy-sci-vm-managed-cloud#connect-to-the-vm,1077,167
Cluster-as-a-service - Kubernetes,"This article describes how to deploy and use a Kubernetes cluster using JASMIN
Cluster-as-a-Service (CaaS).
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes,108,15
Introduction,"{{<link ""https://kubernetes.io/"">}}Kubernetes{{</link>}} is an open-source system for automating
the deployment, scaling and management of containerised applications.
Kubernetes is an extremely powerful system, and a full discussion of its
capabilities is beyond the scope of this article - please refer to the
Kubernetes documentation. This article assumes some knowledge of Kubernetes
terminology and focuses on things that are specific to the way Kubernetes is
deployed by CaaS.
In CaaS, Kubernetes is deployed in a single-master configuration using
{{<link ""https://github.com/rancher/rke"">}}Rancher Kubernetes Engine (RKE){{</link>}}. This
configuration was chosen so that a single external IP can be used for SSH
access to the cluster and for
{{<link ""https://kubernetes.io/docs/concepts/services-networking/ingress/"">}}ingress{{</link>}} \-
external IPs are a scarce resource in the JASMIN Cloud and the number
available to each tenancy is limited. It is for this reason that load-balancer
services are also not available. Highly-available (HA) configurations may be
available in the future.
All externally-exposed services, including the Kubernetes API, are
authenticated using the [Identity Manager]({{% ref ""cluster-as-a-service-identity-manager"" %}}), meaning that FreeIPA groups can be used to control
access to the cluster.
The following services are also configured by CaaS (described in more detail
later):
- The {{<link ""https://kubernetes.github.io/ingress-nginx/"">}}Nginx Ingress Controller{{</link>}}
- The {{<link ""https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#openstack"">}}Openstack Cloud Provider{{</link>}} (Block Storage and Metadata only)
- {{<link ""https://docs.cert-manager.io/en/latest/"">}}Jetstack's cert-manager{{</link>}}
- The {{<link ""https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/"">}}Kubernetes dashboard{{</link>}}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes#introduction,1911,213
Cluster configuration,"The following variables are available when creating a Kubernetes cluster:
Variable |  Description  |  Required?  |  Can be updated?  
---|---|---|---  
Identity manager  |  The CaaS Identity Manager that is used to control access to the cluster.  |  Yes  |  No  
Version  |  The Kubernetes version to use. The available versions are determined by the RKE version used by the CaaS configuration. This can be changed after initial deployment to upgrade a cluster to a newer Kubernetes version. Before doing this, you should back up your cluster - in particular, you should [take a snapshot of the etcd database](https://rancher.com/docs/rke/latest/en/etcd-snapshots/) and make sure any data in persistent volumes is backed up.  |  Yes  |  Yes  
Worker nodes  |  The number of worker nodes in the cluster. This can be scaled up or down after deployment. When scaling down, there is currently no effort made to drain the hosts in order to remove them gracefully: we rely on Kubernetes to reschedule terminated pods. This may change in the future.  |  Yes  |  Yes  
Master size  |  The size to use for the master node. The master node is configured to be unschedulable, so no user workloads will run on it (just system workloads).  |  Yes  |  No  
Worker size  |  The size to use for worker nodes. Consider the workloads that you want to run and pick the size accordingly. The capacity of the cluster can be increased by adding more workers, but the size of each worker **cannot** be changed after the first deployment.  |  Yes  | No  
Root volume size  |  The size of the root volume of cluster nodes, in GB. This volume must be sufficiently large to hold the operating system (~3GB), all the Docker images used by your containers (which can be multiple GBs in size) and all the logs and ephemeral storage for your containers. For reference, [Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine/) deploys hosts with 100GB root disks by default. At least 40GB is recommended.  |  Yes  |  No  
External IP  |  The external IP that will be attached to the master node. This IP is where the Kubernetes API will be exposed, and can be used for SSH access to the nodes.  |  Yes  |  No
Admin IP ranges |  One or more IP ranges from which admins will access the Kubernetes API and dashboard (if enabled), in [CIDR notation](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#CIDR_notation). Any attempt to access the API or dashboard from an IP address that is not in these ranges will be blocked. Access to the Kubernetes API may allow the creation of resources in your cluster, so it is recommended that this range be as small as possible. If you are not sure what value to use here, contact your local network administrator to find out the appropriate value for your network.  |  Yes  | Yes  
Kubernetes dashboard  |  Indicates whether to deploy the Kubernetes dashboard.  If selected, the Kubernetes dashboard will be available at the configured domain (see below).  |  Yes  |  Yes  
Dashboard domain  |  The domain to use for the Kubernetes dashboard. If left empty, `dashboard.<dashed-external-ip>.sslip.io` is used. For example, if the selected external IP is `192.171.139.83`, the domain will be `dashboard.192-171-139-83.sslip.io`.  If given, the domain must already be configured to point to the selected **External IP** , otherwise configuration will fail. Only use this option if you have control over your own DNS entries - the CaaS system or Kubernetes will not create a DNS entry for you.  |  No  |  No
{.table .table-striped}  
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes#cluster-configuration,3565,580
Accessing the cluster,"Kubernetes is configured to use the OpenID Connect support of the [Identity
Manager]({{% ref ""cluster-as-a-service-identity-manager"" %}}) for
authentication and authorisation. This means that all interactions with the
cluster are authenticated and authorised against the users in FreeIPA, via the
Keycloak integration.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes#accessing-the-cluster,319,41
Using the dashboard,"If the option to deploy the dashboard was selected, the Kubernetes dashboard
will be available at `https://<dashboard domain>`. Upon visiting the
dashboard, you will be redirected to Keycloak to sign in:
{{<image src=""img/docs/cluster-as-a-service-kubernetes/file-FQoiCe1v8G.png"" caption=""Keycloak sign-in screen"">}}
Any user that exists in your FreeIPA database is able to log in to the
dashboard, but only those with permissions assigned for the cluster will be
able to see or do anything. Here is an example of what a user with no
permissions will see:
{{<image src=""img/docs/cluster-as-a-service-kubernetes/file-JWVgCWB0YZ.png"" caption=""View for user with no permissions"">}}
And here is an example of what a user with full admin rights will see (see
Using Kubernetes RBAC below):
{{<image src=""img/docs/cluster-as-a-service-kubernetes/file-CXcLW5mzk0.png"" caption=""View for user with full admin rights"">}}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes#using-the-dashboard,910,119
Using kubectl,"This section assumes that you have
[kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/), the
Kubernetes command-line client, installed on your workstation. In order to
authenticate with Keycloak, you must also install the
[kubelogin](https://github.com/int128/kubelogin) plugin, which provides OpenID
Connect authentication for kubectl.
In order to configure OpenID Connect, you need to know the client ID and
secret of the OpenID Connect client for your Kubernetes cluster in Keycloak.
If you are an admin, you can [find this information in the Keycloak admin
console]({{% ref ""cluster-as-a-service-identity-manager"" %}}) \- the client
will be named after the cluster. If you are **not** an admin, your admin
should provide you with this information.
Use the following commands to configure kubectl to connect to your Kubernetes
cluster using your Identity Manager, replacing the variables with the correct
values for your clusters:
{{<command>}}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes#using-kubectl,962,130
Put the configuration in its own file,"export KUBECONFIG=./kubeconfig
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes#put-the-configuration-in-its-own-file,31,2
Configure the cluster information,"kubectl config set-cluster kubernetes \
    --server https://${KUBERNETES_EXTERNAL_IP}:6443 \
    --insecure-skip-tls-verify=true
(out)Cluster ""kubernetes"" set.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes#configure-the-cluster-information,161,12
Configure the OpenID Connect authentication,"kubectl config set-credentials oidc \
    --auth-provider=oidc \
    --auth-provider-arg=idp-issuer-url=https://${ID_GATEWAY_DOMAIN}/auth/realms/master \
    --auth-provider-arg=client-id=${CLIENT_ID} \
    --auth-provider-arg=client-secret=${CLIENT_SECRET}
(out)User ""oidc"" set.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes#configure-the-openid-connect-authentication,280,15
Configure the context and set it to be the current context,"kubectl config set-context oidc@kubernetes --cluster kubernetes --user oidc
(out)Context ""oidc@kubernetes"" created.
kubectl config use-context oidc@kubernetes
(out)Switched to context ""oidc@kubernetes"".
{{</command>}}
Once kubectl is configured, use the oidc-login plugin to authenticate with
Keycloak and obtain an ID token. Running this command launches a temporary
lightweight web server on your workstation that performs the authentication
flow with Keycloak. A browser window will open where you enter your username
and password:
{{<command>}}
kubectl oidc-login
(out)Open http://localhost:8000 for authentication
(out)You got a valid token until 2019-07-11 21:58:06 +0100 BST
(out)Updated ./kubeconfig
{{</command>}}
You can now use kubectl to query the Kubernetes resources to which you have
been granted access:
{{<command>}}
kubectl get nodes
(out)NAME                  STATUS   ROLES               AGE   VERSION
(out)kubernetes-master-0   Ready    controlplane,etcd   12h   v1.13.5
(out)kubernetes-worker-0   Ready    worker              12h   v1.13.5
(out)kubernetes-worker-1   Ready    worker              12h   v1.13.5
(out)kubernetes-worker-2   Ready    worker              12h   v1.13.5
{{</command>}}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes#configure-the-context-and-set-it-to-be-the-current-context,1217,135
Using Kubernetes RBAC,"Kubernetes includes a powerful [Role-Based Access Control (RBAC)
system](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). A full
discussion of the RBAC system is beyond the scope of this documentation, but
this section gives some examples of how RBAC in Kubernetes can be used in
combination with [FreeIPA groups]({{% ref ""cluster-as-a-service-identity-manager"" %}}) to allow fine-grained access to the cluster.
For every Kubernetes cluster that is deployed, CaaS automatically creates a
group in FreeIPA called `<clustername>_users`. This group, along with the
`admins` group, are assigned the `cluster-admin` role using a
`ClusterRoleBinding`, which grants super-user access to the entire cluster. In
order to grant a user super-user access to the cluster, they just need to be
added to one of these groups (depending on whether you want them to be a
super-user on other clusters as well).
It is also possible to create additional groups in FreeIPA and attach more
restrictive permissions to them.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes#using-kubernetes-rbac,1015,145
Example 1: Read-only cluster access,"For example, suppose you have some auditors who require read-only access to
the entire cluster in order to know what workloads are running. The first
thing to do is [create a group in FreeIPA]({{% ref ""cluster-as-a-service-identity-manager"" %}}) \- in this case, you might create a group called
`kubernetes_auditors`. Once the group is created, you can reference it in
Kubernetes by using the prefix `oidc:` \- in this case the group would be
referenced in Kubernetes as `oidc:kubernetes_auditors`. To grant read-only
access to this group for the entire cluster, create a `ClusterRoleBinding`
linking that group to the built-in `view` role:
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-auditors-read
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: view
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: oidc:kubernetes_auditors
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes#example-1:-read-only-cluster-access,931,121
Example 2: Read-write namespace access,"As another example, suppose you have some developers who want to deploy their
app in your cluster, and you want to grant them read-write access to a single
namespace to do this. Again, the first thing you would do is create a group in
FreeIPA called, for example, `myapp_developers`. You can then assign this
group the built-in `edit` role, but this time use a `RoleBinding` that is tied
to a particular namespace:
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: myapp-developers-edit
  namespace: myapp
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: edit
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: oidc:myapp_developers
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes#example-2:-read-write-namespace-access,711,96
Using the cluster,"It is beyond the scope of this documentation to discuss how to use Kubernetes: please refer to the Kubernetes documentation for that. This section
describes some things about the way Kubernetes is deployed by CaaS that will
make a difference to how your applications are deployed.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes#using-the-cluster,281,46
Ingress,"In CaaS Kubernetes, [Ingress
resources](https://kubernetes.io/docs/concepts/services-networking/ingress/)
are handled by the [Nginx Ingress
Controller](https://kubernetes.github.io/ingress-nginx/), which is exposed at
the external IP used by the master node. The Ingress Controller supports a
wide range of `Ingress` annotations that can be used to customise the
behaviour for particular services - visit the documentation for more details.
In order to expose a service using an `Ingress` resource, each `host` given in
the resource specification must have a DNS entry that points to the external
IP of the master node (where the Ingress Controller is listening). CaaS or
Kubernetes will **not** create these DNS records for you, and it is not
possible to use an IP address as a `host`. If you cannot create or edit DNS
records, you can use [xip.io](http://xip.io/) (or similar services) - these
are ""magic domains"" that provide DNS resolution for any IP address using
domains of the form `[subdomain.]<ip>.xip.io`.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes#ingress,1016,148
TLS with cert-manager,"CaaS Kubernetes deployments also include [Jetstack's cert-
manager](https://docs.cert-manager.io/en/latest/), which provides Kubernetes-
native resources for obtaining and renewing SSL certificates - visit the
documentation for more information. CaaS installs a `ClusterIssuer` called
`letsencrypt` that can automatically fetch and renew browser-trusted SSL
certificates from [Let's Encrypt](https://letsencrypt.org/) using
[ACME](https://en.wikipedia.org/wiki/Automated_Certificate_Management_Environment).
By using annotations, certificates can be fetched automatically for `Ingress`
resources:
apiVersion: extensions/v1
kind: Ingress
metadata:
  name: myapp
  annotations:
    kubernetes.io/ingress.class: ""nginx""
    cert-manager.io/cluster-issuer: ""letsencrypt""
spec:
  tls:
    - hosts:
        - example.example.com
      secretName: myapp-tls  # This secret will be created by cert-manager
  rules:
    - host: example.example.com
      http:
        paths:
          - path: /
      pathType: prefix
            backend:
              service
    name: myapp
                port: 
      number: 8080
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes#tls-with-cert-manager,1110,103
Storage,"CaaS Kubernetes is also configured to take advantage of the fact that it is
running on [Openstack](https://www.openstack.org/). In particular, a [storage
class](https://kubernetes.io/docs/concepts/storage/storage-classes/) is
installed that can dynamically provision
[Cinder](https://docs.openstack.org/cinder/) volumes in response to
[persistent volume
claims](https://kubernetes.io/docs/concepts/storage/persistent-
volumes/#persistentvolumeclaims) being created. This storage class is called
`csi-cinder-sc-delete`, and is consumed like this:
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: myclaim
    spec:
      storageClassName: csi-cinder-sc-delete
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
If there is enough quota available, this persistent volume claim should result
in a new Cinder volume being provisioned and bound to the claim. The Cinder
volume will show up in the **Volumes** tab of the JASMIN Cloud Portal with a
name of the form `kubernetes-dynamic-pvc-<uuid>`.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-kubernetes#storage,1070,113
Cluster-as-a-Service - Shared storage,"This article describes how to deploy and use shared storage clusters using
JASMIN Cluster-as-a-Service (CaaS).
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-shared-storage,111,15
Introduction,"CaaS provides shared storage clusters that can be mounted on multiple nodes to
provide common storage across all those nodes.
These storage clusters are not intended to be directly consumed by users, but
are taken as cluster configuration options by other clusters. In particular,
[Slurm clusters]({{% ref ""cluster-as-a-service-slurm"" %}}) take a shared
storage cluster as a configuration option - the shared storage is mounted on
each cluster node for user home directories.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-shared-storage#introduction,476,72
NFS,"{{<link ""https://en.wikipedia.org/wiki/Network_File_System"">}}Network File System (NFS){{</link>}}
is a protocol for accessing remote network-attached storage. NFS is also used
to refer to the implementation of the protocol in the Linux kernel.
A CaaS NFS shared storage cluster provides a simple
NFS server. A volume is
attached of the specified size, formatted as an {{<link ""https://en.wikipedia.org/wiki/XFS"">}}XFS filesystem{{</link>}}, mounted at `/srv` and exported with no authentication.
NFS servers do not get an external IP, and so are only accessible from the
tenancy's internal network.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-shared-storage#nfs,600,80
Cluster configuration,"The following variables are available to configure an NFS cluster:
Variable |  Description  |  Required?  |  Can be updated?  
---|---|---|---  
Identity manager  |  The CaaS Identity Manager that is used to control access to the cluster.  |  Yes  |  No  
Volume size  |  The size of the NFS data volume in GB.  |  Yes  |  No  
Size  |  The size to use for the NFS server.  |  Yes  |  No
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/cluster-as-a-service-shared-storage#cluster-configuration,412,72
Adding and removing SSH keys from an External Cloud VM,"When you create a machine in a JASMIN External Cloud tenancy, the SSH key
associated with your JASMIN account is uploaded to the machine to grant you
access to the machine as `root`:
{{<command>}}
ssh -A root@<external ip>
{{</command>}}
However, this is a one-time operation when the machine is created. **Updating
your SSH key in the JASMIN Accounts Portal is not reflected in External Cloud
VMs.** Once initial access has been granted, you as the tenancy admin are
responsible for **all** configuration of the machine, including the SSH keys
allowed to access the machine. For example, you may choose to grant access to
a user who does not have a JASMIN account by adding their SSH key to the
machine.
**IMPORTANT:** When you change your SSH key in the JASMIN Accounts Portal, you
**must retain your old private key** until you have added your new key to all
External Cloud VMs that you administer. Failing to do so will result in you
being locked out of those machines.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/adding-and-removing-ssh-keys-from-an-external-cloud-vm,974,168
Adding and removing SSH keys,"The allowed SSH keys for a user can be found in `$HOME/.ssh/authorized_keys`
for the user. For `root`, this is `/root/.ssh/authorized_keys`.
To grant access to a user, they must first {{<link ""../getting-started/generate-ssh-key-pair"">}}generate an SSH key pair{{</link>}}. Once they have done this, they should give you
their **public** key. **The private key should never leave the user's
local machine.** Once you have added this public key as a new line to the
`authorized_keys` file for the target user on your External Cloud VM, the user
will be able to SSH to the machine.
Similarly, to disable access for a user, just remove their public key from the
`authorized_keys` file on the machine.
**IMPORTANT:** When replacing an old key with a new one, make sure that you
add the new key before removing the old one or you may accidentally lock
yourself out of your machine.
",https://help.jasmin.ac.uk/docs/for-cloud-tenants/adding-and-removing-ssh-keys-from-an-external-cloud-vm#adding-and-removing-ssh-keys,877,143
Check network details,"{{<alert type=""danger"">}}
DEPRECATED: This article will shortly become obsolete following JASMIN's [migration to the Rocky 9 operating system](../software-on-jasmin/rocky9-migration-2024). The previous restriction on network domains and reverse DNS does not apply to the set of new machines (and old ones are being replaced).
{{</alert>}}
This article explains how to:
- check that your network domain is able to access JASMIN resources
- check that the particular host from which you are intending to connect to JASMIN has the required network configuration
",https://help.jasmin.ac.uk/docs/interactive-computing/check-network-details,559,80
Check network domain (non .ac.uk users),"In order to maintain a secure and reliable scientific infrastructure for its
users, JASMIN restricts login access by maintaining an ""allow list"" of network
domains that are allowed to make SSH connections to the JASMIN login gateways
and data transfer servers.
All `.ac.uk` network domains (i.e. UK universities and ""academic"" institutions)
are already registered.
If your institution's network domain is not .ac.uk, please request for it to
be added to the allow list by contacting the
[JASMIN Helpdesk](mailto:support@jasmin.ac.uk), after reading the
information in the following section about forward and reverse DNS lookup.
",https://help.jasmin.ac.uk/docs/interactive-computing/check-network-details#check-network-domain-(non-.ac.uk-users),628,92
Check IP address resolves to network domain (all users),"In addition to being on the allowed IP list there is an additional requirement
that the address of your local computer must have **forward and reverse DNS
lookup enabled**. This means that the hostname must resolve to an IP address,
and the IP address must resolve to the fully-qualified hostname.
One easy way to do this is to access the following URL **from the machine
which will be used to make the SSH connection to JASMIN** :
<https://accounts.jasmin.ac.uk/services/reverse_dns_check/>
If you don't have a web browser on that machine, you can use the 'curl' or
'wget' Linux commands to make an HTTP request to that URL, and inspect the
output. A successful response will look like this:
External IP address: 130.246.123.456
Resolved to host: vpn-3-167.rl.ac.uk
Whereas an unsuccessful response will look like this:
External IP address: 130.246.123.456 
Reverse DNS lookup failed
If your IP address does not resolve, please contact your local IT technical
support desk and show them this article to help explain the context.
It is important that the network domain to which the IP address resolves is
part of the network domain which has been allowed. If there is no obvious
relationship between the network domain of the host and that of your
institution (derived from your email address), you may be asked to provide
additional justification or your connection may be denied. Some institutions
prefer not to provide public DNS listings: in this case please ask the
technical support representative to contact the JASMIN helpdesk on your behalf
to see if a technical solution can be found.
This can be a problem if you attempt to connect directly from a commercial
home or business internet service provider. Wherever possible, please connect
to your institution (which is likely to be on the allow list already) before
making an outgoing SSH connection to a JASMIN server.
As long as the IP address resolves to a fully-qualified hostname within the
allowed domain, it does not matter whether the host has a static or
dynamically-assigned (DHCP) IP address.
If you cannot obtain an IP address that resolves in this way, you may still be
able to access (only) `login2.jasmin.ac.uk`, which has been configured to
enable access without the reverse DNS restriction. However, this is likely to
be the only entry point available to you and will limit what you can do on
JASMIN. Doing so can be useful as a temporary solution, but to gain full use
of JASMIN you will need to have an IP address that resolves to the domain of
your institution. For access to graphical desktops, equivalent servers `nx-
login2.jasmin.ac.uk` and `nx-login3.jasmin.ac.uk` have been provided and for
transfer tasks, an additional transfer server `xfer3.jasmin.ac.uk` is
available. See [login servers]({{% ref ""login-servers"" %}}) and [transfer
servers]({{% ref ""transfer-servers"" %}}), but note the additional access role
required in the case of the transfer server.
",https://help.jasmin.ac.uk/docs/interactive-computing/check-network-details#check-ip-address-resolves-to-network-domain-(all-users),2944,475
Tenancy Sci Analysis VMs,"{{<alert type=""danger"">}}
**Deprecated feature** documented here to provide information about how to access existing instances.
Tenancy sci analysis machines are no longer part of the JASMIN Cloud service, as most use cases can be
served by using the shared [sci servers]({{% ref ""sci-servers"" %}}).
{{</alert>}}
This document explains how to access a tenancy-based sci machine. These are
normally provisioned by an admin/manager representing a particular community/institution.
Check which institutions/group you belong to:
NCAS, NCEO, UKMO, RSG, ...
The admin/manager will provide you the name of the tenancy.
You can then search for it on your JASMIN accounts portal under the JASMIN
service named `Sci Analysis VMs`.
",https://help.jasmin.ac.uk/docs/interactive-computing/tenancy-sci-analysis-vms,721,106
How to request access,"**Step 1:** Find the Sci Analysis VMs under the Menu 'Discover services' at https://accounts.jasmin.ac.uk/services/
**Step 2:** Check the name of the service and the description that your supervisor
or PI recommended you to choose and click ""More information""
{{<image src=""img/docs/tenancy-sci-analysis-vms/file-EUW05EGJj3.png"" caption=""Locate the correct service"" >}}
{{<image src=""img/docs/tenancy-sci-analysis-vms/file-pyW8yyQboJ.png"" caption=""Click +Apply"" >}}
**Step 3:** Apply for `USER` role and provide details on your project and a
reference then click ""Apply""
{{<image src=""img/docs/tenancy-sci-analysis-vms/file-QyGyIDjEcM.png"" caption=""Apply for USER role"" >}}
**Step 4:** Notification
{{<image src=""img/docs/tenancy-sci-analysis-vms/file-pY8gr70WNK.png"" caption=""Status PENDING"" >}}
Once your request was approved, you will get a notification
{{<image src=""img/docs/tenancy-sci-analysis-vms/file-j2xhRrluyc.png"" caption=""Notification"" >}}
{{<image src=""img/docs/tenancy-sci-analysis-vms/file-BLPGdEfAD5.png"" caption=""Status updated to ACTIVE"" >}}
If your request was rejected, then reapply and provide further supporting
information
{{<image src=""img/docs/tenancy-sci-analysis-vms/file-un7XlTQ5JU.png"" caption=""Rejected: further info requested"" >}}
Click on the rejection notification. This will take you to the following page
where you can ""Apply again""
{{<image src=""img/docs/tenancy-sci-analysis-vms/file-jtLIeTwWez.png"" caption=""Request for further detail"" >}}
",https://help.jasmin.ac.uk/docs/interactive-computing/tenancy-sci-analysis-vms#how-to-request-access,1479,148
How to login,"The machine will not be accessible directly externally, so you need to access it via a JASMIN login machine: don't forget the -A (agent forwarding) option on your initial connection.
{{<command user=""localuser"" host=""localhost"">}}
ssh -A user@login2.jasmin.ac.uk
{{</command>}}
Access the VM using the IP address (not the hostname) of the virtual machine. Your manager should be able
to provide you with this, since they created (provisioned) the virtual machine.
{{<command user=""user"" host=""login2.jasmin.ac.uk"">}}
ssh -A user@<IP-ADDRESS-OF-VM>
{{</command>}}
",https://help.jasmin.ac.uk/docs/interactive-computing/tenancy-sci-analysis-vms#how-to-login,563,75
How to report issues,"Users should report issues to the `ADMIN` and/or `MANAGER`/`DEPUTY` of the
tenancy based SCI VM initially, rather than the JASMIN team. If the issue cannot be
resolved by the `ADMIN` and/or `MANAGER`, **they** should contact the JASMIN helpdesk. You can find the name of the current holders of the `MANAGER`/`DEPUTY` roles by going to the page which described the service on the accounts portal, as above.
",https://help.jasmin.ac.uk/docs/interactive-computing/tenancy-sci-analysis-vms#how-to-report-issues,406,66
Creating a virtual environment in the JASMIN Notebooks Service,"Creating a virtual environment is useful to allow a discrete set of extra
packages to be installed to meet specific requirements. This allows a user to
run multiple environments with different dependencies without conflicts.
There are a number of ways to create a virtual environment to use with the
Notebooks Service. This document outlines the most common and recommended
methods, and then some other ways which you might find useful.
Please note that environments created for the Notebooks Service **will not work**
on the JASMIN scientific analysis servers or the LOTUS batch processing.
",https://help.jasmin.ac.uk/docs/interactive-computing/creating-a-virtual-environment-in-the-notebooks-service,592,93
Step 1: Creating a virtual environment,"This step creates a Python virtual environment, and allows you to install
packages into it.
{{<alert type=""danger"">}}
These commands are intended for use at the Jupyter
Notebooks shell, **not on the JASMIN sci machines**
{{</alert>}}
To get started, open the [JASMIN Notebooks Service](https://notebooks.jasmin.ac.uk/)
and in the launcher click the terminal button.
{{<image src=""/img/docs/creating-a-virtual-environment-in-the-jasmin-notebooks-service/notebook-terminal.png"" caption=""Opening the terminal"">}}
{{<alert type=""info"">}}
Don't worry if you see this: this is a known issue but should not cause you a problem.
id: cannot find name for user ID NNNNN
[I have no name!@jupyter-user notebooks-misc]$ 
{{</alert>}}
Then, type these commands at the bash shell which appears.
First, make a directory in which to store your virtual environments. You can
put this wherever you like, as long as you reference the same place later. You
could store several virtual environments within this directory, for different
purposes. Then, change into that directory.
{{<command user=""user"" host=""jupyter-user"">}}
mkdir ~/nb_envs
cd ~/nb_envs
{{</command>}}
Next, create a new empty virtual environment. We recommended including the
`--system-site-packages` argument which will allow you to add packages on top
of `jaspy`, rather than starting completely from scratch.
{{<command user=""user"" host=""jupyter-user"">}}
python -m venv name-of-environment --system-site-packages
{{</command>}}
Then, activate the specific virtual environment created above, which will
allow you to install packages.
{{<command user=""user"" host=""jupyter-user"">}}
source name-of-environment/bin/activate
{{</command>}}
If you want to be able to use your virtual environment as a Jupyter Notebook
kernel (recommended), you should install `ipykernel` using pip.
{{<command user=""user"" host=""jupyter-user"">}}
pip install ipykernel
{{</command>}}
You can then install whatever packages you need in the environment.
{{<command user=""user"" host=""jupyter-user"">}}
pip install pyjokes
{{</command>}}
If you change your mind and need to add more packages in the future, it is
simple to activate the virtual environment in the same way as above and use
`pip` to install more packages.
",https://help.jasmin.ac.uk/docs/interactive-computing/creating-a-virtual-environment-in-the-notebooks-service#step-1:-creating-a-virtual-environment,2240,295
Step 2: Making the notebooks service recognise your new kernel,"These steps are also run from the notebooks' service shell, as above.
If you aren't still there from the last step, cd to the location of your `venv`.
{{<command user=""user"" host=""jupyter-user"">}}
cd ~/nb_envs
{{</command>}}
If it isn't already active, activate the virtual environment.
{{<command user=""user"" host=""jupyter-user"">}}
    source name-of-environment/bin/activate
{{</command>}}
Running the following command will make the Notebooks Service notice your new
virtual environment, and include it in the list of kernels which you can run
code with. You only have to do this once.
{{<command user=""user"" host=""jupyter-user"">}}
python -m ipykernel install --user --name=name-of-environment
{{</command>}}
",https://help.jasmin.ac.uk/docs/interactive-computing/creating-a-virtual-environment-in-the-notebooks-service#step-2:-making-the-notebooks-service-recognise-your-new-kernel,712,94
Step 3: Using your new kernel,"{{<image src=""/img/docs/creating-a-virtual-environment-in-the-jasmin-notebooks-service/197739637-1e75ce45-c0de-49ec-b168-d2dc101ca7fe.png"" caption=""Select kernel, in this case: 'name-of-environment'"" wrapper=""col-6 mx-auto"">}}
You can then choose this kernel from the Jupyter Notebook homepage, or from the top
right of any open notebook. No changes to the Python code within are required.
{{<image src=""/img/docs/creating-a-virtual-environment-in-the-jasmin-notebooks-service/197740127-074abd6d-f0f2-4450-8c4c-232a5800137c.png"" caption=""Kernel name shown in notebook title tab"">}}
",https://help.jasmin.ac.uk/docs/interactive-computing/creating-a-virtual-environment-in-the-notebooks-service#step-3:-using-your-new-kernel,582,48
Other tips & useful knowledge,,https://help.jasmin.ac.uk/docs/interactive-computing/creating-a-virtual-environment-in-the-notebooks-service#other-tips-&-useful-knowledge,0,0
Activating an environment without it being a kernel,"If you follow Step 1 above to create a virtual environment, it is possible to
use the packages from this environment in a Python file without making it a
kernel. While this can be useful, it has the very distinct disadvantage of
hardcoding the path to your virtual environment in your Python code. For this
reason we discourage using this method with a medium level of severity. To do
this, simply add the following code to your Python file **before** any
imports. Adjust the `venv_path` variable to be correct for the `venv` you
created.
import sys
import pathlib
import platform
venv_path = ""~/nb_envs/name-of-environment""
py_version = platform.python_version_tuple()
sys.path.append(
    str(
        pathlib.Path(
            f""{venv_path}/lib/python{py_version[0]}.{py_version[1]}/site-packages/""
        ).expanduser()
    )
)
Explanation: this adds the `site-packages` folder from your `venv` directly to the
path Python uses to search for packages (`$PYTHONPATH`). This lets Python
find them to import.
",https://help.jasmin.ac.uk/docs/interactive-computing/creating-a-virtual-environment-in-the-notebooks-service#activating-an-environment-without-it-being-a-kernel,1011,140
Can I install packages from inside my Python code?,"We very strongly recommend **NOT** trying to install Python packages from
inside notebook code. `pip` isn't designed for it, and it is almost always
easier to activate the `venv` as above and install things that way.
If you wish to record the set of packages inside your `venv` so you can install
them en-masse later, `pip` has the facility to do this. To export a list of
packages that exist inside a `venv`, from the notebook's bash shell with the
virtual environment in question activated:
{{<command user=""user"" host=""jupyter-user"">}}
pip freeze > requirements.txt
{{</command>}}
To install a list of packages which have been exported:
{{<command user=""user"" host=""jupyter-user"">}}
pip install -r requirements.txt
{{</command>}}
Exporting packages in this way is also useful for sharing your environment
with others, reinstalling when it breaks etc. It's a good idea to keep the
requirements file alongside the code in version control. If your code becomes
more complex it is probably more sensible to make it a Python package, and
install it as one, but doing that is outside the scope of this document.
If you really must, you can call `pip` from inside your notebook like this:
(after first updating the packages variable to be the ones you want to
install.)
import sys
import subprocess as sp
packages = ['pyjokes']
sp.check_call([sys.executable, '-m', 'pip', 'install'] + packages)
",https://help.jasmin.ac.uk/docs/interactive-computing/creating-a-virtual-environment-in-the-notebooks-service#can-i-install-packages-from-inside-my-python-code?,1391,220
Can I use conda instead of a virtual environment?,"Yes, no problem.
To create a conda environment, simply run the following at the JASMIN
Notebook shell:
{{<command user=""user"" host=""jupyter-user"">}}
conda create --name name-insert-here ipykernel
{{</command>}}
Install any packages you which to use in the environment:
{{<command user=""user"" host=""jupyter-user"">}}
conda install --name name-insert-here pyjokes
{{</command>}}
Make the Notebooks Service recognise your environment as a kernel:
{{<command user=""user"" host=""jupyter-user"">}}
conda run --name name-insert-here python -m ipykernel install --user --name name-insert-here
{{</command>}}
",https://help.jasmin.ac.uk/docs/interactive-computing/creating-a-virtual-environment-in-the-notebooks-service#can-i-use-conda-instead-of-a-virtual-environment?,597,70
Can I get rid of my old kernels from the Notebooks Service?,"Yes.
To list the names of kernels you have installed, run the following at the
JASMIN Notebook shell:
{{<command user=""user"" host=""jupyter-user"">}}
jupyter kernelspec list
{{</command>}}
To remove one of them, run:
{{<command user=""user"" host=""jupyter-user"">}}
jupyter kernelspec uninstall insert-name-here
{{</command>}}
",https://help.jasmin.ac.uk/docs/interactive-computing/creating-a-virtual-environment-in-the-notebooks-service#can-i-get-rid-of-my-old-kernels-from-the-notebooks-service?,322,39
Login problems,"Having problems connecting to a host on JASMIN? Details of how to login to
JASMIN can be found [here]({{% ref ""how-to-login"" %}}), but this article may help to
resolve login problems. It provides information for the following issues:
- Unable to login to a `login` server
- Can login to `login` server but can't login to a subsequent server
- `ssh-add` command gives error: ""Could not open a connection to your authentication agent.""
- Errors when trying to connect with MobaXterm
",https://help.jasmin.ac.uk/docs/interactive-computing/login-problems,481,81
Unable to login to login server,"If you are unable to login to a login server e.g. `login-01.jasmin.ac.uk` then
look carefully at any error messages displayed as this can help diagnose what
is wrong:
**1) ""Connection reset by peer""**
This suggests a problem with the configuration of your machine or local
network. We no longer restrict access to JASMIN by network domain, and no longer
require registration of non-`*.ac.uk` domains, so you should be able to connect from
anywhere. If your local admin team is not able to resolve the issue, please
contact JASMIN support.
**2) ""Permission denied""**
Here, the most likely cause is that the SSH key which your client is
presenting does not match the one in your JASMIN account. This can be for a
number of reasons:
  * **Your SSH client is old and needs updating**
    * You can check this with `ssh -V` and comparing to the [versions mentioned here]({{% ref ""login-servers#recent-changes"" %}}).
    * You will need to update your client before you can connect to JASMIN securely. Ask your local admin team for help: this is
    not something that we can ""fix"" at the JASMIN end.
  * **You have omitted to specify the username in your SSH connection**
    * In this case, you will be attempting to connect with the username you have on your local machine, which may not be the same.
  * **You have only recently uploaded your SSH key (it can take 20 to 60 minutes before the key propagates to all the places it needs to on JASMIN).**
    * Try waiting a few minutes before trying again.
  * **You don't have your key loaded in your local authentication agent (e.g. ssh-agent).**
    * Check that you are following the method suitable for your operating system
      * The article ""[How to login]({{% ref ""how-to-login"" %}})"" has instructions for linux, mac and windows.  
    * Note that connections using NoMachine NX don't require an authentication agent: this can be a good alternative if you're having problems.
  * **You have not yet been granted jasmin-login access or your access has expired.**
    * To check, go to [My services](https://accounts.jasmin.ac.uk/services/my_services/?page=1&active=1&_apply_filters=1) on the JASMIN accounts portal and check that ""Login services: jasmin-login"" is listed. If not then you either need to [apply for jasmin-login access](https://accounts.jasmin.ac.uk/account/login/?next=/services/login_services/jasmin-login/), or if you have already done this recently you may simply need to wait for it to be approved. Note that if you have applied for access to a group workspace you still need jasmin-login access in order to connect to jasmin machines.
**3) ""The authenticity of host 'nnnn ( <ip address>)' can't be established.""
or ""key for host nnnn has changed""**
Your local computer stores a list of all the other SSH hosts which it has
successfully connected to in the past. If you use an intermediate host like a
login server to make onward connections to a sci machine, the login host will
maintain another such list. In both cases there should be a
file`~/.ssh/known_hosts` (so one in your local home directory on your own
machine, and one in your JASMIIN home directory)
When the SSH client first contacts the host for the SSH connection, it checks
to see if the remote host is one that it recognises. If this check fails, you
may get a message like the following:
**Message 1:**
{{<command>}}
(out)The authenticity of host 'nnnn (<ip address>)' can't be established.
(out)ECDSA key fingerprint is SHA256:8QY9iBcOQFyEYkpOtBUU8WQGeADb0DyMff01BRuvYls.
(out)ECDSA key fingerprint is MD5:f9:19:c4:5f:2b:fa:ed:aa:34:86:c9:23:dd:1c:44:30.
(out)Are you sure you want to continue connecting (yes/no)?
{{</command>}}
**Message 2:**
{{<command>}}
(out)@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
(out)@       WARNING: POSSIBLE DNS SPOOFING DETECTED!          @
(out)@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
(out)The ECDSA host key for nnnn has changed,
(out)and the key for the corresponding IP address <IP address>
(out)has a different value. This could either mean that
(out)DNS SPOOFING is happening or the IP address for the host
(out)and its host key have changed at the same time.
(out)Offending key for IP in /home/users/username/.ssh/known_hosts:62
(out)@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
(out)@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
(out)@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
(out)IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
(out)Someone could be eavesdropping on you right now (man-in-the-middle attack)!
(out)It is also possible that a host key has just been changed.
(out)The fingerprint for the ECDSA key sent by the remote host is
(out)SHA256:Evr7U40sEGSLVypfafLYtbF2oYvGDuBxTyrALdx11pk.
(out)Please contact your system administrator.
(out)Add correct host key in /home/users/username/.ssh/known_hosts to get rid of this message.
(out)Offending ECDSA key in /home/users/username/.ssh/known_hosts:115
(out)ECDSA host key for nnnn has changed and you have requested strict checking.
(out)Host key verification failed.
{{</command>}}
This can happen when:
- machines are re-installed (as part of maintenance by the JASMIN team)
- when you modify your `~/.ssh/known_hosts` file  
- when you access a ""known"" host but via a different name (e.g. `sci-vm-01` vs `sci-vm-01.jasmin.ac.uk`)
Message 1 means that you don't have an entry for that host in your
`~/.ssh/known_hosts` file. In most cases, you can safely reply ""yes"" and the
SSH connection should proceed as normal from then on.
If you get message 2, and are confident that the change is for a legitimate
reason, the solution is to modify your `~/.ssh/known_hosts` file, removing the
entries for that host (there may be more than one, as above for `sci1` vs
`sci-vm-01.jasmin.ac.uk`) by deleting those lines. Next time you try and connect,
you will get message 1, but can reply ""yes"" and the SSH connection should
proceed as normal.
**Note:** If you're using a graphical SFTP or SCP client for data transfers,
the error messages above may be hidden and so it can be harder to establish
the reason for failure. Using a terminal session (in MobaXterm on Windows, or
Mac/Linux terminal) to the problem host will likely reveal the messages and
enable you to follow the steps above to solve the problem.
",https://help.jasmin.ac.uk/docs/interactive-computing/login-problems#unable-to-login-to-login-server,6331,942
Can login to login server but can't login to a subsequent host,"Here, there are 3 main possibilities:
**1) You have not set up agent forwarding correctly on your local machine.**
****This allows your ssh key to be used for logging in from the login server to
other machines. To check, run the following command on the login server:
{{<command user=""user"" host=""login-01"">}}
echo ""$SSH_AUTH_SOCK""
{{</command>}}
This should display something that looks similar to (but not identical to)
{{<command user=""user"" host=""login-01"">}}
(out)/tmp/ssh-RNjiHr2844/agent.2844
{{</command>}}
If nothing is displayed then it indicates
that agent forwarding is not working. Please read 
[how to login]({{% ref ""how-to-login"" %}}) and make sure
 you are running ssh-agent (or similar), have loaded
your private key and are using the `-A` option on your ssh command for the
connection to the login server. NX users should make sure that the ""agent
forwarding"" option is ticked when setting up a connection profile.
**2) Some hosts within JASMIN are restricted to particular (groups of)
users.**
The [`sci` servers]({{% ref ""sci-servers""%}}) and [`xfer` servers]({{% ref ""transfer-servers"" %}})
should be available to all with `jasmin-login` access
(see above). However, some other machines are restricted to particular project
participants and require special permission to use. For example, **old** the high-
performance transfer servers `hpxfer[12].jasmin.ac.uk` require the
{{<link ""../data-transfer/hpxfer-access-role"">}}hpxfer access role{{</link>}}, which can be applied for at the JASMIN accounts portal,
as can most roles currently in use.
**3) There is a problem with the host you are trying to connect to.**
Occasionally there may be problems with the host (machine) which you are
trying to connect to. The sci servers (particularly physical/high-memory hosts
`sci-ph-[12]`) experience very high usage loads and occasionally run out of
resources. This may prevent you from logging in. In some circumstances ask you
for a password: this is normally a sign that something is wrong with the
machine, since passwords are not used for SSH logins on JASMIN, so there is
no point in trying to enter your account password or SSH passphrase at this
stage. In this case please contact the JASMIN helpdesk.
If you still have problems then please contact us using the help beacon below.
It would be helpful if you can include as much of the following information as
possible:
- The IP address and full hostname of the machine you are trying to connect from.
- The date and time that you tried connecting (to the nearest minute if possible). This will help us to identify any relevant messages in any log files.
- The exact command you were using
- Add `-vvv` to your `ssh` command and send us the the output (please include the command itself)
- List the SSH keys directory on your local machine. On a linux machine this can be done with the command: `ls -l ~/.ssh`
",https://help.jasmin.ac.uk/docs/interactive-computing/login-problems#can-login-to-login-server-but-can't-login-to-a-subsequent-host,2883,455
"ssh-add command gives error: ""Could not open a connection to your authentication agent.""","On some terminal sessions the usual instructions for starting the ssh-agent
session and adding the key may give the following error:
{{<command user=""user"" host=""localhost"">}}
ssh-add ~/.ssh/id_rsa_jasmin
(out)Could not open a connection to your authentication agent.
{{</command>}}
If you get this message, please try either:
modifying the method you use to start the ssh-agent, to:
{{<command user=""user"" host=""localhost"">}}
eval $(ssh-agent -s)
{{</command>}}
(and then trying to load the key again)
or see below (if using MobaXterm) which now has a better way of loading the SSH key.
","https://help.jasmin.ac.uk/docs/interactive-computing/login-problems#ssh-add-command-gives-error:-""could-not-open-a-connection-to-your-authentication-agent.""",588,86
Errors when connecting with Mobaxterm,"Please follow the [instructions for MobaXterm]({{% ref ""mobaxterm"" %}}) (which
include a video to show how to load your key into its own ssh-agent, `MobAgent`).
These instructions have changed with more recent versions of MobaXterm, and
replace the need to use the `ssh-add` command, so please make sure that both the
version you are using, and your method, are up to date!
Please note that even if your initial connection to (for example) your
university host does not require your JASMIN SSH key, you should still load
the key AND enable agent forwarding, for your initial connection to that host,
so that the key can be used for the subsequent connection to the JASMIN login
host. This actually applies to any connection method, not just MobaXterm.
",https://help.jasmin.ac.uk/docs/interactive-computing/login-problems#errors-when-connecting-with-mobaxterm,752,126
Introduction,"[Dask Gateway](https://gateway.dask.org/) is a service which manages [Dask](https://dask.org) clusters for users.
On JASMIN, it creates a Dask cluster in {{<link ""../batch-computing/lotus-overview.md"">}}LOTUS{{</link>}}, our batch computing cluster. It automatically creates a Dask for you, scheduling Slurm jobs to create Dask schedulers and workers as appropriate.
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#introduction,367,43
Prerequisites,"Before using Dask Gateway on JASMIN, you will need:
1. An existing JASMIN account and valid `jasmin-login` access role: {{<button size=""sm"" href=""https://accounts.jasmin.ac.uk/services/login_services/jasmin-login/"">}}Apply here{{</button>}}
2. **Subsequently** (once `jasmin-login` has been approved and completed), the `dask` access role: {{<button size=""sm"" href=""https://accounts.jasmin.ac.uk/services/additional_services/dask/"">}}Apply here{{</button>}}
The `jasmin-login` access role ensures that your account is set up with access to the LOTUS batch processing cluster, while the `dask` role grants access to the special LOTUS partition used by the Dask Gateway service.
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#prerequisites,677,76
Creating a Dask cluster,,https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#creating-a-dask-cluster,0,0
In the JASMIN Notebooks service,"In the {{<link ""jasmin_notebooks_service"">}}JASMIN notebooks service{{</link>}}, authentication to `dask-gateway` happens automatically. You can use the snippet below to create a cluster and get a Dask client which you can use:
import dask_gateway
# Create a connection to dask-gateway.
gw = dask_gateway.Gateway(""https://dask-gateway.jasmin.ac.uk"", auth=""jupyterhub"")
# Inspect and change the options if required before creating your cluster.
options = gw.cluster_options()
options.worker_cores = 2
# Create a Dask cluster, or, if one already exists, connect to it.
# This stage creates the scheduler job in Slurm, so it may take some
# time while your job queues.
clusters = gw.list_clusters()
if not clusters:
    cluster = gw.new_cluster(options, shutdown_on_close=False)
else:
    cluster = gw.connect(clusters[0].name)
# Create at least one worker, and allow your cluster to scale to three.
cluster.adapt(minimum=1, maximum=3)
# Get a Dask client.
client = cluster.get_client()
#########################
### DO DASK WORK HERE ###
#########################
# When you are done and wish to release your cluster:
cluster.shutdown()
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#in-the-jasmin-notebooks-service,1135,151
Elsewhere on JASMIN,"The following explains how to use the Dask Gateway elsewhere on JASMIN, for example, on the `sci` machines.
{{<alert type=""info"">}}
It is not necessary to do this if you only want to use Dask in the JASMIN notebook service.
{{</alert>}}
At the current time, it is still necessary to use the notebooks service to generate an API token to allow you to connect to the gateway server.
{{<alert type=""danger"">}}
It is very important that your API token is not shared between users and remains secret. With it, another user could submit Dask jobs to LOTUS as you, and they could exploit this to see anything in your JASMIN account.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#elsewhere-on-jasmin,639,110
Setup,"1. Make a Dask configuration folder in your home directory
    {{<command user=""user"" host=""sci-vm-01"">}}
    mkdir -p ~/.config/dask
    {{</command>}}
2. Create a configuration file for `dask-gateway`
    {{<command user=""user"" host=""sci-vm-01"">}}
    touch ~/.config/dask/gateway.yaml
    {{</command>}}
3. Change the permissions on the file so that only you can read it
    {{<command user=""user"" host=""sci-vm-01"">}}
    chmod 600 ~/.config/dask/gateway.yaml
    {{</command>}}
4. Head to the {{< link ""https://notebooks.jasmin.ac.uk/hub/token"" >}}API token generator page{{</link>}}, put a note in the box to remind yourself what this token is for, press the **big orange button**, then {{<mark>}}copy{{</mark>}} the {{<mark>}}token{{</mark>}}.
5. Paste the following snippet into `~/.config/dask/gateway.yaml`, replace the entry on the final line with the API token you just created.
    ```yaml
    gateway:
      address: https://dask-gateway.jasmin.ac.uk
      auth:
        type: jupyterhub
        kwargs:
          api_token: replaceWithYourSecretAPIToken
    ```
6. You're done. You can now use `dask-gateway` from the command line.
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#setup,1146,129
Access the Dask dashboard,"To get the link to your Dask dashboard, run the following:
print(client.dashboard_link)
Currently the Dask dashboard is not accessible from a browser outside the JASMIN firewall. If your browser fails to load the dashboard link returned,
please use our [graphical desktop service]({{% ref ""graphical-linux-desktop-access-using-nx"" %}}) to run a Firefox browser inside the firewall to view your dashboard.
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#access-the-dask-dashboard,405,57
Use a custom Python environment,"By default the JASMIN Notebooks service and Dask Gateway use the latest version of the `jaspy` software environment. However, often users would like to use their own software environments.
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#use-a-custom-python-environment,189,29
Understanding the problem,"When Dask Gateway creates a dask cluster for a user, it runs a setup command to activate a conda environment or python `venv`.
To have Dask use your packages, you need to create a custom environment which you can pass to `dask-gateway` to activate.
However, for technical reasons, it is not currently possible to use the same virtual environment in both the notebook service and on JASMIN. So you will need to make two environments, one for your notebook to use and one for Dask to use.
{{<alert type=""info"">}}
It is VERY important that these environments have the same packages installed in them, and that the packages are exactly the same version in both environments.
If you do not keep packages and versions in-sync you can expect many confusing errors.
{{</alert>}}
If you use a self-contained conda environment this is not a problem, and you can use this as a kernel in the notebooks service and on the `sci` machines. You can skip to {{<link ""#putting-it-all-together"">}}Putting it all together{{</link>}} below.
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#understanding-the-problem,1020,170
Creating a virtual environment for Dask,"- Login to one of the JASMIN `sci` machines.
- Activate `jaspy`
{{<command>}}
module load jaspy
{{</command>}}
- Create your environment in the normal way
{{<command>}}
python -m venv name-of-environment
{{</command>}}
- Activate the environment
{{<command>}}
source name-of-environment/bin/activate
{{</command>}}
- Install dask and dask gateway and dependencies: without this step your environment will not work with dask.
{{<command>}}
pip install dask-gateway dask lz4
{{</command>}}
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#creating-a-virtual-environment-for-dask,488,64
Creating a virtual environment for the notebooks service,"- Follow the instructions [here]({{% ref ""creating-a-virtual-environment-in-the-notebooks-service"" %}}) to create a virtual environment.
- Install Dask and Dask Gateway and dependencies: without this step your environment will not work with Dask.
{{<command>}}
pip install dask-gateway dask lz4
{{</command>}}
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#creating-a-virtual-environment-for-the-notebooks-service,310,38
Putting it all together,"- Set your notebook virtual environment as the kernel for the notebook in question as shown in the instructions linked above.
- Set `options.worker_setup` to a command which will activate your Dask virtual environment. For example
options.worker_setup = ""source /home/users/example/name-of-environment/bin/activate""
- If you have an existing Dask cluster, close it and ensure all LOTUS jobs are stopped before recreating it using the new environment.
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#putting-it-all-together,451,64
Code Examples,"Examples of code and notebooks which can be used to test the JASMIN Dask Gateway service are
{{< link ""https://github.com/cedadev/jasmin-daskgateway/tree/main/examples"" >}}available on GitHub{{</link>}}.
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#code-examples,204,23
Introduction,"[Dask Gateway](https://gateway.dask.org/) is a service which manages [Dask](https://dask.org) clusters for users.
On JASMIN, it creates a Dask cluster in {{<link ""../batch-computing/lotus-overview.md"">}}LOTUS{{</link>}}, our batch computing cluster. It automatically creates a Dask for you, scheduling Slurm jobs to create Dask schedulers and workers as appropriate.
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#introduction,367,43
Prerequisites,"Before using Dask Gateway on JASMIN, you will need:
1. An existing JASMIN account and valid `jasmin-login` access role: {{<button size=""sm"" href=""https://accounts.jasmin.ac.uk/services/login_services/jasmin-login/"">}}Apply here{{</button>}}
2. **Subsequently** (once `jasmin-login` has been approved and completed), the `dask` access role: {{<button size=""sm"" href=""https://accounts.jasmin.ac.uk/services/additional_services/dask/"">}}Apply here{{</button>}}
The `jasmin-login` access role ensures that your account is set up with access to the LOTUS batch processing cluster, while the `dask` role grants access to the special LOTUS partition used by the Dask Gateway service.
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#prerequisites,677,76
Creating a Dask cluster,,https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#creating-a-dask-cluster,0,0
In the JASMIN Notebooks service,"In the {{<link ""jasmin_notebooks_service"">}}JASMIN notebooks service{{</link>}}, authentication to `dask-gateway` happens automatically. You can use the snippet below to create a cluster and get a Dask client which you can use:
import dask_gateway
# Create a connection to dask-gateway.
gw = dask_gateway.Gateway(""https://dask-gateway.jasmin.ac.uk"", auth=""jupyterhub"")
# Inspect and change the options if required before creating your cluster.
options = gw.cluster_options()
options.worker_cores = 2
# Create a Dask cluster, or, if one already exists, connect to it.
# This stage creates the scheduler job in Slurm, so it may take some
# time while your job queues.
clusters = gw.list_clusters()
if not clusters:
    cluster = gw.new_cluster(options, shutdown_on_close=False)
else:
    cluster = gw.connect(clusters[0].name)
# Create at least one worker, and allow your cluster to scale to three.
cluster.adapt(minimum=1, maximum=3)
# Get a Dask client.
client = cluster.get_client()
#########################
### DO DASK WORK HERE ###
#########################
# When you are done and wish to release your cluster:
cluster.shutdown()
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#in-the-jasmin-notebooks-service,1135,151
Elsewhere on JASMIN,"The following explains how to use the Dask Gateway elsewhere on JASMIN, for example, on the `sci` machines.
{{<alert type=""info"">}}
It is not necessary to do this if you only want to use Dask in the JASMIN notebook service.
{{</alert>}}
At the current time, it is still necessary to use the notebooks service to generate an API token to allow you to connect to the gateway server.
{{<alert type=""danger"">}}
It is very important that your API token is not shared between users and remains secret. With it, another user could submit Dask jobs to LOTUS as you, and they could exploit this to see anything in your JASMIN account.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#elsewhere-on-jasmin,639,110
Setup,"1. Make a Dask configuration folder in your home directory
    {{<command user=""user"" host=""sci-vm-01"">}}
    mkdir -p ~/.config/dask
    {{</command>}}
2. Create a configuration file for `dask-gateway`
    {{<command user=""user"" host=""sci-vm-01"">}}
    touch ~/.config/dask/gateway.yaml
    {{</command>}}
3. Change the permissions on the file so that only you can read it
    {{<command user=""user"" host=""sci-vm-01"">}}
    chmod 600 ~/.config/dask/gateway.yaml
    {{</command>}}
4. Head to the {{< link ""https://notebooks.jasmin.ac.uk/hub/token"" >}}API token generator page{{</link>}}, put a note in the box to remind yourself what this token is for, press the **big orange button**, then {{<mark>}}copy{{</mark>}} the {{<mark>}}token{{</mark>}}.
5. Paste the following snippet into `~/.config/dask/gateway.yaml`, replace the entry on the final line with the API token you just created.
    ```yaml
    gateway:
      address: https://dask-gateway.jasmin.ac.uk
      auth:
        type: jupyterhub
        kwargs:
          api_token: replaceWithYourSecretAPIToken
    ```
6. You're done. You can now use `dask-gateway` from the command line.
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#setup,1146,129
Access the Dask dashboard,"To get the link to your Dask dashboard, run the following:
print(client.dashboard_link)
Currently the Dask dashboard is not accessible from a browser outside the JASMIN firewall. If your browser fails to load the dashboard link returned,
please use our [graphical desktop service]({{% ref ""graphical-linux-desktop-access-using-nx"" %}}) to run a Firefox browser inside the firewall to view your dashboard.
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#access-the-dask-dashboard,405,57
Use a custom Python environment,"By default the JASMIN Notebooks service and Dask Gateway use the latest version of the `jaspy` software environment. However, often users would like to use their own software environments.
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#use-a-custom-python-environment,189,29
Understanding the problem,"When Dask Gateway creates a dask cluster for a user, it runs a setup command to activate a conda environment or python `venv`.
To have Dask use your packages, you need to create a custom environment which you can pass to `dask-gateway` to activate.
However, for technical reasons, it is not currently possible to use the same virtual environment in both the notebook service and on JASMIN. So you will need to make two environments, one for your notebook to use and one for Dask to use.
{{<alert type=""info"">}}
It is VERY important that these environments have the same packages installed in them, and that the packages are exactly the same version in both environments.
If you do not keep packages and versions in-sync you can expect many confusing errors.
{{</alert>}}
If you use a self-contained conda environment this is not a problem, and you can use this as a kernel in the notebooks service and on the `sci` machines. You can skip to {{<link ""#putting-it-all-together"">}}Putting it all together{{</link>}} below.
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#understanding-the-problem,1020,170
Creating a virtual environment for Dask,"- Login to one of the JASMIN `sci` machines.
- Activate `jaspy`
{{<command>}}
module load jaspy
{{</command>}}
- Create your environment in the normal way
{{<command>}}
python -m venv name-of-environment
{{</command>}}
- Activate the environment
{{<command>}}
source name-of-environment/bin/activate
{{</command>}}
- Install dask and dask gateway and dependencies: without this step your environment will not work with dask.
{{<command>}}
pip install dask-gateway dask lz4
{{</command>}}
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#creating-a-virtual-environment-for-dask,488,64
Creating a virtual environment for the notebooks service,"- Follow the instructions [here]({{% ref ""creating-a-virtual-environment-in-the-notebooks-service"" %}}) to create a virtual environment.
- Install Dask and Dask Gateway and dependencies: without this step your environment will not work with Dask.
{{<command>}}
pip install dask-gateway dask lz4
{{</command>}}
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#creating-a-virtual-environment-for-the-notebooks-service,310,38
Putting it all together,"- Set your notebook virtual environment as the kernel for the notebook in question as shown in the instructions linked above.
- Set `options.worker_setup` to a command which will activate your Dask virtual environment. For example
options.worker_setup = ""source /home/users/example/name-of-environment/bin/activate""
- If you have an existing Dask cluster, close it and ensure all LOTUS jobs are stopped before recreating it using the new environment.
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#putting-it-all-together,451,64
Code Examples,"Examples of code and notebooks which can be used to test the JASMIN Dask Gateway service are
{{< link ""https://github.com/cedadev/jasmin-daskgateway/tree/main/examples"" >}}available on GitHub{{</link>}}.
",https://help.jasmin.ac.uk/docs/interactive-computing/dask-gateway#code-examples,204,23
Project-specific servers,"{{<alert type=""danger"">}}
**Deprecated feature** documented here to provide information about how to access existing instances.
Project-specific servers are no longer provided on request, as most use cases can be satisfied by 
a tenancy in the JASMIN cloud.
{{</alert>}}
This article introduces the project specific servers. It covers:
",https://help.jasmin.ac.uk/docs/interactive-computing/project-specific-servers,336,47
What is a project-specific server?,"Previously, projects could be provided with dedicated servers
built with specific software required by the project, or where
access needs to be restricted to members of a project or institution.
",https://help.jasmin.ac.uk/docs/interactive-computing/project-specific-servers#what-is-a-project-specific-server?,195,30
Get Access,"On the JASMIN accounts portal, you can apply for access to an existing Project
VM.
Step 1: Select Project VMs from the Discover Services menu on the left
{{<image src=""img/docs/project-specific-servers/file-sXxKyvM4tC.png"" caption=""discover services menu"">}}
Step 2: You can either browse the page listing or do a search on the Project
VMs. For example here using CEDA as a keyword search and to apply for access
to CEDA_scisup, click apply or look into More information
{{<image src=""img/docs/project-specific-servers/file-sXxKyvM4tC.png"" caption=""locate relevant server"">}}
{{<image src=""img/docs/project-specific-servers/file-1VcK9TxAb7.png"" caption=""apply for access"">}}
{{<image src=""img/docs/project-specific-servers/file-6pFYZos6OQ.png"" caption=""provide supporting information and submit request"">}}
{{<image src=""img/docs/project-specific-servers/file-bfV2a3mLsK.png"" caption=""request pending"">}}
",https://help.jasmin.ac.uk/docs/interactive-computing/project-specific-servers#get-access,905,95
Transfer servers,"This article lists the JASMIN data transfer servers and provides links to how
they can be used.
Please see further articles in the [Data Transfer category]({{% ref ""data-transfer-overview"" %}}) for details on managing your data transfers.
The standard transfer servers provide a basic and functional service for
moving small amounts of data over relatively short distances. However, the
high-performance data transfer servers shown above are also available for
those with particular requirements.
{{<alert type=""info"" >}}
Please make sure you use the dedicated transfer servers and not the scientific
analysis or login servers for any significant data transfers. The transfer servers have been configured to achieve the best transfer rates and will perform significantly better than other servers on jasmin, while maintaining the performance of analysis servers for interactive use by other users.
{{</alert>}}
JASMIN provides specific servers for managing data transfers. These are:
",https://help.jasmin.ac.uk/docs/interactive-computing/transfer-servers,984,143
`xfer` servers,"Server  |  Purpose  |  Access requirements  |  Further information  
---|---|---|---  
`xfer-vm-01.jasmin.ac.uk` |  Virtual machine for general purpose data transfers.  |`jasmin-login` | Accessible from any network
`xfer-vm-02.jasmin.ac.uk` |  Virtual machine for general purpose data transfers.  |`jasmin-login` | Accessible from any network
`xfer-vm-03.jasmin.ac.uk` |  Virtual machine for general purpose data transfers.<br>Has `cron` for scheduled transfers, see notes.  | `jasmin-login`| Accessible from any network.
{.table .table-striped}
Notes:
- Similar config on all 3 (no domain or reverse DNS restrictions now)
- Same advice applies re. **SSH client version**, see [login nodes]({{% ref ""#login-nodes"" %}})
- If using cron on `xfer-vm-03`, you must use [crontamer]({{% ref ""using-cron/#crontamer"" %}})
- Throttle any automated transfers to avoid many SSH connections in quick succession, otherwise you may get blocked.
- Consider using [Globus]({{% ref ""#globus-data-transfer-service"" %}}) for any data transfer in or out of JASMIN
- A new software collection `jasmin-xfer` has now been added to these servers, providing these tools:
emacs-nox
ftp
lftp
parallel
python3-requests
python3.11
python3.11-requests
rclone
rsync
s3cmd
screen
xterm
",https://help.jasmin.ac.uk/docs/interactive-computing/transfer-servers#`xfer`-servers,1254,166
`hpxfer` servers,"Server  |  Purpose  |  Access requirements  |  Further information  
---|---|---|---  
`hpxfer3.jasmin.ac.uk` | Physical machine for higher-performance data transfers | `jasmin-login` access role | 
`hpxfer4.jasmin.ac.uk` | Physical machine for higher-performance data transfers | `jasmin-login` access role | 
{.table .table-striped .w-auto}
Notes:
- Tested with `sshftp` (GridFTP over SSH) from ARCHER2
- Same applies re. **SSH client version**, see [login nodes]({{% ref ""#login-nodes"" %}})
- The software collection `jasmin-xfer` available as per [xfer servers, above]({{% ref ""#xfer-servers"" %}})
- The `hpxfer` role, and the supplying of client IP addresses is no longer required for these **new** servers.
",https://help.jasmin.ac.uk/docs/interactive-computing/transfer-servers#`hpxfer`-servers,713,95
Avoid getting locked out with SSH transfers,"Access rules in place to enable secure SSH connections from any network are
**sensitive to repeated, rapid connection attempts, which can result in being ""locked out""**.
This may happen when attempting the transfer of several small files in quick succession with separate SSH
connections for each.
All the above servers behave the same way in this respect, so please be aware of this when
managing your transfers.
{{<alert type=""danger"" >}}
Users are **not permitted to execute commands which require
administrative privileges.** This applies to all hosts in the managed part of
JASMIN where users have SSH login access (for example `login`, `nx-login`,
`sci`, `xfer` and `hpxfer` machines). In other words, the **use of`su` or
`sudo` is not permitted**. Please be careful when typing commands,
particularly if you have multiple terminal windows open on your own computer,
that you do not accidentally attempt `sudo`on a JASMIN machine: expect some
follow-up from the JASMIN team if you do!
{{</alert>}}
",https://help.jasmin.ac.uk/docs/interactive-computing/transfer-servers#avoid-getting-locked-out-with-ssh-transfers,1004,157
GridFTP server,"For users of the (now legacy), certificate-based GridFTP only (specifically, `gsiftp://` using the `globus-url-copy` client), there is a new server:
Server  |  Purpose  |  Access requirements  |  Further information  
---|---|---|---  
`gridftp1.jasmin.ac.uk` | Physical machine for legacy GridFTP transfers.  | No SSH login access.<br>Apply for additional access role [here](https://accounts.jasmin.ac.uk/services/additional_services/hpxfer). | Old machine, continue using for now
`gridftp2.jasmin.ac.uk` | Physical machine for legacy GridFTP transfers. | No SSH login access.<br>`jasmin-login` access role | New machine, NOT YET AVAILABLE
{.table .table-striped}
**Users of this service are now strongly advised to migrate to using [Globus]({{% ref ""globus-transfers-with-jasmin"" %}}) instead.**
",https://help.jasmin.ac.uk/docs/interactive-computing/transfer-servers#gridftp-server,798,95
JASMIN Notebooks Service,"The JASMIN Notebooks Service provides access to
{{<link ""https://jupyter.org/"">}}Jupyter Notebooks{{</link>}} in a web browser.
",https://help.jasmin.ac.uk/docs/interactive-computing/jasmin-notebooks-service,128,14
What is a Jupyter Notebook?,"A Jupyter Notebook is an interactive document containing live code and
visualisations that can be viewed and modified in a web browser. These
documents can be shared, often using GitHub, and many projects distribute
example code as Jupyter notebooks. Users interact with their notebooks using
the open-source Jupyter Notebook server application.
Jupyter has support for many languages including Python, R, Scala and Julia,
which are implemented by plugins known as ""kernels"". The JASMIN Notebook
Service currently provides one kernel - Python 3.10 with the latest
[Jaspy software environment]({{% ref ""jaspy-envs"" %}}) installed. This environment is
active by default, so there is no need for the `module` commands described in
the linked article. You can also install and use your own Python environments
as explained below.
The JASMIN Notebook Service uses
{{<link ""https://jupyter.org/hub"">}}JupyterHub{{</link>}} to manage multiple Jupyter Notebook
servers. After authenticating with a JASMIN account (with the correct access),
a user gets access to **their own notebook server**. The notebook server runs as
the authenticated user, and the user can access their home directory, Group
Workspaces and CEDA Archive data as they would from a scientific analysis
server.
",https://help.jasmin.ac.uk/docs/interactive-computing/jasmin-notebooks-service#what-is-a-jupyter-notebook?,1271,187
Getting access to the JASMIN Notebook Service,"In order to access the JASMIN Notebook service, first, follow the steps in
[Getting started with JASMIN]({{% ref ""get-started-with-jasmin"" %}}) to get a
JASMIN account and the `jasmin-login` service.
{{<alert type=""info"">}}
From 16/6/2021, access to the JASMIN Jupyter Notebook service
is controlled simply by having a valid `jasmin-login` grant.
[2-step verification](http://notebooks.jasmin.ac.uk/) is still required as shown
below, but it is no longer necessary to apply for and be granted the
additional `jupyter-notebooks` role, which is now deprecated.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/interactive-computing/jasmin-notebooks-service#getting-access-to-the-jasmin-notebook-service,572,78
Using the JASMIN Notebook Service,"To use the JASMIN Notebook Service, navigate to
<https://notebooks.jasmin.ac.uk>. This will redirect you to the JASMIN
Accounts Portal, where you should sign in with your JASMIN Account. If you
have previously accessed the notebook service from the computer you are using,
this may happen automatically.
The first time you access the JASMIN Notebook Service from a new computer or
browser, you will be asked to verify your email address after signing in. To
do this, make sure the ""Email"" method is selected and click ""Send me a code"":
{{<image src=""img/docs/jasmin-notebooks-service/file-9fa76fPQdE.png"" caption=""Sign in with 2-factor authentication"">}}
This will result in an email being sent to your registered email address
containing a six-digit verification code. Enter this code and press ""Verify
code"". Each code lasts between 15 and 30 minutes.
Upon successfully signing in and verifying your email if required, you will be
redirected back to the JASMIN Notebook Service. The service will then create a
notebook server for you to use, and you will see a loading page:
{{<image src=""img/docs/jasmin-notebooks-service/file-nkDFYlwSg1.png"" caption=""Loading page"">}}
After a few seconds, or in some rare cases a minute or two, you will be taken
to the JupyterLab interface:
{{<image src=""img/docs/jasmin-notebooks-service/file-fhTnvJz3xx.png"" caption=""JupyterLab interface"">}}
The folder shown in the left-hand panel will be your home directory on JASMIN,
exactly as if you had logged in to a scientific analysis server. Any changes
made in JupyterLab will be immediately reflected on the scientific analysis
servers and LOTUS, and vice-versa. You can then launch, edit and run Python
notebooks using the JupyterLab interface. Notebooks can access files from the
CEDA Archive, and also have read-only access to group workspaces. For example,
this notebook reads a file belonging to the CCI project from the CEDA Archive
and plots the data on a map:
{{<image src=""img/docs/jasmin-notebooks-service/file-LvHEu70CM6.png"" caption=""Example notebook"">}}
A full discussion of the power of the JupyterLab interface is beyond the scope
of this documentation, but the
{{<link ""https://jupyterlab.readthedocs.io/en/stable/"">}}JupyterLab documentation{{</link>}} is excellent, and
there are many tutorials available on the internet.
",https://help.jasmin.ac.uk/docs/interactive-computing/jasmin-notebooks-service#using-the-jasmin-notebook-service,2326,329
Intended usage and limitations,"The JupyterLab environment provided by the JASMIN Notebooks Service is
powerful, but it has some limitations that reflect the type of usage that we
want to encourage.
The JASMIN Notebooks Service is primarily intended for interactively producing visualisations
of existing data, not for processing vast amounts of data. As such, the
resources made available to each notebook server are limited.
For larger processing tasks with Notebooks, users
should consider using the {{<link ""https://github.com/cedadev/jasmin-daskgateway"">}}JASMIN Dask Gateway service{{</link>}}, which provides an easy way for processing managed from within a Jupyter Notebook to be scaled out to multiple LOTUS jobs in parallel.
Alternatively, use the [LOTUS batch processing cluster]({{% ref ""lotus-overview"" %}})
separately, before using the notebooks service to visualise the output.
Doing this means that the bulk of the resource usage will be shared by the LOTUS cluster rather than the Notebooks service itself.
{{<alert type=""info"">}}
Although it was previously the case that Group Workspaces were only available in Notebooks read-only, this is no longer the case (as of 2023), so you should have full read-write access to any group workspace volume.
{{</alert>}}
",https://help.jasmin.ac.uk/docs/interactive-computing/jasmin-notebooks-service#intended-usage-and-limitations,1245,181
Common issues and questions,,https://help.jasmin.ac.uk/docs/interactive-computing/jasmin-notebooks-service#common-issues-and-questions,0,0
"I get ""403 forbidden"" when I try to access the JASMIN Notebook Service","This error occurs when you do not have the correct permissions to access the
JASMIN Notebook service. Please ensure you have been granted the `jasmin-login` and allow some time for the changes to propagate through the system.
","https://help.jasmin.ac.uk/docs/interactive-computing/jasmin-notebooks-service#i-get-""403-forbidden""-when-i-try-to-access-the-jasmin-notebook-service",226,37
Which software environment is used by the Notebook Service?,"The JASMIN Notebook Service currently uses the **default Jaspy environment**
listed on the [Jaspy page]({{% ref ""jaspy-envs"" %}}).
",https://help.jasmin.ac.uk/docs/interactive-computing/jasmin-notebooks-service#which-software-environment-is-used-by-the-notebook-service?,131,18
Can I install additional packages?,"The recommended way to do this is to
[create your own virtual environment]({{% ref ""creating-a-virtual-environment-in-the-notebooks-service"" %}})
in the notebooks service and install additional packages into that.
You can make that virtual environment persist as a kernel to use again next time you
use the Notebooks service.
If you believe that the package is more widely applicable, you can request an
update to Jaspy.
",https://help.jasmin.ac.uk/docs/interactive-computing/jasmin-notebooks-service#can-i-install-additional-packages?,421,63
Can I use a different software environment?,"Yes, the article linked above also describes how to use Conda to create your
own custom environment.
",https://help.jasmin.ac.uk/docs/interactive-computing/jasmin-notebooks-service#can-i-use-a-different-software-environment?,101,17
"I get ""503 service unavailable"" when I try to access the JASMIN Notebook","Service
This error occurs when your notebook server is unable to start. By far the
most common cause of this error is when you are over your quota in your home
directory. As part of starting up, Jupyter needs to create some small files in
your home directory in order to track state. When it is unable to do this
because you are over your quota, you will see the ""503 service unavailable""
error.
If you see this error, try connecting to JASMIN using SSH and checking your
home directory usage. After clearing some space in your home directory, your
notebook server should be able to start again.
","https://help.jasmin.ac.uk/docs/interactive-computing/jasmin-notebooks-service#i-get-""503-service-unavailable""-when-i-try-to-access-the-jasmin-notebook",596,108
I get the following message when my notebook is queued for spawning,"{{<image src=""img/docs/jasmin-notebooks-service/file-NHYStoV3nD.png"" caption=""Error message"">}}
The message above indicates that the Notebook service
is oversubscribed -busy- and there are no resources available to start your
Notebook server. Please try again later!
Please use the scientific analysis server and LOTUS for processing and Jupyter
Notebook for lighter visualisation.
",https://help.jasmin.ac.uk/docs/interactive-computing/jasmin-notebooks-service#i-get-the-following-message-when-my-notebook-is-queued-for-spawning,382,46
Example Notebooks,"In support of the JASMIN Notebook Service, we have developed a GitHub
repository with a collection of Notebooks that demonstrate some of the
important features of the service. The following provides a general
introduction:
<https://github.com/cedadev/ceda-notebooks/blob/master/notebooks/training/intro/notebook-tour.ipynb>
Other Notebooks can be found within the repository, under:
<https://github.com/cedadev/ceda-notebooks/tree/master/notebooks>
",https://help.jasmin.ac.uk/docs/interactive-computing/jasmin-notebooks-service#example-notebooks,449,45
Webinar / video tutorial,"A {{<link ""https://www.ceda.ac.uk/events/past/jasmin-notebook-service-webinar/"">}}CEDA webinar on 16th June 2020{{</link>}}
demonstrated how to use the service. A recording of the event is available:
{{< youtube nle9teGLAb0 >}}
",https://help.jasmin.ac.uk/docs/interactive-computing/jasmin-notebooks-service#webinar-/-video-tutorial,228,25
Further info,"Here are a set of links to learn more about Jupyter Notebooks and the JASMIN
Notebook Service:
- {{<link ""https://notebooks.jasmin.ac.uk/"">}}JASMIN Notebook Service{{</link>}}
- {{<link ""https://jupyter.org/"">}}Intro to Jupyter Lab{{</link>}}
- {{<link ""https://jupyter.org/try"">}}Try a Jupyter Lab Notebook in your browser{{</link>}} (this link does not use the JASMIN Notebook Service)
- {{<link ""https://github.com/cedadev/ceda-notebooks/tree/master/notebooks/training/intro"">}}Tour of the JASMIN Notebooks Service{{</link>}} \- set of notebooks highlighting different features
- {{<link ""https://www.ceda.ac.uk/events/past/jasmin-notebook-service-webinar/"">}}Intro to the JASMIN Notebooks Service{{</link>}} \- video of a webinar from June 2020
- {{<link ""https://github.com/cedadev/ceda-notebooks/blob/master/notebooks/training/rerunnable-virtualenv-maker.ipynb"">}}Instructions to create a Python virtual environment{{</link>}} \- example notebook
- {{<link ""https://github.com/cedadev/ceda-notebooks/blob/master/notebooks/docs/add_conda_envs.ipynb"">}}Instructions to create a Conda environment for use with the Notebook Service{{</link>}} \- example notebook
",https://help.jasmin.ac.uk/docs/interactive-computing/jasmin-notebooks-service#further-info,1165,107
Intro,"The scientific analysis (sci) servers are provided for general purpose use by all users with the `jasmin-login` access role.
The sci servers are not directly accessible outside the firewall of the STFC network (JASMIN's host organisation) so most* users will need to access them via a [login server]({{% ref ""login-servers"" %}}).
Users inside the STFC network (e.g. STFC staff on site, or remotely using the STFC VPN) should be able to access them directly.
",https://help.jasmin.ac.uk/docs/interactive-computing/sci-servers#intro,458,74
Available sci servers,"The following sci servers are available:
Server name  |  Virtual/Physical |  Processor model  |  CPU Cores  |  RAM (GB)  | /tmp max per user | /tmp size
---|---|---|---|---|---|---
`sci-vm-01` | virtual | Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz | 8 | 32 GB | 512 MB | 80 GB virtual disk
`sci-vm-02` | virtual | Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz | 8 | 32 GB | 512 MB | 80 GB virtual disk
`sci-vm-03` | virtual | Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz | 8 | 32 GB | 512 MB | 80 GB virtual disk
`sci-vm-04` | virtual | Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz | 8 | 32 GB | 512 MB | 80 GB virtual disk
`sci-vm-05` | virtual | Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz | 8 | 32 GB | 512 MB | 80 GB virtual disk
`sci-vm-06` | virtual | Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz | 8 | 32 GB | 512 MB | 80 GB virtual disk
`sci-ph-01` | physical | AMD EPYC 74F3 | 48 | 2 TB | 20 GB | 2 x 446 GB SATA SSD
`sci-ph-02` | physical | AMD EPYC 74F3 | 48 | 2 TB | 20 GB | 2 x 446 GB SATA SSD
{.table .table-striped}
",https://help.jasmin.ac.uk/docs/interactive-computing/sci-servers#available-sci-servers,1009,218
Notes,,https://help.jasmin.ac.uk/docs/interactive-computing/sci-servers#notes,0,0
1\. Access,"Sci servers are not exposed outside the STFC network, so from external locations you need to access
them via a login server.
For users within the STFC network, there is no longer any reverse DNS restriction, so all
should be accessible directly within that network without need to go via a login node.
See [connecting to a sci server via a login server]({{% ref ""login-servers#connecting-to-a-sci-server-via-a-login-server"" %}})
for some alternative methods of connecting.
",https://help.jasmin.ac.uk/docs/interactive-computing/sci-servers#1\.-access,473,72
2\. Physical servers,"Physical servers are actually re-configured nodes within the LOTUS cluster and as such have different a network
configuration from the virtual `sci` servers, with limited outward connectivity.
Outbound internet access (via NAT) is only
for HTTP(S), so outbound SSH **will not work (to hosts outside of
JASMIN) on these machines**. If you try to `git pull/clone` from external repositories e.g. Github, the operation will timeout with error `fatal: Could not read from remote repository`. The solution in this case is to access `git pull/clone` over **HTTPS** instead (check the repo for alternative access details).
",https://help.jasmin.ac.uk/docs/interactive-computing/sci-servers#2\.-physical-servers,616,94
3. /tmp on VMs,"The local `/tmp` of the virtual sci servers is not available (N/A) for users
as this is used by the VM itself. It also provides no performance advantage as it is not local to the server.
",https://help.jasmin.ac.uk/docs/interactive-computing/sci-servers#3.-/tmp-on-vms,187,36
4. Arbiter,"A monitoring utility ""**Arbiter**"" is implemented across
all sci machines to control CPU and memory usage. This utility
records the activity on the node, automatically sets limits on the resources
available to each user. Users' processes are thus capped from
using excessive resources, and can be slowed or have memory reduced in response to repeated violations.
",https://help.jasmin.ac.uk/docs/interactive-computing/sci-servers#4.-arbiter,363,56
5. Privileges,"Users are **not permitted to execute commands which require
administrative privileges.** This applies to all hosts in the managed part of
JASMIN where users have SSH login access (for example `login`, `nx-login`,
`sci`, `xfer` and `hpxfer` machines). In other words, the **use of `su` or
`sudo` is not permitted**. Please be careful when typing commands,
particularly if you have multiple terminal windows open on your own computer,
that you do not accidentally attempt `sudo` on a JASMIN machine: expect some
follow-up from the JASMIN team if you do!
See also [software installed]({{% ref ""#software-installed"" %}}), below.
",https://help.jasmin.ac.uk/docs/interactive-computing/sci-servers#5.-privileges,625,96
Purpose,"Scientific analysis servers are designed for interactive and ad-hoc analysis
of data in group workspaces, the CEDA archive, and users' home directories.
**For long-running and resource-intensive jobs, users are required to use
the LOTUS cluster** which offers better I/O performance, parallelism, and
fair-share scheduling.
The following guidelines should be considered when using the scientific analysis
servers:
- Check available resources before your process starts and choose a sci server that is suitable (check average load in the list displayed at the login screen on the login servers, or by using the Linux monitoring commands: `top`, or `free -h` )
- Execution/processing time should be less than 1 hour
- Serial jobs only
- High memory jobs should be executed on the physical servers which have more memory (labelled P in [above table](#available-sci-servers)).
- Monitor your process on a sci server using `top` or `ps` Linux commands
- Report if there is a user's process affecting the performance of a scientific server
",https://help.jasmin.ac.uk/docs/interactive-computing/sci-servers#purpose,1034,161
Software installed,"Each sci server has the following features:
- Rocky 9 operating system with development tools.
- Software packages that make up the [JASMIN Analysis Platform]({{% ref ""software-overview"" %}}) are all installed - providing commonly-used open-source analysis tools. These packages include NCO, CDO, Python (with netCDF4, matplotlib, numpy etc.,) and R.
- Access to proprietary tools, e.g. IDL and Intel Fortran, through the `module` system.
- Editors: `emacs`, `vim`, `nedit`, `geany` and `nano` are installed.
- For a more richly-featured editor or Integrated Development Environment (IDE), consider using
a remote editor locally, for example {{<link href=""https://code.visualstudio.com/docs/remote/ssh"">}}VSCode{{</link>}} or
{{<link ""https://www.jetbrains.com/pycharm/"">}}PyCharm{{</link>}}: these can be installed and customised on your own machine
rather than needing central installation and management on JASMIN. Watch this space for
further advice about how to configure and use VSCode in this way.
- Ability to run graphical applications: use the
[NX graphical desktop service]({{% ref ""graphical-linux-desktop-access-using-nx"" %}}) for best performance.
See [[Note 4]](#5-privileges) above about privileges: you can only install software for yourself if it can be done with user-level privileges.
",https://help.jasmin.ac.uk/docs/interactive-computing/sci-servers#software-installed,1305,172
Access to storage,"Each sci server has:
- Access to the LOTUS `/work/scratch-pw*` and `/work/scratch-nopw2` volumes.
- Read access to the [CEDA Archive]({{% ref ""ceda-archive"" %}}) according to permissions on your CEDA account.
- Read/Write access to Group Workspaces according to membership.
",https://help.jasmin.ac.uk/docs/interactive-computing/sci-servers#access-to-storage,274,39
Introduction,"{{<link ""https://code.visualstudio.com/docs/remote/ssh"" >}}Visual Studio Code{{</link>}} is a richly-featured editor and Integrated Development Environment (IDE)
which has remote access and other useful features that can be used with JASMIN.
This article is in response to requests from users about how use VSCode with JASMIN and is not a product endorsement. Other IDEs with similar features are also available, for example {{<link ""https://www.jetbrains.com/pycharm/"" >}}PyCharm{{</link>}}.
The following demonstrates how to use VSCode to connect to JASMIN, and mentions some of its features for further reading.
",https://help.jasmin.ac.uk/docs/interactive-computing/access-from-vscode#introduction,615,82
Obtaining VSCode,"Follow the ""setup"" link from the page linked above
to obtain VSCode for your platform: this should be your **local** machine, **not** JASMIN.
{{<alert type=""info"">}}
**There should be no need for you to install VSCode on JASMIN, and we ask you not to.**
We don't provide it centrally, because the multitude of different extensions and configurations 
would be too difficult to manage in a multi-user environment, so it makes more sense if
you install your own local instance of it, which you can configure as you like: the remote capabilities
of VSCode make a central installation unnecessary.
{{</alert>}}
Read about the following extensions and decide which you think you need (you can always ""upgrade"" later)
- {{<link ""https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh"">}}Remote-SSH extension{{</link>}}
  - enables you connect & remotely edit files
- {{<link ""https://aka.ms/vscode-remote/download/extension"" >}}Remote Development extension pack{{</link>}}
  - this includes the Remote-SSH extension as well as others which enable a raft of other features.
You should read about those extensions first, at the links above, but actually install the one
you choose by using the Extensions menu within VSCode once you've installed the application.
You will also almost certainly want the following for working with Python locally and on JASMIN.
- {{<link ""https://marketplace.visualstudio.com/items?itemName=ms-python.python"">}}Python{{</link>}}  (includes Pylance and Python Debugger).
There are many, many other extensions to add, but you now have the most relevant ones to get you started.
**These extensions are best installed locally, before connecting to any remote hosts.**
",https://help.jasmin.ac.uk/docs/interactive-computing/access-from-vscode#obtaining-vscode,1713,237
How to connect?,"You will need to set up at least one SSH connection profile to a remote host on JASMIN: we'll cover **which** host(s), shortly.
VSCode has a tool to help you set this up, and creates entries in your `~/.ssh/config` file for the SSH client that you're using.
1. First consider your SSH client: VSCode will connect using the SSH client of your operating system: there isn't one built into
VSCode itself. See [presenting your ssh key]({{% ref ""present-ssh-key"" %}}) for details of the ""agent"" method: this is more convenient
as this is persistent across sessions and you won't be asked for your passphrase on each connection. If that doesn't work for you,
note the extra configuration below which can be added to specify the location of your key instead.
2. Next, consider which remote host(s) on JASMIN you want to connect to:
- `login` servers are available from any location, but don't have any software or storage mounted other than your home directory
- `sci` servers are probably where you want to work, but aren't directly accessible from outside of the STFC network.
- `xfer` servers might be a good choice if it's just editing you're likely to be doing, since they're directly accessible from anywhere
and have all filesystems mounted (except scratch). But they're not for doing processing.
So the ideal setup might be 2 profiles as follows:
1. `sci` server, accessed via a login server
1. `xfer` server, accessed directly
If we use the tool provided by VSCode to create these, we can customise them further.
The following video demonstrates these steps, and the initial setup of 2 connection profiles, on Windows. But the interface
is almost identical on Mac and Linux.
{{< video id=""PAwSWtHwhSQ"">}}
Notes:
- the sequences have been shortened slightly while installing extensions and initiating the connection.
- the demo shown assumes the SSH key is already loaded in an ssh-agent. To specify the key location instead, see below.
- the `-A` option is only needed IF you will be making onward SSH connections from the remote host ...this can be omitted if not
",https://help.jasmin.ac.uk/docs/interactive-computing/access-from-vscode#how-to-connect?,2067,349
Essential steps,"- Install the extensions
- Create a connection profile
  - start by entering the one-liner command you would use to connect to a sci server via a login server, i.e.
    ```bash
    ssh -A username@sci-vm-01.jasmin.ac.uk -J username@login-01.jasmin.ac.uk
    ```
  - this creates an entry in `~/.ssh/config` (it's recommended to choose that location)
- This entry can be customised, by editing that file
  - note that the `Host` is a ""friendly name"" which you can define, whereas `HostName` is the actual full name of the host including domain, e.g. `sci-vm-01.jasmin.ac.uk`
- Add other entries as needed
  - in this case, we added a second profile for `xfer-vm-01` which, being an `xfer` server, is directly accessible so does not need the `ProxyJump`
- Save the file `~/.ssh/config` and restart VSCode
- The new profiles are available next time you open it
- Connect to one of the remote hosts you just made
- Open a terminal on that host
- ...and now you're able to work on JASMIN
",https://help.jasmin.ac.uk/docs/interactive-computing/access-from-vscode#essential-steps,983,164
Specifying the key location,"Alternative method if you can't get the ""agent"" method to work (but means that you may be prompted for the key passphrase each time you connect):
- edit `~/.ssh/config` and add the line with `IdentityFile` as shown:
Host sci-vm-01-via-login-01
  Hostname sci-vm-01.jasmin.ac.uk
  User username
  ProxyJump username@login-01.jasmin.ac.uk
  ForwardAgent yes
  IdentityFile ~/.ssh/id_rsa_jasmin
",https://help.jasmin.ac.uk/docs/interactive-computing/access-from-vscode#specifying-the-key-location,392,49
Further tips,"- Editing large files may be slow compared to editing them in place on the remote server.
- If you're using VSCode locally **without** the remote host connection described above, you can still open a Terminal to use SSH commands to connect to remote hosts, but this will be without the integration that the full remote host connection provides (e.g. won't display your remote directories and files in the explorer bar).
- There's a lot more that you can do with VSCode locally (even without the remote connection that this article describes).
For example:
- syntax colouring and code auto-completion for a huge range of languages
- {{<link ""https://code.visualstudio.com/docs/sourcecontrol/overview"" >}}Git integration{{</link>}}, note you need `git` installed locally(see {{<link ""https://git-scm.com/downloads/win"" >}}git{{</link>}} for Windows)
- {{<link ""https://code.visualstudio.com/docs/datascience/jupyter-notebooks"" >}}Working with Jupyter notebooks{{</link>}} locally in VSCode
and many other features beyond the scope of this article.
This makes an IDE such as VSCode a good choice to install locally, rather than using the (fairly basic) editors available on JASMIN.
",https://help.jasmin.ac.uk/docs/interactive-computing/access-from-vscode#further-tips,1179,165
Troubleshooting,,https://help.jasmin.ac.uk/docs/interactive-computing/access-from-vscode#troubleshooting,0,0
Permission denied,"If you get `permission denied` when connecting, you should troubleshoot this as you would any other SSH connection.
Open a terminal within VSCode and check that your SSH key is being presented correctly. If it's not listed when you do
ssh-add -l
then:
- go back to [presenting your ssh key]({{% ref ""present-ssh-key"" %}}) and check your setup
- alternatively, [specify the location of your key]({{% ref ""#specifying-the-key-location"" %}}) as detailed above
",https://help.jasmin.ac.uk/docs/interactive-computing/access-from-vscode#permission-denied,457,72
Can't connect to a remote host that previously worked,"VSCode starts a small server process on the remote host when you connect. Occasionally this can get
stuck. One method of fixing this is to:
- quit VSCode
- from a separate SSH client terminal, log in to the **same** host
- identify any `.vscode-server` process running on that host, e.g.
ps -ef | grep $USER | grep vscode
- note the process ID (PID) number (1st numerical column) and kill that process
kill <PID>
- recursively delete the `~/.vscode-server` directory which VSCode created in your JASMIN home directory
rm -rf ~/.vscode-server
- Retry connecting to the remote host
- If that doesn't work, try rebooting your own machine, then repeating the above steps.
",https://help.jasmin.ac.uk/docs/interactive-computing/access-from-vscode#can't-connect-to-a-remote-host-that-previously-worked,668,114
Introduction,"- This service provides a graphical Linux desktop on JASMIN, ideal
for use with graphics-heavy tasks like interactive work with large images.
- The desktop environment includes a Firefox web browser which can be used to access
internal-only web resources.
Using graphical applications over a wide-area network can be very slow, and is
not recommended or supported on JASMIN. This service provides a better alternative with
graphical desktop **within** the JASMIN environment itself, rather than on the user's local machine.
A small client application, available for you to install on your local machine,
enables you to connect to specific servers within JASMIN. Graphics are then relayed to the client
application in a more efficient form, resulting in much better performance particularly if you need
to interact with what's being displayed.
The service provides an improved user experience and is strongly
recommended over standard X11 graphics.
The following servers have the NX service available and can be
used as described below. These now have **identical configuration**, so you can use any one of them from any network location.
name | notes
--- | ---
`nx1.jasmin.ac.uk` |
`nx2.jasmin.ac.uk` |
`nx3.jasmin.ac.uk` |
`nx4.jasmin.ac.uk` | (not yet converted to Rocky 9)
{.table .table-striped .w-auto}
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#introduction,1308,200
Notes,"- The `nx*` servers should only be used with Nomachine Enterprise Client as described below, other than for testing your connection, as this preserves system resources for their intended purpose.
- Although the graphical desktop session should persist when you close the client (unless you specifically log out), you should not rely on this feature, so please don't report this as a problem: occasionally machines run out of resources and sessions get killed. Keeping sessions open consumes resources on the server even when you're not using the session, which may mean that other users can't use the service.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#notes,610,98
Installing NoMachine Enterprise Client,"Download the
{{<link ""https://www.nomachine.com/download-enterprise#NoMachine-Enterprise-Client"" >}}appropriate version of the NoMachine Enterprise Client{{</link>}}
from NoMachine.
That page contains links to several different products: 
**The only one you need to install is NoMachine Enterprise Client.**
Versions are available for Windows, Mac and Linux. You may need privileges on
your local machine in order to install the software so you may need to ask for
help from your local IT helpdesk.
Note that **Nomachine Enterprise Client** is a different application to the
""Nomachine Enterprise Desktop"" or ""Nomachine"" available from the more publicised download
links on the NoMachine website or other applications in the NoMachine suite:
the desktop edition contains additional components to enable remote access to
your **own** (local) machine from a remote location: perhaps convenient
but not what we are trying to enable for you here.
The **NoMachine Enterprise Client** is a cut-down client to which connects to a remote
server: in your case, the server is at the JASMIN end, where the desktop session will exist.
Remember to check for updates for the enterprise client to ensure you always
have the latest stable version. You can configure the application to check for
updates (and optionally apply them automatically) by going to Settings /
Updates in the menu.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#installing-nomachine-enterprise-client,1373,205
Setting up your connection,"There are 2 methods of using your SSH key which should work with JASMIN, these affect how you set up the connection:
1. Specify the location of your SSH private key
2. Using your key stored in a local ssh-agent
For a simple terminal connection to JASMIN, you would follow the instructions in [presenting your ssh key]({{% ref ""present-ssh-key/#1-specifying-the-key-location-each-time"" %}}), but the NoMachine client needs you to do it a slightly different way. The same principles apply however.
Until recently, we recommended only method (1) but with the move to Rocky 9 Linux, there is a problem with this approach for Windows 11 users, yet to be resolved by the software vendor. So we have also described a method based on (2) which, although requiring careful (one-off) configuration, should also be useful if you're planning to use other applications, like VSCode to connect to JASMIN. Using this method, Windows users no longer need to create and use a reformatted version of the SSH private key.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#setting-up-your-connection,1003,165
Method 1: specifying key location,"Videos for each platform (click the tab for your operating system):
{{< nav type=""tabs"" id=""tabs-create-key"" >}}
  {{< nav-item header=""Windows"" show=""true"" >}}
    {{< video id=""O2tzHD0iCYY"" >}}
Notes:
- This profile may not work currently for Windows 11 users
- Windows 10 users need may need to first [create a reformatted formatted version of private key]({{% ref ""#authentication-error-windows-users"" %}})
- Please try [Method 2]({{% ref ""#method-2-using-an-agent"" %}}) if if this does not work fully for you.
  {{< /nav-item >}}
  {{< nav-item header=""Mac"" >}}
    {{< video id=""tXSLXvdKHFE"" >}}
  {{< /nav-item >}}
  {{< nav-item header=""Linux"" >}}
    {{<video id=""b2-Ho1onU5I"">}}
  {{< /nav-item >}}
{{< /nav >}}
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#method-1:-specifying-key-location,722,100
Step-by-step instructions,"{{< accordion id=""accordion-create-with-key"" >}}
  {{< accordion-item header=""Step by step instructions (click to expand)"" show=""false"" >}}
  1. Open the NX client
      1. On Mac and Windows, click the NoMachine Icon
      1. On Linux, the default location for the executable once installed is `/usr/NX/bin/nxplayer`, so you may want to add this to your `$PATH`. Your desktop environment may enable you to add an icon to your desktop.
  1. In the ""Machines"" view, select ""Add""
      1. You're now in the ""Address"" tab. Type a name for this connection profile, and the full hostname, e.g. `nx1.jasmin.ac.uk`. Set the Protocol to ""SSH"", which will change the port to 22.
  1. Go to the ""Configuration"" tab.
      1. Choose **""Use key-based authentication with a key you provide""** , then click the Modify button to the right.
      1. The default is ""Use password authentication"": **don't** use this.
      1. Use the button to the right to navigate to your private key, or type the path in the box.
      1. Your private key may be in a hidden directory e.g. `~/.ssh` (see {{<link ""#cant-find-your-private-key"">}}Troubleshooting{{</link>}})
      1. For security, it is recommended NOT to ""import the private key to the connection file"" (store it in an **encrypted** password manager instead).
      1. Make sure you tick the box ""Forward Authentication"" **IMPORTANT**
  1. Go back to the ""Add connection"" dialog
      1. If all is correct, click ""Add""
  {{< /accordion-item >}}
{{</accordion>}}
Once you have created the connection profile, go to [Connecting]({{% ref ""#connecting"" %}}), below, and continue from there.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#step-by-step-instructions,1621,245
Method 2: using an agent,"The alternative profile for using an agent instead, is very similar but we need to select option **""Use key-based authentication with a SSH agent""**.
Videos for each platform:
{{< nav type=""tabs"" id=""tabs-create-agent"" >}}
  {{< nav-item header=""Windows"" show=""true"" >}}
    {{< video id=""wDZKV8lIY5M"" >}}
  {{< /nav-item >}}
  {{< nav-item header=""Mac"" >}}
    {{< video id=""wBIBtBLGE1g"" >}}
  {{< /nav-item >}}
  {{< nav-item header=""Linux"" >}}
    {{<video id=""5Yrk8XrmAAE"">}}
  {{< /nav-item >}}
{{< /nav >}}
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#method-2:-using-an-agent,513,68
Step-by-step instructions,"{{< accordion id=""accordion-create-with-agent"" >}}
  {{< accordion-item header=""Steps in more detail (click to expand)"" show=""false"" >}}
- In the ""Machines"" view, select ""Add""
- You're now in the ""Address"" tab. Type a name for this connection profile, and the full hostname, e.g. `nx1.jasmin.ac.uk`. Set the Protocol to ""SSH"", which will change the port to 22.
- Go to the ""Configuration"" tab.
- Choose **""Use key-based authentication with a SSH agent""** , then click the Modify button to the right.
- Make sure you tick the box ""Forward Authentication"" **IMPORTANT**
- Go back to the ""Add connection"" dialog
- If all is correct, click ""Add""
  {{< /accordion-item >}}
{{</accordion>}}
Once you have created the connection profile, go to [Connecting]({{% ref ""#connecting"" %}}), below, and continue from there.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#step-by-step-instructions,810,125
Connecting,,https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#connecting,0,0
Connecting with method 1 (key),"Follow the steps in the video to show how to connect to the desktop on the `nx` server, and to make the onward connection to the `sci` server:
{{< nav type=""tabs"" id=""tabs-connect-key"" >}}
  {{< nav-item header=""Windows"" show=""true"" >}}
    {{< video id=""-bxqj6jWPJk"" >}}
Notes:
- This profile may not work currently for Windows 11 users
- Windows 10 users need may need to first [create a reformatted formatted version of private key]({{% ref ""#authentication-error-windows-users"" %}})
- Please try [Method 2]({{% ref ""#method-2-using-an-agent"" %}}) if if this does not work fully for you.
  {{< /nav-item >}}
  {{< nav-item header=""Mac"" >}}
    {{< video id=""kNu4oInzEb8"" >}}
  {{< /nav-item >}}
  {{< nav-item header=""Linux"" >}}
    {{<video id=""3ndUx8JFp0U"">}}
  {{< /nav-item >}}
{{< /nav >}}
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#connecting-with-method-1-(key),798,117
Step-by-step instructions,"{{< accordion id=""accordion-connect-with-key"" >}}
  {{< accordion-item header=""Steps in more detail (click to expand)"" show=""false"" >}}
- You'll be asked for your username and the passphrase for your key. It is NOT recommended to save your passphrase in the connection file.
- Click OK
- You may see a list of all the other desktop sessions currently in progress from other users. Ignore these and click ""New desktop"".
- Select ""Create a new virtual desktop"", then click ""Create""
- **Note the instructions for how to reach the NX menu once in the session, and select screen settings from the list of icons: Recommended setting is ""Fit to window"" (leftmost icon)**
- Click OK on this and subsequent screens giving information about the NX and desktop environments.
- You should be presented with a linux deskop on the server to which you connected, e.g. `nx1.jasmin.ac.uk`
- You should be presented with a linux deskop on the server to which you connected, e.g. `nx1.jasmin.ac.uk`
- Locate the icon to open the ""Terminal"" application (bottom of window in Rocky 9 desktop)
- The video demonstrates making an onward connection to a `sci` server and testing the graphics functionality by opening the `xterm` application on that server, before exiting and logging out of the NX desktop.
{{</accordion-item>}}
{{</accordion>}}
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#step-by-step-instructions,1321,213
Connecting with method 2 (agent),"**Overview of this method:**
In summary, we need to:
- Load the SSH private key into a local ssh-agent
- Edit the NX configuration file to use the native ssh client instead of the NoMachine one
- Use the connection profile we created earlier, to connect.
1\. Load your SSH private key into your authentication agent
- Follow the [instructions for your platform here]({{% ref ""present-ssh-key/#1-loading-your-key-into-an-agent"" %}}), then return once you have successfully loaded your key.
  - for **Windows**, this **must** be the Windows ""OpenSSH Client"" optional feature, not any other ssh-agent. The MobaXterm agent will not work for this purpose.
  - for **Linux**, you may find that a ""local"" ssh-agent does not work: for example using Gnome desktop, you may need to use the global one for your desktop environment, e.g.  `gnome-keyring-daemon --start` instead, before doing `ssh-add <key>`
2\. Edit the NX configuration file
- Open the file `.nx/config/player.cfg` in a simple text editor (e.g. Windows Notepad). The `.nx` directory should be in your home directory.
- Towards the end of the file, you should see two lines like this:
  ```xml
  <option key=""SSH client mode"" value=""library"">
  <option key=""SSH Client"" value=""nxssh.exe"">
  ```
  Change them as follows:
  (the changes are slightly different for each platform)
{{< nav type=""tabs"" id=""tabs-os2"" >}}
  {{< nav-item header=""Windows"" show=""true"" >}}
  <option key=""SSH client mode"" value=""native"" />
  <option key=""SSH Client"" value=""C:\Windows\System32\OpenSSH\ssh.exe"" />
  {{< /nav-item >}}
  {{< nav-item header=""Mac"" >}}
  <option key=""SSH client mode"" value=""native"" />
  <option key=""SSH Client"" value=""/usr/bin/ssh"" />
  {{< /nav-item >}}
  {{< nav-item header=""Linux"">}}
  <option key=""SSH client mode"" value=""native"" />
  <option key=""SSH Client"" value=""/usr/bin/ssh"" />
  {{< /nav-item >}}
{{< /nav >}}
  **Save** and **close** the file before opening NoMachine Enterprise Client.
Next, follow the video below for actually connecting, or see the step-by-step instructions below:
{{< nav type=""tabs"" id=""tabs-connect-agent"" >}}
  {{< nav-item header=""Windows"" show=""true"" >}}
    {{< video id=""BwKG_dGGtUU"" >}}
  {{< /nav-item >}}
  {{< nav-item header=""Mac"" >}}
    {{< video id=""Q7JrBPacBao"" >}}
  {{< /nav-item >}}
  {{< nav-item header=""Linux"" >}}
    {{<video id=""37iof53jpUM"">}}
  {{< /nav-item >}}
{{< /nav >}}
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#connecting-with-method-2-(agent),2397,330
Step-by-step instructions,"{{< accordion id=""accordion-connect-with-agent"" >}}
  {{< accordion-item header=""Steps in more detail (click to expand)"" show=""false"" >}}
- Enter your JASMIN username in the box
- Click OK
- You may see a list of all the other desktop sessions currently in progress from other users. Ignore these and click ""New desktop"".
- Select ""Create a new virtual desktop"", then click ""Create""
- **Note the instructions for how to reach the NX menu once in the session, and select screen settings from the list of icons: Recommended setting is ""Fit to window"" (leftmost icon)**
- Click OK on this and subsequent screens giving information about the NX and desktop environments.
- You should be presented with a linux deskop on the server to which you connected, e.g. `nx1.jasmin.ac.uk`
- Locate the icon to open the ""Terminal"" application (bottom of window in Rocky 9 desktop)
- The video demonstrates making an onward connection to a `sci` server and testing the graphics functionality by opening the `xterm` application on that server, before exiting and logging out of the NX desktop.
{{</accordion-item>}}
{{</accordion>}}
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#step-by-step-instructions,1116,178
Using the graphical desktop environment,"Once you have set up the environment to your liking, you can
  - use the web browser on that system to access web-based resources available only within JASMIN
  - make SSH connections to other systems within JASMIN such as `sci-vm-01.jasmin.ac.uk`
  - use graphical applications on other systems within JASMIN and send the output bask to this desktop
  - the ""connecting"" videos above show the steps involved for this.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#using-the-graphical-desktop-environment,419,68
Notes,"  * The number of ""virtual desktops"" which can be created per user is limited to 1 in order to preserve system resources.
  * Although in theory sessions and desktop windows should persist when you close down the NoMachine client and when you re-open it to the same connection, you should not rely on this feature. Keeping long-running sessions open reduces resources available to other users.
  * The option to shut down the machine does not work for a ""regular"" user (only admins). Please just ""Log out"" instead.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#notes,515,87
Troubleshooting,,https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#troubleshooting,0,0
Authentication error (Windows users),"* Try reformatting your private key using a tool available within MobaXterm. 
    * If you have tested and can successfully connect to `nx1.jasmin.ac.uk` from an SSH client in a terminal window, but **NOT** from the NoMachine NX client (""Authentication Failed""), then try following the steps below: these instructions are for Windows users.
    * Note that this doesn't make you a new key, it just makes a reformatted version of your key, which can sit alongside your existing private key file. The public key stays the same, so there's no need to upload anything new to your JASMIN profile.
    * Open the MobaKeyGen tool: MobaXterm menu / Tools / **MobaKeyGen (SSH Key Generator)**.
    * Click Load, navigate to your existing private key file (you may need to change the filter to show all files (*.*) instead of just (*.ppk). Select your key, click Open
    * You will be prompted for the passphrase which you set when creating the key.
    * In the MobaKeyGen menu, select Conversions / Export OpenSSH key (the 2nd option)
    * Choose a new name for the file (it is recommended to just append something on to the end of your existing key name, so that it's clear that they are formats of the same key, e.g. id_rsa_jasmin -> id_rsa_jasmin_f (but the name is not significant).
    * Close the ""MobaXterm SSH Key Generator"" window
    * Make yourself a new NX connection profile, pointing to the new file
    * You should now be able to connect.
    * The following video shows this sequence in action (although the key is named slightly differently)
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#authentication-error-(windows-users),1554,263
Transposed symbol keys,"After the first connection (particularly for Mac users), subsequent connections to the same connection profile sometimes have some symbols keys e.g. `@` and `""` transposed.
Click the ""settings"" option (in the menu, top-right), then go to ""settings"" and search for ""input"" to look for alternative keyboard layouts.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#transposed-symbol-keys,314,47
Connection timeout,"Please do not try and connect using the proprietary ""NX"" protocol. Select ""SSH"" as the protocol. If you mistakenly use ""NX"" as the protocol you may see an error similar to the following when you try to connect (The correct port for **SSH** connections is 22)
A connection timeout has occurred while trying to connect to 'nx1.jasmin.ac.uk' on port '4000'.
The issue could either be caused by a networking problem, by a firewall or NAT blocking incoming
traffic or by a wrong server address. Please verify your configuration and try again.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#connection-timeout,538,91
Authentication method,"Please use ""Private key"" as the authentication method, not ""Authentication agent"", as the former has been found to work more consistently and reliably, particularly for onward connections to other machines.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#authentication-method,207,30
Client version,"Make sure you have installed and are using the correct and most recent version of the {{<link ""https://www.nomachine.com/download-enterprise#NoMachine-Enterprise-Client"" >}}NoMachine Enterprise Client{{</link>}} (not the NoMachine Enterprise Desktop or any other applications from NoMachine).
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#client-version,293,32
Key format,"If you created your SSH private key using the ""PuTTYgen"" application, and are getting ""Authentication failed"" for a key pair that you know works OK for simple terminal connections, it could be that you need to convert the private key to ""OpenSSH format"" for use here. By doing this you would be creating an alternately-formatted version of the same private key, so the public key stays the same (and you don't need to re-upload that to JASMIN).
- Open PuTTYgen and ""Load an existing private key file"" (click ""Load"")
- Ignore the notice about saving it in PuTTY's own format (this is not useful here) (click ""OK"")
- In the PuTTYgen menu, select ""Conversions"", then ""Export OpenSSH key""
- Save the newly-formatted private key file locally. The passphrase needed to unlock it should not have changed.
- Use this newly-formatted key file with NoMachine NX.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#key-format,853,145
Passphrase vs Password,"Be sure to use the PASSPHRASE associated with your SSH private key, and not the PASSWORD associated with your JASMIN account, when prompted using the NX client.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#passphrase-vs-password,161,27
Can't find your private key?,"The location of your private key on your local machine may be in a hidden directory for example `~/.ssh`. In order to navigate to it to provide the location when setting up your connection profile, you may need to enable the display of hidden directories/files in your local desktop environment first. On a Mac you can do this with the shortcut {{<kbd ""CMD+SHIFT+."">}}. In Windows this is under File Explorer / View / Hidden Items. It's also possible that your home directory itself (normally `/Users/<username>`) is not configured to be displayed by default in `Finder`. If this is case, go to Finder / Preferences / Sidebar / Show these items and tick the box next to the item representing your username: this should make it appear, and `.ssh` should be a subdirectory of this.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#can't-find-your-private-key?,780,134
Can't make an onward connection,"Previous versions of the Windows client had problems with ""forward authentication"" enabled, but this is required for onward connection to other machines. If this happens, try:
* Does your username have > 8 characters? If so, try using `nx4` (centos7), or any of the new Rocky 9 NX servers, `nx[1-3]`
* Uninstalling the NoMachine Enterprise Client
* Deleting the `C:\Users\<username>\.nx` directory on your machine
* Re-installing and trying again.
* Deleting and making a new connection profile for the nx server you're connecting to.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#can't-make-an-onward-connection,535,84
Can't display graphics from sci machine or other onward connection,"Did you omit the `-X` option from the SSH command when you made the onward connection to that machine? Try `-Y` if `-X` doesn't work for you.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#can't-display-graphics-from-sci-machine-or-other-onward-connection,142,27
Disk space,"Check your disk usage in your JASMIN home directory: if this is over the 100G limit, you may not be able to write any temporary files and this could prevent NoMachine from being able to start a new session or even reconnect to an existing virtual desktop session. Clear out some space and re-check with `pdu -sh $HOME` to find out how much space you're using.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#disk-space,360,66
Can't connect or gets stuck connecting to a previous session,"Sometimes, you can't connect because you have a previous session which did not terminate correectly, or you might have problems reconnecting to a previous desktop session. Sometimes the client will get stuck with a ""spinning wheel"" before eventually timing out. You can terminate your own previous session as follows:
- Follow instructions in {{<link ""#connecting"">}}Connecting{{</link>}} until step 2 where all the users' sessions on the machine are displayed.
- Find the one corresponding to your username
- Right-click it and select ""Terminate session""
Note that you may lose any unsaved work in the session that you terminate, but it should clear the stuck session and allow you to reconnect. Please try this first before asking the support team, as this is the first thing that they will try in order to clear your session.
",https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#can't-connect-or-gets-stuck-connecting-to-a-previous-session,829,134
"""It worked yesterday""","For occasions where ""it worked last time I tried to connect, but now doesn't"", please first try the above step to clear any previous session which might have got stuck, otherwise the time-honoured IT support advice of ""turning it off and on again"" is applicable: try restarting your local machine where the NX client is running, as this can sometimes clear issues with the client, your machine or your network connection. Don't forget to re-connect via your VPN if available.
","https://help.jasmin.ac.uk/docs/interactive-computing/graphical-linux-desktop-access-using-nx#""it-worked-yesterday""",476,80
Available login servers,"There are four login servers available to access resources within JASMIN.
Users with the `jasmin-login` access role can access the following servers via
{{<abbr SSH>}}.
{{<alert type=""info"" >}}
All four login servers now have identical configuration and should be accessible from any network.
{{</alert>}}
name |
--- |
`login-01.jasmin.ac.uk` |
`login-02.jasmin.ac.uk` |
`login-03.jasmin.ac.uk` |
`login-04.jasmin.ac.uk` |
{.table .table-striped .w-auto}
",https://help.jasmin.ac.uk/docs/interactive-computing/login-servers#available-login-servers,455,59
Features of login servers,"Login servers have minimal resources and software installed. They provide:
- a means to access other resources within JASMIN (inside the {{<abbr STFC >}} firewall)
- access to your home directory (`/home/users/<username>`)
- no analysis software
- no access to group workspaces
",https://help.jasmin.ac.uk/docs/interactive-computing/login-servers#features-of-login-servers,278,42
Recent changes,"- There is no longer any requirement for forward/reverse DNS lookup or any restriction by
institutional domain.
- You no longer need to register non-`*.ac.uk` domains with the JASMIN team.
- This means all users can access all login servers (previously some users could only use particular ones)
- As before, no filesystems other than the home directory are mounted.
- Use only as a ""hop"" to reach other servers within JASMIN.
- **Make sure your SSH client is up to date**. Check the version with `ssh -V`. If
it's significantly older than `OpenSSH_8.7p1, OpenSSL 3.0.7`, speak to your local
admin team as it may need to be updated before you can connect securely to JASMIN.
See also [How to login]({{% ref ""how-to-login"" %}}) and other articles in the [Getting started]({{% ref ""getting-started"" %}}) category.
See also [NoMachine NX service]({{% ref ""graphical-linux-desktop-access-using-nx"" %}}) which provides login to a graphical Linux desktop, rather than a single terminal window.
",https://help.jasmin.ac.uk/docs/interactive-computing/login-servers#recent-changes,988,157
How to use the login servers,"For full details of how to log in, including making onward connections to
other machines, please see the article [""How to login""]({{% ref ""how-to-login"" %}}).
{{<alert type=""danger"">}}
Users are **not permitted to execute commands which require
administrative privileges**. This applies to all hosts in the managed part of
JASMIN where users have SSH login access (for example `login`, `nx`,
`sci`, `xfer` and `hpxfer` machines).
In other words, the **use of `su` and `sudo` is not permitted**.
Please be careful when typing commands,
particularly if you have multiple terminal windows open on your own computer,
that you do not accidentally attempt `sudo`on a JASMIN machine: expect some
follow-up from the JASMIN team if you do!
{{</alert>}}
",https://help.jasmin.ac.uk/docs/interactive-computing/login-servers#how-to-use-the-login-servers,744,115
Connecting to a sci server via a login server,"The connection via a login server can be done either with 2 hops, or using a login server as a Jump Host (-J):
- 2 hops method:
{{<command user=""user"" host=""localhost"">}}
ssh -A fred@login-01.jasmin.ac.uk
{{</command>}}
{{<command user=""user"" host=""login-01"">}}
ssh fred@sci-vm-01.jasmin.ac.uk
",https://help.jasmin.ac.uk/docs/interactive-computing/login-servers#connecting-to-a-sci-server-via-a-login-server,294,39
"no -A needed for this step, if no onward connections from sci server","{{</command>}}
{{<command user=""user"" host=""sci-vm-01"">}}
","https://help.jasmin.ac.uk/docs/interactive-computing/login-servers#no--a-needed-for-this-step,-if-no-onward-connections-from-sci-server",58,4
now on sci server,"{{</command>}}
- Jump Host method:
{{<command user=""user"" host=""localhost"">}}
ssh -A fred@login-01.jasmin.ac.uk -J fred@login-01.jasmin.ac.uk
{{</command>}}
{{<command user=""user"" host=""sci-vm-01"">}}
",https://help.jasmin.ac.uk/docs/interactive-computing/login-servers#now-on-sci-server,200,17
now on sci server,"{{</command>}}
Alternatively, the same effect can be achieved with a ProxyJump directive in your local `~/.ssh/config` file:
Host Sci1ViaLogin01
  User fred
  ForwardAgent yes
  HostName sci-vm-01.jasmin.ac.uk
  ProxyJump fred@login-01.jasmin.ac.uk
You could then simply connect to `Sci1ViaLogin01`:
{{<command user=""user"" host=""localhost"">}}
ssh Sci1ViaLogin01
{{</command>}}
{{<command user=""user"" host=""sci-vm-01"">}}
",https://help.jasmin.ac.uk/docs/interactive-computing/login-servers#now-on-sci-server,420,43
now on sci server,"{{</command>}}
This sort of configuration is useful for connections needed by remote editing/development tools such 
as VSCode. The example above relies on having your key loaded locally in an ssh-agent.
An alternative is to include a line specifying
the location of your key, so you'll then be prompted for your passphrase whenever you connect:
Host Sci1ViaLogin01
  User fred
  ForwardAgent yes
  HostName sci-vm-01.jasmin.ac.uk
  ProxyJump fred@login-01.jasmin.ac.uk
  IdentityFile ~/.ssh/id_rsa_jasmin
",https://help.jasmin.ac.uk/docs/interactive-computing/login-servers#now-on-sci-server,506,66
Interactive computing overview,"This article introduces the resources on JASMIN available for interactive
computing (as opposed to [batch computing]({{% ref ""slurm-scheduler-overview"" %}})). It covers:
- Login servers
- Scientific Analysis servers
- Data Transfer Servers
",https://help.jasmin.ac.uk/docs/interactive-computing/interactive-computing-overview,240,32
Login Severs,"The [login (also known as gateway or bastion) servers]({{% ref ""login-servers"" %}}) provide external* users with access to services inside of JASMIN.
*external = outside of the STFC/RAL firewall.
",https://help.jasmin.ac.uk/docs/interactive-computing/interactive-computing-overview#login-severs,196,29
Scientific Analysis Servers,"The [scientific analysis servers]({{% ref ""sci-servers"" %}}) are the main
resource for most users' everyday work. They have a [standardised software
environment]({{% ref ""software-overview#common-software"" %}}) and are ideal for development and testing of
processing tasks which can then be submitted to the [LOTUS batch processing
cluster]({{% ref ""slurm-scheduler-overview"" %}}) for larger processing runs.
",https://help.jasmin.ac.uk/docs/interactive-computing/interactive-computing-overview#scientific-analysis-servers,409,53
Data Transfer Servers,"General [data transfer servers]({{% ref ""transfer-servers"" %}}) are provided
for simple or smaller data transfer tasks. For larger data flows or where high
performance is required, more sophisticated tools and services are available.
See also [data transfer overview]({{% ref ""data-transfer-overview"" %}}).
",https://help.jasmin.ac.uk/docs/interactive-computing/interactive-computing-overview#data-transfer-servers,307,41
Terms and Conditions for data and information provided by the NERC or STFC through the Centre for Environmental Data Analysis (CEDA),,https://help.ceda.ac.uk/article/4642-disclaimer#terms-and-conditions-for-data-and-information-provided-by-the-nerc-or-stfc-through-the-centre-for-environmental-data-analysis-(ceda),0,0
Exclusion of Liability,"Changing circumstances may cause the STFC or NERC to have to change the
information and contents of CEDA\'s pages at any time. Whilst every
effort has been made to ensure the accuracy of information presented,
both STFC and NERC disclaim all responsibility for and accept no
liability for any errors or losses caused by any inaccuracies in such
information or the consequences of any person acting or refraining from
acting or otherwise relying on such information.
Your use of information provided by STFC or NERC through the Centre for
Environmental Data Analysis is at your own risk. Please read any
warnings given about the limitations of the information.
Neither NERC nor STFC gives any warranty as to the quality or accuracy
of the information or its suitability for any use. All implied
conditions relating to the quality or suitability of the information,
and all liabilities arising from the supply of the information
(including any liability arising in negligence) are excluded to the
fullest extent permitted by law.
Neither NERC nor STFC gives any warranty as to the accuracy or
completeness of data or images in the form in which they are cached or
downloaded to your computer, as they may be affected by on-line
conditions over which NERC or STFC has no control.
",https://help.ceda.ac.uk/article/4642-disclaimer#exclusion-of-liability,1277,213
Notes on Limitations,"-   Scientific observations are made according to the prevailing
    understanding of the subject at the time. The quality of such
    observations may be affected by subsequent advances in knowledge,
    improved methods of interpretation, and better access to sampling
    locations.
-   Raw data may have been transcribed from analogue to digital format,
    or may have been acquired by means of automated measuring
    techniques. Although such processes are subjected to quality control
    to ensure reliability where possible, some raw data may have been
    processed without human intervention and may in consequence contain
    undetected errors.
-   Detail clearly defined and accurately depicted on large-scale maps
    may be lost when small-scale maps are derived from them.
-   Although samples and records are maintained with all reasonable
    care, there may be some deterioration in the long term.
-   The most appropriate techniques for copying original records are
    used, but there may be some loss of detail and dimensional
    distortion when such records are copied.
-   Data may be compiled from the disparate sources of information at
    STFC's or NERC\'s disposal, including material donated to STFC or
    NERC by third parties, and may not have been subject to any
    verification or other quality control process.
-   Data, information and related records which have been donated to
    STFC or NERC have been produced for a specific purpose, and that may
    affect the type and completeness of the data recorded and any
    interpretation. The nature and purpose of data collection, and the
    age of the resultant material may render it unsuitable for certain
    applications/uses. You must verify the suitability of the material
    for your intended usage.
-   The data, information and related records supplied by STFC or NERC
    should not be taken as a substitute for specialist interpretations,
    professional advice and/or detailed site investigations. You must
    seek professional advice before making technical interpretations on
    the basis of the materials provided.
-   If a report or other output is produced for you on the basis of data
    you have provided to STFC or NERC, or your own data input into a
    STFC or a NERC system, please do not rely on it as a source of
    information about other areas or features.
",https://help.ceda.ac.uk/article/4642-disclaimer#notes-on-limitations,2382,363
Accessibility,"This page states CEDA\'s intention to provide websites that are usable
and accessible to all and details some of the measures taken. CEDA\'s
objective on accessibility is to conform to the guidelines for UK
Government Websites, which support the W3C\'s [Web Content Accessibility
Guidelines](http://www.w3.org/WAI/standards-guidelines/wcag/glance/) at
Level AA. These are to ensure that we achieve and maintain an
appropriate Web accessibility standard and that our web sites are
inclusive.
This accessibility statement applies only to the Centre for
Environmental Data Analysis (CEDA) websites and relates to all new and
reviewed public facing websites, but does not apply to any other site,
including:
-   Legacy websites operated by or for CEDA
-   Other websites operated by partner organisations
-   Any site that is linked from one of our pages
Please note, some sections of this web site are controlled by third
parties and so it has not been possible for the same standards of
accessibility to be applied to them.
We are making every effort to make CEDA\'s websites accessible,
including layout, and easy to use for everyone, no matter what browser
you choose to use, and whether or not you have any disabilities.
",https://help.ceda.ac.uk/article/4641-accessibility#accessibility,1222,192
Recruitment,"CEDA recruitment is handled by our parent organisation the Science and
Technology Facilities Council (STFC) which currently recruits through
the i-recruitment Portal. Please refer to the [STFC Accessibility
Statement](https://stfc.ukri.org/About+STFC/5809/) regarding that
recruitment site.
",https://help.ceda.ac.uk/article/4641-accessibility#recruitment,291,33
PDFs,"Most complex Adobe® Acrobat® Portable Document Format (PDF) files on
this site added after November 2008 are tagged to allow basic
accessibility pre-existing PDFs will be tagged on request.
For more information and help about accessibility and changing your
browser settings please visit the BBC\'s [My Web, My
Way](http://www.bbc.co.uk/accessibility/) pages.
More information about PDF accessibility can be found on the [Adobe
website accessibility
section](https://www.adobe.com/accessibility.html).
To download a copy of Acrobat reader, please visit the [acrobat download
page](https://get.adobe.com/uk/reader/otherversions/) on the Adobe
website.
If you require any further help and advice regarding assistive software
and hardware, please contact the [Shaw
Trust](https://www.shaw-trust.org.uk/page/9/).
",https://help.ceda.ac.uk/article/4641-accessibility#pdfs,809,99
Changing font size,"Font size can be changed by setting your browser settings to your own
preference - this option is usually available from the \""view\"" option
in your browser toolbar.
<div>
",https://help.ceda.ac.uk/article/4641-accessibility#changing-font-size,172,29
Video Content,"</div>
CEDA videos are hosted on YouTube and presented with captions. To play a
video, users can click the triangle button on the player or the image.
Alternatively, keyboard-only and assistive software users can use the
""\[Video Autoplay\]"" link for this purpose.
When a video is playing, the users can use the following keys on the
keyboard to control the video player:
-   Spacebar: Play / Pause;
-   Right Arrow / Left Arrow: go forward / go backward;
-   Up Arrow / Down Arrow: Louder / Softer.
Mac users browsing with Safari can use CTRL with the above keys to
fulfil the same operations.
<div>
",https://help.ceda.ac.uk/article/4641-accessibility#video-content,601,105
Contact Information,"</div>
Maintaining an accessible site is an ongoing process and we are
continually working to offer a user friendly experience. However, if you
have any problems using this web site please [contact
us](http://www.ceda.ac.uk/contact/).
",https://help.ceda.ac.uk/article/4641-accessibility#contact-information,235,33
Introduction,"This page gives a brief introduction to the BADC-CSV file format, which
is covered in greater depth in the \"" [BADC-CSV Text File Guide for
Users and Produces](https://zenodo.org/records/7355840)\"" . 
Note - for more details on how to read more generic comma- or -tab- (csv
or tsv) separated data please see our [ASCII data
format](https://help.ceda.ac.uk/article/4429-ascii-formats) page.
",https://help.ceda.ac.uk/article/105-badc-csv#introduction,390,55
What is the BADC-CSV format?,"A BADC-CSV formatted file is simply a csv file with a fancy file header
full of information about the data.
It is used for simple 1-D data, such as:
-   A collection of surface meteorological data from one or more
    stations over a period of time
-   A time series of data from one instrument
-   Simple model output
",https://help.ceda.ac.uk/article/105-badc-csv#what-is-the-badc-csv-format?,319,59
How do I format my data?,"The BADC-CSV file is broken down into two sections; header and data. The
header includes metadata, information about the data themselves and
supplementary information that helps users to understand the data and
how the data were produced, plus other useful information. The header
has various components, some components are compulsory, while others are
encouraged to make the data more understandable/usable. 
",https://help.ceda.ac.uk/article/105-badc-csv#how-do-i-format-my-data?,411,60
Header,"The header includes various pieces of information about the data, known
as metadata. Each metadata item contains a reference to the data column
it applies to. A ""G"" indicates that the metadata applies globally - i.e.
to all the data, while a ""number"" or ""string"" represents  a particular
column of data to which it applies. The links to specific columns could
be, for example:
-   numbers: 1, 2\... etc
-   letters: a, b, c, etc
-   labels: temp, wind speed, etc. 
These will correspond to the column headings at the start of the data
section (see below for more information).
An example of a header section is shown below.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/65d38a17505a5d724b734d51/file-M6fkqtNq2s.png)
Below is a table of commonly used metadata labels that can be used in
your data file. Note that the 'Conventions', 'long_name' and
'coordinate_variable' labels are compulsory. For a full list please see
\"" [BADC-CSV Text File Guide for Users and
Produces](https://zenodo.org/records/7355840)\"".
<div>
\
<div>
  --------------------- ----------------------------
  **Label**             **Reference**
  Conventions           G, BADC-CSV,version number
  title                 G
  creator               G
  contributor           G
  activity              G
  coordinate_variable   G
  long_name             Column number or name
  standard_name         Column number or name
  type                  Column number or name
  flag_value            G, column number or name
  flag_meanings         G, column number or name
  comments              G, column number or name
  last_revised_date     G
  history               G
  source                G
  observation_station   G
  height                G, column number or name
  location              G
  date_valid            G
  rights                G
  reference             G
  --------------------- ----------------------------
</div>
</div>
All words within the label should be joined by underscores (\""\_\""),
should not have any white space and should be entirely lower case. Data
providers may add their own labels, but additional labels will not be
automatically interpreted by software designed to understand BADC-CSV
formatted files.
",https://help.ceda.ac.uk/article/105-badc-csv#header,2245,278
Data,,https://help.ceda.ac.uk/article/105-badc-csv#data,0,0
,"The data section comes after the header and includes 3 markers to
indicate the start and end of the data.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/65d38a3b505a5d724b734d52/file-8DwaP5mnZr.png)
",https://help.ceda.ac.uk/article/105-badc-csv#,239,21
Examples,,https://help.ceda.ac.uk/article/105-badc-csv#examples,0,0
,"[Simple
Example:](https://artefacts.ceda.ac.uk/formats/badc-csv/simple-example.csv)
This simple example shows a basic BADC-CSV file to demonstrate the
layout and also a simple sample of data. This has all the basic required
elements in it to be acceptable.
[Full Example
1:](https://artefacts.ceda.ac.uk/formats/badc-csv/badc-csv-full-example1.csv)This
is a sample taken from the UK Met Office MetDB dataset showing the
AMDARS message type and also includes some additional metadata elements
beyond the standard.
",https://help.ceda.ac.uk/article/105-badc-csv#,513,62
How do I get started?,,https://help.ceda.ac.uk/article/105-badc-csv#how-do-i-get-started?,0,0
,"A basic BADC-CSV template can be found
[here](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/attachments/5e15c3a52c7d3a7e9ae5e88d/badc-csv_template.csv).
",https://help.ceda.ac.uk/article/105-badc-csv#,183,8
Uploading BADC-CSV files to CEDA,"For programmes currently submitting BADC-CSV formatted files, CEDA
provides a web-based file uploader [BADC-CSV file checker
service](https://archive2.ceda.ac.uk/cgi-bin/badccsv/csvchecker). In the
process, files are checked for compliance with the BADC-CSV standard.
Once your data have passed you can upload your data following the guide.
",https://help.ceda.ac.uk/article/105-badc-csv#uploading-badc-csv-files-to-ceda,341,42
Further reading,"The BADC-CSV format has been structured to comply with a number of data
standards. More information about these standards can be found through
\"" [BADC-CSV Text File Guide for Users and
Producers](https://zenodo.org/records/7355840)\"".
\
For more information and guidance on formatting your data please visit
our [FAQs
page](https://help.ceda.ac.uk/article/4661-depositing-data-faqs?preview=5b5af56c2c7d3a03f89d101b#What%20data%20format%20should%20I%20use?).
",https://help.ceda.ac.uk/article/105-badc-csv#further-reading,459,47
Introduction,"::: {#wikipage children-count=""0""}
To ensure that MOLES Metadata entries link logically through to the
endpoints within the archive it is important that the archive structure
used for any dataset follows a recognised structure model. The CEDA
Archive Structure Model has been developed from examining a number of
typical use cases from within the archive and then seeing how these map
over to the MOLES 2/MOLES 3 concepts. Commonalities between the existing
\""datasets\"" within the CEDA archive permit an Archive Structure Model
to be formulated and this presents the opportunity to align the MOLES
archive with MOLES 3 concepts and other archive structure paradigms,
e.g. CMIP5 archive structure.\
The [attached
file](http://team.ceda.ac.uk/trac/ceda/attachment/wiki/opman/CEDAArchiveStructureModel/CEDAArchiveStructureModel_v2.pptx)
is a presentation is a summary of the conversations that took place
during the development of the CEDA Archive Structure Model. For further
clarification discuss this with Sam Pepler, Graham Parton and Spiros
Ventouras.
**NOTE:** 2013-04-23: how to deal with super-collections of data, such
as ESA CCI products, has yet to be fully formulated
",https://help.ceda.ac.uk/article/4158-ceda-archive-structure-model#introduction,1178,164
Basic Structure {#BasicStructure},"The archive structure basically breaks down as follows:
data centre/data entity path/{data}/Reference Path/File Management Path
or more explicitly:
/<data centre>/<primary Data Entity>/<metadata|data>/<M1>/<M2>/<deployment>/<split1>/<split2>/<files>
**NB** all parts after \<metadata\|data> level listed above apply only
to \<data> path.
Where :\
**\<data centre>** will be the data centre that the data are part of..
e.g. neodc/badc/cmip5.\
**\<primary data entity>** is the principal MOLES data entity under
which the data are associated. *NB* this may not be the most logical
place at the end of a project, but should be established as best as
possible before data arrive based on available knowledge\
**\<metadata\|data>** is used at provide a top level split between the
actual data and other types of files, e.g. CMSL metadata files\
**\<Reference Path>** is a metadata hierarchical splitting that finishes
up at a referenced object - the **deployment** . See below for further
information on how this links to MOLES 3 concepts.\
**\<File Management Path>** is a locally useful way of splitting down
files within a given **deployment** , e.g. by YYYY/MM/DD. The depth of
splitting should be to have a balanced of enough files to be sensible,
but not excessive\
",https://help.ceda.ac.uk/article/4158-ceda-archive-structure-model#basic-structure-{#basicstructure},1267,189
Links with MOLES entries {#LinkswithMOLESentries},"Within the MOLES catalogue there are now two areas where links to the
archive can be made. These are from the Data Entity and, to provide the
opportunity to prepare for MOLES 3 concepts being implemented, from the
the Deployments.
Data Entities should have the URLs pointing down to the /\<data
centre>/\<data entity>/ part of the archive, while Deployments should
follow the relevant \<Reference Path> to the relevant \<deployment>
reference.
",https://help.ceda.ac.uk/article/4158-ceda-archive-structure-model#links-with-moles-entries-{#linkswithmolesentries},444,70
Examples {#Examples},"The following illustrate how this model is used in practice:
/badc/faam/data/YYYY/<flight-number>/<products type>/files
e.g.      /badc/faam/data/2009/b462-jun-24/core_processed/<files|sub-directories>
/badc/appraise/data/<project>/
e.g.      /badc/appraise/data/adient/bae-146/b462-2009-jun-24 -> /badc/faam/data/2009/b462-jun-24
/badc/fgam/data/instrument/data/YYYY/MM/files
e.g.    /badc/fgam/data/man-radar-1290Mhz/data/2002/10/man-radar-1290Mhz_20021017_low.nc
/badc/ecmwf-era-interim/data/<grid types>/<analysis|forecast><level type>/YYYY/MM/DD/files
e.g. /badc/ecmwf-era-interim/data/gg/as/2009/09/26/ ggas200909261200.nc
:::
",https://help.ceda.ac.uk/article/4158-ceda-archive-structure-model#examples-{#examples},633,29
Depositing data FAQs,"[What and how much data can I
deposit?](#What%20and%20how%20much%20data%20can%20I%20deposit?)
[Why should I archive data?](#Why%20should%20I%20archive%20data?)
[What data format should I
use?](#What%20data%20format%20should%20I%20use?)
[Why is metadata important?](#Why%20is%20metadata%20important?)
[What metadata should I include in my
files?](#What%20metadata%20should%20I%20include%20in%20my%20files?)
[How do I check my files?](#How%20do%20I%20check%20my%20files?)
[How should I name my files?](#How%20should%20I%20name%20my%20files?)
[Can I get a DOI for my
dataset?](#Can%20I%20get%20a%20DOI%20for%20my%20dataset?)
[How long will it take to get a
DOI?](#How%20long%20will%20it%20take%20to%20get%20a%20DOI?)
[How do I tell CEDA about my
data?](#How%20do%20I%20tell%20CEDA%20about%20my%20data?)
[How do I supply catalogue record information and other
information?](#How%20do%20I%20supply%20catalogue%20record%20information%20and%20other%20information?)
[What is a data management plan
(DMP)?](#What%20is%20a%20data%20management%20plan%20(DMP)?)
[Is my data worthy of
archiving?](#Is%20my%20data%20worthy%20of%20archiving?)
[What if I want to change my
data?](#What%20if%20I%20want%20to%20change%20my%20data?)
[How can I send my data to
CEDA?](#How%20can%20I%20send%20my%20data%20to%20CEDA?)
[How do I get data usage
information?](#How%20do%20I%20get%20data%20usage%20information?)
[Where do I go if my data is not suitable to be stored at
CEDA?](#Where%20do%20I%20go%20if%20my%20data%20is%20not%20suitable%20to%20be%20stored%20at%20CEDA?)
[Can I archive compressed and tar-ed
data?](#Can%20I%20archive%20compressed%20and%20tar-ed%20data?)
[My data is stored on a group/project workspace does that mean it is
stored on the
archive?](#My%20data%20is%20stored%20on%20a%20group/project%20workspace%20does%20that%20mean%20it%20is%20stored%20on%20the%20archive?)
[What licence will my data be made available to other
under?](#What%20licence%20will%20my%20data%20be%20made%20available%20to%20other%20under?)
[Who owns my data?](#Who%20owns%20my%20data?)
[What levels of archiving service do CEDA
offer?](#What%20levels%20of%20archiving%20service%20do%20CEDA%20offer?)
[How long will CEDA keep my
data?](#How%20long%20will%20CEDA%20keep%20my%20data?)
[Can I restrict who can use my
data?](#Can%20I%20restrict%20who%20can%20use%20my%20data?)
[How is dataset access
controlled?](#How%20is%20dataset%20access%20controlled?)
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs,2418,196
What and how much data can I deposit? {#What and how much data can I deposit?},"CEDA will archive data relevant to Atmospheric Sciences and Earth
Observation fields, including data from NERC funded grants. CEDA will
also accept small volumes of non-NERC funded projects to act as a
reference only source. This includes a wide range of instrumental,
satellite, aircraft, observations, analyses and model datasets of
interest to the scientific community.
<div>
Projects can archive up to 10TB of data, either as data for publication
reference purposes or for re-use by the community. 
</div>
<div>
**Reference data**: archive data to referencing in a peer-reviewed
publication. 
</div>
<div>
These are data that needs to be discoverable and downloadable, but is
left to the user to work out some of the usability issues. CEDA will
make a catalogue entry and add the data files to the archive. Its
principal use will be to provide evidence for publication.
</div>
<div>
**Reusable**: data to be held on the CEDA archive for re-use by the UK
research community
</div>
<div>
The data must be provided in community supported format (e.g. NetCDF),
with a defined file and directory naming convention, CEDA can provide
support to organise the data. It must also have associated documentation
and file level metadata (preferably adhering to the Climate and Forecast
(CF) metadata conventions). 
</div>
<div>
Note that for projects with large volumes of model data in general CEDA
will only archive a small subset of the data to be used for reference in
publications. Data that is perceived to have a high value to the wider
scientific community (outside of the specific project) may be allocated
a larger volume of space, however, there is a finite amount of disk
space and it has to be shared across the community.\
</div>
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#what-and-how-much-data-can-i-deposit?-{#what-and-how-much-data-can-i-deposit?},1735,283
Why should I archive data? {#Why should I archive data?},"Sending data to a secure long-term archive is increasingly a necessity
for science projects due to the funding body requirements and to fulfil
citation stipulations when publishing work.
It is NERC's data policy that all environmental data of long-term value
generated through NERC-funded activities must be submitted to NERC for
long-term management and dissemination and will be made openly available
for others to use.
It is also good practice for long term scientific aims and to enable the
preservation and re-use of valuable research data as it is unique and
cannot be replaced if destroyed or lost.
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#why-should-i-archive-data?-{#why-should-i-archive-data?},606,97
What data format should I use? {#What data format should I use?},"It is essential that the data are provided in suitable  [standard data
formats](https://help.ceda.ac.uk/article/104-file-formats) such as
[NetCDF](https://help.ceda.ac.uk/article/106-netcdf), [NASA
Ames](https://help.ceda.ac.uk/article/4692-nasa-ames) or
[BADC-CSV](https://help.ceda.ac.uk/article/105-badc-csv) -- preferably
following the CF metadata convention. Conforming to standard formats
will maximise the potential for re-use and long-term usability and
interoperability of your data in CEDA\'s archives.
Everyone has their own favourite formats but with 14PB of data for
\>5000 datasets and \>550 million files in the archive it is essential
that correctly-applied standard data formats are used in the archive to
enable efficient curation and preservation of your data.
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#what-data-format-should-i-use?-{#what-data-format-should-i-use?},780,91
Why is metadata important? {#Why is metadata important?},,https://help.ceda.ac.uk/article/4661-depositing-data-faqs#why-is-metadata-important?-{#why-is-metadata-important?},0,0
,"Metadata is information about the data itself. Metadata is used for data
discovery and for explaining what the specifics of the data are. The
archived data may be re-used in the future by parties not involved with
the project and could help support their science - so it is essential
that data producers provide as much context with their data as possible.
Metadata should answer these questions: who, what, why, where, when and
how your data was produced.
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#,457,78
What metadata should I include in my files? {#What metadata should I include in my files?},"Metadata should answer these questions: who, what, why, where, when and
how your data was produced. CEDA encourages the use of international
standards such as the Climate and Forecasting Conventions (
[CF-convention](https://help.ceda.ac.uk/article/4507-the-cf-metadata-convention))s
and Dublin Core. Most of our standard formats are already set up to
follow these standards and depositors are encouraged to follow these.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5b5f1d350428631d7a8972cf/file-UTVWIkYm7r.png)
For more information see  [Metadata
Basics](https://help.ceda.ac.uk/article/4428-metadata-basics)
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#what-metadata-should-i-include-in-my-files?-{#what-metadata-should-i-include-in-my-files?},653,62
How do I check my files? {#How do I check my files?},"There are tools available for some formats to allow people to check
their files for compliance to format and/or metadata standards. These
include the following:
  ----------------------------------- ---------------------------------------------------------------------
  Format                              File compliance tool
  BADC-CSV                            [BADC-CSV file checker
                                      service](http://archive.ceda.ac.uk/cgi-bin/badccsv/csvchecker).
  NASA Ames                           [NASA Ames file checker service](http://archive.ceda.ac.uk/nachecker)
  NetCDF                              [CEDA netCDF CF-compliance checking
                                      service](http://wps-web1.ceda.ac.uk/submit/form?proc_id=CFChecker)\
                                      [NCAS-CMS CF-netCDF checking
                                      service](http://puma.nerc.ac.uk/cgi-bin/cf-checker.pl)
  ----------------------------------- ---------------------------------------------------------------------
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#how-do-i-check-my-files?-{#how-do-i-check-my-files?},1047,55
How should I name my files? {#How should I name my files?},"CEDA file naming convention given below to enable quick access to
pertinent metadata and avoids the need to open and read the file in
order to assess its contents.
The CEDA file-naming convention for observation data:
**instrument\|model\_\[location\|platform\|modelnumber\]\_YYYYMMDD\[hh\]\[mm\]\[ss\]\[\_extra\].ext**
For further information please see the  [File Names
explained](https://help.ceda.ac.uk/article/103-filenames).
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#how-should-i-name-my-files?-{#how-should-i-name-my-files?},431,46
Can I get a DOI for my dataset? {#Can I get a DOI for my dataset?},"CEDA is able to assign DOIs (Digital Object Identifiers) to datasets
held within its archives. Publishers are increasingly requesting that
researchers ensure that their data are lodged in a recognised data
repository, preferably with DOIs assigned to the data to aid linking the
published article and the referenced data resource. 
 In order to do this the data must be:
1.  Complete - the data can be a whole dataset (or can apply to an
    ongoing time series).
2.  Unchanging - whereby the actual content is fixed and will not be
    further amended to correct for inconsistencies, with the exception
    of additional files clearly labeled as errata.
3.  Properly archived in a persistent location - such as the archives
    held by CEDA.
4.  Have all the necessary information about the dataset to compile a
    suitable CEDA dataset catalogue page. This forms the DOI landing
    page to which the DOI will resolve and bring the person following
    the DOI to the referenced resource. For this reason we don\'t change
    the title or the authors after we give the data a DOI.
CEDA data scientists will work with you to ensure that your data meet
these criteria. While this may also necessitate some additional work on
your behalf we\'ll try to guide you through the process and make it as
easy as possible. 
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#can-i-get-a-doi-for-my-dataset?-{#can-i-get-a-doi-for-my-dataset?},1316,220
How long will it take to get a DOI? {#How long will it take to get a DOI?},"This will depend on the work needed to ensure the data and associated
dataset catalogue page are suitably in place for the DOI landing page to
be created. 
In all cases we recommend that you contact CEDA at the earliest
available opportunity to discuss your requirements as we may not be able
to meet short deadlines.
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#how-long-will-it-take-to-get-a-doi?-{#how-long-will-it-take-to-get-a-doi?},318,56
How do I tell CEDA about my data? {#How do I tell CEDA about my data?},"If you have a NERC grant relating to atmospheric or earth observation
fields, CEDA will contact you at the beginning of your project for
further details and discussion about archival. Appropriate data
management must occur for each funded project, NERC data policy guidance
can be found 
[here](https://nerc.ukri.org/research/sites/data/policy/). 
For Non-NERC projects please contact CEDA to discuss your requirements.
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#how-do-i-tell-ceda-about-my-data?-{#how-do-i-tell-ceda-about-my-data?},420,57
How do I supply catalogue record information and other information? {#How do I supply catalogue record information and other information?},"To help users find data in CEDA archives it is important that we have
correct information about the data. This includes information about
dataset itself, like a description and the geographic area covered by
the data, but also information about the instruments or model used to
create the data and other project background. These details can be
sent via a text file with the 
[data](https://arrivals.ceda.ac.uk/intro/). We will assume a file in the
top level of the delivered dataset with the
name `metadata.yaml` contains the details of the dataset, project and
instrument or model. You can either use this simple metadata file
creation
[utility](http://artefacts.ceda.ac.uk/tools/dataset_ingest_labeler/dataset_labeler.html),
or edit one of the examples below.
[example dataset details YAML
file](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/attachments/5b4cc8ae0428631d7a88f39a/marius.yaml).  
[station-data_metadata_example.yaml](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/attachments/5ba24c0a2c7d3a16370f4656/station-data_metadata_example.yaml)
[instrument_metadata_example.yaml](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/attachments/5ba24bf82c7d3a16370f4654/instrument_metadata_example.yaml)
[model_metadata_example.yaml](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/attachments/5ba24bef2c7d3a16370f4653/model_metadata_example.yaml)
Adding the information this way helps us keep the record and the data
together even if its supplied over non-web based channels like FTP.
Other useful information
Besides the information areas covered in the above links it is also
helpful to provide the following types of information to CEDA to help
curate your data for long-term use: \
-   links to useful websites 
-   script to read in/plot the data 
-   documentation related to particular data formats
-   copies of project logos 
-   photographs of instruments and sites where the instrument is
    deployed 
-   calibration information 
-   links to related articles
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#how-do-i-supply-catalogue-record-information-and-other-information?-{#how-do-i-supply-catalogue-record-information-and-other-information?},2094,219
What is a data management plan (DMP)?,"A data management plan is a formal document that outlines how data are
to be handled both during a research project, and after the project is
completed. [ ]{style=""font-size: 10.5px;""}The goal of a data management
plan is to consider the many aspects of data
management, metadata generation and data preservation before the project
begins; this ensures that data are well-managed in the present, and
prepared for preservation in the future.
For NERC grants, the outline data management plan from the proposal will
be the start point for creation of the full data management plan. This
full DMP should be mutually agreed between the Data Centre and the
Principal Investigator within three months of the start date of the
grant.
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#what-is-a-data-management-plan-(dmp)?,727,119
Is my data worthy of archiving? {#Is my data worthy of archiving?},"Whatever the scale of the project it is useful to first determine the
value of the data and whether there are any requirements to formally
archive them. The  [NERC Data Value
CheckList](https://nerc.ukri.org/research/sites/data/policy/data-value-checklist/)
is an excellent and quick series of questions to help evaluate any data
to see if they should be archived. While aimed primarily at NERC funded
research, it can be used for non-NERC funded work too. Here is a flavour
of the questions to address:
-   Is there any potential onward benefit for the data to others?
-   Am I mandated by a research council policy or legal requirement to
    make the data available?
-   Do I wish to obtain recognition for the data product in its own
    right? If so, how can I get this?
-   For how long does the data need to persist?
-   Have the data been, or will they be, used in publications?
    Therefore, is there a case to archive the data for reproducibility
    of the results
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#is-my-data-worthy-of-archiving?-{#is-my-data-worthy-of-archiving?},977,164
What if I want to change my data? {#What if I want to change my data?},"CEDA strongly encourages to archive data that is complete and finalised.
However, we are aware that the data may change due to error\'s found
later on. In this instance you should get in touch with CEDA to discuss
further. New versions of the data can be added by indication of the
version number in the file name and information about what has changed
in the history comments of the metadata.
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#what-if-i-want-to-change-my-data?-{#what-if-i-want-to-change-my-data?},394,70
How can I send my data to CEDA? {#How can I send my data to CEDA?},"There are 3 ways you can send data to CEDA (HTTP, FTP and RSYNC). A
member of the CEDA team will be assigned to liaise with the data
provider to provide advice on data preparation and to help set up the
delivery and ingestion route.
[HTTP:](http://arrivals.ceda.ac.uk/) File uploader service which is
suitable for small scale data providers and short lived projects. A step
by step guide can be found
[here](https://help.ceda.ac.uk/article/4660-depositing-data-at-ceda-a-step-by-step-guide?preview=5b5af0c50428631d7a896169)
and [video tutorial](https://www.youtube.com/watch?v=b2iiQp1PeiU).
FTP: Suitable for small - medium scale data uploads for suitable
projects where RSYNC is not an option. More information can be found
[here](https://help.ceda.ac.uk/article/142-sending-data-to-ceda) and
[video tutorial](https://www.youtube.com/watch?v=KiZ_2xRlVPY).
RSYNC: This route is particularly suited to regular, automated data
uploading and is especially useful for very large files and dataset
transfers. More information can be found
[here](https://help.ceda.ac.uk/article/142-sending-data-to-ceda) and
[video tutorial](https://www.youtube.com/watch?v=RFSDyt88ICg).
How do I get data usage information?
CEDA will be able to provide data usage information for use in reporting
to funding agencies to data providers where suitable statistics are
available. All data usage information that we hold is covered by the
Data Protection Act and as such data providers can only have access to
the data use information for which permission has already been granted
by the CEDA facility user.
If you have a specific data usage reporting requirement please discuss
this prior to archiving data with CEDA to ensure that we can set up the
required access control mechanisms to capture the required information.
Otherwise it may not be possible for us to retrospectively either
provide the information required as we do not have a suitable record or
that user permission to make the information available to the data
provider may not be in place.
The information that we are able to provide will depend on the type of
access set (note - this is mainly for BADC and NEODC data, less
information is held about UKSSDC and IPCC-DDC users). CEDA groups access
as follows:
-   Open Access (Publicly available data)  - no registration is required
    so limited information available
-   Restricted Access - All registered users - usage logs provide
    additional anonymised userbase profiling information (e.g. research
    fields supported by dataset, countries of users making use of
    dataset). Although CEDA is able to identify individual user accounts
    within internal logs, user details are not available to data
    providers.
-   Restricted Access - Application required - specific research
    descriptions available to permit authorisation of access to
    restricted resources in addition to user base profiling
  ------------------------- ------------------------------------------------------------------------------------------ ------------------------------------------
  Available Information     Use                                                                                        Covers
  Download statistics       number of files and volume for a set period of time.                                       public, registered user, restricted user
  Userbase information      To permit anonymised profiling of user base for dataset - e.g. research fields supported   registered user, restricted user
  Application information   For dataset authorisers to determine if access should be granted to restricted resource    restricted user only
  ------------------------- ------------------------------------------------------------------------------------------ ------------------------------------------
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#how-can-i-send-my-data-to-ceda?-{#how-can-i-send-my-data-to-ceda?},3810,473
Where do I go if my data is not suitable to be stored at CEDA? {#Where do I go if my data is not suitable to be stored at CEDA?},"NERC has a network of 5 environmental  [data
centres](https://nerc.ukri.org/research/sites/data/) covering a range of
disciplines including: 
-   British Oceanographic Data Centre (Marine)
-   Centre for Environmental Data Analysis which includes:
    -   British Atmospheric Data Centre (Atmospheric)
    -   NERC Earth Observation Data Centre (Earth observation)
    -   UK Solar System Data Centre (Solar and space physics)
-   Environmental Information Data Centre (Terrestrial and freshwater)
-   National Geoscience Data Centre (Geoscience)
-   Polar Data Centre (Polar and cryosphere)
The range of data held within the data centres is vast, covering all
aspects of environmental science. Some centres also hold physical
specimens and sample materials collected during NERC\'s activities, as
well as material supplied by third parties (sometimes under statute).
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#where-do-i-go-if-my-data-is-not-suitable-to-be-stored-at-ceda?-{#where-do-i-go-if-my-data-is-not-suitable-to-be-stored-at-ceda?},868,115
Can I archive compressed and tar-ed data? {#Can I archive compressed and tar-ed data?},"Moving and storing files can often be made more efficient by compressing
the files themselves (e.g. .gz, .bz, .zip), sometimes in addition to
\""tar\"" which is used to ball together various files.
For archiving purposes, CEDA does not generally apply such compressions
or balling together of files. Where this has been done it is usually for
one of the the following reasons:
-   To make it easier for end users to access and download large numbers
    of files and/or large individual files. For example, NIMROD rain
    radar products tend to be gzipped and then tarred together into
    daily tarballs to avoid users having to deal with many hundreds of
    files for any given day, reduce the overall number of file objects
    in the archive and to reduce the overall volume of the data in the
    archive.
-   For consistency with older data. If a dataset have always been
    compressed we will generally continue to compress additions to that
    dataset so that anyone with automated processing of the data is not
    suddenly disrupted. 
-   To follow an established convention. For example Sentinel data uses
    zip as part of its SAFE packaging. The sentinel user community has
    software that is expecting the zipped version. 
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#can-i-archive-compressed-and-tar-ed-data?-{#can-i-archive-compressed-and-tar-ed-data?},1242,202
My data is stored on a group/project workspace does that mean it is stored on the archive? {#My data is stored on a group/project workspace does that mean it is stored on the archive?},"CEDA supports projects through shared storage spaces such as JASMIN
group workspaces or FTP project spaces. Users of these services should
understand that:
-   this is NOT the archive - placing data into these areas will not
    constitute having deposited data in the CEDA archive
-   the group-workspaces/project-spaces are NOT managed by CEDA and so
    content should be considered at risk
However, it is possible to prepare a dataset in these areas for eventual
ingestion into the archive. If you wish to do this please contact your
CEDA support officer in the first instance to discuss ingestion into the
archive as it may be possible to ingest directly from these areas.
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#my-data-is-stored-on-a-group/project-workspace-does-that-mean-it-is-stored-on-the-archive?-{#my-data-is-stored-on-a-group/project-workspace-does-that-mean-it-is-stored-on-the-archive?},678,111
What licence will my data be made available to other under? {#What licence will my data be made available to other under?},"Generally the data produced by NERC funded research will be released
under the open government licence, as per NERC data policy. Data from
other sources may use another licence, but we strongly encourage open
licences such as creative commons licence. 
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#what-licence-will-my-data-be-made-available-to-other-under?-{#what-licence-will-my-data-be-made-available-to-other-under?},253,40
Who owns my data? {#Who owns my data?},"Ownership of the data lies with the data creator, but NERC has the right
to exploit data resulting from NERC funded research. Data creators will
be required to agree to the data deposit conditions before the data are
added to the archive.
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#who-owns-my-data?-{#who-owns-my-data?},239,42
What levels of archiving service do CEDA offer? {#What levels of archiving service do CEDA offer?},"CEDA offers 3 levels of data archiving (reference, structured and
compatible). Once CEDA has decided the data is suitable for deposit, one
of the following categories will be assigned depending on the level of
interest to the scientific community. The categories are reference,
structured, compatible,
**Reference**: These are data that needs to be discoverable and
downloadable, but is left to the user to work out some of the usability
issues. CEDA will make a catalogue entry and add the data files to the
archive. This is a suitable solution if there is not likely to be mass
interest in the data and its principal use will be to provide evidence
for publication. Minimum qualification: a paper referencing the
dataset.  
**Structured:** As well as the reference, these data are in a community
supported format, with a defined file and directory naming convention.
It may also have specified file level metadata attribute conventions.
The data are more useable by third parties and is suitable for a dataset
where there is an intention to make the data more reusable. 
Minimum qualification: evidence of use of similar datasets by CEDA core
communities. 
**Compatible:** In addition to being structured, these data are
connected to specified community tools or systems that enable better
discovery or processing. For example, climate model data in ESGF, MIDAS
data in the WPS or aircraft data in the Flight Finder.
Minimum qualification: evidence of use of similar datasets by CEDA core
communities and community tool specifications. Some evidence that the
data will fit the tools.
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#what-levels-of-archiving-service-do-ceda-offer?-{#what-levels-of-archiving-service-do-ceda-offer?},1586,254
How long will CEDA keep my data? {#How long will CEDA keep my data?},"Most unique observations of the environment for which the CEDA archive
is the primary repository, data will be kept indefinitely. CEDA are
constrained by resources, man power and archive size, so we do have to
make some chooses on what we keep and whether it is selected for
archiving in the first place.
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#how-long-will-ceda-keep-my-data?-{#how-long-will-ceda-keep-my-data?},305,53
Can I restrict who can use my data? {#Can I restrict who can use my data?},"For NERC funded projects a MAXIMUM of 2 YEARS embargo can be put in
place for project participants to work exclusively on, and publish the
results of, the data they have collected. Access to all data submitted
to the data centre will be restricted to project participants for
following the data production date, after which they will be released
into the public domain. During the embargo period, access may be
extended to external collaborators who have been authorised by the PI or
their delegated authority. Potential users of the archive will be
required to agree to the Conditions of Use.
For non NERC projects/datasets this depends on the licencing for that
dataset. We strongly encourage open goverment licencing and are less
likely to archive data with a locked down licence.
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#can-i-restrict-who-can-use-my-data?-{#can-i-restrict-who-can-use-my-data?},784,130
How is dataset access controlled? {#How is dataset access controlled?},"CEDA recognises both the requirements to make data as open and as free
possible for use by the wider community but also the need to protect
data providers\' IPR and right to first use of data provided. For all
NERC funded research data providers are covered by the NERC Data Policy,
whereby access can be limited to a maximum of 2 years after the date of
collection/production - i.e. the date when the data were collected for
observational data or the date when the simulation code produced data. 
CEDA therefore has various mechanisms to control access to data held in
our archives to ensure that these two requirements are managed
appropriately over time. The levels of control are as follows:
NOTE - conditions of use for the data are independent of the access
control mechanism and all data users are required to adhere to the
general CEDA conditions of use and the specific data licences pertaining
to the data that they wish to use.
  ----------------------------------- -----------------------------------
  Publically available data           Data are freely available to
                                      access, with no requirement for the
                                      user to register
  Registered user accessible data     Data are freely available to access
  Restricted data for limited period  Data are restricted for a set
                                      period of time, for example until 2
                                      years after the date of production,
                                      after which access can be opened up
                                      to public/registered user. Access
                                      to such datasets is usually
                                      verified by a designated authoriser
                                      for applications.
  Permanently restricted\             Typically only applying to
  \                                   third-party datasets that CEDA
  \                                   obtains for use by the wider
  \                                   research community where access
  \                                   needs to be controlled to ensure
  \                                   use is within permitted conditions
                                      (e.g. only UK academics; those with
                                      NERC funding). Applications for
                                      these data are usually verified by
                                      CEDA staff.\
  ----------------------------------- -----------------------------------
",https://help.ceda.ac.uk/article/4661-depositing-data-faqs#how-is-dataset-access-controlled?-{#how-is-dataset-access-controlled?},2598,296
Introduction,"On the [CEDA Help site](https://help.ceda.ac.uk/) you\'ll find a range
of help articles to answer many user questions about using [CEDA archive
and services](http://www.ceda.ac.uk/) and the [JASMIN
infrastructure](http://www.jasmin.ac.uk/).
If you are a new or existing user of the CEDA archives and wondering
where to start, please see the related articles below or use the search
function to find other relevant help articles.
The following CEDA Archive Users Guide is designed to help new and
returning users find, access and make use of data held in the CEDA
archives. This page applies mainly to users of the
[atmospheric](http://archive.ceda.ac.uk/) and [earth
observations](http://archive.ceda.ac.uk/) archives, but users of the
[UKSSDC](https://www.ukssdc.ac.uk/) and
[IPCC-DDC](http://www.ipcc-data.org/) may also find useful information
here.
",https://help.ceda.ac.uk/article/136-users-quick-start#introduction,853,113
Dataset specific question,"Whilst the following \"" [CEDA Archive User
Guide](https://help.ceda.ac.uk/category/12-using-ceda-data-archive)\""
pages have been designed to address many common questions, users will
[find answers to most dataset specific
questions](https://help.ceda.ac.uk/article/234-data-help-docs) on the
relevant dataset catalogue pages. Please see the \""[Discovering
Datasets](https://help.ceda.ac.uk/article/137-ceda-data-catalogue)\""
section for details on how to do this.
------------------------------------------------------------------------
-   [Archive Users Quick Start
    Guide](https://help.ceda.ac.uk/article/136-users-quick-start)
-   [User Registration and Account
    Administration](https://help.ceda.ac.uk/article/39-ceda-account)
-   [CEDA account](https://help.ceda.ac.uk/article/39-ceda-account)
-   [Finding
    data](https://help.ceda.ac.uk/article/137-ceda-data-catalogue)
-   [Data Access - public and restricted
    data](https://help.ceda.ac.uk/article/98-accessing-data)
-   [Navigating the CEDA
    archives](https://help.ceda.ac.uk/article/100-navigating-ceda-archives)
-   [Download Data from CEDA
    archives](https://help.ceda.ac.uk/article/99-download-data-from-ceda-archives)
-   [Data Help Docs](https://help.ceda.ac.uk/article/234-data-help-docs)
-   [Data Tools: preparation,
    visualisation](https://help.ceda.ac.uk/article/101-data-tools-preparation-visualisation)
-   [Citing Data in
    Publications](https://help.ceda.ac.uk/article/102-data-citation)
-   [File Names
    Explained](https://help.ceda.ac.uk/article/103-filenames)
-   [File Formats
    Demystified](https://help.ceda.ac.uk/article/104-file-formats)
------------------------------------------------------------------------
",https://help.ceda.ac.uk/article/136-users-quick-start#dataset-specific-question,1722,106
Further Assistance,"CEDA runs a dedicated helpdesk to assist users in a variety of ways from
account issues, finding and making use of specific data to scientific
and data format enquiries. Users are asked to check this user guide,
dataset and format documentation initially as most answers will be
already documented, but are welcome to [contact
us](http://www.ceda.ac.uk/contact/) for further assistance.
We will aim to give an initial response to the query within one working
day and to resolve the query as quickly as possible, depending on
available resources and query complexity.
",https://help.ceda.ac.uk/article/136-users-quick-start#further-assistance,567,89
Synda,"Synda stands for Synchronise Data it is a command line utility that can
manage the downloading of data through the Earth System Grid Federation
(ESGF). Synda is composed of two components a transfer module (sdt) and
a post-processing module (sdp). The process for installing on JASMIN the
transfer module is outlined here. Full documentation on usage can be
found in the main [Synda
documentation](http://prodiguer.github.io/synda/index.html). 
The Synda installation instructions can be found
[here](http://prodiguer.github.io/synda/sdt/src_install.html#installation). However,
some useful additional installation instructions for the transfer module
are provided below. 
",https://help.ceda.ac.uk/article/4664-synda-synchronise-data#synda,673,85
**Before downloading**:,"1.  Make a directory where you want the software installed 
2.  Create a \`setup_paths.sh\` file in the synda directory with the
    following content (uses a general version of ) :  
3.  Before running  synda  this file must be executed every time
<!-- -->
    $ chmod u+x ./setup_paths.sh 
    $ source setup_paths.sh
",https://help.ceda.ac.uk/article/4664-synda-synchronise-data#**before-downloading**:,320,50
Download,"<div>
Now following the online Synda
[instructions](http://prodiguer.github.io/synda/sdt/src_install.html#installation):
</div>
1.  wget the install script, and make it executable. 
2.  run the install script:
<!-- -->
    $ ./install.sh -d <version-number> -t <directory path to where sdt is>
",https://help.ceda.ac.uk/article/4664-synda-synchronise-data#download,294,35
Configurations,,https://help.ceda.ac.uk/article/4664-synda-synchronise-data#configurations,0,0
(1) In the sdt configurations file,"    sdt/conf/sdt.conf:
<div>
Set:
</div>
-   Default directory path for selection files: selection_path=\<\>
    default=synda/sdt/selection
-   Default directory path for default selection
    files: default_path=\<\> default=synda/sdt/selection
-   Default data directory path: data_path=\<\> default=synda/sdt/data;
    likely want to change this
-   Default database directory: db_path=\<\> default=synda/sdt/db/
-   Change default index to be CEDA by commenting out DKRZ and
    uncommenting CEDA
",https://help.ceda.ac.uk/article/4664-synda-synchronise-data#(1)-in-the-sdt-configurations-file,502,55
(2) In the sdt credentials file,"    sdt/conf/credentials.conf
-   Supply a valid openid and username
",https://help.ceda.ac.uk/article/4664-synda-synchronise-data#(2)-in-the-sdt-credentials-file,69,8
MOLES 3 Editor User Guide,"::: {#wikipage}
[Original trac
page](http://team.ceda.ac.uk//trac/ceda/wiki/opman/MOLESEditorUserGuide)
::: wiki-toc
1.  [MOLES 3 Editor User Guide](#MOLES3EditorUserGuide)
    1.  [Overview of MOLES 3 structure](#OverviewofMOLES3structure)
    2.  [MOLES2 -\> MOLES3 and the archive](#MOLES2-MOLES3andthearchive)
    3.  [Editing notes for MOLES3
        content](#EditingnotesforMOLES3content)
    4.  [A Note About Adding DOIs](#ANoteAboutAddingDOIs)
    5.  [Helpful Hints & Tools](#HelpfulHintsTools)
    6.  [](#a)
:::
This guide is a collection of notes on editing MOLES3 records with the
new Django interface. The old MOLES2 guide is available
[here?](http://team.ceda.ac.uk//trac/ceda/wiki/opman/MOLES2EditorUserGuide){.missing
.wiki} .
You can go straight to the MOLES3 editor interface at: [[]{.icon}
http://catalogue.ceda.ac.uk/admin/](http://catalogue.ceda.ac.uk/admin/){.ext-link}
. Once logged in you can also use the \""edit record\"" link on each
record.
There are a short set of videos explaining various aspects of MOLES
below for quick guides.
Further detailed notes are added from this page too. In particular there
are lists of the questions to refer to when editing/creating MOLES3
content to ensure that the content conform to the NERC Metadata
Guidelines where possible.
",https://help.ceda.ac.uk/article/4138-moles-3-editor-user-guide,1294,151
Overview of MOLES 3 structure {#OverviewofMOLES3structure},"MOLES3 has the following principal components:
[![moles3 outline
structure](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5820776a90336042578d2bd3/IMAGE.png ""moles3 outline structure""){width=""600px""}](http://team.ceda.ac.uk//trac/ceda/attachment/wiki/opman/MOLESEditorUserGuide/moles3_layout.png)
You can find the presentation put together for the MOLES3 [evaluation
here](http://team.ceda.ac.uk/trac/ceda/attachment/wiki/opman/MOLESEditorUserGuide/m2-m3_presentation_AS.pptx)
- useful for further background information and comparison with MOLES2.
",https://help.ceda.ac.uk/article/4138-moles-3-editor-user-guide#overview-of-moles-3-structure-{#overviewofmoles3structure},586,34
MOLES2 -\> MOLES3 and the archive {#MOLES2-MOLES3andthearchive},"Having migrated from MOLES1-\>MOLES2 and finally to MOLES3 over the
years, there is a need to review the MOLES3 content. In particular if
the Observation level records are fit for purpose (i.e. really represent
items in the archive). [This
video](http://team.ceda.ac.uk/trac/ceda/attachment/wiki/opman/MOLESEditorUserGuide/m3_v_m2_v_archive.mp4)
gives a quick example and a few suggestions about things to consider
when carrying out a review of a dataset collection in MOLES3 and seeing
if it truly matches what is in the archive.
The ideal that we should be aiming for is a set of Observations
(datasets in the user view) that cover all of the CEDA archive holdings
in a unique fashion - i.e. the files/resources (if physical data)
covered by an observation should be:
1.  A cohesive set of data - i.e. they have common features, e.g. all
    from one instrument for a given campaign; from the same flight; from
    the same period (e.g. year). Bascically, they share a common set of
    answers to who, what, when, where, how.
2.  There is one point below which all the collection can then be found
    which provides the \""download\"" point to connect to from the
    Observation
3.  If they were to be citable with a DOI the Observation makes a
    sensible landing page (though you can also add DOIs to an
    Observation Collection too, but this requires all members of the
    collection to be well defined -i.e. this is hard to accomplish!)
4.  They share the same security requirements - e.g. they are all open
    data or covered by the same access group and will change at the same
    time
Unfortunately, when we went from MOLES1-\>MOLES2 there was a
proliferation of Deployment records created as a bunch of DPTs and Obs
Station records were split off into different Deployments. Whilst some
of that was amended in MOLES2 by aggregating some deployments together,
lots were not and have been migrated over to Observations in MOLES3. In
addition, the focus of MOLES2 was at the Data Entity level and often the
Deployment level was left incomplete and things were further complicated
by the Activity, DPT and Obs Station records also being points which
connected to the archive.
**[ MOLES3 breaks away from this by ONLY permitting Observations to
connect to the archive. ]{.underline}**
When examining the archive, the [CEDA archive structure
model](http://ceda-internal.helpscoutdocs.com/article/4158-ceda-archive-structure-model){.wiki}
may be a useful point of reference.
",https://help.ceda.ac.uk/article/4138-moles-3-editor-user-guide#moles2--\>-moles3-and-the-archive-{#moles2-moles3andthearchive},2485,388
Editing notes for MOLES3 content {#EditingnotesforMOLES3content},"  -------------------- ------------------------ -------------------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------------------------------------
  **User view name**   **MOLES3 name**          **Link to video**                                                                                                          **Link to Metadata Guidelines**                                                                                                       **Note**
  Dataset              Observation              [Video here](http://team.ceda.ac.uk/trac/ceda/attachment/wiki/opman/MOLESEditorUserGuide/m3_editting_obsCollections.mp4)   [observation guidelines here](http://ceda-internal.helpscoutdocs.com/article/4187-observation-records){.wiki}                         This is where to link to the archive and have the access application link. Link these to Project type of project records (not Programme)
  Dataset Collection   Observation Collection   [Video here](http://team.ceda.ac.uk/trac/ceda/attachment/wiki/opman/MOLESEditorUserGuide/m3_editting_obsCollections.mp4)   [observation collection guidelines here](http://ceda-internal.helpscoutdocs.com/article/4303-observation-collection-records){.wiki}   
  Project              Project                  [Video here](http://team.ceda.ac.uk/trac/ceda/attachment/wiki/opman/MOLESEditorUserGuide/m3_editting_project.mp4)          [project guidelines here](http://ceda-internal.helpscoutdocs.com/article/4154-opmanmoleseditoruserguideproject-ceda){.wiki}           This can have Programme-\>Project-\>Field Campaign hierarchy. Link to projects not programme level records to Observation Collections
  Instrument           Instrument                                                                                                                                          [instrument guidelines here](http://ceda-internal.helpscoutdocs.com/article/4185-instrument-records){.wiki}                           
  Platform             Platform                                                                                                                                            [platform guidelines here](http://ceda-internal.helpscoutdocs.com/article/4333-platform-records){.wiki}                               
  Computation          Computation                                                                                                                                         [computation guidelines here?](http://team.ceda.ac.uk//trac/ceda/wiki/opman/MOLESEditorUserGuide/computation){.missing .wiki}         
  Composite Process    Composite Process                                                                                                                                   [composite guidelines here?](http://team.ceda.ac.uk//trac/ceda/wiki/opman/MOLESEditorUserGuide/composite){.missing .wiki}             
  -------------------- ------------------------ -------------------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------------------------------------
",https://help.ceda.ac.uk/article/4138-moles-3-editor-user-guide#editing-notes-for-moles3-content-{#editingnotesformoles3content},3613,111
A Note About Adding DOIs {#ANoteAboutAddingDOIs},"DOIs can be minted for Observation and Observation Collections.
Instructions on minting DOIs are given on the [DOI opman
page](http://ceda-internal.helpscoutdocs.com/article/4359-datacitation-ceda){.wiki}
",https://help.ceda.ac.uk/article/4138-moles-3-editor-user-guide#a-note-about-adding-dois-{#anoteaboutaddingdois},205,20
Helpful Hints & Tools {#HelpfulHintsTools},"Here\'s a list of useful things to help speed up your MOLES editing:
-   Need to edit a number of fields accross a number of similar records?
    Open them all up in seperate tabs in your browser.. then do one
    field for all the records at a time; then the next field for all
    opened records - this way you\'ll be more consistent and you\'ll not
    get caught weighting whilst saving after each edit.
-   A couple of handy tools (for Chrome users):
  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------------
  [[]{.icon} Tab Scissors](https://chrome.google.com/webstore/detail/tab-scissors/cdochbecpfdpjobpgnacnbepkgcfhoek){.ext-link}                                            splits your browser into two side-by-side windows on the selected tab
  [[]{.icon} Check my links](https://chrome.google.com/webstore/detail/check-my-links/ojkcdipcgfaekbeaelaapakgnjflfglf){.ext-link}                                        A quick way to check all the links (and highlights them too!) on your page aren\'t broken whist you work
  [[]{.icon} Link Clump](https://chrome.google.com/webstore/detail/linkclump/lfpjkncokllnfokkgpkobnkbkmelfefj?utm_source=chrome-app-launcher-info-dialog){.ext-link}      A FAST way to open lots of links to pages you need to check - simply Click and drag selection of lots of link to open as a bunch of new tabs
  [[]{.icon} Tab-sidebar](https://chrome.google.com/webstore/detail/tab-sidebar/egmcoemabahnkbkfgdifiafkhebcfpfo?utm_source=chrome-app-launcher-info-dialog){.ext-link}   A handy way to keep track (and switch to!) of which tab is which when you\'ve got lots open and so can\'t read their names anymore in Chrome\|\|
  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------------
",https://help.ceda.ac.uk/article/4138-moles-3-editor-user-guide#helpful-hints-&-tools-{#helpfulhintstools},2231,190
{#a},":::
",https://help.ceda.ac.uk/article/4138-moles-3-editor-user-guide#{#a},4,1
CEDA Satellite Data Finder Refining Results,"At the moment there are more than 1.5 million satellite scenes in the
archive and this is growing every day.
In order to get the most use out of the Satellite Data Finder, it is
important to refine your search. The number of hits, for your current
search parameters, is displayed below the Export Results button. Only
1000 will be drawn on the map or are extractable via the Export
Results dialog to increase performance.
::: {.section .callout-blue .dashed}
",https://help.ceda.ac.uk/article/4494-cedageosearch-refining-results,459,79
Jump to section:,,https://help.ceda.ac.uk/article/4494-cedageosearch-refining-results#jump-to-section:,0,0
,"-   [Temporal Filter](#temporal-filter)
-   [Change Map Centre](#change-map-centre)
-   [Rectangle Search](#rectangle-search)
-   [Satellite Filter](#satellite-filter)
-   [Apply Filters](#apply-filter)
-   [Clear Filters](#clear-filter)
-   [Related Articles](#related-articles)
:::
",https://help.ceda.ac.uk/article/4494-cedageosearch-refining-results#,284,23
Temporal Filter,"This element will allow you to refine your search with temporal
parameters. You can enter a start time and end time, click **Apply
Filters** and the interface will search for satellite data that was
recorded within that period.
To help you see the distribution of data over time, there is a
convenient histogram underneath the start and end time box. A larger bar
means that more data files were recorded within that time period. Note
that the histogram displays counts for ***all* available data**, and is
not modified by changing the filters.
Clicking the histogram bars is a shortcut to selecting a specific year
but it will likely be necessary to refine your search further to get the
most benefit from the interface.
",https://help.ceda.ac.uk/article/4494-cedageosearch-refining-results#temporal-filter,722,123
Change Map Centre,"This element allows you to search for a location. If it finds what
you\'re searching for, the map will centre on the matching location. You
can enter several different types of things to find a location. For
example, all of the things below are valid location searches:
-   Place names (e.g. \""Scotland\"", \""London\"", or \""Turl Street\"")
-   Postcodes (e.g. \""90210\"" or \""OX11 0QN\"")
-   Coordinates (e.g. \""51.2W, 31.0N\"" or \""51.7595, -1.2325\"")
",https://help.ceda.ac.uk/article/4494-cedageosearch-refining-results#change-map-centre,449,71
Rectangle Search,"Found in the top right corner of the map. Toggle **On** by clicking the
button. When turned on, draw on the map using the cursor. Once your
drawing is complete, the map will search for results which intersect the
box drawn.
Only 100 are drawn to avoid overburdening the map.
You can resize the rectangle using the white nodes. You can also move
the rectangle by grabbing one of the sides, marked by the black line.
The coordinates of the NW and SE corners are automatically updated in
the **Rectangle Search** pane on the left hand side of the screen.
The **Clear Results** button will remove all the coloured polygons from
the screen. This can make it easier to edit the rectangular bounding
box. Simply click **Apply Filters** to get results matching your
bounding box.
The rectangle and all other filters can be removed by clicking \""Clear
Filters\"".
When the rectangle tool is **On**, panning is disabled on the map. Turn
the tool off to resume panning.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5981d8442c7d3a73488b921f/file-cMjanOwX3y.png)
",https://help.ceda.ac.uk/article/4494-cedageosearch-refining-results#rectangle-search,1091,168
Satellite Filter,"This element provides checkboxes to filter down the specific satellites
which you want to retrieve data from. The number displayed to the right
of the element indicates the number of datasets available given the
current filters.
For example:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5981d859042863033a1b9279/file-3OHflsSv9W.png)
Given the current selection, there are no datasets from the Landsat
Missions 5 and 7. You can try changing the time range or search area.
",https://help.ceda.ac.uk/article/4494-cedageosearch-refining-results#satellite-filter,513,64
Apply Filters {#apply-filter},"This button simply applys any search criteria you may have entered. For
example, if you have supplied a date range and a keyword, clicking
\""Apply Filters\"" will apply these criteria to your search.
Certain things are done automatically - the Geographical Search will
reresh your results once you click **Go** and the Satellite Filter is
applied without needing to click \""Apply Filters\"".
",https://help.ceda.ac.uk/article/4494-cedageosearch-refining-results#apply-filters-{#apply-filter},390,62
Clear Filters {#clear-filter},"This clears the map of all objects, data polygons and rectangles, and
resets all the filters to a neutral state.
",https://help.ceda.ac.uk/article/4494-cedageosearch-refining-results#clear-filters-{#clear-filter},113,20
Citing Data in Publications,"Data from CEDA should be cited in any article or presentation making use
of the data. A full citation, akin to academic papers, will be found on
the dataset catalogue page. 
A typical citation looks like this:
> Met Office (2006): MIDAS: UK Hourly Weather Observation Data. NCAS
> British Atmospheric Data Centre, *date of
> citation*. [https://catalogue.ceda.ac.uk/uuid/916ac4bbc46f7685ae9a5e10451bae7c](https://catalogue.ceda.ac.uk/uuid/916ac4bbc46f7685ae9a5e10451bae7c/)
When citing in a publication or presentation please replace the \""*date
of citation\""* part with the date that you downloaded the data from CEDA
as that will reference the state of the archive on the day of download. 
",https://help.ceda.ac.uk/article/102-data-citation,692,96
Data with DOIs,"Data held in the CEDA archives may be assigned a Digital Object
Identifier - or DOI. A DOI enables scientists to cite datasets in the
same manner as a scientific journal article, thereby enabling credit to
be assigned to the dataset creators, and ensuring the discovery,
permanence and stability of the dataset. For further details on DOIs
and, if you are a data provider, how to obtain your own DOI for data
archived at a NERC data centre please see the [DOI Guidelines for
researchers](https://eds.ukri.org/sites/default/files/2024-01/Data%20Citation%20Guidelines.pdf)
or refer to the full [NERC EDS DOI
Policy](https://eds.ukri.org/sites/default/files/2024-01/NERC%20EDS%20DOI%20Policy.pdf)
This is also covered in our \""[Depositing data
FAQ\'s](https://help.ceda.ac.uk/article/4661-depositing-data-faqs)\""
page.
A typical citation to use for data with a DOI is
> Met Office; Hollis, D.; Carlisle, E.; Kendon, M.; Packman, S.;
> Doherty, A. (2024): HadUK-Grid Gridded Climate Observations on a 1km
> grid over the UK, v1.3.0.ceda (1836-2023). NERC EDS Centre for
> Environmental Data Analysis, *18 July 2024*.
> doi:10.5285/b963ead70580451aa7455782224479d5. <https://dx.doi.org/10.5285/b963ead70580451aa7455782224479d5>
Note, in this case the Date of Citation to use is the one given in the
citation as this is the date the DOI was assigned to the dataset and
thus the data are then fixed.
",https://help.ceda.ac.uk/article/102-data-citation#data-with-dois,1393,194
Additional Requirements,"Some individual datasets may also require additional acknowledgement.
This should be indicated within the licence for the data and/or the
dataset catalogue page.
For example for Met Office data the NERC - Met Office agreement required
that:
>  \""scientific papers must give due credit to Met Office either through
> acknowledgement or if the data provided a significant basis of the
> work, co-authorship.\""
------------------------------------------------------------------------
\
Some CEDA Services also require slightly different acknowledgement.
For example:
-   CEDA Trajectory Service:
-   -   \""I am grateful to the Centre for Environmental Data Analysis,
        for providing access to calculated trajectories using data from
        the European Centre for Medium Range Weather Forecasts.\"" 
    -   \""I wish to thank the Centre for Environmental Data Analysis for
        the use of its calculated trajectory data. I also thank the
        European Centre for Medium Range Weather Forecasts who supplied
        the initial data via CEDA.\"" 
    -   \""Thanks to the Centre for Environmental Data Analysis for the
        calculation of trajectories and access to ECMWF data.\""
Please contact the  [CEDA Helpdesk](http://www.ceda.ac.uk/contact/) if
you require advice.
",https://help.ceda.ac.uk/article/102-data-citation#additional-requirements,1280,174
New Datamad,"login: [Datamad](https://datamad.ceda.ac.uk/)
",https://help.ceda.ac.uk/article/4969-new-datamad,46,2
What is Datamad?,"[DataMAD](https://cedadev.github.io/datamad2/) is a NERC system for
managing NERC grants and data archiving amongst the NERC
datacentres. The application provides processes to claim grants from
data producing projects and generate tickets in JIRA to manage the
pipeline from project start to data production, delivery and archiving.
",https://help.ceda.ac.uk/article/4969-new-datamad#what-is-datamad?,333,45
How do I get a login?,"To request a login you will need to contact admin: Wendy
She will then ask you to login and reset your password
",https://help.ceda.ac.uk/article/4969-new-datamad#how-do-i-get-a-login?,112,22
Does Datamad have a user guide?,"The user guide can be found
[here](https://cedadev.github.io/datamad2/tutorial.html)
",https://help.ceda.ac.uk/article/4969-new-datamad#does-datamad-have-a-user-guide?,85,7
How do I find CEDA\'s grants?,"Grants can be filtered by Data Centre at the top and the side.
Individual grants can be searched.
",https://help.ceda.ac.uk/article/4969-new-datamad#how-do-i-find-ceda\'s-grants?,98,18
How do I create a DMP?,"To create a DMP you firstly need to add data products. This can be added
on the right-hand side: 
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fbbd56e4cedfd001610f920/file-xw86AmUpX7.png)Next,
add in the relevant data product information note that at CEDA usually
the Digitial data products and model source code sections are used. You
will also see other options.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fbbd5d94cedfd00165b30c5/file-ix0cE4yUh6.png)
The next step is to go back on to the grant page and click \'Generate
Document from template\' on the right-hand side:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fbbd62b46e0fb0017fceb0c/file-LmbVmkF6CH.png)Select
the relevant template. A downloadable document will be produced. Make
any amendments that are needed and add the DMP to the grant. Note this
is no longer a google doc so you will need to save it on your desktop to
upload.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fbbd6904cedfd00165b30c9/file-6iWM3cIHFQ.png)Add
the relevant tags. I usually add the version number and draft.
Once the PI has agreed to the DMP update your product information and
attach the final DMP. Also, make a note on JIRA and add the DMP agreed
component. 
",https://help.ceda.ac.uk/article/4969-new-datamad#how-do-i-create-a-dmp?,1373,156
How do I link to JIRA?,"On the grant page on the right-hand side, there is a button to convert
the grant to a JIRA ticket. The Grant will then appear in JIRA under the
\'NEW unassigned projects\' in the CEDA dashboard
",https://help.ceda.ac.uk/article/4969-new-datamad#how-do-i-link-to-jira?,194,36
How do I claim an unassigned grant?,"In the unassigned filters, there are grants associated with every
datacentre. Once you have confirmed the grant is relevant to CEDA by
checking the information and the ODMP, you can \'claim\' the grant and
\'convert to JIRA\'. Make sure you do remember to convert to JIRA as
this is the point where grants can get lost.
",https://help.ceda.ac.uk/article/4969-new-datamad#how-do-i-claim-an-unassigned-grant?,320,56
CEDA Archive Terms and Conditions,"The Centre for Environmental Data Analysis (CEDA) Archive (hereafter the
CEDA Archive) is one of the designated data centres operated for the
Natural Environment Research Council (NERC). This service is run by the
Science and Technology Facilities Council (STFC). Both NERC and STFC are
part of the UK Research and Innovation (UKRI). The CEDA Archive refers
to both the data archive and its associated services.
These Terms and Conditions of Access are presented to new users when
they register using the the CEDA Archive Accounts.
These Terms and Conditions of Access are subject to modifications and
additions, for example to reflect changes to the CEDA Archive and
services. However, we anticipate they will change rarely, if at all. You
will be notified of any modifications to these conditions using your
registered email address. Any modifications or additions to these
conditions will be effective immediately. If you do not agree to the
modified conditions, you should discontinue your use of the CEDA Archive
where required to log in. Note that in all cases the Terms and
Conditions include an undertaking by you as to your conduct, and an
agreement by you as to what we can do with your personal data.
Please read these terms and conditions. If you do not agree to these
Terms and Conditions, we will not be able to allow you to register
and/or use the service requested. These Terms and Conditions are offered
only in English.
The following classes of ""CEDA user"" are covered in this document:
1.  Standard user of the CEDA Archive and associated services.
2.  Resource authorisers and nominated deputies with responsibility for
    approving or rejecting requests for access to a particular resource
    or service (e.g. a restricted dataset or service), including those
    with ""view only"" access to see user details, but not approving
    applications.
All use of the CEDA Archive and associated services requires users to
agree the general terms and conditions, including the acceptance of the
conditions under which the service is operated.
",https://help.ceda.ac.uk/article/4640-ceda-archive-terms-and-conditions,2058,332
General Terms and Conditions,"You agree:
1.  To observe the [JANET Acceptable Use
    Policy](https://community.ja.net/library/acceptable-use-policy).
2.  Not to disrupt the working of the service, for example by knowingly
    introducing malicious software into it, nor to try to breach its
    security or use resources which aren\'t assigned to you.
3.  Not to interfere with other users\' work, corrupt their data or
    invade their privacy.
4.  Not to infringe copyright or other intellectual property rights.
5.  Not to take or access data from any database or dataset without
    explicit or implied license.
6.  Not to use the service for illegal or immoral purposes, such as
    theft, fraud, drug-trafficking, money-laundering, terrorism,
    pornography, violence, cruelty, incitement to racial hatred,
    prostitution, paedophilia, or for offensive, obscene, abusive,
    menacing, defamatory or insulting behaviour.
7.  To observe any further requirements as laid down if you fall into
    one of the additional classes of CEDA Archive user given below.
You accept:
1.  That the service is provided \""as is\"" and we can\'t guarantee 100%
    perfection. In legal terms, this means that we are excluding all
    warranties and conditions applying to the service, including those
    implied by law. We are not liable if things go wrong and you suffer
    damage as a result, although if our negligence results in anyone\'s
    death or personal injury we do not limit or exclude our liability
    for that.
2.  That you are responsible for your use of any advice or information
    we may give you. We will take reasonable steps to ensure that it\'s
    true and useful, but we can\'t guarantee this. In legal terms, this
    means that we expressly disclaim any and all liability for all
    representations, statements, conditions or warranties to that or any
    other effect except to the extent that such liability may not be
    lawfully excluded.
3.  That we may make changes in the service.
4.  That you alone are responsible for what you do when using the
    service. If you break the law you alone must answer for it, and if
    you cause damage to anyone else, you alone are liable, not us.
5.  That the use of the service by nationals of certain countries is
    controlled by special regulations laid down by the UK Government in
    connection with the [Wassenaar
    Arrangement](https://www.wassenaar.org/).
6.  That we may make reasonable changes to these Terms and Conditions at
    any time, and, once we have posted those changes on our website and
    emailed you at the latest email address we have for you, the new
    version will then apply to you.
",https://help.ceda.ac.uk/article/4640-ceda-archive-terms-and-conditions#general-terms-and-conditions,2659,414
Archive User,"In addition to the general terms and conditions, you also agree:
1.  To provide us information about your work and yourself as and when
    requested so that appropriate resource management decisions (e.g.
    applications for restricted datasets) can be made, and to let us
    know if this information changes (for example, you move
    institution).
2.  Only to use the service/resource for the purposes for which you were
    given access, for example, to work on NERC funded project covered in
    your resource application.
3.  Not to use another person\'s account, nor to let other people use
    your accounts, except by agreement with us.
4.  To comply with any special conditions that may apply to particular
    resources, e.g. datasets.
5.  To inform us if you believe your credentials have been compromised,
    or if you become aware that the security of our systems is
    compromised in any way.
You also accept:
1.  That we will use the personal details which you supply to us,
    together with records of your use of the service, as described in
    our [Personal Data and Privacy
    Policy](https://www.ceda.ac.uk/privacy-and-cookies).
2.  That we may suspend your access to the service/resource if it seems
    to us that you are breaking these Terms and Conditions and that we
    have the right to close your account.
",https://help.ceda.ac.uk/article/4640-ceda-archive-terms-and-conditions#archive-user,1342,211
Data Providers,"::: {#siteloader}
:::
1.  The CEDA Archive is one of the Natural Environment Research Council
    (NERC) Data centres. The depositor confirms that they are the owner
    of the data and/or has the right to deposit the data in a NERC
    archive.
2.  Your ownership of the data remains unchanged when deposited in the
    archive. However, NERC reserves the right to store the data, and
    make the data available under an appropriate licence.
3.  The depositor is also giving NERC permission to translate the data
    to any medium or format for the purpose of future preservation and
    accessibility. Any preservation actions taken will try not to change
    the meaning of the data significantly.
4.  The description of the data provided for catalogue records will be
    made freely available for any purpose.  This may be harvested by
    other cataloging and indexing services.
5.  If the dataset needs to be withdrawn, for any reason, a tombstone
    catalogue record will be kept in its place. Any request for
    withdrawal of a dataset will need to be justified before it is taken
    down.
6.  It's the data providers responsibility to make sure that malicious,
    offensive or illegal content is not deposited in the archive.
7.  Any personal information such as data creator email addresses should
    only be included where appropriate to enable traceability of the
    data.
",https://help.ceda.ac.uk/article/4640-ceda-archive-terms-and-conditions#data-providers,1393,221
Application Approvers and Viewers,"CEDA Archive users may be designated as application approver or viewer
with regards to restricted data in the CEDA Archive. Approvers have
responsibility to review and approve applications for access to their
restricted resource. Viewers are granted permission to view user
details, including provided research information, of users utilising
their restricted data. A user may hold either or both of these roles.
As such a user, in addition to the general terms and conditions, you
also agree, where applicable:
1.  Not to disclose the personal information of any CEDA Archive user to
    a third party.
2.  To make decisions in a fair and unbiased way, based solely on their
    eligibility to access the service or resource.
You also accept:
1.  Failure to comply with these terms and conditions may result in the
    withdrawal of your approver or view role.
",https://help.ceda.ac.uk/article/4640-ceda-archive-terms-and-conditions#application-approvers-and-viewers,862,139
The CEDA Archive Team agrees:,"1.  As far as we reasonably can, to provide a 24-hour service as
    described on this website, it being understood that there will be
    times when the service is unavailable, for example as a result of
    unexpected failures, maintenance work or upgrades.
2.  To take reasonable steps to protect your data from being lost or
    corrupted whilst in our care.
3.  To protect the security and privacy of the data we hold about you,
    as described in our [Personal Data and Privacy
    Policy](https://www.ceda.ac.uk/account/privacy-and-cookies/).
4.  That we will acquire no intellectual property rights over your data,
    documentation or software.
5.  To respond promptly to any complaints or suggestions you make about
    the service.
These Terms and Conditions are governed by the laws of England and the
English courts.
If you have any questions about these Terms and Conditions, please
[contact us](http://www.jasmin.ac.uk/contact/).
Version: 1
Published: 25 May 2018
Last Revised: 25 May 2018
",https://help.ceda.ac.uk/article/4640-ceda-archive-terms-and-conditions#the-ceda-archive-team-agrees:,1006,151
opmandatabasePolicy CEDA,"::: {#wikipage}
\[Sam - should be in another dos?\]
",https://help.ceda.ac.uk,52,9
Storage of Databases within the archive {#StorageofDatabaseswithinthearchive},"\
Databases will be preserved \""as-is\"" and a note produced for users to
inform them that these files will not be readable in the long term. The
information content will also be stored as an SQL dump of the database
or a set of xml files. In some cases where the database would be a
highly desirable dataset and where a database structure provides much
greater functionality than flat files can provide would justify effort
being put into making the database available via an on-line interactive
interface (e.g. such as for MIDAS). However, where possible CF
conventions should be included in the databases by the data providers.
:::
",https://help.ceda.ac.uk#storage-of-databases-within-the-archive-{#storageofdatabaseswithinthearchive},634,108
CEDA Web Processing Service WPS,"The  [CEDA Web Processing Service](https://ceda-wps-ui.ceda.ac.uk/), or
CEDA WPS, hosts a range of tools written by CEDA staff or imported
external tools to help undertake various data preparation tasks. 
",https://help.ceda.ac.uk,205,28
"What is the WPS Service and what can you do with it? {#what-is-the-wps-service-and-what-can-you-do-with-it children-count=""0""}","The CEDA WPS provides a range of online tools to aid using data in the
CEDA Archive. These include the ability to subset datasets by time,
location and even parameters, such as the MIDAS surface meteorological
dataset or model data from ECMWF. For more details of what other
capabilities the WPS tools offer please refer to their associated
details.
In the following guide you\'ll see the terms `Process` and `Tools`.
Here, a `Process` is a collection of `Tools` provided together for a
common purpose- for example, all those related to working with the data
in the MIDAS Dataset Collection are provided under the \'MIDAS Extract\'
process (e.g. finding relevant stations and extracting sub-sets of data
based on some selection criteria).
","https://help.ceda.ac.uk#what-is-the-wps-service-and-what-can-you-do-with-it?-{#what-is-the-wps-service-and-what-can-you-do-with-it-children-count=""0""}",739,120
"User Guide {#user-guide children-count=""0""}","This *general* user guide to the CEDA WPS outlines basic information
about the user interface with a brief summary of each Process (i.e.
collection of tools) and links to specific documentation page. Note,
although Processes and Tools may be listed in the WPS pages, access to
using the tools may depend on which datasets you have access to in the
CEDA Archive. Where required, such access can be applied for via the
relevant page in the [CEDA Data Catalogue
service](https://help.ceda.ac.uk/article/137-ceda-data-catalogue).
Please see the individual process and tool documentation for further
details.
::: {.section .callout children-count=""0""}
","https://help.ceda.ac.uk#user-guide-{#user-guide-children-count=""0""}",647,95
In this guide,"-   Getting started: [Sign In](#login)
-   Service Load Overview: The [Dashboard](#dashboard) view
-   Using the tools: The [Processes](#processes) page
-   [Monitor](#monitor) your submitted jobs
-   [Suggest/Request/Write a tool](#write)
-   [Further Reading about the CEDA WPS](#further)
-   [Browser compatibility](#browser)
:::
",https://help.ceda.ac.uk#in-this-guide,333,39
"Getting Started: Sign In {#login children-count=""0""}","![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/60391afb8502d1120e90b9ff/file-s2oX6aGroN.png)
In order to make use of the CEDA Web Processing Service, and so we can
provide support if needed, you will first need to login with your CEDA
Archive user account. If you don\'t yet have such an account these are
free and easy to set up. See our [User Registration and Account
Administration](https://help.ceda.ac.uk/article/81-registering) help
page for guidance on how to do this.
To login go to the CEDA Web Processing Service at:
<https://ceda-wps-ui.ceda.ac.uk/> and press the  `Sign in` button in the
upper right corner. This will take you to a page presenting you with the
option to sign in with your CEDA User Account:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fd0db48a72c62318a4c0684/file-VMOI0Bzdaw.png)
**Note:** the first time you do this you may be asked to authorise the
CEDA-WPS-UI application to access your account details:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fd0dc8f80488e39007cbb92/file-84XiHwsIfX.png)
Click `Authorise`
Once signed in your view will change slightly to look like this at the
top - note, your username will be displayed:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/603919c424d2d21e45eda905/file-shSWpVKceu.png)
","https://help.ceda.ac.uk#getting-started:-sign-in-{#login-children-count=""0""}",1420,154
"Service Load Overview: The Dashboard View {#dashboard children-count=""0""}","![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/60391bf1661b720174a714d1/file-ABX1XE5o5Y.png)
The dashboard (accessed by clicking on the dial icon top right) shows
some statistics about present and past jobs and users, useful to get an
idea of the overall usage and in particular the present load on the
service: larger loads may result in longer job run times.
The **Dashboard** view looks like this:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fd0de4280488e39007cbb9e/file-NUdm9c18mJ.png)The
**Overview** gives you an indication of the present load on the CEDA WPS
service as a whole - with the number of signed in users, number of jobs
available and the number of Web Processing Services (i.e. collections of
tools to use) presently available through the service.
The **Jobs** page gives numbers of jobs presently running, those which
have succeeded and the number that have failed.
The **People** page provides the number of users logged in now, the day
so far and for the past week. 
","https://help.ceda.ac.uk#service-load-overview:-the-dashboard-view-{#dashboard-children-count=""0""}",1072,142
"Using the service: The Processes page {#processes children-count=""0""}","![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/60391e3e661b720174a714e1/file-MU5l5x5XAp.png)
The  `Processes` tab gives a list of available processes (i.e.
collections of tools around a common purpose - e.g. a dataset) within
the CEDA WPS service. A padlock symbol next to the service name
indicates that making use of tools within that service will require your
CEDA user account to have additional access to restricted dataset(s).
The Processes page looks like this:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6112953db37d837a3d0e261a/file-arAOAXiBRx.png)
Choosing one of the WPS processes will get a list of available Tools
within that Process. In the user interface, each Process has a short
description at the top and lists of available tools, each with its own
description and the version of the tool presently available. In the
example below there are two tools available in the MIDAS Extract
process.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fe1e2f1f24ccf588e3fd96d/file-UGdMu37n5u.png)
To use one of the tools simply click on its title. This will take you to
a web form where you can input your selections to use the tool. At the
bottom of the form, there will be a  `Submit` button. Clicking this will
submit your process to a processing queue. When the process is submitted
you can see your job\'s progress in using the `Monitor` section of the
WPS service. 
Note: you can also label your jobs on submission which can help later
when listing them under the Monitor tool.
","https://help.ceda.ac.uk#using-the-service:-the-processes-page-{#processes-children-count=""0""}",1613,217
"Monitor {#monitor children-count=""0""}","![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/60391fcb8502d1120e90ba21/file-HfcI0RKqVF.png)
In  `Monitor` all your running or finished jobs are listed. The list
shows the status and progress of your jobs.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fe0d8ae0b11ce44f63943ae/file-Egy4S5kynm.png)When
a job has finished with success you can see the results by clicking the 
`Details` button.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fe0d90a0b11ce44f63943b3/file-YhNbTeMrpZ.png)If
the result has an output file (XML, text, NetCDF, ...) you can view or
download this output file by using the `Download` button under
the `Outputs` tab. The `Inputs` tab details the form selections made,
whilst the `Job Log` tab shows the log for the process as it ran, which
may be helpful if there is an issue. Details can also be viewed in XML
if desired.
","https://help.ceda.ac.uk#monitor-{#monitor-children-count=""0""}",968,106
"Suggest/Request/Write a tool {#write children-count=""0""}","The CEDA WPS is a service that has been designed to scale with both
usage, and with available tools. If you would like to see a specific
tool in the CEDA WPS, please feel free to suggest this to us or ask
about how you can write a Python tool to be included.
","https://help.ceda.ac.uk#suggest/request/write-a-tool-{#write-children-count=""0""}",259,53
"Further Reading about the CEDA WPS {#further children-count=""0""}","The present CEDA WPS is based on the Birdhouse WPS developed by DKRZ and
utilises \'Pyramid Phoenix\', a web application built with the Python
web framework Pyramid. For more information about the WPS and the
underlying technology, including how to install it on other systems, see
<https://pyramid-phoenix.readthedocs.io/en/latest/index.html>
","https://help.ceda.ac.uk#further-reading-about-the-ceda-wps-{#further-children-count=""0""}",344,47
"Browser compatibility {#browser children-count=""0""}","The CEDA WPS has been tested with the following browsers:
-   Firefox 3+
-   Chrome v 86 +
-   Internet Explorer 8
If you test the CEDA WPS in other browsers and it works, please let
[CEDA Support](javascript:void(0);){#beacon} know and we will add that
to the above list.
","https://help.ceda.ac.uk#browser-compatibility-{#browser-children-count=""0""}",273,48
Using the command line for CREPP,"The CREPP CEDA Receive to publish pipeline was developed by Alan Iwi and
Ag Stephens to ingest CMIP6 data and then subsequently publish this to
ESGF. Data for ingest will be from either the Met Office MASS archive,
already on JASMIN, or replicated international data. 
",https://help.ceda.ac.uk/article/4822-crepp,269,45
Managing CREPP transfers,"CREPP dataset status\' can be seen through the CREPP application web
interface at [ppln.ceda.ac.uk](https://ppln.ceda.ac.uk/) most commonly
using the dataset view. 
",https://help.ceda.ac.uk/article/4822-crepp#managing-crepp-transfers,165,20
Debugging,"Logs are on the two machines used by CREPP some logs are only available
on appropriate machine ( confirm with Alan?) 
    ingest3
    ~/software/crepp/crepp/logs
    esgf-pub 
    /usr/local/crepp/logs
If a dataset is found to be in error if you have admin rights the
dataset can be retried using the web interface, however this is only
feasible for a very small number of datasets. 
For bulk dataset retries use the dataset_actions.sh script located in
the scripts dir, note this script only resets flags and so it is
irrelevant which server this is initiated from: 
On this can be done from the scripts/ directory using
    dataset_actions.sh -h
    usage: dataset_actions.py [-h]
                              (-R | -S | -P <priority> | --clear-priority | -p | -r | -u | -q | -W | --republish)
                              [-v]
                              config [datasets [datasets ...]]
    positional arguments:
      config                CREPP configuration name (e.g. 'esgf-prod')
      datasets              dataset ID(s); if none, they are read from stdin
    optional arguments:
      -h, --help            show this help message and exit
      -R, --retry           retry failed step
      -S, --skip            skip failed step
      -P <priority>, --set-priority <priority>
                            set dataset priority
      --clear-priority      clear dataset priority
      -p, --pause           pause dataset
      -r, --release         release dataset pause
      -u, --update          update processing status and last event
      -q, --query           query status
      -W, --withdraw        withdraw dataset
      --republish           republish withdrawn dataset
      -v, --verbose         more verbose output
",https://help.ceda.ac.uk/article/4822-crepp#debugging,1742,213
Removing from the archive -  (esgdrs errors),"If a dataset needs removing from the archive to clear an esgdrs.out 
error use or other reason then on:
    ingest3
    ~/software/crepp/crepp/scripts/remove-from-archive.sh < <dataset_list>
then retry the ingest step.
",https://help.ceda.ac.uk/article/4822-crepp#removing-from-the-archive-- -(esgdrs-errors),219,28
Observation Collection Records,"::: {#wikipage}
[Original trac
page](http://team.ceda.ac.uk//trac/ceda/wiki/opman/MOLESEditorUserGuide/observationCollection)
::: wiki-toc
",https://help.ceda.ac.uk/article/4303-observation-collection-records,139,7
Page Contents,"1.  [Observation Collection Records](#ObservationCollectionRecords)
:::
**Each Observation Collection should describe a set of datasets held in
the archive. These may include datasets used by other collections too.**
**Also note:**
-   **the publication of the dataset and the Observation Collection
    record should go hand-in-hand**
-   **there is no longer a place to link to the archive or to set the
    apply for access link at this level - it is now down on the Dataset
    level**
  -----------------------------------------------------------------------------------
  Field ( **bold =  Description            Checklist/\*Controlled   Y/N
  mandatory field**                        vocab explanations/      
  )                                        *Notes*                  
  ----------------- ---------------------- ------------------------ -----------------
  **Title**         Short text title for   Do you understand the    
                    the dataset            title?\                  
                    collection.            Is the title brief and   
                                           simple?\                 
                                           Does the title contain   
                                           unexplained technical    
                                           terms or acronyms?\      
                                           Does the title describe  
                                           the resource rather than 
                                           the activity/project     
                                           which produced it?\      
                                           Is the title in sentence 
                                           case?\                   
                                           Is the title in          
                                           English?\                
                                           Does the title contain   
                                           non-standard characters  
                                           (e.g. © ° ± ² ³ µ)?      
  **Abstract**      Short text only entry  Do you understand the    
                    that will be the first abstract?\               
                    information that users Is the abstract written  
                    will see about the     in plain English?\       
                    text.                  Does the abstract        
                                           describe the resource    
                                           rather than an           
                                           activity/project which   
                                           produced it?\            
                                           Do the first few         
                                           sentences summarise the  
                                           contents of the          
                                           resource?\               
                                           Does the abstract also   
                                           explain \'Where\',       
                                           \'When\', \'How\',       
                                           \'Why\' and \'Who\' (if  
                                           appropriate)?\           
                                           Does the abstract        
                                           contain unexplained      
                                           technical terms or       
                                           acronyms?\               
                                           Does the abstract        
                                           contain non-standard     
                                           characters (e.g. © ° ± ² 
                                           ³ µ)?                    
  Image details     link to an image to be                          
                    used as the logo                                
  Publication State There is a drop down   \* Old\                  
                    box for the            \* working - use this    
                    publication state of   while preparing the      
                    the data in the        record prior to dataset  
                    archive (and also the  release\                 
                    Observation Collection \* published - dataset   
                    record). Options are   details are available to 
                                           users, but the content   
                                           of the dataset and/or    
                                           metadata may still be    
                                           changing\                
                                           \* Citable - this is     
                                           ONLY to be used for      
                                           dataset that are static  
                                           and thus have a DOI      
                                           assigned to them         
  Data publication  Date-Time of the data  *should be set once and  
  date              publication - i.e.     NOT changed. Not to be   
                    moved publication      used for DOI date!*      
                    status to                                       
                    \""published\"" for the                           
                    first time                                      
  Observations      A filter and select                             
                    option to choose which                          
                    Observations form this                          
                    collection. If the                              
                    desired Observation                             
                    doesn\'t exist then                             
                    use the + button to                             
                    start creating a new                            
                    one (which is                                   
                    immediately added to                            
                    the selection too)                              
  Identifiers       Unless adding a DOI,   *the moles2 url is to    
                    or a new abbreviation, ensure that users who    
                    please leave these as  find an reference in a   
                    given.                 paper to a MOLES2 record 
                                           using the URLs given     
                                           then can confirm to      
                                           themselves that they are 
                                           on the equivalent MOLES3 
                                           page.*                   
  Parties           A list of              \* **Author** - party    
                    people/organisations   who contributed to the   
                    (called collectively a dataset authoring\       
                    \""party\"") involved    \* **CEDA Officer** -    
                    with the data creation internal CEDA person     
                    or curation.\          responsible for the      
                    Select (or create) a   record/resource in       
                    named party from the   question\                
                    list, select their     \* Curator - party       
                    role from the          responsible for curating 
                    controlled vocabulary  the content (data        
                    and, if more than one  centre)\                 
                    party carrying out     \* Custodian - party     
                    that role, use the     responsible for          
                    priority number to     maintaining the resource 
                    sequence them in this  (data centre)\           
                    role.                  \* Distributor - party   
                                           responsible for          
                                           distributing/providing   
                                           access to the resource   
                                           (data centre)\           
                                           \* **Metadata Owner** -  
                                           party maintaining this   
                                           metadata record (data    
                                           centre)\                 
                                           \* Point of Contact -    
                                           default is the data      
                                           centre, but may be used  
                                           in due course for a      
                                           named person from the    
                                           author list too\         
                                           \* **Publisher** -       
                                           organisation that        
                                           published the data first 
                                           (most of the time one of 
                                           our datacentres, but     
                                           could be external group  
                                           where we are mirroring   
                                           the data only)\          
                                           \                        
  Notes             present and past news  *latest news item will   
                    items particular to    be at the top of the     
                    the record             user view, with past     
                                           items listed in the main 
                                           body of the record*      
  Online Resources  Online link to         Do resource locators     
                    relevant resource -    include titles which are 
                    e.g. documentation,    concise, accurate        
                    WPS.\                  accounts of the resource 
                    There are set          in question?\            
                    categories that should Do resource locators     
                    be used.\              include descriptions     
                    Do NOT use: DMP,       which fully explain      
                    Download, Apply for    their purpose?           
                    Access - these are                              
                    handled elsewhere.                              
  Reviews           For carrying out       *For the time being      
                    metadata content       please refer all new     
                    reviews - not in       records to Anabelle or   
                    proper operation just  Graham for a review      
                    yet.                   before they are          
                                           published. This is to    
                                           ensure consistent        
                                           quality checks are       
                                           carries out*             
  Migration         All the old legacy     *Please leave the        
  Properties        content from the       \""moles2citation\"" at    
                    MOLES2 \""Content\""     present, but look to     
                    section is in here -   move other items into    
                    split into the various other parts of MOLES if  
                    div tags.              possible - e.g. add      
                                           items under the links    
                                           section to the online    
                                           resources section. Once  
                                           migrated feel free to    
                                           delete content*          
  -----------------------------------------------------------------------------------
:::
",https://help.ceda.ac.uk/article/4303-observation-collection-records#page-contents,12278,838
Data Scientists Crib-Sheet,"This page provides a simple 1-stop page to pull together useful links
for documetation about tools data scientist need for their work:
  ----------------------------------------------------------------------------------------------------- ----------------------- -------------------------------------------------------------------------------------------------------------
  **Wiki Entry**                                                                                        **Notes**               **Relevant Tools**
  [SVN Users Guide](http://ceda-internal.helpscoutdocs.com/article/4197-subversion-user-guide){.wiki}   Getting code in,        [Browse svn repo](http://team.ceda.ac.uk/trac/ceda/browser)
                                                                                                        checking it out         
  [MOLES entry guidance                                                                                 Notes on how to style   [[ ]{.icon} MOLES 3 Editor](http://catalogue.ceda.ac.uk/admin/){.ext-link}\
  notes](http://ceda-internal.helpscoutdocs.com/article/4138-moles-3-editor-user-guide){.wiki}          MOLES entries (note -   [MOLES Review
                                                                                                        was written for MOLES2, Form](https://docs.google.com/forms/d/e/1FAIpQLScmiyOrEDB_JGqNPPIIMMw1CDzQy2KnntDzsaFvxAkrpwOg9Q/viewform)\
                                                                                                        but can be applied to   
                                                                                                        MOLES3 objects too!)    
  [New JASMIN ingestion                                                                                 How to set up a new     
  system](http://ceda-internal.helpscoutdocs.com/article/4168-ingesting-into-the-ceda-archive){.wiki}   data stream set up      
                                                                                                        CEDA internal dataset   [[ ]{.icon} CEDA Info DB](http://cedadb.ceda.ac.uk/){.ext-link}
                                                                                                        info                    
                                                                                                        Controlling access to   [[ ]{.icon} CEDA Security DB](http://securitydb.ceda.ac.uk/admin/){.ext-link}
                                                                                                        the archive             
                                                                                                        Info on who has access  [[ ]{.icon} CEDA User DB](http://cedadb.ceda.ac.uk/admin/udbadmin/){.ext-link}
                                                                                                        to what                 
  [Putting our Python packages on                                                                       How to put Python       [[ ]{.icon} ceda conformance checker](https://pypi.python.org/pypi/ceda-cc/){.ext-link}
  \""PYPI\""](http://team.ceda.ac.uk/trac/ceda/wiki/DevGroup/PythonPackages/PuttingOnPyPI#no1)            Packages on \""PYPI\""    
                                                                                                        for users to download   
  ----------------------------------------------------------------------------------------------------- ----------------------- -------------------------------------------------------------------------------------------------------------
",https://help.ceda.ac.uk#data-scientists-crib-sheet,3639,145
Some Useful Links. {#SomeUsefulLinks.},"  -------------------------------------------------------------------------------------------------------------------------------- ---------------------------- --------------------------------------------------------------------------------------------------------------
  **Link**                                                                                                                         **Description**              **Useful for**
  [[ ]{.icon} http://tartley.com/?p=1349](http://tartley.com/?p=1349){.ext-link}                                                   Python regex cheatsheet      useful reminder about regexes
  [[ ]{.icon} http://www.pythonregex.com/](http://www.pythonregex.com/){.ext-link}                                                 Python regex checker         trialing regexes against sample text
  [[ ]{.icon} http://www.movable-type.co.uk/scripts/latlong.html](http://www.movable-type.co.uk/scripts/latlong.html){.ext-link}   Lat lon calculation helper   working our things like lat lon at a set distance from a given point for geographic extents in MOLES records
  -------------------------------------------------------------------------------------------------------------------------------- ---------------------------- --------------------------------------------------------------------------------------------------------------
",https://help.ceda.ac.uk#some-useful-links.-{#someusefullinks.},1373,58
About the CRU data,"The University of East Anglia (UEA) archive the Climatic Research Unit
(CRU) data with CEDA.  The CRU data contains two different products, the
gridded time-series (TS) data and Year-by-Year Variation of Selected
Climate Variables by Country (CY) data. 
In 2017 a new major version of the CRU data was released, version 4 of
the CRU data has an updated methodology compared with version 3.
Complimentary versions of version 3 CRU data were released alongside the
first two versions of the new version 4 data for comparison. Therefore
there exists CRU TS/CY 3.24.01 and 4.00 that both cover the same time
period but differ in methodology and similarly CRU TS/CY 3.25 and 4.01.
CRU TS and CRU CY 3.25 are the final versions of the CRU version 3 data,
no further version 3 data will be released and users should move to
using version 4 of the CRU data.
-   [Downloading the CRU data](#Downloading%20the%20CRU%20data)
-   [Missing data values](#Missing%20data%20values)
-   [A step by step guide to downloading data from the
    WPS](#downloadGuide)
-   [Geographic Information Systems
    (GIS)](#Geographic%20Information%20Systems)
-   [Known issues with gzipped
    files](#Known%20issues%20with%20gzipped%20files)
-   [Further information](#Further%20information)
",https://help.ceda.ac.uk/article/4472-cru-data-user-guide#about-the-cru-data,1264,183
Downloading the CRU data {#Downloading the CRU data},"You can download the CRU data using the various CEDA data download tools
as detailed below:
1.  Using the standard CEDA data download through the [web
    interface](https://help.ceda.ac.uk/article/4431-ceda-archive-web-download-and-services),
    [scripted interactions using the OPeNDAP
    service](https://help.ceda.ac.uk/article/4442-ceda-opendap-scripted-interactions)
    or [ftp](https://help.ceda.ac.uk/article/280-ftp). 
2.  Using the CEDA Web Processing Service (WPS)
    1.  [Guide to downloading data from the WPS ](#downloadGuide)
    2.  [Guide on data formats from the WPS CSV output](#csvGuide)
If you are having difficulty please see the  [Known
Issues](#Known%20issues%20with%20gzipped%20files) section
",https://help.ceda.ac.uk/article/4472-cru-data-user-guide#downloading-the-cru-data-{#downloading-the-cru-data},722,73
Missing data values {#Missing data values},"The CRU TS gridded data provides data over land only, missing values are
given over the oceans and seas.
-   The \"".dat\"" data files have a missing value of -999
-   The \"".csv\"" files downloaded through the web processing service
    (WPS) have a missing value of -9999.999
-   NetCDF files have a missing value of 9.9692e+36 
Since there can be many data values per file and that more than half the
values are over the oceans or seas then it may appear that the file has
no real data, however by locating the geographic region that you are
interested in you will be able to find the real data values. If you are
using a text file (\"".dat\"") or a \"".csv\"" file you may be able to zoom
out at which point you will see that the file contains values other than
the missing value
Note that a global data file is ordered from -90S to 90N i.e. the data
begins over Antarctica, no data are available there so the first rows
will only contain missing values.
",https://help.ceda.ac.uk/article/4472-cru-data-user-guide#missing-data-values-{#missing-data-values},952,177
A step by step guide to downloading data from the WPS {#downloadGuide},"1.   Go to the [CEDA WPS](https://ceda-wps-ui.ceda.ac.uk/) site and
    click 'Sign In' at the upper right corner of the page. This will
    take you to a page presenting you with an option to sign in with
    your CEDA User Account.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6152d8030754e74465f1453f/file-zY4pcbgnyF.png)
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6152d83000c03d672075b18a/file-hSXhMFDdAk.png)
-   **Note:** The first time you do this you may be asked to authorise
    the CEDA-WPS-UI application to access your account details. Click
    'Authorise' in the lower right corner of the page. 
2.  Once signed in, your view will change slightly to show your username
in the top right.  Next, click the 'Processes' tab at the top of the
page.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6152da632b380503dfdf5471/file-nf3P06hTdM.png)
-   The 'Processes' tab gives a list of available processes (i.e. a
    collection of tools around a common purpose -- e.g. a dataset)
    within the CEDA WPS service.
3.  The 'Processes' page will look like this; showing a set of
processes. Click the 'Data Subsetter' process to access the tools needed
for the CRU Time Series.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6152daba12c07c18afdd8356/file-Kn97APapGr.png)
-   Within the 'Data Subsetter', click 'Subset CRU Time Series' 
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6152dade0754e74465f1454e/file-FNYHJj0vZV.png)
4.  Ensure that the Dataset is set to 'Climate Research Unit (CRU) TS
(time-series) datasets 4.04 (4.04 is the available version at the time
of writing). Then select a variable from the drop-down list provided.
Currently, the available variables are:
-   a.       Precipitation
-   b.       Near-surface temperature
-   c.       Near-surface temperature maximum
-   d.       Potential evapotranspiration
-   e.       Ground frost frequency
-   f.        Cloud cover
-   g.       Wet day frequency
-   h.       Diurnal temperature range
-   i.         Vapour pressure
-   j.         Near-surface temperature minimum
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6152dea812c07c18afdd8363/file-cDerGaGCEO.png)
5.  Select a start date/time and an end date/time using the blue Time
Period range bar. Drag each end of the bar to the required dates. 
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6152df7b2b380503dfdf5488/file-DsCMG4D9qr.png)
6.  Select a boundary box by adding coordinates to the boxes provided.
Alternatively, use the interactive map to draw a boundary box. **NOTE**
this is optional and defaults as the globe.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6152dfaa2b380503dfdf548a/file-O5c2NkfVe7.png)
",https://help.ceda.ac.uk/article/4472-cru-data-user-guide#a-step-by-step-guide-to-downloading-data-from-the-wps-{#downloadguide},2976,314
{#csvGuide},"7. Select an output format using the drop-down option. Options include:
-   NetCDF
-   csv
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6152e01900c03d672075b1aa/file-nUozomP17Z.png)
",https://help.ceda.ac.uk/article/4472-cru-data-user-guide#{#csvguide},224,16
{#csvGuide},"8\. Press 'Submit' at the bottom of the page.
9\. You will be taken to the 'Job Monitor' page. The list shows the
status and progress of your jobs. Once a job has finished with success,
you can see the results by clicking the 'Details' button.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6152e60b0754e74465f1458c/file-Pa0TD5N989.png)
10.  You can download and view the output by clicking the 'Outputs' tab.
The 'Inputs' tab details the form selections made, whilst the 'Job Log'
tab shows the log for the process as it ran, which may be helpful if
there is an issue. Details can also be viewed in XML if desired. 
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6152e65b0754e74465f1458f/file-SZQADfVWmf.png)
",https://help.ceda.ac.uk/article/4472-cru-data-user-guide#{#csvguide},790,101
How to read CSV files from the web processing service {#csvGuide},"The CEDA web processing service (WPS) will have provided a comma
separated value (CSV) file in a  [NASA Ames
format](http://cedadocs.ceda.ac.uk/73/4/index.html), all NASA-Ames files
have a header and then the data.
",https://help.ceda.ac.uk/article/4472-cru-data-user-guide#how-to-read-csv-files-from-the-web-processing-service-{#csvguide},215,30
The header section,"The header has lines that describe the data in the file (metadata):
Here an example of CRU precipitation data has been downloaded from the
CEDA WPS and the header information is as shown below:\
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/59ae55fe2c7d3a73488c9672/file-ubbwjXOAxM.png)
(You may wish to expand the second and third columns to see all the
metadata.)
The header information can be decoded by using the following as a guide:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/59ae570b042863033a1c92d5/file-APdvzPNxUp.png)
So for the example above this would be:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/59b0fffe2c7d3a73488caa1b/file-NaDWgjRg03.png)
The data section
After the header, the data is displayed as shown in the image below.
Each repeating data section starts with the time stamp (usually the
number of days since 1900-01-01) and then a large section of latitudes
and longitudes. Where a large geographic region (or global region) has
been selected the data may look similar to: 
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/59b10032042863033a1ca5ea/file-Fw1ysUaOAK.png)
where initially it may look like a file of missing values as described
in the header:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/59ae58af2c7d3a73488c967f/file-KMoVsszd05.png)
However, if a month of the yearly files were selected and zoomed
out, then the geographic region selected can be seen more clearly:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/59ae58e1042863033a1c92df/file-iq7NDqbySh.png)
For the CRU data each value represents one grid box (part of the globe)
and a subsection is shown above. 
",https://help.ceda.ac.uk/article/4472-cru-data-user-guide#the-header-section,1850,193
Geographic Information Systems (GIS) {#Geographic Information Systems},"The CRU data are available in the 
[NetCDF](https://help.ceda.ac.uk/article/106-netcdf) format. NetCDF
files of the CRU data can be downloaded through the data browser for a
given dataset selecting the \"".nc\"" file or by using the Web Processing
Service (WPS) and selecting WPS as the output format. The CRU NetCDF
files can then be loaded into various GIS platforms such as
[ArcGIS](http://desktop.arcgis.com/en/arcmap/10.3/manage-data/netcdf/reading-netcdf-data-as-a-raster-layer.htm)
or QGIS. 
Here an example shows how to open a NetCDF file in QGIS, it is assumed
that you have a NetCDF file ready for use in QGIS:
1.  Install the NetCDF browser plugin by selecting \""Manage and Install
    Plugins\"" under \""Plugins\"" and searching NetCDF browser.
2.  Click on the NetCDF browser
    plugin![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5ad5da672c7d3a0e93676511/file-zumCvd2OMT.png)
3.  Select your previously downloaded NetCDF file and then from the time
    selection drop-down select the timestep you require frames and click
    add
    selection.![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5ad5daec2c7d3a0e93676519/file-G7GQ83B9Er.png)
4.  Select a coordinate system to display the data, if you are unsure a
    reasonable default choice would be WGS
    84.![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5ad5db440428630750927e28/file-f0JAzs5t4B.png)
5.  A map of the data should then be loaded.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5ad5dbbd2c7d3a0e9367651e/file-mk8ZsVcH3h.png)
",https://help.ceda.ac.uk/article/4472-cru-data-user-guide#geographic-information-systems-(gis)-{#geographic-information-systems},1662,169
Known issues with gzipped files {#Known issues with gzipped files},"<div>
It has been reported that some web browsers (eg. Chrome) have been
unzipping the compressed files (e.g. those with \"".dat.gz\"" or
\"".nc.gz\"" extensions) on download, but without renaming the files to
remove the \"".gz\"" file extension. Consequently, users may encounter
issues with the downloaded file not being recognised by unzipping
programmes. To avoid this issue users can manually rename the files to
remove the \"".gz\"" suffix.
</div>
<div>
Due to this for each variable in each version of the  CRU  TS data one
unzipped NetCDF file has been provided. It is possible to interact with
this file directly or via a script using the  [CEDA OPeNDAP
service](https://help.ceda.ac.uk/article/4431-ceda-archive-web-download-and-services).
Simply find the file in the list that is unzipped (i.e. has extension
\"".nc\"") on the right hand side you will see a wheel with a button named
\""subset\"" click this button and then follow the instructions (or refer
to the OPeNDAP documentation) to download a subset of the data that you
require. You can download data in either NetCDF or ASCII (text) format.
 (Note if you download CRU data through this OPeNDAP service no scaling
of any variables have been applied, this is not the case if you download
the gzipped text files \"".dat.gz\"" in which case you should refer to the
version specific file formats guide.) 
</div>
",https://help.ceda.ac.uk/article/4472-cru-data-user-guide#known-issues-with-gzipped-files-{#known-issues-with-gzipped-files},1365,217
Further information {#Further information},"-   The Climate Research Unit
    [homepage](https://crudata.uea.ac.uk/cru/data/hrg/)
-   Scientific
    [paper](https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/joc.3711)
    associated with the CRU data
-   Contact CEDA via support\@ceda.ac.uk
",https://help.ceda.ac.uk/article/4472-cru-data-user-guide#further-information-{#further-information},251,19
Nearline archive NLA,"Sometimes we keep data on tape only. This is used for very voluminous
data sets such as Sentinel.
",https://help.ceda.ac.uk,98,18
Overview,,https://help.ceda.ac.uk#overview,0,0
,"Whilst much of the CEDA archive is stored on disk we do not have the
storage capacity to keep it all online. We therefore migrate many data
sets on to tape. If the data you require is held on tape then you can
use the Near-line archive (NLA) tool to request the files are
restored to disk.
",https://help.ceda.ac.uk#,290,57
Accessing the tape archive from JASMIN,,https://help.ceda.ac.uk#accessing-the-tape-archive-from-jasmin,0,0
1. Requesting a tape quota,"Firstly, you need to request a tape quota by emailing the [CEDA
Helpdesk](mailto:support@ceda.ac.uk). 
",https://help.ceda.ac.uk#1.-requesting-a-tape-quota,103,13
2. Installing the tool (on a JASMIN server),"Recently (in 2020), the nla command line tool was updated to be
compatible with both Python 3 and Python 2.7.\
This requires a change to the previously published method of
installation, and (slightly) different methods of installing for Python
2.7 and Python 3.
",https://help.ceda.ac.uk#2.-installing-the-tool-(on-a-jasmin-server),262,43
2a.  Installing on Python 2.7,"1.  On the **jasmin-sci?.ceda.ac.uk** servers, you do **not** need to
    load the JASPY Python 2.7 module.\
    On the  **sci?.jasmin.ac.uk** you do need to load the JASPY Python
    2.7 module:\
        module load jaspy/2.7<br>
2.  Create, or use an existing, Python 2.7 virtual environment:\
        virtualenv ./nla_venv
3.  Activate the Python 2.7 virtual environment:\
        source ./nla_venv/bin/activate
4.  Download the command-line interface from the [GitHub
    NLA](https://github.com/cedadev/nla_client), by typing:
        git clone https://github.com/cedadev/nla_client
5.  Install the command-line interface from the downloaded repository:
        pip install ./nla_client
",https://help.ceda.ac.uk#2a. -installing-on-python-2.7,692,79
2b. Installing on Python 3.x,"1.  For both the j**asmin-sci?.ceda.ac.uk** servers
    and **sci?.jasmin.ac.uk** servers you need to load the JASPY Python
    3 module:
        module load jaspy
2.  Create, or use an existing, Python 3 virtual environment:\
        python3 -m venv  ./nla_venv
3.  Activate the Python 3 virtual environment:\
        source ./nla_venv/bin/activate
4.  Download the command-line interface from the  [GitHub
    NLA](https://github.com/cedadev/nla_client):
        git clone https://github.com/cedadev/nla_client
5.  Install the command-line interface from the downloaded repository:
        pip install ./nla_client
",https://help.ceda.ac.uk#2b.-installing-on-python-3.x,617,68
3. Using the command line tool,"Run the command-line tool by first activating the virtual environment
(step 3 above) and then running the nla command.
    source ./nla_venv/bin/activate
    nla
This opens an interactive tool. You can type ""help"" to get a list of
help topics / commands. For example:
    ===========================
    CEDA Near line tape utility.
    NLA>>> help
    Documented commands (type help <topic>):
    ========================================
    EOF     listing_request  notify_first     quit   requested_files
    expire  ls               notify_last      quota  requests
    label   notify           pattern_request  req    retainType ""help <command>"" to get help using a particular command.
The first stage is to determine the names of the files in the NLA that
you want to restore. Use the ""ls"" command to do this, which can also
take a sub-string to search for, e.g. ""ls sentinel1a"".
",https://help.ceda.ac.uk#3.-using-the-command-line-tool,886,118
4. Requesting files to be restored to disk,"Once you know which files you want to restore to disk you can issue a
request. There are two ways to make a request:
1.  **Listing request. ** Here you supply a list of file names to
    restore in a file.  E.g. ""`listing_request request_1.txt`"". (you can
    supply any path to the listing file, here it is in the same
    directory as nla.py, which is probably not how you would typically
    invoke it).
2.  **Pattern request. ** Here you supply a sub-string that must appear
    in the filename.  E.g. ""pattern_request 2015"" will recover all files
    with ""2015"" in the filename.  This particular request (\""2015\"") is
    not recommend as it will restore a lot of files!  Something more
    specific like ""S2A_OPER_PRD_MSIL1C_PDMC_20161007"" would be better.
",https://help.ceda.ac.uk#4.-requesting-files-to-be-restored-to-disk,764,122
5. Additional commands,"You can view your requests using the ""requests"" command. This will show
you how much of your quota you have used.
You can view the details of a request by ""req \<request_number>"".
You can check which files are in a request using: ""requested_files
\<request_number>"".
Requests have a retention date.  After this date the restored files will
be removed.  You can extend this retention date by using ""retain
request-number yyyy-mm-dd"".
You can expire a request early using ""expire request-number"".  This will
remove your restored files within 24 hours and free up some of your
quota.
You can label a request by using ""label label-name"".  The default label
name is either the first file in a Listing Request or the pattern in a
Pattern Request.
You can be informed via email when your files are ready using the
""notify"" command.  The system knows your email address so simply doing
""notify"" will inform you when the first files arrive and the last files
arrive.  To notify someone else use ""notify \<email_address>"".  To
notify different people when the first and last arrive use ""notify_first
\<email_address>"" and ""notify_last \<email_address>\"".
You can check your quota via ""quota"".
",https://help.ceda.ac.uk#5.-additional-commands,1183,191
6. Additional documentation,"There also exists a library and REST-API that can be used to interact
with NLA programmatically.\
The documentation for these is at:\
[Additional NLA
documentation](https://cedadev.github.io/django-nla_control/)
",https://help.ceda.ac.uk#6.-additional-documentation,212,25
Archiving Overview,"Data management takes place at all points in the data lifecycle. Some of
these points exist upstream of the data centre and may not lead to data
being archived for long-term preservation. However, the archive may
provide guidance, support and services at some or all points as
required.
Various items in the CEDA Archive Operational Manual cover work
undertaken by CEDA Archive staff and exist to document workflows,
policies and services as needed to aid data management.
",https://help.ceda.ac.uk/article/4682-archiving-overview,473,77
"Data Lifecycle - CEDA Archive roles and links to sections {#data-lifecycle---ceda-archive-roles-and-links-to-sections children-count=""0""}","With regards to the data that are handled by the CEDA Archive the
following points of the data lifecycle are important. Links to relevant
documentation each point are indicated as needed. Note, there may be
ad-hoc engagements with specific projects/facilities with regards to
data management that fall out of the scope of these operational manual
pages.
","https://help.ceda.ac.uk/article/4682-archiving-overview#data-lifecycle---ceda-archive-roles-and-links-to-sections-{#data-lifecycle---ceda-archive-roles-and-links-to-sections-children-count=""0""}",354,56
"Initial scoping 1- Grant awards and Outline Data Management Plans {#initial-scoping-1--grant-awards-and-outline-data-management-plans children-count=""0""}","**For NERC grant awards:** As part of the submission process for a NERC
award applicants are required to submit an Outline Data Management Plan.
Requirements and guidance on preparing ODMPs are available in the award
system.
-   Details on ODMPs etc for applicants:
    <https://nerc.ukri.org/research/sites/environmental-data-service-eds/dmp/>
-   Enquiries regarding additional support (e.g. additional services
    such as database development or a specialist in project data
    management), and costing to be included in the grant proposal may be
    received by the helpdesk. These should be passed to the Archive
    Manager
-   See DataMad documentation for related details.
**For Non-NERC awards:** There isn\'t an official role in the grant
application process as for NERC awards, but enquiries on costs etc. may
be made via the helpdesk or other routes. These should be passed to the
Archive Manager in the first instance to determine if the request
is within scope (see [Acquisitions and Retention
policy](https://help.ceda.ac.uk/article/4857-acquisition-policy)) of the
data centre and if so associated costings and subsequent actions.
","https://help.ceda.ac.uk/article/4682-archiving-overview#initial-scoping-1--grant-awards-and-outline-data-management-plans-{#initial-scoping-1--grant-awards-and-outline-data-management-plans-children-count=""0""}",1149,161
"Initial scoping 2 - Data Management Plans {#initial-scoping-2---data-management-plans children-count=""0""}","For successful NERC applications or agreed work for non-NERC grant
awards a second scoping phase will then take place to establish the
required Data Management Plan. See [DMP op man
pages](https://ceda-internal.helpscoutdocs.com/category/4524-dmp). The
[NERC data value
checklist](https://nerc.ukri.org/research/sites/environmental-data-service-eds/policy/data-value-checklist/)
may be useful to aid discussions to determine which data should be
archived. Additionally, for model/computational output see the
[selection
guidance](https://help.ceda.ac.uk/article/4300-archiving-of-simulations-guide)
notes where data volumes may be problematic. 
","https://help.ceda.ac.uk/article/4682-archiving-overview#initial-scoping-2---data-management-plans-{#initial-scoping-2---data-management-plans-children-count=""0""}",645,64
"Continuing communications {#continuing-communications children-count=""0""}","Once the DMP has been agreed communications with the data providers
should continue as required to ensure that timely support is provided to
aid their data management, preparation for archival and delivery to the
archive. Additionally, supporting documentation should be captured. See
[JIRA
guidance](https://ceda-internal.helpscoutdocs.com/article/4905-jira-data-management-tracking-tool#gsc.tab=0)
and [data management
helpdesk](https://ceda-internal.helpscoutdocs.com/article/4684-data-management-helpdesk#gsc.tab=0)
pages for details on stages of comms and how to use the system
","https://help.ceda.ac.uk/article/4682-archiving-overview#continuing-communications-{#continuing-communications-children-count=""0""}",583,61
"Capturing Supporting Docs {#capturing-supporting-docs children-count=""0""}","Data stored for long-term and re-use require supporting information to
aid their interpretation. Besides catalogue information additional
supporting material may need to be captured. Such details are best
captured during data production if possible. However, not all supporting
material is the same in terms of the types of archiving needed. See the
\' [Where to put data
docs](https://ceda-internal.helpscoutdocs.com/article/4827-where-to-put-data-docs#gsc.tab=0)\'
guidance for a workflow to establish where to put supporting materials. 
","https://help.ceda.ac.uk/article/4682-archiving-overview#capturing-supporting-docs-{#capturing-supporting-docs-children-count=""0""}",540,69
"Ingest {#ingest children-count=""0""}","Guidance for users is available under the [Archiving data with
CEDA](https://help.ceda.ac.uk/category/13-archiving-data-with-ceda) help
docs site.
For the CEDA side see the [General Ingest Process
documentation](https://ceda-internal.helpscoutdocs.com/category/4525-general-ingest-process#gsc.tab=0).
If there are specific detailed/ongoing processes that require additional
documentation please create an entry under the [Specific Ingest
Processes](https://ceda-internal.helpscoutdocs.com/category/4522-specific-dataset-process#gsc.tab=0)
category using the template provided.
If a new format is required then undertake a [format
review](https://ceda-internal.helpscoutdocs.com/article/4953-format-review-procedure).
Tools to use for ingestion, checking etc. are listed under the
[tools](https://ceda-internal.helpscoutdocs.com/category/4521-tools)
section.
","https://help.ceda.ac.uk/article/4682-archiving-overview#ingest-{#ingest-children-count=""0""}",858,72
"Cataloging and indexing {#cataloging-and-indexing children-count=""0""}","Archiving data also requires the content to be catalogued to aid
discovery and indexing to enable their use/access through archive tools.
See [CEDA data catalogue (MOLES)
docs](https://ceda-internal.helpscoutdocs.com/category/4523-catalogue#gsc.tab=0)
for details on preparing content for the data catalogue. Indexing of
content automatically happens through the File Based Index system,
triggered via ingest and associated functions that utilise the Deposit
Client. See \'[Filling in missing datasets for
data.ceda](https://ceda-internal.helpscoutdocs.com/article/4829-filling-in-missing-datasets#gsc.tab=0)\`
documentation items require re-indexing (e.g. to pick up parameters).
Where metadata scraping is not possible from the FBI (e.g. incomplete
metadata, missing content, removed content, external content, offline
content) enter details into the [CEDA Manual Metadata Store
(CMMS)](https://ceda-internal.helpscoutdocs.com/article/4816-cmms).
","https://help.ceda.ac.uk/article/4682-archiving-overview#cataloging-and-indexing-{#cataloging-and-indexing-children-count=""0""}",949,103
"Archive management {#archive-management children-count=""0""}","Storage media varies in the CEDA archive and some data are stored in the
Near-Line Archive. See
[here](https://ceda-internal.helpscoutdocs.com/article/4987-nla) for
details on using the NLA for storage. Move to tape. User help guide for
using the NLA is available [here](Near-line%20archive%20(NLA))
","https://help.ceda.ac.uk/article/4682-archiving-overview#archive-management-{#archive-management-children-count=""0""}",300,39
"End of data lifecycle {#end-of-data-lifecycle children-count=""0""}","Data may be deaquisitioned according to the [CEDA Archive withdrawal
policy](https://help.ceda.ac.uk/article/4730-withdrawal-policy) In such
cases it is essential follow the \'[Remove data
procedure](https://ceda-internal.helpscoutdocs.com/article/4543-remove-data-procedure)\'
both for simple removal of empty directories (ie. data failed to arrive)
and actual removal of content. This ensures that a full audit trail is
maintained related to data management activities.
","https://help.ceda.ac.uk/article/4682-archiving-overview#end-of-data-lifecycle-{#end-of-data-lifecycle-children-count=""0""}",472,53
Ingest and Publication of ESGF Datasets,"::: {#wikipage}
[Original trac
page](http://team.ceda.ac.uk//trac/ceda/wiki/opman/ingest/ESGFIngestAndPublication)
::: wiki-toc
1.  [Ingest and Publication of ESGF
    Datasets](#IngestandPublicationofESGFDatasets)
    1.  [Introduction](#Introduction)
:::
",https://help.ceda.ac.uk/article/4215-ingest-and-publication-of-esgf-datasets,257,17
Introduction {#Introduction},"This page provides details of the ingest and publication workflow for
datasets that are bound for the Earth System Grid Federation (ESGF)
services. The basic workflow is as follows:
1.  A collection of files arrives (a \""batch\"")
2.  Run the ceda-cc compliance checker on the batch.
3.  Run the drs_tool to ingest the data into the archive.
4.  Run the post_ingest_processor.py script to post-process/check the
    data once in the archive.
5.  Run the generate_mapfiles.py script to generate mapfiles to be used
    for ESGF publication.
6.  Run the ESGF publisher to scan the data.
7.  Run the ESGF publisher to generate THREDDS catalogues.
8.  Run the ESGF publisher to put the data in the ESGF Search Catalogue.
Whilst developing the documentation we are documenting some examples:
-   [opman/ingest/ESGFIngestAndPublication/SPECSIngest](http://ceda-internal.helpscoutdocs.com/article/4169-specs-ingest-and-publication-to-esgf){.wiki}
There is also a page on [setting up the software
environment](http://ceda-internal.helpscoutdocs.com/article/4382-setting-up-the-ingest-toolsenvironment){.wiki}
. The following page looks at what needs to be done to automate this
process:
[opman/ingest/ESGFIngestAndPublication/SPECSIngestAutomation](http://ceda-internal.helpscoutdocs.com/article/4240-automation-of-specs-ingest){.wiki}
:::
",https://help.ceda.ac.uk/article/4215-ingest-and-publication-of-esgf-datasets#introduction-{#introduction},1331,155
JIRA Data Management tracking tool,"[Log in - CEH Project Issue Tracking](https://jira.ceh.ac.uk/)
",https://help.ceda.ac.uk/article/4905-jira-data-management-tracking-tool,63,7
"What is JIRA? {#what-is-jira children-count=""0""}","JIRA is a software that is designed to help teams manage work and track
issues. We are using the paid NERC version run by CEH for all the data
centres. You will find there are a lot of projects and different
settings depending on the data centre and project. The
projects/components and dashboards for CEDA all start with \'CEDA\' at
the beginning to make it easier to search. Individual dashboards and
filters should start with your first name to make it easier to search
for.
","https://help.ceda.ac.uk/article/4905-jira-data-management-tracking-tool#what-is-jira?-{#what-is-jira-children-count=""0""}",478,85
"How do I get a login? {#how-do-i-get-a-login children-count=""0""}","To request a login you will need to contact CEH: jirahelp\@ceh.ac.uk
You will then receive an email to set up a password.
","https://help.ceda.ac.uk/article/4905-jira-data-management-tracking-tool#how-do-i-get-a-login?-{#how-do-i-get-a-login-children-count=""0""}",122,22
"How is it set up? {#how-is-it-set-up children-count=""0""}","There are 2 main CEDA projects:
-   CEDA NERC Grants - This is used to track all the NERC projects.
-   CEDA Non NERC projects - This is used to track all the other non
    NERC projects.
These can be found under \'Projects\' - \'View all projects\' - Search
\'CEDA\'.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5f75b4e046e0fb0017988a5d/file-LaqYjg1wpn.png)
There is one main \'CEDA Data Management (DMP) Overview\' dashboard
where everyone\'s projects/tasks can be viewed (similar to the summary
page on the old DMP tool). This can be found under \'Dashboards\' -
\'Manage dashboards\' - Search \'CEDA\'. When you click on the dashboard
make sure the star symbol is colored in. This will then add the
dashboard to your favorites and will appear under your dashboards.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5f75b5124cedfd0017dcd59a/file-ZwhxW4hqDl.png)
**A main issue ticket is equivalent to a project page and the sub-tasks
are equivalent to the reminders. **
","https://help.ceda.ac.uk/article/4905-jira-data-management-tracking-tool#how-is-it-set-up?-{#how-is-it-set-up-children-count=""0""}",1054,138
"How do issues get added? {#how-do-issues-get-added children-count=""0""}","There are two different ways and that depends on the project i.e.
whether it is NERC/NON- NERC.
For the CEDA NERC Grants: Issues (projects) are generated from the new
datamad system and will appear under the \'CEDA New Unassigned Grants\'
filter in the \'CEDA Data Management (DMP) overview. Subtasks are
automatically generated for the initial contact, create DMP, check
progress with PI and Project nearing end date (this is very similar to
the reminders in the old DMP tool). Other subtasks can be manually
added. Kate will initially triage these projects and will make contact
with the PI and set up a DMP. Projects may then be passed on to other
Data scientists.
For the CEDA Non NERC project: Issues are added in manually. At the top
of the page press \'Create\'. This will take you to create issue page.
The project must be \'CEDA Non NERC\' project and the issue type must be
\'Data Management Tracking\'. You can then add in the relevant
information, note, not all the fields are relevant to a non-NERC
project. This can\'t be changed. Once you have created an issue you can
then add in the relevant sub-tasks to your project. Make sure you assign
yourself the project and subtasks, they are assigned to Kate
initially.![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5f75cc844cedfd0017dcd60d/file-xephOSgEuG.png)
","https://help.ceda.ac.uk/article/4905-jira-data-management-tracking-tool#how-do-issues-get-added?-{#how-do-issues-get-added-children-count=""0""}",1361,212
"What is the workflow? {#what-is-the-workflow children-count=""0""}","There are two different workflows at present. One workflow for the main
issue (project page) and another for the sub-tasks (reminders).
The main issue workflow follows the main stages of the NERC data
management process. New Grant - Initial Contact- DMP Comms - Progress -
Data Delivery - Completed. Note that you can move backward and forwards
in the workflow.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fbbcc3c4cedfd00165b3073/file-1NuiznzH9m.png)The
Sub tasks follow a different workflow and follow a standard structure.
To do - Scheduled - Hold - In progress - Resolved - Completed.
Definitions for each status:
-   To do - A task comes in that needs to be done
-   Hold - the task has been put on hold
-   In Progress - When the task is in progress
-   Resolved - When the task has been completed
-   Closed - When you are happy that the task is completed and nothing
    else is needed to be done. This can also be used as a won\'t fix and
    a comment can be added to the subtask.
","https://help.ceda.ac.uk/article/4905-jira-data-management-tracking-tool#what-is-the-workflow?-{#what-is-the-workflow-children-count=""0""}",1034,169
"![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fbbc96bcff47e0017d347b5/file-qS7YO6jW2n.png) {#section children-count=""0""}",,"https://help.ceda.ac.uk/article/4905-jira-data-management-tracking-tool#![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fbbc96bcff47e0017d347b5/file-qs7yo6jw2n.png)-{#section-children-count=""0""}",0,0
"What does an issue look like? {#what-does-an-issue-look-like children-count=""0""}","The main issue (project page) is shown below. I am going to explain each
part starting at the top.
At the top there is \'CEDA NERC Grants\' in Blue this tells you what
project the issue is in. It will either be in the NERC grants or Non
NERC grants project.
\'CEDA-92\' - This is the number given to the issue this is the unique
identifier. This can be used to link up the helpdesk query to the JIRA
Issue and can also be used in filters.
Next is the title of the project/grant.
On the next line are various buttons: Edit is used to edit the grant and
project details and dates. Comment is used to add a comment to the
project page. Assign is where you can assign the issue to a data
scientist. More is where other actions can take place. The rest of the
buttons are to do with the workflow i.e. in the case below as I will
click on the initial contact button when I am contacting the PI for the
first time. This will keep track of which projects are at which stage.
Type: this is always set to \'Data Management tracking\'.
Priority: this function is not being used.
Components: are a defined set of labels. Note that all the other
datacentre comments are visible as well. We are only using the ones that
have \'CEDA\' at the beginning. This is where we can mark a \'CEDA DMP
agreed\', \'CEDA Unrepsondive\' , \'CEDA ended with data to come\'. For
a full list start to type CEDA in the box.
Labels: are not a controlled list and will not be used.
Update frequency option can also be added
Then are the details of the project/grant and description/abstract.
Taken from datamad.
An attachment section where you can attach any files/images.
Issues to links - this is where you can add any useful links. The link
to datamad will already have been created. The link to helpscout query
will appear here too (see below how to link a query).
Sub-tasks- This is equivalent to the reminder section in the old DMP
tool. Each has it\'s own page and a workflow.
Activity- this is where you can record and comments
On the right-hand side is where you will find who is assigned the
issue/task and dates relevant to the task/project/issue. 
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fbbca0d4cedfd00165b305a/file-LXtEudkjRZ.png) sub-task
looks similar but doesn\'t have all the details and descriptions in
unless you add it in by hand. It is linked to the main issue at the top.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5f75d8f7c9e77c00162136f8/file-Zjx5sbosvP.png)
","https://help.ceda.ac.uk/article/4905-jira-data-management-tracking-tool#what-does-an-issue-look-like?-{#what-does-an-issue-look-like-children-count=""0""}",2542,424
"How do I create a dashboard? {#how-do-i-create-a-dashboard children-count=""0""}","Every data scientist should have their own to monitor their projects and
subtasks. 
To create a dashboard go to \'Dashboards\' - \'Manage dashboards\' and
click \'Create new dashboard\'. 
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5f75b4a052faff0016aeb8b8/file-7yA9RF7cLY.png)
Call your dashboard by your \'\<First name> Data Management board\'\'.
This will make it easier when searching.
You can then control the settings on who can view and edit your board,
by selecting project/group and using the add button.
A new blank dashboard will appear. Gadgets can then be added by using
filters. You can have as many gadgets as you like. The best advice is to
play about with and work out what you want to see.
Here is an example of Kate\'s dashboard
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fbbcc02cff47e00160bca2b/file-bT2BOfV8iV.png)
","https://help.ceda.ac.uk/article/4905-jira-data-management-tracking-tool#how-do-i-create-a-dashboard?-{#how-do-i-create-a-dashboard-children-count=""0""}",924,115
"How do I create filters? {#how-do-i-create-filters children-count=""0""}","Filters are used to search for certain types of projects/sub-tasks at
different stages, assignees etc. They can be saved and are used in the
gadgets for the dashboards and are also good for providing stats. 
To create filters go to \'Issues\' - \'Search for issues\'.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5f75b790cff47e001a58a07d/file-bczbYZFM0h.png)
Make sure you filter by projects first as this will remove all the other
datacentres issues. At projects select CEDA NERC and Non NERC projects. 
Then you can filter by types. There are two main types used in the CEDA
projects:
-   All Standard Issue Types - these represent the project pages in the
    old DMP tool
-   All Sub-task Issue Types - these represent the reminders in the old
    DMP tool
Then you can filter on Status (Note that all the statuses from all
datacentres appear in the list. Check out our workflows first above to
decide on what to filter).
Then you can filter on Assignee. 
Note. More advanced searches can take place.
Once you are happy with your search press save as at the top of the page
and label it by your \'\<First name> or \'CEDA\' followed by a sensible
name. You can edit the filters after.
You can add the filters to your dashboard. Go into the dashboard and
press \'add gadget\'. Choose \'Filter Results\'. Then you can add your
relevant filter.
","https://help.ceda.ac.uk/article/4905-jira-data-management-tracking-tool#how-do-i-create-filters?-{#how-do-i-create-filters-children-count=""0""}",1387,223
"How do I link my query to helpscout? {#how-do-i-link-my-query-to-helpscout children-count=""0""}","The link is made in helpscout. In the data management helpdesk, on a
query to the right handside there is a JIRA icon with \'link issue\'.  
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5f75d977c9e77c00162136ff/file-n7TeM2eFIn.png)This
will take you to a search box. Note you will be able to see every issue
in JIRA from all the other datacentres. In the box search for the name
of the project or NERC grant number or the unique identifier
(CEDA-\*\*). Then click Link issue. The query will then be linked to the
project or subtask and will appear in the issue links in JIRA. There can
be more than one link.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5f75d9ac52faff0016aeb9b5/file-i2w4wckBvI.png)
","https://help.ceda.ac.uk/article/4905-jira-data-management-tracking-tool#how-do-i-link-my-query-to-helpscout?-{#how-do-i-link-my-query-to-helpscout-children-count=""0""}",784,101
"How do I track my projects using JIRA? {#how-do-i-track-my-projects-using-jira children-count=""0""}","To track your projects you use the workflow buttons at the top of the
Project page and subtasks.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fbbcb4acff47e00160bca25/file-xRbzuIsL4k.png)
Components are when we can mark projects with the labels below:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5fbbcacc4cedfd001610f8d4/file-lqckvPTnq9.png)
You can keep track of all your tickets by setting up your dashboard.
","https://help.ceda.ac.uk/article/4905-jira-data-management-tracking-tool#how-do-i-track-my-projects-using-jira?-{#how-do-i-track-my-projects-using-jira-children-count=""0""}",496,44
"Who has admin rights/ who do ask to make changes? {#who-has-admin-rights-who-do-ask-to-make-changes children-count=""0""}","Kate has got admin rights to edit the projects, main CEDA dashboard, and
filters. If you want something changing ask Kate first. If she cannot do
the relevant action contact CEH: jirahelp\@ceh.ac.uk.
","https://help.ceda.ac.uk/article/4905-jira-data-management-tracking-tool#who-has-admin-rights/-who-do-ask-to-make-changes?-{#who-has-admin-rights-who-do-ask-to-make-changes-children-count=""0""}",200,32
Background,"The commands in this document will index information about files and
directories so that it can be displayed in `data.ceda.ac.uk`. By virtue
of doing these steps the files will also get submitted to the \'slow\'
queue for eventual in-depth scanning to try and pull our internal file
metadata if possible.
**dap.ceda.ac.uk** - Is a live file listing of the archive (similar to
an `ls`). This is used to serve the files for download and provide
OPeNDAP services.
**data.ceda.ac.uk** -  Is based on the Elasticsearch indices. This is
the browse interface for the archive. There are two indices but for the
purposes of this system, the use of \'FBI\' below relates to the whole
indexing process itself.
Due to historical issues with the indexing system it is possible that
there are still some datasets which have not been fully indexed. To
address this there is a process which is set to go round the entire
archive, storage spot by spot, and check the index against the file
system but this will take months to complete a circuit of the archive.
In the meanwhile, where parts of the archive are known to exist, but are
not visible in **data.ceda.ac.uk** a** **forced scan can be initiated on
targeted areas to give more immediate indexing and visibility.
",https://help.ceda.ac.uk#background,1253,213
**Contents:**,"[How long will things take?](#system)\
[- Check current queue size](#current_queue)[\
Activating the environment](#activate-environment)\
[Choosing the right command](#choosing-right-command)\
[Commands](#commands)\
- [fbi_directory_check](#fbi-directory-check)\
- [fbi_rescan_dir](#fbi-rescan-dir)\
- [fbi_q\_check](#fbi-q-check)
",https://help.ceda.ac.uk#**contents:**,331,24
How long will things take? A rough overview of the system.,"Events processed by the deposit server such as deposit, removal,
directory creation and removal are, **as long as they use the correct
tools**, passed onto the FBI exchange to make changes to the
Elasticsearch indices which provide data to data.ceda.ac.uk. (As such
direct archive interaction using ls, rm, mkdir etc should be avoided).
All messages are passed to the fast and slow queues. The fast queue gets
as much information as it can from the message without touching the
filesystem. This should be enough that it displays in data.ceda.ac.uk in
a short time frame. The slow queue follows this and adds richer
metadata.
From the time you can see the files in the archive, you should expect to
see your new files in **data.ceda.ac.uk** **within 1 day.** It will
likely be much quicker than this (could be immediate) but 1 day gives
sufficient lag to allow any surges in ingesting traffic to be processed
as this is an asynchronous process.
The tools provided in this document feed into the FBI exchange and are
used to manually modify the indices. 
All of these commands go through a queuing system so how long it will
take will depend on how many items are in each of the queues.
**You can check the current queue size:**
[**https://archdash.ceda.ac.uk/current/es_queue**](https://archdash.ceda.ac.uk/current/es_queue)
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5d9db31e04286364bc902c17/file-vhtpep0G0c.png)
",https://help.ceda.ac.uk#how-long-will-things-take?-a-rough-overview-of-the-system.,1457,215
How do the tools work?,"The **fbi_directory_check** submits a list of directories to a local
queue. A process (crawler) then takes a directory off the queue, checks
the archive against the index and sends the difference to the FBI
exchange. \
**Note: There can be a delay from submitting using fbi_directory_check
to seeing things appear due to this intermediate step**
**fbi_rescan_dir** treats the directory in question as if being scanned
for the first time and bypasses the checks. This can be useful if the
item is present but the data is incorrect or if there is a whole
directory which is not showing up so you can skip the checks.
",https://help.ceda.ac.uk#how-do-the-tools-work?,615,105
Activating the correct environment {#activate-environment},"1.  Login to ingest machine
2.  activate the environment\
    \
        conda activate ingest_py3
3.  Run command
",https://help.ceda.ac.uk#activating-the-correct-environment-{#activate-environment},114,16
Choosing the right command {#choosing-right-command},"In general, if you can see partially complete information then you
should use **fbi_directory_check**. If there are large swathes missing,
then **fbi_rescan_dir** will add them. If you wish to overwrite out of
date file and directory metadata (MOLES catalogue labels, sizes,
locations, variables) then use **fbi_rescan_dir.** Look at the decision
tree below to help you pick the right tool.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5db992322c7d3a7e9ae33f94/file-5MEo9ilJEp.png)
",https://help.ceda.ac.uk#choosing-the-right-command-{#choosing-right-command},524,60
fbi_directory_check {#fbi-directory-check},"**If you wish to overwrite the metadata (e.g. MOLES catalogue is
incorrect, use [fbi_rescan_dir](#fbi_rescan_dir))**
Submit directories to be checked for consistency between the archive and
the indices. This command does not check the content, only decides
whether something should be added or removed from the index based on the
files. This command submits items to a local queue for checking. If you
believe the content to be incorrect, you should use 
[fbi_rescan_dir](#fbi-rescan-dir)
**Note: There can be a delay from submitting using fbi_directory_check
to seeing things appear due to the checking step. Especially if the
number of submitted directories is high. You can check how many items
there are in this queue using **[fbi_q\_check](#fbi-q-check)** and look
at the user-submitted queue count.**
Usage:
`fbi_directory_check (--dir <dir> | --file <file>) [-r] --conf <conf>`
Examples:
`fbi_directory_check --conf ~/software/fbi_directory_check/fbi-directory-check/fbi_directory_check/conf/index_updater.ini --dir /neodc/esacci/cloud/data/phase-2/L3C -r `
`fbi_directory_check --conf ~/software/fbi_directory_check/fbi-directory-check/fbi_directory_check/conf/index_updater.ini --file list_of_directories.txt`
Options:
  Option    Description
  --------- -----------------------------------------
  -r        Will search all directories recursively
  \--dir    Accepts a directory path
  \--file   Accepts a file input
  \--conf   Path to configuration file
",https://help.ceda.ac.uk#fbi_directory_check-{#fbi-directory-check},1467,169
fbi_rescan_dir {#fbi-rescan-dir},"Rescan the given directory. This will overwrite the content in the
indices for this directory - useful if you know the metadata is out of
date/incorrect.
Usage:\
`fbi_rescan_dir <dir> [-r] [--no-files] [--no-dirs] --conf <conf>`
Examples:
`fbi_rescan_dir --conf ~/software/fbi_directory_check/fbi-directory-check/fbi_directory_check/conf/index_updater.ini /neodc/esacci/cloud/data/phase-2/L3C --no-files` -
Add all the directories in the given directory
`fbi_rescan_dir --conf ~/software/fbi_directory_check/fbi-directory-check/fbi_directory_check/conf/index_updater.ini /neodc/esacci/cloud/data/phase-2/L3C -r --no-dirs ` -
Add all the files below the given directory
Options:
  Option        Description
  ------------- -----------------------------------------------------------------
  -r            Will search all directories recursively
  \--no-files   Will exclude files from the results and only change directories
  \--no-dirs    Will exclude directories from the results and only change files
  \--conf       Path to configuration file
",https://help.ceda.ac.uk#fbi_rescan_dir-{#fbi-rescan-dir},1047,103
fbi_q\_check {#fbi-q-check},"Display the current number of directories in the user-submitted and bot
queues. These are processed to check the difference between the index
and the archive and then actions are submitted to update the indices.\
The user-submitted queue is given priority over the bot queue and is
built from items submitted through **fbi_directory_check**
Usage:\
`fbi_q_check`
",https://help.ceda.ac.uk#fbi_q\_check-{#fbi-q-check},363,54
{#system},,https://help.ceda.ac.uk#{#system},0,0
Introduction,"This page links to information about software utilities that are either
provided by the CEDA or are freely available from elsewhere. Please note
that much of the available software is documented within the individual
dataset documentation and software directories.
",https://help.ceda.ac.uk/article/101-data-tools-preparation-visualisation#introduction,265,39
Software,"Software packages not described elsewhere on our website are listed
below:
-   [Xconv/Convsh](https://ncas-cms.github.io/xconv-doc/html/index.html) -
    for format conversion, simple display, grid interpolation and
    subsetting. Xconv/Convsh reads Met Office PP and UM formats, GRIB,
    NetCDF, GrADS and DRS.
-   [CEDA Web Processing Service
    (WPS)](https://help.ceda.ac.uk/article/4977-ceda-wps) - A web tool
    of data processing and subsetting services.
-   **Deprecated: **[\'Using IDL in
    Meteorology\'](http://cms.ncas.ac.uk/documents/IDL/ncas_idl.pdf) \*\*
    NCAS-CMS have shifted to using Python so this guide is no longer
    current.\
    This 133 page guide written by NCAS-CMS is a good starting point to
    using IDL. It includes many example plots and tips on using IDL.
",https://help.ceda.ac.uk/article/101-data-tools-preparation-visualisation#software,800,97
DataCitation CEDA,"::: {#wikipage}
[](http://team.ceda.ac.uk//trac/ceda/wiki/DataCitation)
::: wiki-toc
\[Alison w - review and keep\]
",https://help.ceda.ac.uk/article/4359-datacitation-ceda,116,11
Page Contents,"1.  1.  [Data Citation and DOIs](#DataCitationandDOIs)
    2.  [If you want to assign a DOI to a Dataset (Observation in
        MOLES):](#IfyouwanttoassignaDOItoaDatasetObservationinMOLES:)
    3.  [If you want to assign a DOI to a Dataset Collection
        (Observation
        ...](#IfyouwanttoassignaDOItoaDatasetCollectionObservationCollectioninMOLES:)
    4.  [Adding DOI string to MOLES 3
        record:](#AddingDOIstringtoMOLES3record:)
:::
",https://help.ceda.ac.uk/article/4359-datacitation-ceda#page-contents,451,43
Data Citation and DOIs {#DataCitationandDOIs},"**Why this is a good idea:**
-   Encourages users to submit data to us
    -   with complete metadata
    -   in useful formats
<!-- -->
-   Encourages collaboration, data reuse, improves scientific record,
    better value-for-money, etc. etc. etc\....
Citation (i.e. assigning a DOI) is something that we can do in-house
(and is valuable to our users in it\'s own right).
Publication (involving full academic peer review) has to be done in
collaboration with a journal. See
[here](http://team.ceda.ac.uk/trac/ceda/attachment/wiki/DataCitation/LibraryTutorial_21May2013.pdf)
for a presentation (aimed at data producers) all about data citation and
publication.
**Why DOIs?**
-   They are actionable, interoperable, persistent links for (digital)
    objects
-   Scientists are already used to citing papers using DOIs
-   Pangaea assign DOIs, and ESSD use DOIs to link to the datasets they
    publish
-   The British Library gave us an allocation of 500 DOIs to assign to
    datasets as we saw fit.
**Choosing datasets to cite**
Dataset has to be:
-   Stable (i.e. not going to be modified)
-   Complete (i.e. not going to be updated)
-   Permanent -- by assigning a DOI we're committing to make the dataset
    available for posterity
-   Good quality -- by assigning a DOI we're giving it our data centre
    stamp of approval, saying that it's complete and all the metadata is
    available
**Authors\' permissions**
For legacy datasets, we will need to ask permission to add a DOI to the
dataset. For new datasets, permission to add a DOI will be part of the
Data Management Plan.
**The Rules**
The official NERC data citation guidance documents are attached to this
page.
-   The guidelines for scientists document can be found at [[ ]{.icon}
    http://www.nerc.ac.uk/research/sites/data/doi.asp](http://www.nerc.ac.uk/research/sites/data/doi.asp){.ext-link}
-   The guidance document and procedures for minting a DOI can be found
    in the Operational Policies and Procedures section of the EDC
    central iShare site at [[ ]{.icon}
    https://ishare.apps.nerc.ac.uk/teams/edcs/default.aspx](https://ishare.apps.nerc.ac.uk/teams/edcs/default.aspx){.ext-link}
When a dataset is cited that means:
-   There will be bitwise fixity
-   With no additions or deletions of files
-   No changes to the directory structure in the dataset ""bundle""
A DOI should point to a html representation of some record which
describes a data object.
-   Upgrades to versions of data formats will result in new editions of
    datasets -- rules about how we do that to be decided.
-   If there is a new version of the dataset, a new DOI is needed.
-   We can only cite datasets where we have full authorization to
    distribute it in perpetuity (i.e. we won\'t assign DOIs to Met
    Office data, as Met Office reserve the right to get us to delete it
    whenever they want.)
We will need an inventory of files and their associated metadata
records.
We can give separate DOIs to datasets and their metadata records (but we
probably don't want to).
DOI metadata will be scraped from the MOLES metadata records. It will
follow the
[DataCite?](http://team.ceda.ac.uk//trac/ceda/wiki/DataCite){.missing
.wiki} metadata schema as given at [[ ]{.icon}
http://schema.datacite.org/](http://schema.datacite.org/){.ext-link}
**Landing page rules**
Landing pages tend to have metadata about the object being referenced;
e.g. author, abstract, publication date\...
We decided to use the deployments records in our metadata catalogue as
our DOI landing page. So, for the GBS dataset, we're using: [[ ]{.icon}
http://team.badc.rl.ac.uk:50001/view/badc.nerc.ac.uk\_\_ATOM\_\_dep_11902946270621452](http://team.badc.rl.ac.uk:50001/view/badc.nerc.ac.uk__ATOM__dep_11902946270621452){.ext-link}
And [[ ]{.icon}
http://badc.nerc.ac.uk/view/badc.nerc.ac.uk\_\_ATOM\_\_dep_11902119479621181](http://badc.nerc.ac.uk/view/badc.nerc.ac.uk__ATOM__dep_11902119479621181){.ext-link}
-   We can change the landing page any time we like, but you had better
    be able to get to your digital object from there!
<!-- -->
-   Landing pages can have query based links to other things (papers
    which cite this dataset) etc \...
    -   The (DOI-mandatory) metadata describing the dataset shouldn\'t
        change as it describes the digital object and represents it
        faithfully. It ought not change, since any change to it, ought
        to reflect a change to the digital object (and that should
        trigger a new DOI)
    -   The original landing page can indicate that a newer version of
        the dataset exists, but it should still point to the older
        version!
**File Inventory**
Probably will use checkm, but to be decided.
**Human readable citation string**
The human readable citation string should follow the guidelines laid out
in section 2.2 of the current
[DataCite?](http://team.ceda.ac.uk//trac/ceda/wiki/DataCite){.missing
.wiki} metadata schema ( [[ ]{.icon}
http://schema.datacite.org/](http://schema.datacite.org/){.ext-link} ),
as copied below:
> Because many users of this schema are members of a variety of academic
> disciplines,
> [DataCite?](http://team.ceda.ac.uk//trac/ceda/wiki/DataCite){.missing
> .wiki} remains discipline‐agnostic concerning matters pertaining to
> academic style sheet requirements. Therefore,
> [DataCite?](http://team.ceda.ac.uk//trac/ceda/wiki/DataCite){.missing
> .wiki} recommends rather than requires a particular citation format.
> In keeping with this approach, the following is the recommended format
> for rendering a
> [DataCite?](http://team.ceda.ac.uk//trac/ceda/wiki/DataCite){.missing
> .wiki} citation for human readers using the first five properties of
> the schema:
> Creator (
> [PublicationYear?](http://team.ceda.ac.uk//trac/ceda/wiki/PublicationYear){.missing
> .wiki} ): Title. Publisher. Identifier
> It may also be desirable to include information from two optional
> properties, Version and
> [ResourceType?](http://team.ceda.ac.uk//trac/ceda/wiki/ResourceType){.missing
> .wiki} (as appropriate). If so, the recommended form is as follows:
> Creator (
> [PublicationYear?](http://team.ceda.ac.uk//trac/ceda/wiki/PublicationYear){.missing
> .wiki} ): Title. Version. Publisher.
> [ResourceType?](http://team.ceda.ac.uk//trac/ceda/wiki/ResourceType){.missing
> .wiki} . Identifier
> For citation purposes, the Identifier may optionally appear both in
> its original format and in a linkable, http format, as it is practiced
> by the Organisation for Economic Co‐operation and Development (OECD),
> as shown below.
> Regarding the
> [PublicationYear?](http://team.ceda.ac.uk//trac/ceda/wiki/PublicationYear){.missing
> .wiki} ,
> [DataCite?](http://team.ceda.ac.uk//trac/ceda/wiki/DataCite){.missing
> .wiki} recommends, for resources that do not have a standard
> publication year value, to submit the date that would be preferred
> from a citation perspective. Here are several examples:
-   Irino, T; Tada, R (2009): Chemical and mineral compositions of
    sediments from ODP Site 127‐ 797. Geological Institute, University
    of Tokyo.doi:10.1594/PANGAEA.726855. [[ ]{.icon}
    http://dx.doi.org/10.1594/PANGAEA.726855](http://dx.doi.org/10.1594/PANGAEA.726855){.ext-link}
-   Geofon operator (2009): GEFON event gfz2009kciu (NW Balkan Region).
    [GeoForschungsZentrum?](http://team.ceda.ac.uk//trac/ceda/wiki/GeoForschungsZentrum){.missing
    .wiki} Potsdam (GFZ). doi:10.1594/GFG.GEOFON.gfz2009kciu. [[
    ]{.icon}
    http://dx.doi.org/10.1594/GFZ.GEOFON.gfz2009kciu](http://dx.doi.org/10.1594/GFZ.GEOFON.gfz2009kciu){.ext-link}
-   Denhard, Michael (2009): dphase_mpeps: MicroPEPS LAF‐Ensemble run by
    DWD for the MAP D‐PHASE project. World Data Center for Climate. doi:
    10.1594/WDCC/dphase mpeps. [[ ]{.icon}
    http://dx.doi.org/10.1594/WDCC/dphase_mpeps](http://dx.doi.org/10.1594/WDCC/dphase_mpeps){.ext-link}
**DOI strings** Prefix (unique to NERC): 10.5285 - followed by a unique
string of our choice.
We decided (along with all the other NERC data centres) to use GUIDs
(Globally Unique Identifier) as the unique string.
The value of a GUID is represented as a 32-character hexadecimal string,
such as {21EC2020-3AEA-1069-A2DD-08002B30309D}, and is usually stored as
a 128-bit integer. The total number of unique keys is 2128 or 3.4×1038
--- roughly 2 trillion per cubic millimeter of the entire volume of the
Earth. This number is so large that the probability of the same number
being generated twice is extremely small.
Our DOIs will look something like this:
10.5285/e8f43a51-0198-4323-a926-fe69225d57dd (you can use a website like
[[ ]{.icon}
this](http://www.guidgenerator.com/online-guid-generator.aspx){.ext-link}
to generate a GUID )
",https://help.ceda.ac.uk/article/4359-datacitation-ceda#data-citation-and-dois-{#datacitationanddois},8729,1096
If you want to assign a DOI to a Dataset (Observation in MOLES): {#IfyouwanttoassignaDOItoaDatasetObservationinMOLES:},"> Note that this process is subject to change, depending on the amount
> of DOIs that need to be minted. This document will be updated as the
> process changes. (Last update 7 June
1.  Requester confirms that the dataset to be cited meets the criteria
    specified above for DOI assignment.
    1.  If the dataset does not meet the criteria, the dataset author
        should be informed of what needs to be done to the dataset to
        allow it to achieve the required criteria.
2.  Requester puts a message in the #ceda_doi slack channel
    (<https://ncas-talk.slack.com/messages/C1B02HTRS/convo/C0PAPC5L0-1496742217.927695/>)
    with the URL of the catalogue page for the dataset.
3.  DOI minter picks up the request (and lets the others know by doing a
    thumbs up on the request)
4.  DOI manager checks the the landing page.
5.  DOI issuer mints the DOI.
6.  DOI issuer sends a slack message (on the #ceda_doi channel) to the
    requester confirming that the DOI has been minted.
7.  Requester sends a congratulatory email to dataset authors,
    confirming DOI has been minted.
    1.  Note that there is some latency in the DOI system, and newly
        minted DOIs may not resolve for the first 24 hours. Check back
        the next day, and ping the #ceda_doi channel if there is an
        issue.
",https://help.ceda.ac.uk/article/4359-datacitation-ceda#if-you-want-to-assign-a-doi-to-a-dataset-(observation-in-moles):-{#ifyouwanttoassignadoitoadatasetobservationinmoles:},1315,212
If you want to assign a DOI to a Dataset Collection (Observation Collection in MOLES): {#IfyouwanttoassignaDOItoaDatasetCollectionObservationCollectioninMOLES:},"It is possible to add a DOI to a dataset collection in addition to lower
level datasets. However, before this is possible each component dataset
within the collection MUST also match the required standards for a DOI -
i.e. bitwise fixity etc - in addition to the collection as a whole being
of standard. As such, collections containing 3rd party datasets will not
be able to receive a DOI for the collection as those parts of the
collection are outside the control of the DOI requester.
",https://help.ceda.ac.uk/article/4359-datacitation-ceda#if-you-want-to-assign-a-doi-to-a-dataset-collection-(observation-collection-in-moles):-{#ifyouwanttoassignadoitoadatasetcollectionobservationcollectioninmoles:},487,86
Adding DOI string to MOLES 3 record: {#AddingDOIstringtoMOLES3record:},"Once you have your DOI for the dataset/dataset collection in question
you need to add it to the relevant MOLES
[Observation/Observation?](http://team.ceda.ac.uk//trac/ceda/wiki/Observation/Observation){.missing
.wiki} Collection page that will act as the landing page for the
resource.
This [[ ]{.icon} example
record](http://catalogue.ceda.ac.uk/admin/cedamoles_app/observation/3528/){.ext-link}
demonstrates the fields that have been completed to add the DOI
information to the MOLES record as noted below.
To ensure that the DOI information is correctly included in the
[Observation/Observation?](http://team.ceda.ac.uk//trac/ceda/wiki/Observation/Observation){.missing
.wiki} Collection citation string:
1.  Add the DOI string into the \""Identifiers\"" section, entering the
    full DOI url into the URL field and selecting \""DOI\"" as the
    Indentifier Type. (a shortURL will be auto-generated for you). E.g.
    the full DOI URL is of the form:
    ``` wiki
    http://dx.doi.org/10.5285/E8F43A51-0198-4323-A926-FE69225D57DD
    ```
<!-- -->
2.  Change the \""publication status\"" to \""Citable\""
See
[here](http://team.ceda.ac.uk/trac/ceda/attachment/wiki/DataCitation/HowToRequestADOI_18July2013.pdf)
for a presentation about requesting a DOI.
:::
",https://help.ceda.ac.uk/article/4359-datacitation-ceda#adding-doi-string-to-moles-3-record:-{#addingdoistringtomoles3record:},1255,138
MOLES API End Points,"This page details the various API endpoints for MOLES.
",https://help.ceda.ac.uk/article/4710-moles-api,55,9
"In this article {#in-this-article children-count=""0""}","-   [Catalogue API Server](#api_server)
-   [RESTful API (api/v2)](#rest_api)
-   [MOLES1/MOLES2 redirect API (legacy_lookup)](#leg)
-   [Path to MOLES Record API](#path)
-   [Completeness Check (Ingestion tool)](#complete)
-   [DRS to Archive Path look up](#drs)
-   [All Observation Mappings](#all-observation-mappings)
","https://help.ceda.ac.uk/article/4710-moles-api#in-this-article-{#in-this-article-children-count=""0""}",322,35
"Catalogue API Server {#api_server children-count=""0""}","Though the API end points can be accessed via the main catalogue site
(catalogue.ceda.ac.uk) a dedicated API server is also available which
should be used to support bulk API calls to avoid over burdening the
main catalogue service. This is available at:\
<http://api.catalogue.ceda.ac.uk/>
","https://help.ceda.ac.uk/article/4710-moles-api#catalogue-api-server-{#api_server-children-count=""0""}",291,43
"RESTful API (api/v2) {#rest_api children-count=""0""}","A full RESTful API has been provided for most MOLES object classes. As
this is built within Django the service supports standard Django
arguments. See Django RESTApi online documentation and each class object
under the URL below for details on how to use each class endpoint.
**URL:** <http://api.catalogue.ceda.ac.uk/api/v2/>
","https://help.ceda.ac.uk/article/4710-moles-api#restful-api-(api/v2)-{#rest_api-children-count=""0""}",327,48
"Legacy Lookup {#leg children-count=""0""}","MOLES1, MOLES2 URL look-up service for use by Apache server. Takes OLD
URL and does a look-up to find which MOLES3 record the previous
identifier has been recorded on. This is then returned to allow an
appropriate redirect response to be sent to the
**URL:**
<http://api.catalogue.ceda.ac.uk/api/legacy_lookup/?identifier=%7Burl_to_check%7D>
**Example:**
User requests
[http://badc.nerc.ac.uk/](http://badc.nerc.ac.uk/%3Ca%20href=)[view/badc.nerc.ac.uk\_\_ATOM\_\_obs_11734597270316948](http://esgf-test1.ceda.ac.uk/view/badc.nerc.ac.uk__ATOM__obs_11734597270316948)
Apache script submits this to API call as:
<http://api.catalogue.ceda.ac.uk/api/legacy_lookup/?identifier=http://badc.nerc.ac.uk/view/badc.nerc.ac.uk__ATOM__obs_11734597270316948>
","https://help.ceda.ac.uk/article/4710-moles-api#legacy-lookup-{#leg-children-count=""0""}",747,59
"**Request:** {#request children-count=""0""}","Is a url with the MOLES1/MOLES2 URL passed as an argument
  Name         Type     Required   Default   Options   Notes
  ------------ -------- ---------- --------- --------- -------
  identifier   String   Yes        empty               
","https://help.ceda.ac.uk/article/4710-moles-api#**request:**-{#request-children-count=""0""}",238,27
"**Response:** {#response children-count=""0""}","Is a json object with the following fields:
  Response   Name             Type     Notes
  ---------- ---------------- -------- ------------------------------------------------------------------------
  body       url              String   MOLES3 record URL corresponding to the requested MOLES1/MOLES2 URL
             identifier       String   MOLES1/MOLES2 URL requested by the user
             identifierType   String   type of identifier as recorded in MOLES3: one of moles1_url moles2_url
","https://help.ceda.ac.uk/article/4710-moles-api#**response:**-{#response-children-count=""0""}",496,49
"Positive Response: {#positive-response children-count=""0""}","{""url"": ""http://catalogue.ceda.ac.uk/uuid/5a1076bffc8c4c5d8a2ff3a4cfb29846"", ""identifier"": ""http://badc.nerc.ac.uk/view/badc.nerc.ac.uk__ATOM__obs_11734597270316948"", ""identifierType"": ""moles2_url""}
","https://help.ceda.ac.uk/article/4710-moles-api#positive-response:-{#positive-response-children-count=""0""}",199,6
"Negative Response {#negative-response children-count=""0""}","{""url"": ""UNKNOWN"", ""identifier"": """", ""identifierType"": ""UNKNOWN""}
","https://help.ceda.ac.uk/article/4710-moles-api#negative-response-{#negative-response-children-count=""0""}",66,6
"Path To Record Lookup {#path children-count=""0""}","Attempts to return the required Observation/Collection record for a
given path in the archive where a definitive answer can be provided
**URL:**
<http://api.catalogue.ceda.ac.uk/api/v0/obs/get_info/%7Bpath%7D>
**Example:**
<http://api.catalogue.ceda.ac.uk/api/v0/obs/get_info/badc/ukmo-midas/data/GL>
","https://help.ceda.ac.uk/article/4710-moles-api#path-to-record-lookup-{#path-children-count=""0""}",301,25
"**Request:** {#request-1 children-count=""0""}","Is a url with a CEDA Archive path
  Name   Type     Required   Default   Options   Notes
  ------ -------- ---------- --------- --------- -------
  path   String   Yes        empty               
","https://help.ceda.ac.uk/article/4710-moles-api#**request:**-{#request-1-children-count=""0""}",196,24
"**Response:** {#response-1 children-count=""0""}","Is a json object with the following fields:
  Response   Name          Type     Notes
  ---------- ------------- -------- -------------------------------------------------------------------------------------------------------------------------------------------------------
  body       url           String   MOLES Record URL relevant to the given archive path
             title         String   MOLES record title
             record_type   String   type of MOLES3 record type in end-user speak: one of Dataset or Dataset Collection. These map to Observation and Observation Collection respectively.
             record_path   String   The corresponding archive path as recorded on the Result object when the record_type is Dataset. (Blank for Collections)
","https://help.ceda.ac.uk/article/4710-moles-api#**response:**-{#response-1-children-count=""0""}",760,77
"Positive response: {#positive-response-1 children-count=""0""}",,"https://help.ceda.ac.uk/article/4710-moles-api#positive-response:-{#positive-response-1-children-count=""0""}",0,0
"Dataset (Observation): {#dataset-observation children-count=""0""}","{""url"": ""http://catalogue.ceda.ac.uk/uuid/0ec59f09b3158829a059fe70b17de951"", ""record_type"": ""Dataset"", ""record_path"": ""/badc/ukmo-midas/data/GL"", ""title"": ""MIDAS: Global Weather Observation Data""}
","https://help.ceda.ac.uk/article/4710-moles-api#dataset-(observation):-{#dataset-observation-children-count=""0""}",197,12
"Dataset Collection (Observation Collection): {#dataset-collection-observation-collection children-count=""0""}","{""url"": ""http://catalogue.ceda.ac.uk/uuid/220a65615218d5c9cc9e4785a3234bd0"", ""record_type"": ""Dataset Collection"", ""record_path"": """", ""title"": ""Met Office Integrated Data Archive System (MIDAS) Land  and Marine Surface Stations Data (1853-current)""}
","https://help.ceda.ac.uk/article/4710-moles-api#dataset-collection-(observation-collection):-{#dataset-collection-observation-collection-children-count=""0""}",249,22
"Negative response {#negative-response-1 children-count=""0""}","{""url"": """", ""record_type"": """", ""record_path"": """", ""title"": """"}
","https://help.ceda.ac.uk/article/4710-moles-api#negative-response-{#negative-response-1-children-count=""0""}",63,8
"Observation Completeness Lookup {#complete children-count=""0""}","Gives a complete listing of all observation paths with a \'completed\'
status. For use use by the ingestion system to determine when writing to
the archive is permitted.
*Note: this takes a few seconds to run!*
**URL: **<http://api.catalogue.ceda.ac.uk/api/v0/complete>
**Example:** <http://api.catalogue.ceda.ac.uk/api/v0/complete>
","https://help.ceda.ac.uk/article/4710-moles-api#observation-completeness-lookup-{#complete-children-count=""0""}",333,40
"**Request:** {#request-2 children-count=""0""}","Is a straight url with no arguments
","https://help.ceda.ac.uk/article/4710-moles-api#**request:**-{#request-2-children-count=""0""}",36,7
"**Response:** {#response-2 children-count=""0""}","Is a plain text object where each line has two elements: the path and
then a \'Completeness\' indicator. This is used to indicate whether the
given path (and everything below this) corresponds to a \'Completed\'
dataset. This is determined from the Data Status of the Observation
Record.
*Note: the first 6 lines are hardcoded covering the 6 top level data
centre paths and are always set to False.*
/badc False
/neodc False
/edc False
/sparc False
/bodc False
/ngdc False
/badc/ukmo-tovs/ True
/badc/ukmo-tovs/ True
/badc/csip/data/ukmo-nrt True
/badc/op3/data/op3-1/ceh-temp-gradient/2008 True
/badc/op3/data/op3-1/york-dc-gc-fid2/2008 True
","https://help.ceda.ac.uk/article/4710-moles-api#**response:**-{#response-2-children-count=""0""}",643,90
"DRS to Record lookup {#drs children-count=""0""}","Allows mapping a DRS to an archive path by finding the relevant
Observation record that hold the DRS entry and returning its
Result.dataPath entry.
**URL:** <http://api.catalogue.ceda.ac.uk/api/v0/drs_lookup/>**{drs
id}**
**Example:**
<http://api.catalogue.ceda.ac.uk/api/v0/drs_lookup/esacci.AEROSOL.day.L3C.AER_PRODUCTS.AATSR.Envisat.ORAC.03-02.r1.v20170402>
","https://help.ceda.ac.uk/article/4710-moles-api#drs-to-record-lookup-{#drs-children-count=""0""}",361,29
"**Request:** {#request-3 children-count=""0""}","Is a url ending with the DRS ID to look up.
  Name         Type     Required   Default   Options   Notes
  ------------ -------- ---------- --------- --------- -------
  identifier   String   Yes        empty               
","https://help.ceda.ac.uk/article/4710-moles-api#**request:**-{#request-3-children-count=""0""}",224,27
"**Response:** {#response-3 children-count=""0""}","Is a json object with the following fields:
  Response   Name         Type     Notes
  ---------- ------------ -------- --------------------------------------------------------------------
  body       directory    String   MOLES3 record URL corresponding to the requested MOLES1/MOLES2 URL
             identifier   String   MOLES1/MOLES2 URL requested by the user
             version      String   version of the identifier
","https://help.ceda.ac.uk/article/4710-moles-api#**response:**-{#response-3-children-count=""0""}",427,42
"Positive Response {#positive-response-2 children-count=""0""}","{""directory"": ""/neodc/esacci/aerosol/data/AATSR_ORAC/L3/v3.02/DAILY/"", ""identifier"": ""esacci.AEROSOL.day.L3C.AER_PRODUCTS.AATSR.Envisat.ORAC.03-02.r1.v20170402"", ""version"": ""v20170402""}
","https://help.ceda.ac.uk/article/4710-moles-api#positive-response-{#positive-response-2-children-count=""0""}",186,6
"Negative Response {#negative-response-2 children-count=""0""}","{""directory"": ""UNKNOWN"", ""identifier"": ""esacci.AEROSOL.day.L3C.AER_PRODUCTS.AATSR.Envisat.ORAC.03-02.r1.v2017040"", ""version"": ""UNKNOWN""}
","https://help.ceda.ac.uk/article/4710-moles-api#negative-response-{#negative-response-2-children-count=""0""}",137,6
"DRS to Record listing {#all-observation-mappings children-count=""0""}","An API to see all the DRS to Record results given an optional prefix.
Same as DRS to Record lookup but given a prefix will find all the
matches.
**URL:** <http://api.catalogue.ceda.ac.uk/api/v0/drs_listing/>**{ prefix
}\
**
**Example:
[](http://api.catalogue.ceda.ac.uk/api/v0/drs_listing/esacci)**[http://api.catalogue.ceda.ac.uk/api/v0/drs_listing/esacci\
](http://api.catalogue.ceda.ac.uk/api/v0/drs_listing/esacci)
","https://help.ceda.ac.uk/article/4710-moles-api#drs-to-record-listing-{#all-observation-mappings-children-count=""0""}",419,37
"**Request:** {#request-4 children-count=""0""}","URL with the prefix of a DRS dataset.
  ------------------------------------------------------------------------
  Name\       Type\       Required\   Default\    Options\    Notes\
  ----------- ----------- ----------- ----------- ----------- ------------
  prefix\     String\     Yes\        empty\      \           Empty prefix
                                                              will show\
                                                              all DRS
                                                              datasets.\
  ------------------------------------------------------------------------
","https://help.ceda.ac.uk/article/4710-moles-api#**request:**-{#request-4-children-count=""0""}",623,34
"{#all-observation-mappings children-count=""0""}",,"https://help.ceda.ac.uk/article/4710-moles-api#{#all-observation-mappings-children-count=""0""}",0,0
"Response: {#response-4 children-count=""0""}","Json object with the following fields:
  -----------------------------------------------------------------------
  Response    Name        Type        Notes                   
  ----------- ----------- ----------- ----------- ----------- -----------
  body        **{ DRS     String      Value is                
              dataset }**             archive                 
                                      path of                 
                                      each DRS                
                                      dataset\                
  -----------------------------------------------------------------------
","https://help.ceda.ac.uk/article/4710-moles-api#response:-{#response-4-children-count=""0""}",639,32
"Positive Response: {#positive-response-3 children-count=""0""}","{""esacci.CLOUD.day.L3U.CLD_PRODUCTS.MODIS.Terra.MODIS_TERRA.1-0.r1"": ""/neodc/esacci_cloud/data/L3U/modis_terra"", ""esacci.CLOUD.day.L3U.CLD_PRODUCTS.MODIS.Aqua.MODIS_AQUA.1-0.r1"": ""/neodc/esacci_cloud/data/L3U/modis_aqua""}
","https://help.ceda.ac.uk/article/4710-moles-api#positive-response:-{#positive-response-3-children-count=""0""}",222,4
"Negative Response: {#negative-response-3 children-count=""0""}","{}
","https://help.ceda.ac.uk/article/4710-moles-api#negative-response:-{#negative-response-3-children-count=""0""}",3,1
"All Observation Mappings {#all-observation-mappings children-count=""0""}","Returns all observation data paths with metadata attached as in the
[Path to Record Lookup](#path)
**URL:** <http://api.catalogue.ceda.ac.uk/api/v0/obs/all>
**Example:** <http://api.catalogue.ceda.ac.uk/api/v0/obs/all>
","https://help.ceda.ac.uk/article/4710-moles-api#all-observation-mappings-{#all-observation-mappings-children-count=""0""}",219,19
"**Request:**  {#request-5 children-count=""0""}","No arguments
","https://help.ceda.ac.uk/article/4710-moles-api#**request:** -{#request-5-children-count=""0""}",13,2
"Response:  {#response-5 children-count=""0""}","A JSON object. The top-level keys are the MOLES data path attached to
each observation in the listing. Each of these keys has another JSON
object attached with the following fields:
  Response    Name          Type     Notes
  ----------- ------------- -------- -------------------------------------------------------------------------------------------------------------------------------------------------
  data path   url           String   MOLES Record URL relevant to the given archive path
              title         String   MOLES record title
              record_type   String   type of MOLES3 record type in end-user speak: one of Dataset or Dataset Collection. This API only returns datasets so it will always be Dataset
","https://help.ceda.ac.uk/article/4710-moles-api#response: -{#response-5-children-count=""0""}",734,84
"Positive Response {#positive-response-4 children-count=""0""}","{
    ""/badc/sage3/data/g3assp.004"": {
        ""url"": ""http://catalogue.ceda.ac.uk/uuid/bbf55db100ce4d0995f86cb1d607011b"", 
        ""record_type"": ""Dataset"", 
        ""title"": ""SAGE III: Level 2 Solar species profiles version 4, HDF-EOS formatted""
    }, 
    ""/badc/cmip5/data/cmip5/output1/IPSL/IPSL-CM5B-LR/piControl/"": {
        ""url"": ""http://catalogue.ceda.ac.uk/uuid/cc6bac7b82de4d6aaa56f24bdf123edd"", 
        ""record_type"": ""Dataset"", 
        ""title"": ""WCRP CMIP5: Institut Pierre-Simon Laplace (IPSL) IPSL-CM5B-LR model output for the piControl experiment""
    }, 
    ...
}
","https://help.ceda.ac.uk/article/4710-moles-api#positive-response-{#positive-response-4-children-count=""0""}",586,43
"Negative Response {#negative-response-4 children-count=""0""}","Should always respond with a listing. A negative response would indicate
a fundamental problem with the API or database.
{}
","https://help.ceda.ac.uk/article/4710-moles-api#negative-response-{#negative-response-4-children-count=""0""}",124,20
Requirements for users sending us data on USB drives,"::: {#wikipage}
[](http://team.ceda.ac.uk//trac/ceda/wiki/opman/USBDrives)
*\[Alison W - review and keep\]*
*This page gives details about the what users should be doing when
sending data to us on USB drives.* **Please note that this page might
need editing before sending out to end-users as some of the information
it contains is intended only for internal viewing/use.**
",https://help.ceda.ac.uk/article/4408-requirements-for-users-sending-us-data-on-usb-drives,374,54
Ideally {#Ideally},"-   Users should send the complete drive in the original packaging with
    all cables, power adapter and driver disks as provided by the
    manufacturer. (This means that we don\'t have to go looking on the
    manufacturer\'s website for manuals and drives or trying to find a
    power adapter that will work with the drive.)
-   Users should not attempt to re-format the drive with a new
    filesystem unless they absolutely have to. Most USB drives are able
    to work on Windows systems using NTFS as standard from
    manufacturers. Most Linux systems are easily capable of reading NTFS
    formatted drives.
-   The power adapter should be a UK three-pin plug.
-   As a minimum the drive should support USB 2.0. Anything else faster
    is a bonus for us.
",https://help.ceda.ac.uk/article/4408-requirements-for-users-sending-us-data-on-usb-drives#ideally-{#ideally},767,127
Useful things for us to have {#Usefulthingsforustohave},"If users send us a list of MD5 checksums (from their source system) we
can check this against corresponding MD5 checksums on the files in their
destination directory. One Linux the md5sum utility. On windows the
following tool is recommended: [[ ]{.icon}
http://www.fourmilab.ch/md5/](http://www.fourmilab.ch/md5/){.ext-link}
The following is a listing for md5sum.bat
@ECHO OFF
REM  
REM
REM See also: http://www.computerhope.com/forhlp.htm
REM           http://www.fourmilab.ch/md5/
FOR /R %1 %%f IN (*) DO md5 -l ""%%f%""
@ECHO ON
*TODO: put \*.bat and bash scripts on this page, suitable for
distributing to end users to automate this process.*
",https://help.ceda.ac.uk/article/4408-requirements-for-users-sending-us-data-on-usb-drives#useful-things-for-us-to-have-{#usefulthingsforustohave},646,91
Devices known to work {#Devicesknowntowork},"The following drives has been successfully used in the past (without
many problems):
-   Western Digital Passport 500 GB
-   LACIE (design by F.A.PORSCHE) (formatted with ext3 filesystem) 500
    GB
",https://help.ceda.ac.uk/article/4408-requirements-for-users-sending-us-data-on-usb-drives#devices-known-to-work-{#devicesknowntowork},199,30
Troubleshooting Lacie Big Disks {#TroubleshootingLacieBigDisks},"The following pages have been useful in the past to help troubleshoot
mounting problems on Linux:
-   [[ ]{.icon}
    http://www.lacie.com/support/faq/faq.htm?faqid=10285](http://www.lacie.com/support/faq/faq.htm?faqid=10285){.ext-link}
-   [[ ]{.icon}
    http://www.mail-archive.com/gnhlug-discuss\@mail.gnhlug.org/msg14917.html](http://www.mail-archive.com/gnhlug-discuss@mail.gnhlug.org/msg14917.html){.ext-link}
-   [[ ]{.icon}
    http://ubuntuforums.org/showthread.php?t=543901](http://ubuntuforums.org/showthread.php?t=543901){.ext-link}
",https://help.ceda.ac.uk/article/4408-requirements-for-users-sending-us-data-on-usb-drives#troubleshooting-lacie-big-disks-{#troubleshootinglaciebigdisks},546,28
Virus Protection and Policy {#VirusProtectionandPolicy},"Following a discussion with Paul from SPBU IT Helpdesk a safe way to
transfer files from USB drives (or at least those formatted with FAT32
or NTFS) is to plug it into SSTDWBADC01.space.rl.ac.uk R25 1.117 (the
machine that used to be used by Belinda). Assuming this machine is kept
on to allow security updates to be applied Sophos should do a good job
of protecting against most Windows viruses. To perform the actual
transfer a WinSCP client can be used and the whole process should be
monitored via Remote Desktop.
Where disks are formatted with ext3, Reiser, XFS, &c., these can be done
using any linux workstation that can access the archive. Paul did not
think that viruses for linux boxes were as problematic as for windows
boxes.
Discussions with SDDCS are in the pipeline.
",https://help.ceda.ac.uk/article/4408-requirements-for-users-sending-us-data-on-usb-drives#virus-protection-and-policy-{#virusprotectionandpolicy},782,135
CEDA USB Drives {#CEDAUSBDrives},"We have several 1 and 2 TB Lacie Disks available that can be sent out
for data transfer purposes.
:::
",https://help.ceda.ac.uk/article/4408-requirements-for-users-sending-us-data-on-usb-drives#ceda-usb-drives-{#cedausbdrives},102,20
Using Archive Access Tokens for Scripted Interactions,"Many datasets in the CEDA archive have specific auditing policies or
access restrictions in place which prevent non logged-in or unauthorised
users from downloading their data. This can pose a problem for anonymous
scripted interactions, such as bulk file downloads (with Wget, Python
requests, etc) or data sub-setting (e.g. NetCDF libraries or HTTP range
get requests). To solve this problem, we provide access tokens that can
be used in scripts to act on behalf of your CEDA account for the
purposes of accessing data on the CEDA Archive.
[What is an Access Token?](#intro)
[How to Generate an Access Token](#how)
-   [Access Restrictions](#restrictions)
-   [Generating Tokens from the CEDA Services Website](#website)
-   [Generating Tokens with the Token API](#api)
[Using Access Tokens in Scripts (examples)](#usage)
-   [Finding CEDA Archive File URLs](#urls)
-   [Using Wget](#wget)
-   [Using Curl](#curl)
-   [Using Python Requests](#requests)
-   [Using Python NetCDF4](#netcdf4)
-   [Complete Python Example Script](#script)
",https://help.ceda.ac.uk/article/5100-archive-access-tokens,1038,148
What is an Access Token? {#intro},"The kind of access token we issue is something called a Bearer Token
(from the OAuth 2.0 protocol). It is an opaque string which can be used
in HTTP requests to uniquely identify the owner of the token for the
purposes of verifying their identity. It is added to a request as part
of the HTTP \""Authorization Header\"".
As a security measure, access tokens that we issue have a limited
lifespan.
",https://help.ceda.ac.uk/article/5100-archive-access-tokens#what-is-an-access-token?-{#intro},395,71
How to Generate an Access Token {#how},"First, you will need to log in to your user account (if you don\'t
already have one, you can [Register for a CEDA
Account](https://services.ceda.ac.uk/cedasite/register/info/)).
",https://help.ceda.ac.uk/article/5100-archive-access-tokens#how-to-generate-an-access-token-{#how},178,24
Access Restrictions {#restrictions},"Tokens cannot be used to access data that your CEDA account doesn\'t
already have access to. Many datasets just require login to access, but
some have additional licences that you must first sign-up to before
you\'ll be able to access them. You can find information about the
access restrictions for datasets, along with relevant sign-up links, on
our [Data Catalogue](https://catalogue.ceda.ac.uk/). Note that some
datasets have policy agreements that you must first agree to, and you
may need to wait before your access is approved by a moderator.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/652989171ec9493542047c8b/file-tbuud9a3DM.png)
Once you\'re logged in (and your account has the access permissions you
need) you can now generate access tokens to use in scripted
interactions. See below for details of the different ways to do this.
",https://help.ceda.ac.uk/article/5100-archive-access-tokens#access-restrictions-{#restrictions},885,123
Generating Tokens from the CEDA Services Website {#website},"The simplest way to generate a token is by using the [access token
generator on the CEDA services
website](https://services-beta.ceda.ac.uk/account/token/). Note that,
since these tokens will only have a lifespan of 3 days and cannot be
refreshed, this method may not be suitable for scripts that are intended
to run for a long period of time. For these purposes, see the section
about the token API further down.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/652984f79362491a4094a1cd/file-hZl7ffFnC2.png)
To generate a new token, click the \""Create new access token\"" button,
and enter your CEDA user password and (optionally) a name for the token.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6529844d9362491a4094a1ca/file-0wWrDO1IQw.png)
After generating a token, you can copy it to use in scripts by clicking
\""Copy\"", or delete unneeded tokens using \""Delete\"". You can also see
the expiry date of each of your tokens. You can generate a maximum of 2
active tokens using this method.
",https://help.ceda.ac.uk/article/5100-archive-access-tokens#generating-tokens-from-the-ceda-services-website-{#website},1072,139
Generating Tokens with the Token API {#api},"Fresh tokens can also be requested from our Token API. This makes it
possible to refresh expired tokens inside of your scripts, avoiding the
manual step of generating and copying tokens from the website.
The API endpoint is: 
**<https://services-beta.ceda.ac.uk/api/token/create/>**
This is secured with [HTTP basic
authentication](https://en.wikipedia.org/wiki/Basic_access_authentication),
requiring you to provide your CEDA login credentials in the header of a
POST request.
Below are some examples of scripts that will generate a token from the
API.
Using a bash script and Curl:
<div>
    CEDA_USERNAME='my CEDA username'
    CEDA_PASSWORD='my CEDA password'
    # If successful, this will return a JSON response containing the token
    curl --location --request POST 'https://services-beta.ceda.ac.uk/api/token/create/' --header ""Authorization: Basic $(echo -n ""${CEDA_USERNAME}:${CEDA_PASSWORD}"" | base64)""<br>
</div>
From inside a Python script (uses
[requests](https://pypi.org/project/requests/)):
    import json
    import requests
    from base64 import b64encode
    url = ""https://services-beta.ceda.ac.uk/api/token/create/""
    username = ""my CEDA username""
    password = ""my CEDA password""
    token = b64encode(f""{username}:{password}"".encode('utf-8')).decode(""ascii"")
    headers = {
        ""Authorization"": f'Basic {token}',
    }
    response = requests.request(""POST"", url, headers=headers)
    # If successful, this will return a JSON response containing the token
    response_data = json.loads(response.text)
    print(response.text)
    if response.status_code == 200:
        token = response_data[""access_token""]
",https://help.ceda.ac.uk/article/5100-archive-access-tokens#generating-tokens-with-the-token-api-{#api},1644,180
Using Access Tokens in Scripts {#usage},"There are multiple ways to use tokens in scripted interactions. First,
you\'ll need a valid URL to the file in the CEDA archive that you\'re
interested in. Then your access token must be added to the HTTP header
using the \""Bearer Token Authorization Header\"" standard. In this
section, we\'ve included examples with some common ways to access data.
",https://help.ceda.ac.uk/article/5100-archive-access-tokens#using-access-tokens-in-scripts-{#usage},350,58
Finding CEDA Archive File URLs {#urls},"All of the examples below assume that you have an appropriate URL for a
file in the CEDA archive. 
Here are some steps you can follow to find a file URL:
-   Find a dataset that you wish to access files from in the [CEDA
    Archive Browser](http://data.ceda.ac.uk/) (for example,
    [UKMO-midas-open](http://data.ceda.ac.uk/badc/ukmo-midas-open/data/uk-daily-temperature-obs/dataset-version-201901/aberdeenshire/00145_cairnwell/qc-version-1/)).
    You can also use the [CEDA Data
    Catalogue](https://catalogue.ceda.ac.uk/) to search for files, and
    then press the \""Download\"" button to be sent to the corresponding
    location on the Archive Browser.
-   Down the right hand side of the list of files, there will be
    a download icon for each file in the
    dataset:![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/66d89c83811a2434cb6d321b/file-ecMNNRRrZd.png)
-   Right click this download icon and select \""Copy link address\""
    (this may be different depending on what browser you\'re using).  
-   Now you can paste that link address into your script, or onto the
    command line.  The links will start with
    **<http://dap.ceda.ac.uk/>** followed by the path to the file in the
    archive.
Lastly, make sure that the file you\'re interested in is either **open
access** (can be downloaded anonymously), or that your CEDA account has
permission to access the file (see [Access
Restrictions](#restrictions)).
You can check whether you have permission to access the file by
attempting to download it in a browser. If you\'re able to download the
file in a browser (you may need to login first), then any token that you
generate should also be able to. The exception to this is if you\'re
using a token that was generated before you had access to the data.
",https://help.ceda.ac.uk/article/5100-archive-access-tokens#finding-ceda-archive-file-urls-{#urls},1816,252
Using Wget {#wget},"To use an access token with a Wget operation, add the token to the
Authorisation Header and include the header with the \""\--header\""
command line option.
Below is an example of using the token to download a single file:
    wget https://dap.ceda.ac.uk/badc/csip/data/salford-radiometer-1/2005/06/salford-radiometer-1_faccombe_20050624_hpc.nc --header ""Authorization: Bearer INSERT_ACCESS_TOKEN_HERE""
You can also use the token with a Wget command downloading multiple
files in a directory:
    wget -e robots=off --mirror --no-parent -r https://dap.ceda.ac.uk/badc/csip/data/salford-radiometer-1/2005/06/ --header ""Authorization: Bearer INSERT_ACCESS_TOKEN_HERE""
",https://help.ceda.ac.uk/article/5100-archive-access-tokens#using-wget-{#wget},664,72
Using Curl {#curl},"Similar to Wget, Curl also supports adding a header using the \""-H\""
command line option:
    curl -L -H 'Authorization: Bearer INSERT_ACCESS_TOKEN_HERE' https://dap.ceda.ac.uk/badc/csip/data/salford-radiometer-1/2005/06/salford-radiometer-1_faccombe_20050624_hpc.nc > result
",https://help.ceda.ac.uk/article/5100-archive-access-tokens#using-curl-{#curl},276,24
Using Python Requests {#requests},"The Python requests module allows you to set the Bearer Token
Authorization Header before sending a request. The example below
downloads a single file:
    import requests
    url = ""https://dap.ceda.ac.uk/badc/csip/data/salford-radiometer-1/2005/06/salford-radiometer-1_faccombe_20050624_iwv.nc""
    token = ""INSERT_ACCESS_TOKEN_HERE""
    headers = {
      ""Authorization"": f""Bearer {token}""
    }
    response = requests.request(""GET"", url, headers=headers)
    print(response.text)
",https://help.ceda.ac.uk/article/5100-archive-access-tokens#using-python-requests-{#requests},485,45
Using Python NetCDF4 {#netcdf4},"Normally, it is possible to create NetCDF Dataset objects directly using
an open access file URL from our archive. This avoids having to download
the entire file, which may be very large. However, trying to do this
with a restricted access URL will fail.
To get around this problem, we can request the file separately with the
access token included in the request header, ensuring that our request
is a data stream so that the whole file isn\'t downloaded. Then, that
data stream can be passed to the Dataset object at initialisation to
create an \""in-memory\"" Dataset (read more about this in the [NetCDF4
documentation](https://unidata.github.io/netcdf4-python/#in-memory-diskless-datasets)).
<div>
    import os
    import requests
    from urllib.parse import urlparse
    url = ""https://dap.ceda.ac.uk/badc/csip/data/salford-radiometer-1/2005/06/salford-radiometer-1_faccombe_20050624_iwv.nc""
    token = ""INSERT_ACCESS_TOKEN_HERE""
    headers = {
      ""Authorization"": f""Bearer {token}""
    }
    response = requests.request(""GET"", url, headers=headers, stream=True)
    filename = os.path.basename(urlparse(url).path)
    dataset = Dataset(filename, memory=response.content)
",https://help.ceda.ac.uk/article/5100-archive-access-tokens#using-python-netcdf4-{#netcdf4},1183,140
Complete Python Example Script {#script},"We\'ve written an example Python script which demonstrates the complete
process of fetching a token using the Token API (and caching it for
later use), then opening a restricted access NetCDF file from a URL as a
streamed Dataset.
Feel free to copy and adapt this to suit your workflow. You can find the
script at the following link:
<https://github.com/cedadev/opendap-python-example/blob/master/remote_nc_with_token.py>
</div>
",https://help.ceda.ac.uk/article/5100-archive-access-tokens#complete-python-example-script-{#script},429,61
You can see the status of the CEDA main archive services via:,"-   the [CEDA status page](https://www.ceda.ac.uk/status) (for details
    of current incidents. See also [CEDA
    news](https://www.ceda.ac.uk/news))
-   the [Archive status page in Uptime
    Robot](https://stats.uptimerobot.com/vZPgQt7YnO) (for status of
    individual services)
[](https://stats.uptimerobot.com/LJKgZfn2G)
The link can be used to check whether our different websites are broken
or running fine and see how stable they have been over various time
scales.
Clicking on the different services from the list above will provide more
detailed statistics for that service.
",https://help.ceda.ac.uk/article/4845-ceda-archive-status#you-can-see-the-status-of-the-ceda-main-archive-services-via:,587,72
What are the services covered by the status page?,"The different services monitored are :
  ----------------------- ----------------------------------------------------- -----------------------
  **Service**             **URL**                                               **Description**
  **DAP**                 [https://dap.ceda.ac.uk](https://dap.ceda.ac.uk/)     This is our usual
                                                                                download route. When it
                                                                                is broken you may see a
                                                                                timeout error message.
                                                                                This service is pointed
                                                                                to from our main data
                                                                                index.  \
  **Data index**          [https://data.ceda.ac.uk](https://data.ceda.ac.uk/)   This provides the
                                                                                listing of files and
                                                                                directories in the
                                                                                archive.
  **FTP**                 ftp://ftp.ceda.ac.uk/                                 An alternative route to
                                                                                the archive data. This
                                                                                requests a CEDA
                                                                                account. 
  **FTP Anon**            <ftp://anon-ftp.ceda.ac.uk/%C2%A0>                    Another data download
                                                                                service. It doesn't
                                                                                require a log in, but
                                                                                currently holds very
                                                                                limited datasets. So
                                                                                you may not be able to
                                                                                see the data you are
                                                                                after - we are working
                                                                                on increasing the range
                                                                                of datasets.
  **CEDA Archive**        <https://archive.ceda.ac.uk/>                         The main archive
                                                                                website. Mainly this
                                                                                just aids navigation to
                                                                                the other services. 
  **Arrivals**            <https://arrivals.ceda.ac.uk/intro/>                  This is the service
                                                                                where CEDA accepts data
                                                                                that is deposited by
                                                                                providers into the
                                                                                archive.
  **CEDA Catalogue**      <https://catalogue.ceda.ac.uk>\                       The catalogue of all
                                                                                the data we hold. 
  ----------------------- ----------------------------------------------------- -----------------------
",https://help.ceda.ac.uk/article/4845-ceda-archive-status#what-are-the-services-covered-by-the-status-page?,3934,160
CEDA Satellite Data Finder Introduction,"::: {.section .callout-blue .dashed}
",https://help.ceda.ac.uk/article/4493-cedageosearch-intro,37,4
Jump to section:,"-   [Using the
    Interface](https://secure.helpscout.net/members/login/?jump=https%3A%2F%2Fsecure.helpscout.net%2Fdocs%2F564b4f2490336002f86de436%2Farticle%2F5981d46c042863033a1b9241%2F%3F&jdata=)
-   [Looking at Your Results](#looking-at-your-results)
-   [Export Results](#export-results)
-   [Dataset on Tape](#dataset-on-tape)
-   [Related Articles](#related-articles)
:::
",https://help.ceda.ac.uk/article/4493-cedageosearch-intro#jump-to-section:,379,20
Finding the Satellite Data Finder,"[CEDA Satellite Data Finder](http://geo-search.ceda.ac.uk/)
",https://help.ceda.ac.uk/article/4493-cedageosearch-intro#finding-the-satellite-data-finder,60,4
Using the Interface,"The screen in front of you is split into two sections. On the left is a
side panel with accordion-style expanding elements that contain useful
elements for refining your search parameters. On the right is a large
map.
On first load the map will be blank. Refine your search using the
filters and click **Apply Filters**. Once you have made a selection, the
map will be populated with coloured polygons. These polygons are the
boundaries of scenes extracted from satellite data contained within the
CEDA archive. Up to 1000 scenes will be rendered.
After this first action, you can pan around the map by clicking and
dragging. The map\'s bounding box will automatically change, and the
interface will automatically search for and display scenes within the
area. You can also zoom in and out using the controls in the bottom
right of the map. The map functionality uses Google Maps, so the
interface should be familiar if you have used that before.
Here is a short video which gives a run through of all the main
features.
",https://help.ceda.ac.uk/article/4493-cedageosearch-intro#using-the-interface,1021,177
Looking at Your Results,"Each polygon is clickable and will bring up an information dialogue when
selected. This will provide some basic information about each individual
file including the recorded start and end time for the data, the file
name, the satellite mission, satellite name and instrument name. An
example is shown below:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5981d79d2c7d3a73488b9212/file-sIRgjr1DJx.png)
If there is a quicklook thumbnail image in the archive, this will be
displayed. Clicking on this image will bring up a larger image.
Quicklook images are not available for all datasets. In order to view
some of the quicklooks, you are required to be signed in using your CEDA
account at
[data.ceda.ac.uk](https://auth.ceda.ac.uk/account/signin/?r=http%3A//data.ceda.ac.uk/)
There are also buttons where you can directly download the scene or view
the source directory on the archive.
",https://help.ceda.ac.uk/article/4493-cedageosearch-intro#looking-at-your-results,925,122
Export Results,"This dialogue box will help you export any search hits you may have for
a given area. It will allow you to export the data in 3 given formats:
-   Raw Elasticsearch JSON documents. This is generally intended for
    software developers or people who can directly interact with the
    Elasticsearch installation. This does include an amount of useful
    raw metadata, but generally it\'s not as useful as the other
    options.
-   File Paths. This will export a JSON list of NEODC system file paths.
    This is useful if you have filesystem access to the CEDA archive -
    e.g. on JASMIN or through another entry point.
-   Download URLs. This is probably the most useful option to you. This
    will provide a JSON list of URLs which can be directly used to
    download any matching data files.
Clicking the Copy icon in the top right of the results box will copy the
contents of results to the clipboard.
",https://help.ceda.ac.uk/article/4493-cedageosearch-intro#export-results,912,157
Dataset on Tape,"If the dataset has been moved to tape storage the download buttons will
not be available but there is a link to tell you how to gain access to
the dataset.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5981d583042863033a1b925a/file-4kGB2MSf2V.png)
",https://help.ceda.ac.uk/article/4493-cedageosearch-intro#dataset-on-tape,289,32
CMIP6 data at CEDA,"The World Climate Research Programme (WCRP) Working Group on Coupled
Modelling ([WGCM](https://www.wcrp-climate.org/wgcm-overview)) oversees
the Coupled Model Intercomparison Project Phase 6 (CMIP6). The CMIP
programme is an ongoing project which combines climate modelling
experiments and produces huge quantities of data archived around the
world by members of the Earth System Grid Federation (ESGF) and the
current iteration is CMIP6. CMIP6 has been making data available since
2018 - present and the analyses will inform the IPCC Sixth Assessment
Report (AR6).
::: {.section .callout-red}
**NOTE** -  CMIP6 is currently in progress and data are still being
actively retrieved by CEDA, not all datasets are complete yet. In
addition, CEDA will only permanently hold a subset of the CMIP6 data
(currently CEDA are prioritising data required for the AR6 WG1 report).
Please see below for guidance on how to search what data we hold.
:::
1\. [CMIP6 data structure](#CMIP6%20data%20structure)
2\. [Data Access: CEDA](#Data%20Access:%20CEDA)
3\. [Data Access: ESGF](#Data%20Access:%20ESGF)
",https://help.ceda.ac.uk/article/4801-cmip6-data,1089,154
CMIP6 data structure {#CMIP6 data structure},"The following will describe the directory structure and filename
composition of the CMIP6 data so users can find specific data more
easily:
",https://help.ceda.ac.uk/article/4801-cmip6-data#cmip6-data-structure-{#cmip6-data-structure},140,22
Directories,"The directory structure for CMIP6 is as follows:
    <mip_era>/<activity_id>/<institution_id>/<source_id>/<experiment_id>/<variant_label>/<table_id>/<variable_id>/<grid_label>/<version>
-   *mip_era*: refers to the phase of the project, this will be CMIP6,
-   *activity_id*: is the abbreviated identifier of the Model
    Intercomparison Project (MIP). For example; Aerosols and Chemistry
    Model Intercomparison Project (AerChemMIP), Coupled Climate Carbon
    Cycle Model Intercomparison Project (C4MIP) or Scenario Model
    Intercomparison Project (ScenarioMIP). A full list can be found
    [here](https://github.com/WCRP-CMIP/CMIP6_CVs/blob/master/CMIP6_activity_id.json),
-   *institution_id:* refers to the centre or institute responsible for
    the model,
-   *source_id:*  is the model used. Details for all models should be
    available through [ES-DOC](https://explore.es-doc.org/cmip6/models)
    (note this is still an active piece of work), 
-   *experiment_id:* refers to the set experiments being run for CMIP6.
    For example; PiControl, historical and 1pctCO2 (1 percent per year
    increase in CO2)
-   *variant_label: *is a label constructed from 4 indices (ensemble
    identifiers) r\<k>i\<l>p\<m>f\<n> where:
    -   k = realization_index
    -   l = initialization_index
    -   m = physics_index
    -   n = forcing_index
-   *table_id: *this refers to the MIP table being used. The MIP tables
    are used to organise the variables. For example, Amon refers to
    monthly atmospheric variables and Oday contains daily ocean data.
    Each variable in a MIP table must have a unique output name. To
    understand more about the naming conventions of MIP tables please
    see
    [here](https://www.earthsystemcog.org/projects/wip/mip_table_about),
-   *variable_id: *is the data variable, for example, Near-Surface Air
    Temperature (tas), Surface Air Pressure (ps), Relative Humidity
    (hur),
-   *grid_label: *this describes the model grid used. For
    example; global mean data (gm), data reported on a model\'s native
    grid (gn) or regridded data reported on a grid other than the native
    grid and other than the preferred target grid (gr1),
-   *version: *refers to the data version (for CMIP6 this is normally of
    the form vYYYYMMDD),
",https://help.ceda.ac.uk/article/4801-cmip6-data#directories,2291,288
Filenames,"The filename structure for CMIP6 is as follows:
    <variable_id>_<table_id>_<source_id>_<experiment_id>_<variant_label>_<grid_label>_<time_range>.nc
A number of filename facets are shared with the directory so only new
facet is \`time_range\`:
-   *time_range*: the date range of the data file is given in the format
    YYYYMMDD-YYYYMMDD with optionally additional elements to cover hours
    (HH) and seconds (SS). For example data from Jan 1st 1850 to 31st
    Dec 1899 would be: 18500101-18991231.
",https://help.ceda.ac.uk/article/4801-cmip6-data#filenames,503,65
Version,"As described above, the final part of the CMIP6 identifier is the
version number. To organise data on the filesystem the directory
structure is such that a symbolic link called \""latest\"" is created and
always points to the most recent version. Older versions of the data
retain their version number on the filesystem and so can be accessed if
need.  The \'latest\' directory should be the default that you use as
data may be updated due to known errors. Therefore the data within the
\'latest\' directory may change if a new version of the data are
published.
",https://help.ceda.ac.uk/article/4801-cmip6-data#version,561,97
Data Access: CEDA {#Data Access: CEDA},,https://help.ceda.ac.uk/article/4801-cmip6-data#data-access:-ceda-{#data-access:-ceda},0,0
CEDA data browser,"Data can be obtained from the [CEDA
data browser](http://data.ceda.ac.uk/badc/cmip6/data/CMIP6) by searching
through the directories (see below) and clicking the download button
under \'Actions\' :  
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5dbab9ee04286364bc91224b/file-t49xxczcrD.png)
",https://help.ceda.ac.uk/article/4801-cmip6-data#ceda-data-browser,333,25
Data discovery: catalogue search,"You can search for CMIP6 data through the [CEDA
Catalogue](https://catalogue.ceda.ac.uk/). Each institution has its own
project record which then contains dataset records from the different
models and experiments produced by that institute. The search bar can be
used to refine what is shown in the results, as seen below, you can
search by institution, model, experiment etc. 
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5dcabbe404286364bc91a405/file-TRzjU7bux2.png)From
the dataset record page there is 3 methods of downloading the data you
require, including the data browser as described above.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5dcabe252c7d3a7e9ae3d590/file-dsbybgYeVE.png)
",https://help.ceda.ac.uk/article/4801-cmip6-data#data-discovery:-catalogue-search,775,80
JASMIN users only,"Alternatively, data can be obtained directly from JASMIN under:
     /badc/cmip6/data/
As CEDA will only hold a percentage of the total CMIP6 archive, the
following section gives examples for data discovery on JASMIN to check
if the data you require are available on the CEDA Archive.
If you can\'t find the data you need, you can see if the data are
available from the full CMIP6 distributed archive searchable via the
ESGF ( [see the section below](#Data%20Access:%20ESGF)).
",https://help.ceda.ac.uk/article/4801-cmip6-data#jasmin-users-only,477,76
Data discovery: example searches,"The following section will provide examples of Linux commands which will
identify what CMIP6 data is available. Multiple examples have been
provided dependant on what data is required, these can be modified to
cater to your specific query. The following commands (search methods)
will need to contain the exact CMIP approved terminology when referring
to the names of the directories e.g. scenarioMIP (for guidance see the
[controlled vocabulary lists](https://github.com/WCRP-CMIP/CMIP6_CVs)).
",https://help.ceda.ac.uk/article/4801-cmip6-data#data-discovery:-example-searches,495,69
Example 1: CMIP identifier to path,"If you have the specific CMIP identifier for a file and want to check if
we hold that in the CEDA Archive, the following can be entered into the
command line. You can substitute in the CMIP identifier you have, to
obtain the CEDA Archive path using a translation command. You can then
list the file(s) within this directory.
    cmip_identifier=CMIP6.ScenarioMIP.BCC.BCC-CSM2-MR.ssp370.r1i1p1f1.Amon.tas.gn.v20190314
    ls /badc/cmip6/data/$cmip_identifier | tr . /
",https://help.ceda.ac.uk/article/4801-cmip6-data#example-1:-cmip-identifier-to-path,467,66
Example 2: List files,"The following command will simply list all of the files which exist in
the CEDA Archive under this specific combination of directories. By
adding the last command \' \| wc -l \', the file paths will not be
listed but the number of files will be displayed. This may be useful to
see if there are a large number of files for your query. In this
example, without the last command, a list of file paths will be returned
looking at all the CMIP models and experiments for one parameter.
    ls /badc/cmip6/data/CMIP6/HighResMIP/*/*/*/r1i1p1f1/Amon/tas/gn/latest/ | wc -l
",https://help.ceda.ac.uk/article/4801-cmip6-data#example-2:-list-files,566,95
Example 3: List paths,"In order to obtain a list of paths but not list all the files within
them, \' -I \'\*.nc\' \' will exclude all nc files. This command would
useful if you wanted to produce a list of paths to pass to a script,
which then read in all the files beneath. In this example, a list of
paths will be returned looking at all the CMIP models for one specific
parameter.
    ls -I '*.nc' /badc/cmip6/data/CMIP6/ScenarioMIP/*/*/ssp370/*/Amon/tas/gn/latest
",https://help.ceda.ac.uk/article/4801-cmip6-data#example-3:-list-paths,444,75
Example 4: Replacing wildcard,"If you would like to broaden your search and do not know the specific
directories under each parent, the wildcard (\*) may be useful. This may
also be useful if you want to analyse data across all models or all
parameters but do not know each specific path combination. In this
example, a list of file paths will be returned looking at all the
parameters for one specific model.
    ls /badc/cmip6/data/CMIP6/CMIP/MOHC/HadGEM3-GC31-LL/piControl/r1i1p1f1/Amon/*/*/latest
",https://help.ceda.ac.uk/article/4801-cmip6-data#example-4:-replacing-wildcard,470,71
Data Access: ESGF {#Data Access: ESGF},"If data you need is not available on JASMIN, please visit the Earth
System Grid Federation
([ESGF](https://esgf-index1.ceda.ac.uk/search/cmip6-ceda/)) site which
contains the full CMIP6 Archive. From this site, you can download the
data that you require for analysis. If using JASMIN to analyse data on
your GWS, only do this if there are a limited number of small files.
Otherwise please contact the CEDA helpdesk to request data be retrieved
for the main archive. 
The ESGF site allows you to search through all the available CMIP6 data
by filtering with specific requirements. These filters mimic the
directory structure used to store data in the CEDA Archive (as seen in
the [section above](#CMIP6%20data%20structure)).
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5d7262102c7d3a7e9ae0d7b6/file-H9GckZIw07.png)
To access the NetCDF files, select \'list files\' and from here the file
can be downloaded via HTTP or OpenDAP. Other methods of downloading the
files are available, including a wget script or Globus Connect, for
further information see the [ESGF
website](https://esgf-index1.ceda.ac.uk/search/cmip6-ceda/).
",https://help.ceda.ac.uk/article/4801-cmip6-data#data-access:-esgf-{#data-access:-esgf},1165,154
,,https://help.ceda.ac.uk/article/4801-cmip6-data#,0,0
CEDA Manual Metadata Store CMMS,"The CEDA data catalogue (MOLES) is integrated with other CEDA systems to
automatically pull in useful metadata where possible. For example, file
and parameter information can be harvested from the File Based Index
(FBI).  However, there are cases when this doesn\'t work so
complementary metadata store is required:
The CEDA Manual Metadata Store (CMMS).
",https://help.ceda.ac.uk,355,54
Introduction,"An introduction to using the CMMS with MOLES can be found
[here](https://docs.google.com/presentation/d/1kipaOuIiEvH7UOJFAqi1hpIEpKdjwv9cm0LuQye5tRs/edit?usp=sharing)
Remember - a CMMS entry related to an Observation record in MOLES. No
other record!
",https://help.ceda.ac.uk#introduction,251,27
What metadata can a CMMS entry hold?,"A CMMS entry can hold one or more of the following types of information
for a dataset to augment those scraped from elsewhere:
-   Parameters
-   Temporal range
-   Geographic bounding box
-   Total Volume -- for external, offline or removed content
-   Number of files -- for external, offline or removed content
-   Licence and access details -- for external content, offline or
    removed content
",https://help.ceda.ac.uk#what-metadata-can-a-cmms-entry-hold?,401,66
When to make a CMMS entry,"You will need to create a CMMS entry where you need to
1.  External datasets (not covered by FBI)
2.  Offline datasets (so not covered by FBI)
3.  Removed datasets (so no longer covered by FBI - NOTE need to capture
    the parameters before you delete!)
OR
Where there is an FBI content issue. E.g. :
1.  no parameter information available
2.  missing some information
3.  content is incorrect
",https://help.ceda.ac.uk#when-to-make-a-cmms-entry,395,69
Workflows,"The following are some common workflows that may be needed.
",https://help.ceda.ac.uk#workflows,60,10
Find/Creating a CMMS Entry,"::: {.section .video}
:::
",https://help.ceda.ac.uk#find/creating-a-cmms-entry,26,4
Adding parameters to a catalogue record,"::: {.section .video}
:::
",https://help.ceda.ac.uk#adding-parameters-to-a-catalogue-record,26,4
Weather Data in CEDA Archives,"Users frequently contact the CEDA helpdesk asking about the availability
of meteorological data for a given location. This article will hopefully
direct most people to relevant data that we hold and a few hints and
tips along the way. However, if you still can\'t find the data that you
need, please free to get in touch and we\'ll do what we can to assist.
",https://help.ceda.ac.uk/article/96-weather-data,358,64
,,https://help.ceda.ac.uk/article/96-weather-data#,0,0
Surface Meteorology,"-   Met Office 
    [MIDAS](http://catalogue.ceda.ac.uk/uuid/220a65615218d5c9cc9e4785a3234bd0) -
    is one of CEDA\'s most popular datasets containing historical
    meteorological observations back from the present to the 18th
    century. We have a number of tools and guides to making use of these
    data, including our MIDAS Quick Start Guide - see the related
    articles section below.
-   Met Office [MIDAS
    Open](https://catalogue.ceda.ac.uk/uuid/dbd451271eb04662beade68da43546e1) -
    this is a subset of the UK stations from MIDAS that have been made
    available under the UK Open Government licence, so available for all
    types of use.
-   Met Office 
    [MetDB](http://catalogue.ceda.ac.uk/uuid/8ee156b6ed41b153e85dbf02a4134513) -
    holds data from a limited number of meteorological message types,
    but reaches CEDA archives within 48 hours of observation. These
    include surface meteorology in land and sea SYNOP messages, METARS
    messages and upper air TEMP, PILOT and AMDARs messages. 
-   Met Office
    [HadUK-Grid](https://catalogue.ceda.ac.uk/uuid/4dc8450d889a491ebb20e724debe2dfb) -
    provides gridded surface data derived from a range of observations
    inputs, including the data from MIDAS.
\
Other sites that may have other data sources, including data from
amature observers, include:
[COL](https://www.colweather.org.uk/)/[Met Office Weather Observers
Website (WOW)](http://wow.metoffice.gov.uk/)/[Weather
Underground](https://www.wunderground.com/)
",https://help.ceda.ac.uk/article/96-weather-data#surface-meteorology,1505,172
Upper Air data,"\
-   Balloon-borne radiosonde measurements of temperature, humidity, wind
    speed and direction. CEDA has a number of data collections with
    these data such as:
    -   [Met Office high resolution radiosonde
        data](http://catalogue.ceda.ac.uk/uuid/c1e2240c353f8edeb98087e90e6d832e),
    -   [MetDB](http://catalogue.ceda.ac.uk/uuid/8ee156b6ed41b153e85dbf02a4134513)
        collection holds TEMP messages generated by radiosonde stations
        from sites around the world
-   Weather Radar data held by CEDA comes in a variety of types,
    including:
    -   wind profiling radar -
        [MST](http://catalogue.ceda.ac.uk/uuid/bd095d86e4a9f0c706b08058dbad3b31)
        radar near Aberystwyth,
        [Chilbolton](http://catalogue.ceda.ac.uk/uuid/7cbc3fc19bfa037a48ba4cba4b93544d)
        Observatory in Hampshire  [Met Office Wind profiler
        collection](http://catalogue.ceda.ac.uk/uuid/9b37cafea3a1fa3e6f69b3a85c46ee5c)
    -   Rain radar products from the [Met Office\'s
        NIMROD](http://catalogue.ceda.ac.uk/uuid/82adec1f896af6169112d09cc1174499)
        system include composite rain fall from the UK and Europe as
        well as single and dual polar products from individual sites.
-   LIDAR data held by CEDA include backscartter data from the [Met
    Office;s
    LIDARNET](http://catalogue.ceda.ac.uk/uuid/38a6e76871fca4c58d0f831e532bff41)
    collection
-   Airborne observations are available as part of the
    [FAAM](http://catalogue.ceda.ac.uk/uuid/affe775e8d8890a4556aec5bc4e0b45c)
    and
    [EUFAR](http://catalogue.ceda.ac.uk/uuid/d40e4067ae0121b31bb1ba57e04707de)
",https://help.ceda.ac.uk/article/96-weather-data#upper-air-data,1617,135
MIDAS Quick Start Guide,"The  [Met Office MIDAS dataset
collection](http://catalogue.ceda.ac.uk/uuid/220a65615218d5c9cc9e4785a3234bd0)
is one of CEDA\'s most popular datasets and also one which can cause
users some issues when attempting to use it. To help address this
problem CEDA have put together a MIDAS Users Quick Start Guide available
via the CEDA Document Repository at:
[](http://cedadocs.ceda.ac.uk/1260/2/midas_user_guide_v1.2.pdf)
<http://cedadocs.ceda.ac.uk/1437/>
The guide covers how users can:
-   find the stations reporting the data they need
-   use the CEDA Web Processing Service (CEDA-WPS) to extract the
    desired data
-   use the quality control flags and undertake basic filtering of the
    data
***Note**  - you will need to first register as a CEDA user and apply
for access to the MIDAS dataset collection before you can access these
data.* 
",https://help.ceda.ac.uk/article/94-midas,849,119
MIDAS FAQs,"We have also started to compile a list of answers to Frequently Asked
Questions from users about MIDAS data, which can be viewed
[here](https://help.ceda.ac.uk/article/269-midas-faq). 
",https://help.ceda.ac.uk/article/94-midas#midas-faqs,185,24
"Introduction {#introduction children-count=""0""}","CEDA will accept data relevant to atmospheric and earth observation
fields. This includes a wide range of instrumental, satellite, aircraft,
observations, analyses and model datasets of interest to the scientific
community. To archive data with CEDA please follow the steps below
(please also see the
[FAQ\'s](https://help.ceda.ac.uk/article/4661-depositing-data-faqs),
[video tutorial](https://www.youtube.com/watch?v=b2iiQp1PeiU), and
[webinar](https://www.youtube.com/watch?v=vd_nt4Cslno&list=PLhF74YhqhjqkXNWSgr4m6WSCjKZ2HEhHq)).
","https://help.ceda.ac.uk/article/4660-depositing-data-at-ceda-a-step-by-step-guide#introduction-{#introduction-children-count=""0""}",534,50
"Step by step guide {#step-by-step-guide children-count=""0""}","Once you have collected/produced your atmospheric or earth observation
data; processed, calibrated, validated and quality checked it and
prepared it for archiving using correctly applied [standard data
formats](https://help.ceda.ac.uk/category/4423-formats) (for example,
[NetCDF](https://help.ceda.ac.uk/article/106-netcdf) should adhere to
the [CF
conventions](https://help.ceda.ac.uk/article/4507-the-cf-metadata-convention)
and have good quality
[metadata](https://help.ceda.ac.uk/article/4428-metadata-basics)
including [global
attributes](https://help.ceda.ac.uk/article/4432-ceda-cf-examples)) and
[meaningful filenames](https://help.ceda.ac.uk/article/103-filenames),
you are now ready to upload it to the CEDA archive, using the following
steps.
**1.** Register for a [CEDA
account](https://services.ceda.ac.uk/cedasite/register/info/).
**2**. Go to [arrivals.ceda.ac.uk](http://arrivals.ceda.ac.uk/) and
login with your CEDA username and password.
**3.** Click begin. You need to agree to the [deposit
agreement](https://artefacts.ceda.ac.uk/licences/depositors_agreement)
the first time you use the service.  
**4.** Click browse deliveries.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5b5ed2cd0428631d7a89700a/file-GDZHNBN57G.png)
**5.** Choose an existing delivery directory or create a new delivery
directory by clicking on \'new directory\'.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5b5ed3540428631d7a897010/file-xLBilyUBmB.png)
**6.**  Add a suitable name for your delivery e.g. the project,
instrument name, model name or what the data contains, etc. and either
provide a link to an existing CEDA catalogue dataset record or follow
the circled link to generate one now. Adding the information this way
helps keep the record and the data together. Supplying metadata
alongside a data delivery is an essential component of dataset
management (conforming to principles of good data management) and is
required by CEDA in the majority of cases, with few exceptions. For
upload of intermediate products not intended for immediate archiving at
CEDA please contact the data management helpdesk
(data.management\@ceda.ac.uk) or your CEDA contact to discuss your
requirements.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/64b7e788a9d61472afe09011/file-RQqcFLTYb5.jpg)
**7.** To generate a new record, [fill in the
form](https://catalogue.ceda.ac.uk/metadata_records/add/new/) (example
layout below) with as much information (metadata) about your dataset as
you can. This form can also be accessed by clicking the circled link
mentioned in the previous step - please complete the form only once per
data delivery. Each box has help information if you click on the
\""**i**\"" icon. There are also example templates you can adapt - use the
\""Show Example\"" button to view these.  Further down this form you can
add Project, Instrument or Computation details and can look up and reuse
existing records in our catalogue. Further guidance on the types of
supplementary information to include is available at [this
link](https://help.ceda.ac.uk/article/143-supplementary-info).\
\
(Please note: **one** of either a URL to an existing record, a newly
created record or a metadata.yaml file is required to submit data. The
first two are generally recommended, however if you prefer to supply
metadata using a metadata.yaml file, this can be generated with this
[utility](http://artefacts.ceda.ac.uk/tools/dataset_ingest_labeler/dataset_labeler.html?_ga=2.101872833.1357629677.1697184458-39077896.1694429715))
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/64b7ecacd48f7f58ce212bbd/file-ez8abSORdE.jpg)
Save and preview the record, and go back and edit if required. (Saved,
unsubmitted records can be found under \' [Metadata
records](https://catalogue.ceda.ac.uk/metadata_records/)\' when logged
in). Once it is complete you can submit it for review by the CEDA team,
and proceed to upload your data.
**8.** Upload your files using the following tools if appropriate: unzip
zipped files, fix bad names (this removes unusual characters and
spacing), remove empty directories, remove zero length files and remove
links from filenames. 
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5b5ed3970428631d7a897017/file-KaUHh3dZ2z.png)
You can also upload your data via
[FTP](https://www.youtube.com/watch?v=KiZ_2xRlVPY) or
[RSYNC](https://www.youtube.com/watch?v=RFSDyt88ICg) (these can be found
by clicking the \'other upload methods\' button shown above). **If you
use these methods you still need to come back to the web interface to
submit the uploaded data.** Further information on the delivery tools
accepted by CEDA and written instructions on using these can be found on
this [page](https://help.ceda.ac.uk/article/142-sending-data-to-ceda).
**9.** Click the \'Review submission\' button.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/64b7f11dc2f5ed048130c888/file-rYIoA9lKCB.jpg)
**10.** Confirm Submission (\'Agree and submit\').
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/64b7f33378d5c33f492412cd/file-9mOZ5BVE39.jpg)
**11.** A CEDA contact will check the files and move them to the CEDA
archive. Catalogue records will be created to make your data
discoverable (using the metadata you supplied in step 7), or else
approved by CEDA if you have already created/linked a record. This takes
time, so please be patient with us.
**12.** Finally, we will ask you to review these catalogue records, and
when all checks are complete your dataset will be published.
**13.** If you requested a DOI (Digital Object Identifier) for your
dataset, it can then be provided. Further guidance is available
[here](https://help.ceda.ac.uk/article/146-data-citations-and-dois).
","https://help.ceda.ac.uk/article/4660-depositing-data-at-ceda-a-step-by-step-guide#step-by-step-guide-{#step-by-step-guide-children-count=""0""}",5956,661
NetCDF format,"This is intended as a short introduction to netCDF data. The complete
netCDF documentation can be found on the 
[UNIDATA](https://www.unidata.ucar.edu/software/netcdf/) web site.
Quick links to
-   [Climate and Forecast (CF) Metadata Convention Page from
    CEDA](https://help.ceda.ac.uk/article/4507-the-cf-metadata-convention)
-   [List of downloadable software to write, read and handle NetCDF
    files](#reading)
",https://help.ceda.ac.uk/article/106-netcdf,419,47
Contents,"-   [What is NetCDF?](#what)
-   [Why use NetCDF?](#why)
-   [NetCDF and the Climate and Forecast (CF) Metadata Convention](#cf)
-   [What\'s in a NetCDF file?](#cf_huh) -- Example.
-   [Reading and writing NetCDF files](#reading) -- Including links to
    downloadable software packages.
-   [Further information](#more) -- On NetCDF and the Climate Forecast
    (CF) Metadata Convention.
------------------------------------------------------------------------
",https://help.ceda.ac.uk/article/106-netcdf#contents,463,53
What is netCDF? {#what},"NetCDF (network Common Data Form) is an interface for array-orientated
data access and a library that provides an implementation of that
interface. Many groups have adopted netCDF as a standard way to
represent their scientific data. The netCDF software was developed at
the Unidata Program Center in Boulder Colorado USA.
",https://help.ceda.ac.uk/article/106-netcdf#what-is-netcdf?-{#what},323,50
Why use netCDF? {#why}," The NetCDF format has a wide range of reasons why it is one of CEDA\'s
recommended formats, including:
-   being extensively used within the atmospheric and oceanic science
    communities.
-   being a portable self-describing binary data format.
-   it is network-transparent, meaning that it can be accessed by
    computers that store integers, characters and floating-point numbers
    in different ways.
-   it provides direct-access: a small subset of a large dataset may be
    accessed efficiently, without first reading through all the
    preceding data.
-   it is appendable: data can be appended to a netCDF dataset along one
    dimension without copying the dataset or redefining its structure.
-   datasets can be read and written in a number of languages, these
    include C, C++, FORTRAN, IDL, Python, Perl, and Java.
-   the different language implementations are
    [freely](https://www.unidata.ucar.edu/software/netcdf/software.html)
    available from [the UNIDATA ftp
    area](ftp://ftp.unidata.ucar.edu/pub/netcdf/)
-   several graphics packages support netCDF input, making it very easy
    to display and analyse netCDF datasets. For instance
    [FERRET](https://ferret.pmel.noaa.gov/Ferret/documentation/users-guide/data-set-basics/NETCDF-DATA)
    and [UVCDAT](https://uvcdat.llnl.gov/index.html) provide both
    command line and graphical user interfaces for displaying and
    analysing gridded data.
-   netCDF is completely and methodically documented in UNIDATA\'s
    [NetCDF User\'s
    Guide](https://www.unidata.ucar.edu/software/netcdf/docs/).
-   several groups have defined
    [conventions](https://www.unidata.ucar.edu/software/netcdf/conventions.html)
    for netCDF files, to enable the exchange of data. CEDA has adopted
    the [Climate and Forecasting (CF)
    conventions](https://help.ceda.ac.uk/article/4507-the-cf-metadata-convention)
    for netCDF data.
",https://help.ceda.ac.uk/article/106-netcdf#why-use-netcdf?-{#why},1912,215
NetCDF and CF conventions {#cf},"CEDA supports and strongly recommends the compliance with the [Climate
and Forecast (CF) Metadata Convention](http://cfconventions.org/).
The CF conventions are guidelines and recommendations as to where to put
information within a netCDF file, and they provide advice as to what
type of information you might want to include. CF conventions allow the
creator of the dataset to include information about the data and the
dataset itself (metadata) in a structured way, which makes it easier for
other users to retrieve the information.  *Global attributes* describe
the general properties and origins of the dataset while *local
attributes* are used to characterise the recorded variables.
CEDA provides an [Introduction to the CF
Convention](https://help.ceda.ac.uk/article/4507-the-cf-metadata-convention),
illustrated by some examples. 
",https://help.ceda.ac.uk/article/106-netcdf#netcdf-and-cf-conventions-{#cf},839,114
What\'s in a netCDF file? {#cf_huh},"This is best described with the following [very simple
example](http://zonda1.badc.rl.ac.uk/help/formats/netcdf/nc_example.html)
of a netCDF file with CF conventions applied. In this you will see that
a netCDF dataset (the filename ends in **.nc**) is made up of three
basic components:
-   variables
-   dimensions
-   attributes
The variables store the actual data, the dimensions give the relevant
dimension information for the variables, and the attributes provide
auxiliary information about the variables or the dataset itself. You can
study [an example of a netCDF file](https://help.ceda.ac.uk/) and see
more details of the components in a netCDF dataset. The global
attributes can also be added to the netCDF file, following [CF
conventions](https://help.ceda.ac.uk/article/4432-ceda-cf-examples).
",https://help.ceda.ac.uk/article/106-netcdf#what\'s-in-a-netcdf-file?-{#cf_huh},807,108
Reading and writing netCDF {#reading},"The popularity of netCDF stems from its portability and relative ease of
use. Although it is a binary data format the netCDF distribution comes
complete with the  **ncdump** utility which produces an ASCII dump of
the dataset and thereby provides a quick-look facility. You can also
generate netCDF files from ASCII data files using the **ncgen** utility.
The datasets are commonly written and read using library routines. The
library routines are sensibly named and relatively consistently named
across the different language implementations.
",https://help.ceda.ac.uk/article/106-netcdf#reading-and-writing-netcdf-{#reading},544,82
**Help and tools**,"**1. Guidance and tools provided by CEDA:**
-   Installing the netCDF package provided by Unidata and trying
    out **ncgen** and **ncdump**. -- Converting NetCDF to ASCII. 
-   [XCONV/CONVSH](http://cms.ncas.ac.uk/documents/xconv/ ""Xconv/Convsh - Software for file manipulation/conversion""){.external}
    -- Allows visualisation, conversion and subsetting of NetCDF data on
    Unix/Linux systems. 
-   [NetCDF format
    checker](https://ceda-wps-ui.ceda.ac.uk/processes/list?wps=compliance_checker ""CEDA CF-netCDF file checker service""){.external}
    -- If you wish to check that files are correctly formatted before
    uploading them to CEDA, please use this online tool. 
**2. Selected links to available online software packages to write, read
and handle NetCDF files:**
[Software for Manipulating or Displaying NetCDF
Data](https://www.unidata.ucar.edu/software/netcdf/software.html) -- A 
list of links to a variety of freely available and commercial or
licensed software packages to create or handle NetCDF files, provided by
[Unidata](https://www.unidata.ucar.edu/) at UCAR (USA).
-   [cf-python](https://ncas-cms.github.io/cf-python/): a Python Earth
    Science data analysis library that is built on a complete
    implementation of the CF data model.
-   [iris](https://scitools-iris.readthedocs.io/): a powerful,
    format-agnostic, community-driven Python package for analysing and
    visualising Earth science data.
-   [NCL](http://www.ncl.ucar.edu/): NCAR Command Language (NCL), an
    interpreted language designed for scientific data analysis and
    visualisation. It supports netCDF-3/4, GRIB-1/2, HDF4-SDS, HDF4-EOS,
    HDF5-EOS and shapefiles.
-   [xarray](http://xarray.pydata.org/en/stable/): (formerly *xray*) is
    an open-source project and Python package that makes working with
    labelled multi-dimensional arrays simple, efficient, and fun!
",https://help.ceda.ac.uk/article/106-netcdf#**help-and-tools**,1885,210
Further information {#more},"For a more extensive description you are urged to read the  [UNIDATA
documentation on
netCDF](https://www.unidata.ucar.edu/software/netcdf/docs/).
The UNIDATA documentation also provides the  [File Format
Specification](https://www.unidata.ucar.edu/software/netcdf/docs/file_format_specifications.html)
for netCDF and [Best Practice recommendations on writing netCDF
files](https://www.unidata.ucar.edu/software/netcdf/docs/BestPractices.html).
The  [NetCDF Climate and Forecast (CF) Metadata
Convention](http://cfconventions.org/) defines metadata that provide a
definitive description of what the data in each variable represents, and
of the spatial and temporal properties of the data. The CF conventions
generalize and extend the COARDS conventions developed under the
sponsoring of NOAA.
",https://help.ceda.ac.uk/article/106-netcdf#further-information-{#more},793,82
"Introduction - from Thursday 29th July 2021 {#introduction---from-thursday-29th-july-2021 children-count=""0""}","The CEDA Archive has been a conduit for the research community to access
a range of weather data supplied by the Met Office under the NERC-Met
Office agreement. Until 2021 users had to submit repeat applications for
access to each of the dataset groups (for example radiosonde data, wind
profiler data, surface observations).However, with a combined access
route already available for users of Met Office weather data already in
place for users on the JASMIN data analysis system it has been possible
to apply changes to the access control for a range of Met Office weather
data.
CEDA Users can apply for this new access here:
<https://services.ceda.ac.uk/cedasite/resreg/application?attributeid=ukmo_wx>
","https://help.ceda.ac.uk/article/5001-accessing-met-office-weather-data#introduction---from-thursday-29th-july-2021-{#introduction---from-thursday-29th-july-2021-children-count=""0""}",705,107
"Which data are included and which are not {#which-data-are-included-and-which-are-not children-count=""0""}","The new combined access route has been applied to weather related
datasets in the CEDA Archive including:
-   Surface observations in the full MIDAS dataset collection
-   Ceilometer data from the LIDARNET dataset collection
-   Historic wind profiler and radiosonde data
-   High resolution radiosonde data
-   Model output from the operational Unified Model
-   Met Research Flight (MRF) data
See the bottom of this help page for a [full list](#ukmo_wx_list) of
dataset collections with links to their catalogue entries.
Dataset from the Met Office Hadley Centre or those already available
under the Open Government Licence are **not** covered by this new
combined access route.
","https://help.ceda.ac.uk/article/5001-accessing-met-office-weather-data#which-data-are-included-and-which-are-not-{#which-data-are-included-and-which-are-not-children-count=""0""}",681,105
"For New Users {#for-new-users children-count=""0""}","If you are a new user you simply need to find one of the datasets within
the access group and apply for one of the datasets. Once granted you
will then be able to access all the other datasets within the same
access group. Note, as before if you need to use the data for an
alternative reason then a fresh application will need to be submitted.
Likewise, if you are working with colleagues wishing to use the data too
then, as the access and licence is strictly granted on a per person, per
project basis, they must apply for - and receive - their own access.
","https://help.ceda.ac.uk/article/5001-accessing-met-office-weather-data#for-new-users-{#for-new-users-children-count=""0""}",560,107
"For users with existing or pending applications: {#for-users-with-existing-or-pending-applications children-count=""0""}","Existing users of datasets now covered by the new access route will need
to apply for access to this new combined access group. Parallel access
will be in place for existing access until the end of August 2021 when
the existing access groups will then be removed in order to allow CEDA
to complete the transition to the new access arrangements. We hope that
the length of the transition period will allow uses to transition
smoothly and benefit from the increased access they then achieve. Beyond
greater access existing users should not notice any other changes.
Pending application will be removed from the application system.
","https://help.ceda.ac.uk/article/5001-accessing-met-office-weather-data#for-users-with-existing-or-pending-applications:-{#for-users-with-existing-or-pending-applications-children-count=""0""}",629,105
"For government users {#for-government-users children-count=""0""}","The Met Office also permits access to these data for governmental use
(e.g. by use within local/national government departments) through a
dedicated option as part of the application process. Existing users with
such access will also need to re-apply for access.
","https://help.ceda.ac.uk/article/5001-accessing-met-office-weather-data#for-government-users-{#for-government-users-children-count=""0""}",263,41
"For JASMIN Users {#for-jasmin-users children-count=""0""}","No additional actions are needed with regards to JASMINN user accounts -
the change in the access is managed through the CEDA archive accounts
linked to your account. If you don't already have access to Met Office
data then you may need to link your CEDA account to your JASMIN account
- see
[here](https://help.jasmin.ac.uk/article/4552-update-a-jasmin-account#linkceda)
for instructions.
","https://help.ceda.ac.uk/article/5001-accessing-met-office-weather-data#for-jasmin-users-{#for-jasmin-users-children-count=""0""}",390,56
"Full list of collections covered: {#ukmo_wx_list children-count=""0""}","The following links are the dataset collections which contain one or
more datasets covered by this new combined access route. Only one
application is required to access the Met Office weather data included
in these. The list below has been divided into 3 sections:
","https://help.ceda.ac.uk/article/5001-accessing-met-office-weather-data#full-list-of-collections-covered:-{#ukmo_wx_list-children-count=""0""}",265,44
"Presently live datasets (i.e. continue to be updated - some updates may be pending) {#presently-live-datasets-i.e.-continue-to-be-updated---some-updates-may-be-pending children-count=""0""}","-   [Met Office Mean Sea Level Pressure (MSLP) Charts Collection 1999 to
    present.](https://catalogue.ceda.ac.uk/uuid/267264a0036052fe71f5f25e384f0339)
-   [Met Office Integrated Data Archive System (MIDAS) Land and Marine
    Surface Stations Data
    (1853-current)](https://catalogue.ceda.ac.uk/uuid/220a65615218d5c9cc9e4785a3234bd0)
-   [Met Office MetDB system: Surface, upper air and satellite
    data](https://catalogue.ceda.ac.uk/uuid/8ee156b6ed41b153e85dbf02a4134513)
-   [Met Office Rain Radar Data from the NIMROD
    System](https://catalogue.ceda.ac.uk/uuid/82adec1f896af6169112d09cc1174499)
-   [Met Office high resolution radiosonde data from the UK, Gibraltar,
    St Helena and the Falkland
    Islands](https://catalogue.ceda.ac.uk/uuid/c1e2240c353f8edeb98087e90e6d832e)
-   [Met Office LIDARNET ceilometer network cloud base and backscatter
    data](https://catalogue.ceda.ac.uk/uuid/38a6e76871fca4c58d0f831e532bff41)
-   [Operational Numerical Weather Prediction (NWP) Output from the
    North Atlantic European (NAE) Part of the Met Office Unified Model
    (UM)](https://catalogue.ceda.ac.uk/uuid/220f1c04ffe39af29233b78c2cf2699a)
-   [Operational Numerical Weather Prediction (NWP) Output from the
    Global Atmospheric Part of the Met Office Unified Model
    (UM)](https://catalogue.ceda.ac.uk/uuid/41f061e11217e549a498971725e90520)
-   [Operational Numerical Weather Prediction (NWP) Output from the UK
    Variable (UKV) Resolution Part of the Met Office Unified Model
    (UM)](https://catalogue.ceda.ac.uk/uuid/292da1ccfebd650f6d123e53270016a8)
-   [NWP-Euro: Operational Numerical Weather Prediction (NWP) output
    from the European Atmospheric High Resolution Model; part of the Met
    Office Unified Model
    (UM)](https://catalogue.ceda.ac.uk/uuid/4d6f884bf7cf46df8950b1b570fe8453)
-   [NWP-Global: Operational Numerical Weather Prediction (NWP) output
    from the UK Met Office Global Atmospheric Unified Model
    (UM)](https://catalogue.ceda.ac.uk/uuid/e4ac04e7fa2541278ad4ad06fb4fd5f3)
-   [NWP-UKV: Operational Numerical Weather Prediction (NWP) output from
    the UK Met Office UK Atmospheric High Resolution Unified Model
    (UM)](https://catalogue.ceda.ac.uk/uuid/78f23c539d304591b137cf986b69a525)
","https://help.ceda.ac.uk/article/5001-accessing-met-office-weather-data#presently-live-datasets-(i.e.-continue-to-be-updated---some-updates-may-be-pending)-{#presently-live-datasets-i.e.-continue-to-be-updated---some-updates-may-be-pending-children-count=""0""}",2253,199
"Historic Met Office datasets no longer being updated {#historic-met-office-datasets-no-longer-being-updated children-count=""0""}","-   [Output Runs from the Met Office Limited Area Model for Africa
    (Africa-LAM)](https://catalogue.ceda.ac.uk/uuid/aa1dc1294d8235e7b62a4fc62b3ff3d6)
-   [Met Office\'s European Synoptic stations data (1990 -
    1996)](https://catalogue.ceda.ac.uk/uuid/9f7a888398fcb9b49c9a7e380f713e20)
-   [Met Office Research Unit, Cardington: surface and upper air
    observations](https://catalogue.ceda.ac.uk/uuid/91ddf459b34c8efa4fc6fd711f6f1d88)
-   [Met Office Stratospheric Assimilated
    Data](https://catalogue.ceda.ac.uk/uuid/f0095ccfd57aa3c62b64d3e406ab1f73)
-   [Met Office Global Radiosonde
    Data](https://catalogue.ceda.ac.uk/uuid/f2afaf808b61394b78bd342ff068c8cd)
-   [Met Office vertical wind profiler measurements for the British
    Isles
    (1998-onwards)](https://catalogue.ceda.ac.uk/uuid/9b37cafea3a1fa3e6f69b3a85c46ee5c)
-   [Met Office Land Surface Stations Data
    (1900-2000)](https://catalogue.ceda.ac.uk/uuid/ea2d5d8bce505ad4b10e06b45191883b)
-   [Met Office Cyclone
    database](https://catalogue.ceda.ac.uk/uuid/b0e740cdd68b13f6462f6d5d1a68092e)
","https://help.ceda.ac.uk/article/5001-accessing-met-office-weather-data#historic-met-office-datasets-no-longer-being-updated-{#historic-met-office-datasets-no-longer-being-updated-children-count=""0""}",1074,71
"Project related datasets - (provided in support of specific projects) {#project-related-datasets---provided-in-support-of-specific-projects children-count=""0""}","These datasets may contain one or more Met Office datasets within the
collection, though the link is to the collection as a whole which may
contain data from other sources not covered by the new combined Met
Office weather data access route:
-   [Eyjafjallajokull Volcanic Ash Cloud Measurements and
    Imagery](https://catalogue.ceda.ac.uk/uuid/e6f5502c687f25a6c7009d4704b124b4)
-   [DIAMET: Ensemble of Atmospheric Airborne and Ground-based
    Measurements including Radar
    Data](https://catalogue.ceda.ac.uk/uuid/6ca226c9634e57437f204ad9c5be77e1)
-   [PICASSO: data from radiosonde flights, Chilbolton weather radar and
    in-situ airborne observations by the FAAM BAE-146
    aircraft](https://catalogue.ceda.ac.uk/uuid/400efba73c1d40c78f44918429ce9c99)
-   [Forecasting Rainfall exploiting new data Assimilation techniques
    and Novel observations of Convection (FRANC): rain radar helical
    scan data, assimilation versus model residuals and ensemble member
    model
    output.](https://catalogue.ceda.ac.uk/uuid/333bf4303034426a857515a768387e4f)
-   [GRACES: Collection of remote sensing and surface
    observations](https://catalogue.ceda.ac.uk/uuid/ea21f06880c345e0bba8862c03fdf54a)
-   [Natural Environment Research Council (NERC)
    Mesosphere-Stratosphere-Troposphere (MST) Radar Facility: surface
    and upper air meteorology measurements from Aberystwyth,
    mid-Wales](https://catalogue.ceda.ac.uk/uuid/bd095d86e4a9f0c706b08058dbad3b31)
-   [Hydrological Radar Experiment (HYREX): Radar, Raingauge and Model
    Forecast data on the Brue catchment in Southwest
    England](https://catalogue.ceda.ac.uk/uuid/0e84c6fd72c9a3ea3e772e12e5699307)
-   [Convective Storm Initiation Project (CSIP): surface, remotely
    sensed and airborne atmospheric measurements
    collection](https://catalogue.ceda.ac.uk/uuid/44dfa95b7a2768c006b12fbb2f28799c)
","https://help.ceda.ac.uk/article/5001-accessing-met-office-weather-data#project-related-datasets---(provided-in-support-of-specific-projects)-{#project-related-datasets---provided-in-support-of-specific-projects-children-count=""0""}",1873,168
Privacy and Cookies Notice,"Centre for Environmental Data Analysis (CEDA) and Data Protection
At STFC (being a Council under, and part of, United Kingdom Research and
Innovation), we intend to make sure that we maintain the UK as a
world-leader in science and technology research - integral to this is
our commitment to protecting and safeguarding your data privacy.
It is part of STFC's public purpose to provide services such as the CEDA
Archive and services and JASMIN Services, including the JASMIN Cloud
service. In order to effectively provide that service to you, we need to
collect information about you and may need to contact you from time to
time with regards to its provision.
However, you must not sign up for a CEDA User or JASMIN User account if
you are under the age of 18. If you are under the age of 18 you must get
someone with parental responsibility to open and manage an account on
your behalf.
STFC considers it to be in its legitimate interest that it should be
able to efficiently manage the CEDA and JASMIN services for the benefit
of anyone who has signed up to access them. STFC therefore requests and
securely holds the data which it requires for these purposes. Please
ensure that, for so long as you are accessing these service, the
information that you have provided to us is kept up to date via the CEDA
and JASMIN user accounts or that you notify us if your information needs
to be updated by [contacting us](http://www.ceda.ac.uk/contact/).
We may share your details with others outside CEDA\'s host organisation,
STFC, to aid in the provision of data and services; for example: to gain
third-party approval for your access to particular data and services;
for external assistance in resolving user issues/answering enquiries.
",https://help.ceda.ac.uk/article/4639-privacy-and-cookies#privacy-and-cookies-notice,1734,295
UKRI and Data Protection,"UKRI and STFC comply with the requirements of the GDPR with regard to
the collection, storage, processing and disclosure of personal
information and we are committed to upholding the GDPR\'s core data
protection principles.
A full notice of UKRI's position with regards to Freedom of Information
and the General Data Protection Regulations (GDPR) (EU) (2016/679) can
be found at:
        <https://www.ukri.org/privacy-notice/>
        <https://www.ukri.org/cookie-policy/>
        <https://www.ukri.org/terms-of-use/>
        <https://www.ukri.org/freedom-of-information/>
",https://help.ceda.ac.uk/article/4639-privacy-and-cookies#ukri-and-data-protection,573,63
Cookies,"As is common practice with websites, CEDA and JASMIN websites may place
small files known as ""cookies"" on your device (e.g. computer or
smartphone) in order to improve your user experience.
",https://help.ceda.ac.uk/article/4639-privacy-and-cookies#cookies,190,31
Our use of cookies,"CEDA and JASMIN websites themselves set 'first-party' cookies in order
to identify you as you travel around the sites. This enables us to
provide services that are tailored to your account, such as applying for
access to a resource. These cookies will usually be deleted when you log
out, however in some cases they may remain afterwards to remember your
site preferences.
CEDA and JASMIN sites may also make use of 'third-party' cookies from
the following services:
-   Google Analytics
-   Google Maps
-   YouTube (owned by Google)
-   Help Scout
We use these services to monitor the usage of our websites in order to
continue to improve the services we offer, and to provide effective user
support. For more information about cookies set by Google services, see
the [Google Cookie
Policy](https://www.google.com/policies/technologies/cookies/), and for
Help Scout services see the [Help Scout Privacy
Policy](https://www.helpscout.net/company/legal/privacy/).
",https://help.ceda.ac.uk/article/4639-privacy-and-cookies#our-use-of-cookies,963,143
Disabling cookies,"CEDA and JASMIN websites will not use cookies to collect personally
identifiable information about you.
However, if you wish to restrict or block the cookies which are set by
our websites, or indeed any other website, you can do this through your
browser settings. The 'Help' function within your browser should tell
you how.
Alternatively, you may wish to visit the [About
Cookies](https://www.aboutcookies.org/) website, which contains
comprehensive information on how to do this on a wide variety of
browsers. You will also find details on how to delete cookies from your
machine as well as more general information about cookies.
Please be aware that restricting cookies may have an impact upon the
functionality of CEDA and JASMIN websites.
",https://help.ceda.ac.uk/article/4639-privacy-and-cookies#disabling-cookies,746,118
Questions,"If you have any questions about this policy, please [contact
us](http://www.jasmin.ac.uk/contact/).
Current version: 1
Published on: 23 May 2018
Last revision: 23 May 2018
",https://help.ceda.ac.uk/article/4639-privacy-and-cookies#questions,172,24
General Download Services,"  ---------------------------------------------------------------------------------------- --------------------------------------------------- -----------------------
  **Download Tool**                                                                        **Link to service**                                 **Suitable for:**
  [CEDA                                                                                    [http://data.ceda.ac.uk](http://data.ceda.ac.uk/)   Used for browsing the
  OPeNDAP](https://help.ceda.ac.uk/article/4431-ceda-archive-web-download-and-services)\                                                       archive file system and
  \                                                                                                                                            interactive downloads.
                                                                                                                                               It\'s also a scriptable
                                                                                                                                               interface (API) to the
                                                                                                                                               archive over http,
                                                                                                                                               allowing remote
                                                                                                                                               subsetting within IDL,
                                                                                                                                               Matlab. Python etc. \
  [FTP](https://help.ceda.ac.uk/article/280-ftp)                                           ftp.ceda.ac.uk and anon-ftp.ceda.ac.uk              small - medium scale
                                                                                                                                               data downloads. The
                                                                                                                                               anon-ftp.ceda.ac.uk
                                                                                                                                               only contains a small
                                                                                                                                               subset of data that can
                                                                                                                                               be downloaded
                                                                                                                                               anonymously (with no
                                                                                                                                               login).
  ---------------------------------------------------------------------------------------- --------------------------------------------------- -----------------------
",https://help.ceda.ac.uk/article/99-download-data-from-ceda-archives#general-download-services,3272,74
Metadata Services,"  ----------------------------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------------- -----------------------
  Interface                                             Link to service                                                                                                                                     Suitable for:
  [CEDA Catalogue](http://catalogue.ceda.ac.uk/)\       [http://catalogue.ceda.ac.uk/ ](http://catalogue.ceda.ac.uk/%20)\                                                                                   Browsing information
                                                                                                                                                                                                            about the datasets in
                                                                                                                                                                                                            the archive.   
  [CSW](http://www.opengeospatial.org/standards/cat)\   [https://csw.ceda.ac.uk/geonetwork/srv/eng/csw](https://csw.ceda.ac.uk/geonetwork/srv/eng/csw?SERVICE=CSW&VERSION=2.0.2&REQUEST=GetCapabilities)\   Programatically
                                                                                                                                                                                                            interrogating the CEDA
                                                                                                                                                                                                            Catalogue using
                                                                                                                                                                                                            Catalogue Service for
                                                                                                                                                                                                            the Web service API. 
  [OAI-PMH](https://www.openarchives.org/pmh/)\         <https://csw.ceda.ac.uk/geonetwork/srv/eng/oaipmh?verb=ListRecords&metadataPrefix=oai_dc>\                                                          Programatically
                                                                                                                                                                                                            interrogating the CEDA
                                                                                                                                                                                                            Catalogue using OAI-PMH
                                                                                                                                                                                                            API. This is protocol
                                                                                                                                                                                                            for metadata harvesting
                                                                                                                                                                                                            and is suitable for
                                                                                                                                                                                                            doing federated
                                                                                                                                                                                                            searches. 
  ----------------------------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------------- -----------------------
",https://help.ceda.ac.uk/article/99-download-data-from-ceda-archives#metadata-services,4252,62
Bulk download options,"There are various options to bulk download data. You can see our most up
to date suggestions by clicking on the \'bulk download options\' button
on the [download service webpage](https://data.ceda.ac.uk/) (see image
below). 
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/60e569a78556b07a2884d11b/file-DMlBdgVz9l.png)
Once you have clicked on the button, it will show you our recommended
bulk download options.  
",https://help.ceda.ac.uk/article/99-download-data-from-ceda-archives#bulk-download-options,453,50
Specific data set services,"Tools to subset or aggregate data before downloading are available for
some data. These are linked to from the catalogue page for that dataset.
Examples of these tools are:
-   CEDA WPS service: ** **<http://wps-web1.ceda.ac.uk/ui/home>
-   Earth System Grid Federation (ESGF):
    <https://esgf-index1.ceda.ac.uk/projects/esgf-ceda/>
-   CEDA Satellite Data Finder (Sentinel and Landsat missions
    currently): <http://geo-search.ceda.ac.uk/>
-   Flight Finder (airborne data): <http://flight-finder.ceda.ac.uk/>
[](https://esgf-index1.ceda.ac.uk/projects/esgf-ceda/)
",https://help.ceda.ac.uk/article/99-download-data-from-ceda-archives#specific-data-set-services,570,60
Direct file system access to the archive,"[JASMIN](https://jasmin.ac.uk/) is a globally-unique data analysis
facility. It provides storage and compute facilities to enable
data-intensive environmental science. You can access the CEDA Archive
via JASMIN - this allows direct file system access. You will need to
create a JASMIN account (if you do not already have one). JASMIN users
should consult the JASMIN documentation for more information about
direct [CEDA Archive
access ](https://help.jasmin.ac.uk/article/3838-ceda-archive)and
appropriate [data transfer
mechanisms](https://help.jasmin.ac.uk/category/217-data-transfer). 
",https://help.ceda.ac.uk/article/99-download-data-from-ceda-archives#direct-file-system-access-to-the-archive,588,68
,"*\
*
",https://help.ceda.ac.uk/article/99-download-data-from-ceda-archives#,5,2
Supplying Catalogue Record and Other Information,"To help users find data in CEDA archives it is important that we have
correct information about the data. This includes information about
dataset itself, like a description and the geographic area covered by
the data, but also information about the
[instruments](http://catalogue.ceda.ac.uk/listings/instr/) or model used
to create the data and other project background. These details can be
sent via a text file with the
[data](https://arrivals.ceda.ac.uk/intro/). We will assume a file in the
top level of the delivered dataset with the name `metadata.yaml`
contains the details of the dataset, project and instrument or model.
You can either use this simple metadata file creation
[utility](http://artefacts.ceda.ac.uk/tools/dataset_ingest_labeler/dataset_labeler.html),
or edit one of the examples below:
[example dataset details YAML
file](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/attachments/5b4cc8ae0428631d7a88f39a/marius.yaml).  
[station-data_metadata_example.yaml](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/attachments/5ba24c0a2c7d3a16370f4656/station-data_metadata_example.yaml)
[instrument_metadata_example.yaml](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/attachments/5ba24bf82c7d3a16370f4654/instrument_metadata_example.yaml)
[model_metadata_example.yaml](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/attachments/5ba24bef2c7d3a16370f4653/model_metadata_example.yaml)
Adding the information this way helps us keep the record and the data
together even if its supplied over non-web based channels like FTP.
",https://help.ceda.ac.uk/article/143-supplementary-info,1652,142
Other useful information,"Besides the information areas covered in the above links it is also
helpful to provide the following types of information to CEDA to help
curate your data for long-term use:
-   links to useful websites
-   script to read in/plot the data
-   documentation related to particular data formats
-   copies of project logos
-   photographs of instruments and sites where the instrument is
    deployed
-   calibration information
-   links to related articles
For fixed content these can be uploaded to our [CEDA Document Repository
Zenodo
Community](https://zenodo.org/deposit/new?c=ceda-document-repository)
for long-term archiving which can then be linked up to from the relevant
data catalogue records for data in the CEDA archive. These are items
that users control themselves and can easily update with new versions if
required. Zenodo also automatically issues DOIs for these items allowing
greater traceability and reliable connections.
",https://help.ceda.ac.uk/article/143-supplementary-info#other-useful-information,941,139
Ingest from GWS,"The `badc` user used for ingestion on the ingest machines does not have
access to content in JASMIN group workspace, which can make ingesting
data held on them tricky. The cause is that the `badc` user can not be
added to the linux groups set up for the large number of GWS on JASMIN
*and* also retain active membership of linux groups needed for archive
activities. To get around this world read access needs to be set up for
the required GWS.
",https://help.ceda.ac.uk/article/4999-ingest-from-gws,445,82
"Steps to getting world read access set up for a GWS {#steps-to-getting-world-read-access-set-up-for-a-gws children-count=""0""}","1.  Contact the GWS manager and obtain permission to give world read
    access to the GWS on the JASMIN system via the JASMIN helpdesk
2.  Once permission has been given contact a JASMIN team member with
    root access to the GWSs and ask them to set up world read access for
    the GWS (note to JASMIN team member to set this as `o+rx` as opposed
    to `o+x`
3.  Check that the GWS can be read from the ingest machine
4.  Inform the GWS manager that world read access has been set up and
    that they may wish to ensure that access to other GWS directories
    that should not be ingested from should be set to ensure no world
    read access.
5.  Once ingestion has been complete contact the GWS manager and arrange
    when the top level access should be reverted
Ingestion should be possible with the standard ingest tools on the
ingest machines.
","https://help.ceda.ac.uk/article/4999-ingest-from-gws#steps-to-getting-world-read-access-set-up-for-a-gws-{#steps-to-getting-world-read-access-set-up-for-a-gws-children-count=""0""}",856,153
"Future options {#future-options children-count=""0""}","Note, there are a couple of development items in the pipeline which may
help to alleviate this situation in the future. Documentation will be
needed for these:
-   ingestion initiated from the arrivals service by a reviewer
-   ingest from GWS tool  
","https://help.ceda.ac.uk/article/4999-ingest-from-gws#future-options-{#future-options-children-count=""0""}",251,42
Alternative way to get data from a GWS to archive (a fudge - not officially recommended - but it works),"This method pulls the data from the GWS to /datacentre/processing3 from
where it can then be archived in the usual way.
steps
1.  Agree with the data provider that the data is to be archived
    (deposit conditions), and get the full path to the data on the GWS
2.  Apply for access to the GWS as your own user id  on the JASMIN
    accounts portal <https://accounts.jasmin.ac.uk/>
3.  Log on to a JASMIN sci machine (as your username) and check you can
    access the data and that the path is correct. It is useful to check
    the size/shape of the data to transfer.
4.  As user badc on an ingest machine, make a directory in
    /datacentre/processing3 (or similar staging area) to temporarily
    hold the files.
5.  From this directory (4 above) rsync the data from the GWS to the
    staging area using\
    rsync -a username\@sci\[1234568\].jasmin.ac.uk:/gws/\[full path\]
    /\[directory_wanted\]   . (ie space dot)\
    eg\
    rsync -a
    wgarland\@sci6.jasmin.ac.uk:/gws/nopw/j04/gotham/wp4/cpdn_extracted_data_extra/b778-845_archive
    . &\
    If it fails check the path is correct
Once the rsync is complete and the data have been checked you can move
it to the archive in one of the usual ways. If it is a large dataset and
in a directory structure then the ingest route is recommended as you can
specify a regex, multiple threads to deposit it eg nthreads=10
and arrivals_maxfiles: \
This config file
/home/badc/software/datasets/wgarland/wendyingest.cfg has an example you
can copy/edit to do this - run it manually via ingest_control
",https://help.ceda.ac.uk/article/4999-ingest-from-gws#alternative-way-to-get-data-from-a-gws-to-archive-(a-fudge---not-officially-recommended---but-it-works),1556,240
MIDAS Open User Guide,"[MIDAS
Open](https://catalogue.ceda.ac.uk/uuid/dbd451271eb04662beade68da43546e1)
is one of the most popular datasets in the CEDA Archive, yet also not
the easiest to use. The following guide covers a few aspects of this
dataset collection to aid general use of the data. It includes various
FAQs from our user community.
",https://help.ceda.ac.uk/article/4982-midas-open-user-guide,321,46
"What is MIDAS Open  {#what-is-midas-open children-count=""0""}","[MIDAS
Open](https://catalogue.ceda.ac.uk/uuid/dbd451271eb04662beade68da43546e1)
is a collection of observation datasets made available by the Met Office
each year under the Open Government Licence. The collection contains
data from UK meteorological sites from around the late 19th Century to
recent years (upto the end of the previous full year for each release)
stored in the Met Office\'s \'MIDAS\' database. MIDAS Open, however, is
a series of flat-files for a sub-set of the restricted-access full MIDAS
database, taken at a given \'snapshot\' in time and prepared for general
usage in the way that the files have been formatted and the data split
up. You can read more in our news article when these data were first
released: [UK weather station records now freely available to all: MIDAS
Open](https://www.ceda.ac.uk/blog/midas-open-version-202007-released-more-uk-weather-station-data-added/)
","https://help.ceda.ac.uk/article/4982-midas-open-user-guide#what-is-midas-open -{#what-is-midas-open-children-count=""0""}",902,124
"How does it compare with the full MIDAS collection? {#how-does-it-compare-with-the-full-midas-collection children-count=""0""}","The full MIDAS database holds meteorological data from a wide range of
reporting networks operating both globally and nationally - including
marine meteorological observations too. Additionally, it holds data from
non-Met Office operated sites (e.g. rain gauges operated by water
authorities). The MIDAS Open data are a sub-set of these data for UK,
land-based stations operated by the Met Office. Currently this
represents approximately 95% of available daily temperature and weather
observations, 83% of hourly weather data, and 13% of daily rainfall
within the full MIDAS collection. A large proportion of the UK raingauge
observing network is operated by other agencies so currently excluded
from the Midas-Open set. It does not supersede the full MIDAS collection
also archived at CEDA.\
\
Due to licensing restrictions access to the \'fuller\' MIDAS collection
also held in the CEDA Archives is more restricted, primarily aimed at
supporting academic use.
For more information about the fuller MIDAS collection see the [MIDAS
Dataset
Collection](https://catalogue.ceda.ac.uk/uuid/dbd451271eb04662beade68da43546e1)
and related dataset records in the CEDA Data Catalogue.
","https://help.ceda.ac.uk/article/4982-midas-open-user-guide#how-does-it-compare-with-the-full-midas-collection?-{#how-does-it-compare-with-the-full-midas-collection-children-count=""0""}",1176,168
"Where are the latest data/missing? {#where-are-the-latest-datamissing children-count=""0""}","Not all sites will give you data for the full time period covered by the
collection as MIDAS holds both historical and more recent data, i.e. it
includes data from stations that are now closed. Also, there may be
other reasons for data gaps/missing data from sites. Finally, 2020 data
aren\'t available in MIDAS Open at present - these will be available in
the next release which is due around July this year. You may, however be
able to request more recent data via the National Meteorological Library
and Archive service. For details see the last [MIDAS Meteorological
Data: FAQ](https://help.ceda.ac.uk/article/269-midas-faq)
","https://help.ceda.ac.uk/article/4982-midas-open-user-guide#where-are-the-latest-data/missing?-{#where-are-the-latest-datamissing-children-count=""0""}",629,100
"How do I find the station/data I need? {#how-do-i-find-the-stationdata-i-need children-count=""0""}","Besides the overall dataset splitting (e.g. hourly weather/daily
rainfall) within the MIDAS Open collection determining *which* station
will meet your needs requires some additional work. This is because not
all sites operate for the entire period covered by the datasets nor
reporting all parameters.
To help identify suitable datasets there are a few options available:
-   Within each dataset there are \'station metadata\' files (e.g.
    [midas-open_uk-hourly-weather-obs_dv-202207_station-metadata.csv](https://dap.ceda.ac.uk/badc/ukmo-midas-open/data/uk-hourly-weather-obs/dataset-version-202207/midas-open_uk-hourly-weather-obs_dv-202207_station-metadata.csv?download=1)).
    These CSV files detail each station within that MIDAS Open dataset,
    it\'s county, latitude, longitude and altitude as well as the start
    and end years for the data from that station within the dataset
    (though not the exact temporal coverage).
-   MIDAS Open stations are also listed within the [MIDAS Station Search
    tool](https://archive.ceda.ac.uk/tools/midas_stations), though as
    this contains all sites within the full MIDAS system it is important
    to also select the \'midas open\' option in the search tool to
    select sites just in MIDAS Open.
Using the station metadata will also help to identify where to find the
data from the station within the directory structure as it will
indicate:
-   what historic county the site is listed under
-   the station\'s \'source id\' (a unique identifier for the site
    within MIDAS) and the station name used together in the directory
    structure.
","https://help.ceda.ac.uk/article/4982-midas-open-user-guide#how-do-i-find-the-station/data-i-need?-{#how-do-i-find-the-stationdata-i-need-children-count=""0""}",1607,208
"How are the data structured? {#how-are-the-data-structured children-count=""0""}","The data in the MIDAS Open collection are split at various levels to aid
use of these vast volumes of data. Essentially, the archive structure
is:
`ukmo-midas-open/data|metadata|checksums/<dataset>/<release-version>/<historic-county>/<site>/<qc-version>/files`
where
-   ukmo-midas-open is the top \'collection\' level seen in the archive
    under the \'/badc\' directory as seen either by the web-download
    service ( data.ceda.ac.uk), ftp download service (ftp.ceda.ac.uk) or
    directly on the JASMIN system.
-   data\|metadata\|checksums directory  - at this level you can find
    md5-checksums available for releases following the first release as
    provided by the Met Office. These can be used to verify downloaded
    data if needed. The metadata directory contains files used for a
    \'midas-open\' [station search map
    tool](http://dap.ceda.ac.uk/badc/ukmo-midas-open/metadata/midasmap/map.html)
    \[note, we\'re looking to incorporate this information into our
    general MIDAS Station search tool in due course\]. The data
    directory takes you down to where the datasets themselves are
    archived.
-   dataset - these folders contain the different dataset splits for
    MIDAS Open, reflecting how the data are split up within the Met
    Office\'s MIDAS database.
-   release-version - each year a release of each dataset is made
    available which incorporates data from the previous completed year
    denoted by the year and month of release. For example, the 202007
    release in July 2020 contains data upto the end of 2019.
    Additionally, the release will also contain any new and updates data
    for previous years too - see the associated release details for more
    information about each change. Within these directories you will
    also see 00README_catalogue_and_licence.txt files which give you
    outline information from the associated dataset catalogue page  -
    the link in the file will take you to the dataset\'s catalogue entry
    where you can find lots of useful information.
-   historic-county - MIDAS uses historic county boundaries for
    consistency for all years and to future-proof against further
    boundary alterations. The [map of traditional
    counties](http://archive.ceda.ac.uk/midas_stations/traditional_counties_map)
    may aid users trying to determine which directory may contain their
    station of interest.
-   site - each station in MIDAS received a dedicated MIDAS \'src_id\'
    (source ID) which is used to bring together data from the station as
    reported within different reporting networks where the site may be
    designated with various IDs to aid finding data across the different
    datasets and also to link with the station metadata.
-   qc-version - see below about this important distinction which
    denotes how data are handled within the MIDAS system when quality
    control checks are run and/or data are alternated manually following
    receipt into the MIDAS system.
-   files - each file is split into yearly files to keep individual
    downloads small to aid users. The files themselves are formatted as
    \'BADC-CSV\' files - see the [BADC-CSV Format for Data
    Exchange](https://help.ceda.ac.uk/article/105-badc-csv) help page
    for more information on using this format. However, these are
    essentially comma-seperated-variable files that should readily open
    in common spreadsheet packages or can be easily opened in text
    editors and scripts. They contain two main sections - a header
    section (starting with a \'Conventions\' line) and a data section
    (starting with \'data\' line and finishing with an \'end data\'
    line). The header section gives details about the overall file
    contents (so called \'global\' attributes, marked with a G in the
    second column) and details about each column of data.
","https://help.ceda.ac.uk/article/4982-midas-open-user-guide#how-are-the-data-structured?-{#how-are-the-data-structured-children-count=""0""}",3864,533
"What are the \'qc-version\' directories all about? {#what-are-the-qc-version-directories-all-about children-count=""0""}","When you come to the data directories you\'ll find the actual files are
in sub-folders labelled \'qc-version-#\` according to which \'version\'
of the data line is available in MIDAS. Here the \'qc-version-1\'
directory denotes the latest state of the data lines which may be
original data or may be post-QC-ed data lines - the state of the quality
control is denoted by the values corresponding to the \_q columns (0
means no QC has run on the data when the file was produced, other values
will need to be cross referenced with the documentation). IF the QC
processes at the Met Office result in a change in the data line then the
original (i.e. the one first received by the Met Office from the site)
will be stored in the \'qc-version-0\' files.
You can find out more about the QC information in section 5 of the
[MIDAS User Guide](https://zenodo.org/record/7357335).
","https://help.ceda.ac.uk/article/4982-midas-open-user-guide#what-are-the-\'qc-version\'-directories-all-about?-{#what-are-the-qc-version-directories-all-about-children-count=""0""}",871,149
"Are there any tools to use the data? What about the CEDA WPS MIDAS Tools? {#are-there-any-tools-to-use-the-data-what-about-the-ceda-wps-midas-tools children-count=""0""}","At the moment I\'m afraid CEDA do not have additional tools to use the
MIDAS Open data themselves, unlike the \'MIDAS Extractor\' tools with
the CEDA Web Processing Service (WPS) which work only with data in the
full MIDAS collection. Those existing tools cannot be adapted for the
MIDAS Open data due to the different structures of the two dataset
collections.
","https://help.ceda.ac.uk/article/4982-midas-open-user-guide#are-there-any-tools-to-use-the-data?-what-about-the-ceda-wps-midas-tools?-{#are-there-any-tools-to-use-the-data-what-about-the-ceda-wps-midas-tools-children-count=""0""}",362,61
Introduction,"<div>
There are two FTP archive download services: 
-   ftp.ceda.ac.uk for all the archive. This required you to login using
    your CEDA username and your FTP password (see below).
-   anon-ftp.ceda.ac.uk for a selection of datasets that are anonymously
    downloadable, with no login.
FTP  stands for File Transfer Protocol. It has been a standard for
transferring files between computer systems since the 1970\'s. There are
a large number of FTP client apps designed to help with bulk transfer of
data. Despite its age, the protocol is still used because of its
simplicity and convenience. 
Search the web for more information on FTP client apps or information on
the protocol itself. 
FTP is not a secure protocol. We would like to move away from its use
but are currently maintaining our service because transferring research
data has no intrinsic need for high security and there are still a large
number of our users still using it.
",https://help.ceda.ac.uk/article/280-ftp#introduction,942,155
CEDA FTP Password,"To login to the main CEDA ftp server, ftp.ceda.ac.uk, please use your
CEDA username and your FTP password. Your FTP password is separate from
the password for your CEDA web account. If you do not already have a
password, or if you want to reset your FTP password please go to
[MyCEDA](https://services.ceda.ac.uk/cedasite/myceda) and click on the
\'Configure FTP account\' button.
",https://help.ceda.ac.uk/article/280-ftp#ceda-ftp-password,381,60
Getting started with FTP,"There are a number of ways to start using FTP to download our data. 
-   Find a good FTP app: We recommend
    [Filezilla](https://filezilla-project.org/) as a good fully featured
    FTP client. 
-   At the command line: An FTP command line client is often bundled
    with the operating system. Type ftp at the command prompt, and press
    enter.
-   From a script: There are libraries in all major programming
    languages to use FTP. We have put together a short guide on using
    [FTP within Perl or
    Python](https://help.ceda.ac.uk/article/281-ftp-with-python-and-perl)
    scripts available.
-   Using a web browser. On some web browsers you may be able to use ftp
    by going to the url <ftp://ftp.ceda.ac.uk> However, many browsers
    have now removed ftp support, so this may not work for you.
</div>
",https://help.ceda.ac.uk/article/280-ftp#getting-started-with-ftp,819,125
Introduction,"In addition to the basic data in a netCDF file, the creator of the
dataset needs to include information about the data themselves (e.g.
type, units), and the creator may want to include information about how
the data were collected, and/or warn future users of pitfalls; these
items of information are metadata. Several groups have defined
conventions for netCDF files, to enable the exchange of data. Since
future cataloguing and searching systems will rely on standards in
metadata, CEDA has decided to adopt the  [NetCDF Climate and Forecast
(CF) Metadata Convention](http://cfconventions.org/). The [Unidata
website](https://www.unidata.ucar.edu/software/netcdf/conventions.html)
gives more information about other possible conventions for netCDF files
(and also about the [CF Convention](http://cfconventions.org/)). The
conventions define metadata which provide a description of what the data
in each variable represents. This enables the users of data from
different sources to decide which quantities are comparable. CEDA
receives data from many sources, for example station data, satellite
data, model data, and we aim to describe all netCDF datasets with the CF
conventions.
Whereas netCDF is a binary file format used to order and store data and
metadata, with strict rules (so that software will fail to read your
files unless the data are correctly structured), the CF conventions are
guidelines and recommendations as to where to put metadata within a
netCDF file. The CF conventions also advise on what type of information
you may want to include.
Variables should (ideally) have associated with them a name and units,
and possibly other information such as the direction of increasing
coordinate value and statistical processing (e.g. are data values a
mean, minimum, maximum, etc.). The CF conventions have a list of 
**standard_names** for variables. These are held in the most recent
version of the [CF Standard Name
Table](http://cfconventions.org/Data/cf-standard-names/current/build/cf-standard-name-table.html).
The list includes the units recommended for each standard name (most
common prefixes can be used with the units, e.g. kilo (k), hecto (h),
Mega (M), etc). If a **standard_name** metadata attribute is associated
with a data variable, its value must be chosen from the list published
in the standard name table. It is not compulsory within the CF
conventions to assign a standard name to a data variable, but including
one helps data users to understand the contents of a netCDF file. A
**long_name** attribute can also be used to supply text that describes
the variable more fully (and perhaps provide a handy graph-axis label).
Both standard_name and long_name can be provided for a data variable,
and the CF conventions recommend that at least one of them be supplied
with each variable in a netCDF file.
The guidelines given here are not exhaustive, and some sections of the
CF conventions are not covered. Please consult the  [CF web
pages](http://cfconventions.org/) for further information. There is an
introduction to CF in this
[presentation](http://cfconventions.org/Meetings/2020-workshop/Introduction_to_CF-netCDF.pdf)
from the CF 2020 workshop.
",https://help.ceda.ac.uk/article/4507-the-cf-metadata-convention#introduction,3191,468
CF conventions and the netCDF file,"Figure 1 shows the structure of a netCDF file. The file has several
basic components: dimensions, variables, data, and global attributes.
![Figure 1 showing netCDF
file](http://badc.nerc.ac.uk/help/formats/netcdf/netcdf_fig1.gif)
",https://help.ceda.ac.uk/article/4507-the-cf-metadata-convention#cf-conventions-and-the-netcdf-file,230,26
,,https://help.ceda.ac.uk/article/4507-the-cf-metadata-convention#,0,0
Dimensions,"The dimensions of a variable define the axes of the quantity it
contains. Dimensions can be spatial, temporal, or any other quantity
(even an index). For example, typical dimensions for gridded model data
are latitude, longitude, altitude, and time, while typical dimensions
for radar data are range and time. Dimensions may be of any size,
including unity. Optionally, one dimension in a netCDF file is allowed
to be *\'UNLIMITED\'.* The unlimited dimension allows a data file to be
appended to at a later stage along a particular axis, for example, if
data are still being collected. Most often this facility is used with
the time dimension. The sizes of the various dimensions are declared at
the top of the example shown in Figure 1.
",https://help.ceda.ac.uk/article/4507-the-cf-metadata-convention#dimensions,738,124
Coordinate Variables and Data Variables,"Coordinate variables are special variables in a netCDF file. The name of
a coordinate variable is the same as the name of its dimension. In the
example in Figure 1, the variables dimension1(dimension1) and
dimension2(dimension2) are coordinate variables. The dimensions named in
parenthesis refer to those declared in the first section of the file and
give the size of each coordinate variable. Coordinate variables can
contain regularly or irregularly spaced steps.
Data variables contain the actual measured or modelled quantity, for
example, air temperature. Data variables must not have the same name as
any of their dimensions. In Figure 1 the variable declared as
variable1(dimension1,dimension2) is a two dimensional data variable. The
data values are contained in variable 1 itself and the corresponding
coordinates are contained in the coordinate variables dimension1 and
dimension2. A data variable can also be a scalar quantity with only a
single value, for example radar frequency. Data variables should
be given a *\'standard_name\'* metadata attribute where possible,
otherwise *\'long_name\'* should be used to describe the variable. For
example, a standard name from the [CF standard name
table](http://cfconventions.org/Data/cf-standard-names/current/build/cf-standard-name-table.html)
can be assigned as follows: *tempvar:standard_name =
\""air_temperature\"";* where tempvar is the name of a data variable
containing air temperature values. An overview of CF standard names is
available in this
[presentation](http://cfconventions.org/Meetings/2020-workshop/CF_2020_standard_names_presentation.pdf)
from the CF 2020 workshop.
It is often very useful to include a dummy value for missing data in a
file. The CF conventions suggest that the  *\'\_FillValue\'* attribute
be used, and defined as the same type of variable as the one it
replaces.
There are many other attributes that can be used to provide a detailed
description of the variables inside a netCDF file. Please see the [CF
Conventions
Document](http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/cf-conventions.html)
for the full list of attributes and examples of their usage.
",https://help.ceda.ac.uk/article/4507-the-cf-metadata-convention#coordinate-variables-and-data-variables,2168,296
Global attributes,"These relate to the dataset at the more general level. They might
include such information as instrument name and description, institution
name, processing history, references. One aspect of the CF conventions
is that extra attributes are not outlawed, so you can include further
information in the global attributes, if you think it would be useful
for future users. CEDA recommends you include as much information as
possible.
CF conventions make one global attribute mandatory: \
\
**Conventions** \""CF-1.0\"" \
\
CF conventions recommend the following global attributes:
  ----------------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **title**         A succinct description of what is in the dataset.
  **institution**   Specifies where the original data were produced.
  **source**        The method of production of the original data. If the data are model generated, source should name the model and the version number. If the data are observational, source should characterize them, e.g. surface observation, radiosonde, satellite.
  **history**       Provides an audit trail for modifications to the original data. Well-behaved generic software will automatically append their name, input parameters, and a timestamp.
  **references**    Published or web-based references which describe the data, or the methods used to produce them.
  **comment**       Miscellaneous information about the data or the methods used to produce them.
  ----------------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
We have generated  [some examples](https://help.ceda.ac.uk/) of the
types of information that could be put into the global attributes for
three datasets.
",https://help.ceda.ac.uk/article/4507-the-cf-metadata-convention#global-attributes,2070,221
The CF-checker," There is a web-based CF-checker that allows you to upload a file to
test for compliance with the CF Convention. Visit the  [CF-checker
page](http://puma.nerc.ac.uk/cgi-bin/cf-checker.pl) to use this service.
Alternatively, the CF-checker software can be downloaded and installed
by following the instructions at
<https://github.com/cedadev/cf-checker>.
",https://help.ceda.ac.uk/article/4507-the-cf-metadata-convention#the-cf-checker,354,43
Archiving of Simulations within the NERC Data Management Framework CEDA Policy and Guidelines,"::: {#wikipage}
",https://help.ceda.ac.uk/article/4300-archiving-of-simulations-guide,16,2
Introduction {#Introduction},"Issues associated with archiving information about the environment made
by measurement are relatively well understood. This document outlines a
general policy for the management of simulated and/or statistically
predicted data\[1\] within NERC and provides specific policy and
guidelines for the activities of the CEDA Archive (formally British
Atmospheric Data Centre).
In the remainder of this document we use the term simulation to cover
deterministic predictions (or hindcasts) based on algorithmic models as
well as statistical analyses or composites of either or both of
simulations and real data.
This policy has been developed in response to external legislative
drivers (e.g. Freedom of Information Act and Environmental Information
Regulations), external policy drivers (e.g. the RCUK promulgation on
open access to the products of publicly funded research), as well as the
existing NERC data management policy which is based around ensuring that
NERC funded research is exploited in the most efficient manner possible.
The major question to be answered when considering simulated data is
whether the data products are objects that should be preserved
(archived) in the same way as measured products. In general the answer
to this question is non-trivial, and it will be seen that guidelines are
required to implement a practicable policy.
Whether suitable for archival or not, simulated data are usually
produced by individuals, teams, or projects, and may have potential for
exploitation by the wider community. Some data producers may be able to
support such exploitation by deploying efficient distribution systems.
Others will not. It is therefore also important to develop criteria by
which the scope for programme facilitation or wider applicability or
exploitability can be recognised.
",https://help.ceda.ac.uk/article/4300-archiving-of-simulations-guide#introduction-{#introduction},1804,269
Data Management and Simulated Data {#DataManagementandSimulatedData},"Simulations are generated by either deterministic or statistical models
(or a combination of both). Such modelling activity does not generate
definitive knowledge. Models are continuously developed and hopefully
(but not necessarily) provide improved or more adequate representations
of the physical system as time progresses. This is to be contrasted with
measurements of the earth system, which by definition, cannot be
repeated with the system in the same state and are therefore unique in a
rather different way to simulated data.
In general the information provided by models and the information
provided by measurements are of a different nature. Simulations are
generally, but not always, analogues of the ""real"" world that may
provide insights on physical causal relationships. Where simulations
represent predictions of the real world or where they incorporate real
measurements to improve estimates of the state of the real world (e.g.
assimilation products) their wider value (in the long term, or to a
larger community) is enhanced. Where simulations have more confusing
relationships with the real world (as would be the case with
""sensitivity"" experiments where either the boundary conditions or the
relations within the model are idealised), their wider value is less
obvious.
In addition to the data preservation, and data exploitation roles that
the data management community can provide, there is also a recognised
role for data management to minimise duplication of NERC funded
activities between individuals, teams and projects, and to facilitate
research programmes and collaboration.
The remainder of this document outlines criteria for selecting datasets
for archival or enhanced exploitation, and provides guidelines for the
management of such datasets. It is explicitly expected that a) not all
simulated datasets are suitable for management, and b) not all simulated
datasets will be managed within a NERC designated data centre. Criteria
for Selecting Simulated Data for Management
If the answer to one or more of the following questions is yes, then
simulated data are candidates for professional data management beyond
that provided by the investigating team responsible for producing the
data.
1.  Is there --- or is there likely to be in the future --- a community
    of potential users who might use the data without\[2\] having one of
    the original team involved as co-investigators (or authors)?
2.  Does some particular simulation have some historical, legal or
    scientific importance that is likely to persist? (Some simulations
    may become landmarks, in some way, along the route of scientific
    knowledge. They may also have been quoted to make a statement that
    might be challenged -- either scientifically or legally -- and
    should therefore be kept for evidential reasons.)
3.  Is the management of the data by a project team likely to be too
    onerous for them or result in duplication of effort with other NERC
    funded activities?
4.  Is it likely that the simulation will be included in future
    inter-comparisons for which NERC funding will be sought?
5.  Does the simulation integrate observational data in a manner that
    adds value to the observations?
If the answer to any of the following questions is yes, then the
simulated data should not be archived, but could still be candidates for
data management to aid exploitation within a larger project.
1.  Is the data produced by a trivial algorithm that could be easily
    regenerated from a published algorithm description?
2.  Is the data unlikely to ever be used in a peer-reviewed publication,
    or as evidence to support any public assertions about the
    environment?
3.  Is the data known to be of poor quality or to have little scientific
    validity?
4.  Is it impossible to adequately document the methodology used to
    produce the data in a way that is accessible to users of the data
    outside the producing team?
5.  Is the simulated data produced in a sensitivity experiment rather
    than as a predictive or retrospective analysis of a real system?
6.  Is the data likely to be of short-term use, and in the case of loss,
    more easily (in terms of physical and financial effort) replaceable
    by rerunning the simulation?
If the answer to any of the following questions is yes, then value
judgements will need to be made about how much, if any, of the simulated
data should be archived. Guidelines to assist in this situation appear
below.
1.  Would storage of the data be prohibitively expensive?
2.  Would storage of statistical summaries rather than individual data
    items provide adequate evidential information about the simulation?
    (e.g. while it might normally be desirable to store all ensemble
    members, would ensemble and/or temporal means be adequate in a
    situation where storage of the individual members at full time
    resolution might be prohibitively expensive).
3.  Would historical preservation be satisfied by archiving only the
    data which supported published figures, or is future use likely to
    include data processing?
",https://help.ceda.ac.uk/article/4300-archiving-of-simulations-guide#data-management-and-simulated-data-{#datamanagementandsimulateddata},5119,794
Guidelines for Archiving Simulated Data {#GuidelinesforArchivingSimulatedData},"When simulated data is initially archived, it may be possible for access
to be embargoed in some way for a defined period\[3\]. When this occurs
the following issues need to be addressed:
1.  To which community should it be restricted and for how long?
2.  Should conditions of use apply to the data during and/or after the
    retention period (e.g. communication with investigators, offer of
    co-authorship, acknowledgement in publications)?
In some cases, datasets may be archived by the investigating team at a
national facility, rather than at a NERC designated data centre.
1.  This is most likely to occur when the longevity of the dataset is in
    some doubt, and the added value of using a designated data centre is
    not clear.
2.  Where datasets will initially have restricted access it should
    normally be the case that the data archive is held at a designated
    data centre where procedures are already in place for providing
    secure access to data.
3.  Alternative archives should not be established where the result will
    be that academic staff will be spending significant amounts of time
    carrying out professional data management which should be carried
    out within institutions with more appropriate career structures.
Where the intention is that a dataset be held outside of a NERC
designated data centre, procedures should be in place to ensure that the
data holder (or holders) conform to all the requirements in subsequent
points in this document. Where the dataset fulfils the criteria for
long-term preservation, it should also be ensured that funding is in
place to move the data to a designated data centre when the holder (or
holding facility) is no longer able to archive and distribute the data.
Such datasets will still be the responsibility of a designated data
centre, but those responsible for the remote archives will be
responsible for keeping all metadata required by the designated data
centre up to date, and communicating the results of internal reviews
(especially those which might involve removing or superseding data
holdings).
All simulated datasets will be subject to regular lifetime review
(described below).
Given that a simulation dataset is to be archived, what is involved in
archiving such a dataset?
1.  The simulated data itself should be archived in a format that is
    supported by the designated data centre community (whether or not
    the data is to be initially archived in a designated data centre. It
    is recognised that in taking on data, potentially in perpetuity,
    every new format is a significant ongoing cost.)
2.  Any non-self-describing parameter codes (e.g. stash codes) included
    within the data should be fully documented, either by accompanying
    metadata, or by making reference to appropriate
    dictionaries/thesauri\[4\].
3.  Discovery metadata conforming to appropriate standards and
    conventions\[5\] should be supplied for all datasets to the
    responsible designated data centre.
4.  Where possible, documented computer codes and parameter selections
    should also be provided (e.g. the actual Fortran, and descriptions
    of any parameter settings chosen\[6\]).
5.  Where initial conditions and boundary conditions are themselves
    ancillary datasets, these too should be archived and documented.
6.  Estimates of the difficulty (both practically and financially) of
    recreating the simulation. (This will be needed to inform the
    lifetime review).
7.  Where quantities derived by post-processing are also important,
    these too should be archived, along with as much detail as is
    practicable about how the post-processing was accomplished.
8.  All documents and information (""further metadata"") should conform to
    appropriate archival standards (published open formats, suitable
    metadata structures etc).
Where only a subset of the simulation is to be archived, the following
considerations should be assessed in making decisions:
1.  Potential usage (e.g. if the climate impacts community are involved
    appropriate parameters might include daily min/max temperatures,
    whereas instantaneous values are more likely to be useful if the
    simulation is to be used to generate initial conditions for other
    runs).
2.  Illustrative value (where a simulation is being archived because of
    it's scientific importance, those parameters relative to the
    scientific thesis should be the most important).
3.  Physical Relevance (e.g. case studies, one might only store those
    parameters necessary to make the relevant points, but there are
    obvious risks in retrospectively identifying key parameters).
4.  Volume and cost of storage.
5.  Standard Parameters used in model-intercomparison exercises. Where
    possible and appropriate datasets should always seek to keep these,
    and the designated data centre community will provide guidance on
    current standard lists of parameters.
6.  Can the temporal or spatial resolution be decremented without losing
    impact
Where it is known a priori that simulation data will be archived, they
should normally be archived at the time they are produced. Where
multiple versions are expected within a project, and no other groups are
expecting access to the data before a final version is produced, early
simulations need not be archived. It should never be assumed that any
part of a dataset would be archived after the end of the originating
project.
",https://help.ceda.ac.uk/article/4300-archiving-of-simulations-guide#guidelines-for-archiving-simulated-data-{#guidelinesforarchivingsimulateddata},5466,817
Archive Lifetime {#ArchiveLifetime},"As described in the introduction, continuous model
improvement/development may make obsolete datasets made with previous
versions. All simulated datasets should be subject to more frequent
review procedures than measured datasets.
Review should consider a wide range of metrics to determine the
importance of a dataset. In particular the number of users is a
relatively minor criterion, it is the importance of current and
potential usage that needs to be considered.
Where a dataset is being held for legal reasons, or because of
historical interest, such a dataset might be kept indefinitely.
Where a dataset has been formally cited and formally published, it
should be kept indefinitely, unless it is not possible to migrate the
format to future media.
A suitable timescale for review of simulation datasets held at
designated data centres would be at four-year intervals. Four years
should give time for work to be published and follow-up work to be
performed, and for an initial assessment of the likely longevity of
datasets to be established. Most international programmes (e.g. IPCC)
should have exploited datasets on a timescale of eight years, and again,
further longevity could then be assessed. More frequent reviews may be
appropriate where datasets are held elsewhere.
Reviews should involve at the minimum: the data supplier (if available),
the custodians (especially if not held inside a designated data centre),
representatives of the user community (if it exists), and an external
referee.
Reviews may recommend removing subsets of a dataset.
Reviews may recommend acquiring new datasets to supersede existing
datasets (and to keep multiple versions). Where multiple versions of
datasets are archived, discovery metadata should clearly indicate which
is the most authoritative.
Reviews should consider the availability of tools to manipulate
datasets.
In all cases metadata should be kept for datasets that have been
removed.
",https://help.ceda.ac.uk/article/4300-archiving-of-simulations-guide#archive-lifetime-{#archivelifetime},1944,297
Custodial Responsibilities {#CustodialResponsibilities},"The custodial responsibilities of designated data centres are described
elsewhere. These points are here to provide guidance for the minimum
responsibilities of facilities formally archiving simulation data on
behalf of one or more designated data centres.
All archived data will be duplicated, either in a formal backup archive,
or by complete archive duplication at multiple sites (in which case the
remote sites must support all the same metadata structures, and they
must advise the designated data centre should they consider removing
their copy).
All cataloguing and metadata required by the designated data centre must
be provided and kept up to date.
User support must be provided to include help with any access control,
on how to view and interpret the metadata, and on how to obtain and use
the data in the archive.
Formal dataset reviews must be carried out.
Adequate bandwidth to the data holdings must exist.
Appropriate tools to use and manipulate the data must be provided.
\[1\] The word ""data"" is often claimed by experimental scientists to
exclude simulated information, however, most reputable dictionaries
include simulated products within the definition.
\[2\] This criteria is not intended to exclude co-authorship (which is
always encouraged) but rather to imply that if the dataset can be, and
is likely to be, used without co-authorship, the dataset is more likely
to be suitable for data management.
\[3\] The Freedom of Information Act (2000) and the Environmental
Information Regulations (2004) stipulate that an embargo, if any, can
only apply for some limited amount of time, to allow for ""work in
progress"".
\[4\] Appropriate dictionaries include defacto and dejure standard
vocabularies.
\[5\] In October 2005 this would be NASA GCMD DIF documents with the
Numerical Simulation Extensions.
\[6\] It is hoped that in the near future, the Numerical Model Metadata
Suite being developed at the University of Reading will provide an
appropriate formalism for Unified Model Simulations.
*Originally written by Anne De Rudder, Jamie Kettleborough, Bryan
Lawrence, Kevin Marsh 2005*
:::
",https://help.ceda.ac.uk/article/4300-archiving-of-simulations-guide#custodial-responsibilities-{#custodialresponsibilities},2113,328
Navigating the CEDA archives,"The CEDA archives (including the SPARC data centre) hold over 3Pb worth
of data in more than 180 million files. As a result, the archive is
highly structured to allow data to be carefully archived, which can make
navigation a hurdle for some. The following notes provide some general
guidance on the archive structure to aid navigation.
",https://help.ceda.ac.uk/article/100-navigating-ceda-archives,337,57
A note on supporting information and data discovery,"Though the archive is well structured CEDA have an extensive searchable
catalogue service for users wishing to find data across the archive.
This catalogue service has been designed to complement the archive
content by providing links to supporting documentation and to related
data that may not be connected within the archive itself (e.g. by
finding all data produced by a particular instrument or facility). To
find out more about data discovery see the help page on \""[Finding
Data](https://help.ceda.ac.uk/article/97-finding-data)\"".
",https://help.ceda.ac.uk/article/100-navigating-ceda-archives#a-note-on-supporting-information-and-data-discovery,539,78
From data centres to datasets,"The CEDA archive has a hierarchical structure with a top level splitting
to the data centres and the requests area:
    /badc
    /neodc
    /requests
    /sparc
Below the BADC, NEODC and SPARC data centre paths the archive is
(mostly) structured as follows:
/\<data centre>/\<data collection>/\<metadata\|data\|etc.
splitting>/\<M1>/\<M2>/\<dataset>/\<split1>/\<split2>/\<files>
Where:
-   data centre: is the data centre responsible for archiving the data
-   data collection: is a directory under which data have been collected
    together  - e.g. all data from a project, instrument, facility -
    this collection is usually described by a \""Dataset Collection\"" in
    the CEDA data catalogue
-   metadata\|data\|etc. splitting: below the dataset collection level
    there is (usually) a splitting found with a number of different
    directory types are found: data - contains the actual data for the
    dataset; metadata - contains files that can be used by external
    services to interrogate archive contents; docs - hold relevant
    documentation for the dataset collection; images/quick_looks - will
    have images or plots relevant to the data to aid the user to see
    what is in the archive ahead of downloading or to support an
    external service
-   \<M1>/\<M2>/\<dataset>: is a splitting down to some point in the
    archive below which all the data are related by some common theme.
    The M1/M2/etc splitting will be to aid breaking down the data into
    more manageable, logical groupings. For example, the M1/M2 splitting
    might be for an international modelling comparison project first by
    institute, then by model before arriving at a level that holds all
    the data for a given experiment.  It is this level that is then
    described by the data catalogue\'s \""Dataset\"" entries.
-   \<split1>/\<split2>/\<files>: below the \<dataset> directory there
    is sometimes a need to further break down the data into manageable
    sections (we typically aim for less than 1000 files in the lowest
    level directory to aid archive use). Thus, the \<split1>/\<split2>
    directories could be by year/month/day for example.
",https://help.ceda.ac.uk/article/100-navigating-ceda-archives#from-data-centres-to-datasets,2166,314
Data reuse in the archive,"Most of the time data are only found in one part of the archive - and
indeed data are always only archived at one location! However, sometimes
datasets may be of use within other collections too. In these cases
users may find that they can navigate to the same data by two or more
different roots. This has been done through the use of symbolic linking
in the archive.
",https://help.ceda.ac.uk/article/100-navigating-ceda-archives#data-reuse-in-the-archive,369,69
Access types and limitations,"Large parts of the CEDA archives are open access for anyone to make use
of the data, while other parts (and to use the FTP service for all data)
require users to have a CEDA account first, and yet other parts are
further restricted requiring users to apply for access. The access type
varies across the archive due to the differing requirements of the data
providers in order to respect their intellectual rights to first use of
the data or their ownership of the data (e.g. all Met Office data
remains the property of the Met Office and thus access is strictly on a
licensed basis only). 
Whilst navigating the archive users may be asked to login to their
account to gain further access (or first  [register for a CEDA
account](https://help.ceda.ac.uk/article/81-registering)) and may be
informed that their access rights are not sufficient to proceed further
down the archive at that point. In such cases the user should find the
relevant Dataset entry in the CEDA data catalogue and apply for access
to the restricted resource - for further details about this please see
our [page on data
access](https://help.ceda.ac.uk/article/98-accessing-data).
Further details are available in the \"" [Accessing public and restricted
datasets](https://help.ceda.ac.uk/article/98-accessing-data)\"" section
of the user guide.
",https://help.ceda.ac.uk/article/100-navigating-ceda-archives#access-types-and-limitations,1315,205
CEDA OPeNDAP scripted interactions,"***Using certificates to interact with CEDA Archive data is no longer
supported. We recommend that users instead follow the examples for
token-based scripted interactions found here: [Using Archive Access
Tokens for Scripted
Interactions](https://help.ceda.ac.uk/article/5100-archive-access-tokens#usage)***
As the CEDA Archive has various data access controls in place users
wishing to interact with the CEDA OPeNDAP service will need to do so
using security certificates. The following instructions will guide you
through how to:
-   [Find a download URL](#findurl)
-   [Download data from the CEDA Archive via cURL](#curl)
    -   [Getting started](#start)
    -   [Get a security certificate](#cert)
    -   [Running the cURL command](#runcurl)
-   [Download a file with Python](#python)
    -   [Simple download script example](#simple)
    -   [Connecting to a file in a Python script using the OpenDAP
        service](#opendap)
-   [Known Issues](#issues)
",https://help.ceda.ac.uk/article/4442-ceda-opendap-scripted-interactions,964,124
"Find a download URL {#findurl children-count=""0""}","To discover the URL to download files from:
-   first navigate to the dataset you wish to download in the [CEDA
    archive browser](http://data.ceda.ac.uk) (for example,
    [UKMO-midas-open](http://data.ceda.ac.uk/badc/ukmo-midas-open/data/uk-daily-temperature-obs/dataset-version-201901/aberdeenshire/00145_cairnwell/qc-version-1/)).  
-   Down the right hand side of the list there is a download icon for
    each file in the dataset.  
-   Right click this download icon and select \""copy link address\"".  
-   Now you can paste that link address into your script, or onto the
    command line.  The links will start:
http://dap.ceda.ac.uk/
","https://help.ceda.ac.uk/article/4442-ceda-opendap-scripted-interactions#find-a-download-url-{#findurl-children-count=""0""}",646,78
"Download a file with cURL and a client certificate {#curl children-count=""0""}",,"https://help.ceda.ac.uk/article/4442-ceda-opendap-scripted-interactions#download-a-file-with-curl-and-a-client-certificate-{#curl-children-count=""0""}",0,0
"Getting started {#start children-count=""0""}","-   First create a directory where you will download the scripts to
    generate the security certificates you\'ll be using later
mkdir ~/ceda_pydap_cert_code
-   Switch to that directory
cd ~/ceda_pydap_cert_code
-   Next you\'ll need to download the code from the following Git Hub
    repository:
git clone https://github.com/cedadev/online_ca_client
-   Then navigate to the scripts directory:
cd online_ca_client/contrail/security/onlineca/client/sh/
-   Finally, run the following command to get a folder full of
    trustroots - these will be needed later when we generate your
    temporary certificate:
./onlineca-get-trustroots-wget.sh -U https://slcs.ceda.ac.uk/onlineca/trustroots/ -c ~/trustroots -b
","https://help.ceda.ac.uk/article/4442-ceda-opendap-scripted-interactions#getting-started-{#start-children-count=""0""}",713,86
"Getting a Security Certificate. {#cert children-count=""0""}","Having obtained the necessary code from the git hub repository you now
need a certificate associated with your myCEDA account in order to
access restricted datasets
-   Run the following command to get your temporary (3 day) certificate
    (remember to replace your ceda username below!:
./onlineca-get-cert-wget.sh -U  https://slcs.ceda.ac.uk/onlineca/certificate/ -c ~/trustroots -l <ceda username> -o $PWD/creds.pem
-   NOTE - You\'ll need to input your password after you submit the
    command
-   This will pipe the output (your certificate) into a file of your
    choosing (above it will be a local file: `$PWD/creds.pem`), you can
    alter this if you wish e.g.  `~/tmp/temp_cert.pem`.
","https://help.ceda.ac.uk/article/4442-ceda-opendap-scripted-interactions#getting-a-security-certificate.-{#cert-children-count=""0""}",697,100
"Running the cURL command {#runcurl children-count=""0""}","Finally, you can use your new certificate to download data using any
tool that supports this feature. In our example we will use curl, but
you can also use wget with the \--certificate option.
-   Use your certificate as a parameter to cURL to retrieve something
    from PyDAP:
curl --cert $PWD/creds.pem -L -c /dev/null http://dap.ceda.ac.uk/thredds/fileServer/badc/ukmo-midas-open/data/uk-daily-temperature-obs/dataset-version-201901/aberdeenshire/00145_cairnwell/qc-version-1/midas-open_uk-daily-temperature-obs_dv-201901_aberdeenshire_00145_cairnwell_qcv-1_1994.csv
> The -L option is required for following links and the -c option tells
> cURL to store cookies. On Windows, you will need to use NULL rather
> than /dev/null  * for the -c option input.*
**Note that older versions of cURL may require an absolute path to your
certificate file, as above.**
-   The example above downloads a text file and displays it on the
    console.  To download to file specify the -o option, for example:
curl --cert $PWD/creds.pem -L -c /dev/null http://dap.ceda.ac.uk/thredds/fileServer/badc/ukmo-hadobs/data/insitu/MOHC/HadOBS/HadUK-Grid/v1.0.0.0/1km/tas/ann/v20181126/tas_hadukgrid_uk_1km_ann_188901-188912.nc -o tas_hadukgrid_uk_1km_ann_188901-188912.nc
-   If you are seeing problems with SSL certificates such as:
curl: (60) Peer certificate cannot be authenticated with known CA certificates
*then you can try adding the \--insecure option to the curl command:*
    curl --insecure --cert $PWD/creds.pem -L -c /dev/null ...
","https://help.ceda.ac.uk/article/4442-ceda-opendap-scripted-interactions#running-the-curl-command-{#runcurl-children-count=""0""}",1525,184
"Download a file with cURL and an access token {#python children-count=""0""}","We have recently introduced a simpler way to access with scripts, making
use of OAuth2 access tokens. You can find out more about how to generate
and use access tokens here.
","https://help.ceda.ac.uk/article/4442-ceda-opendap-scripted-interactions#download-a-file-with-curl-and-an-access-token-{#python-children-count=""0""}",174,31
"Download a file with Python {#python children-count=""0""}",,"https://help.ceda.ac.uk/article/4442-ceda-opendap-scripted-interactions#download-a-file-with-python-{#python-children-count=""0""}",0,0
"Simple download script {#simple children-count=""0""}","We have prepared [an example Python
script](https://github.com/cedadev/opendap-python-example/blob/master/simple_file_downloader.py)
which shows how to use the ContrailOnlineCAClient and requests Python
libraries to download a file from the archive.
The script was tested with Python 2.7 and Python 3.8 on both Windows 10
and Linux.
","https://help.ceda.ac.uk/article/4442-ceda-opendap-scripted-interactions#simple-download-script-{#simple-children-count=""0""}",333,41
"Connecting to a file in a Python script using the OpenDAP service {#opendap children-count=""0""}","Please see the example help page on [Reading a NetCDF file from a Python
Script using
OpenDAP](https://help.ceda.ac.uk/article/4712-reading-netcdf-with-python-opendap).
","https://help.ceda.ac.uk/article/4442-ceda-opendap-scripted-interactions#connecting-to-a-file-in-a-python-script-using-the-opendap-service-{#opendap-children-count=""0""}",169,17
Known Issues {#issues},"Presently CEDA\'s DAP implementation has the following known issue.
Whilst updates could be made to the service to address them, these are
not being taken at this time as this will lead to loss of other service
functionality. We hope that these will be addressed in future iterations
of the services and underpinning protocols:
-   int64 data type - files containing data with this type are presently
    unsupported via the DAP protocol deployed within the CEDA Opendap
    service (via Thredds) and thus will lead to a
    `403: message = ""NcDDS Variable data type = long"";` error message.
",https://help.ceda.ac.uk/article/4442-ceda-opendap-scripted-interactions#known-issues-{#issues},592,97
Data Help Docs,"Users can find useful documentation for datasets on their records in
the  [CEDA Data
Catalogue](https://help.ceda.ac.uk/article/137-ceda-data-catalogue).
These documents will cover items such as :
-   file formats
-   related data papers
-   related science articles
-   project information
",https://help.ceda.ac.uk/article/234-data-help-docs,291,37
Where to find Docs,"On a dataset\'s catalogue page you can easily find related documentation
and other online resources under the \""Docs\"" tab located just below the
abstract and citation areas.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/57d18e84c697914ce32d8cde/file-KnuxtEwaWW.png)
",https://help.ceda.ac.uk/article/234-data-help-docs#where-to-find-docs,308,28
Introduction,"A file format is way to encoded information in a computer file. A
format specifies how to interpret the bytes in the files as information
with meaning to the programs and people reading and writing them. Each
format is designed to carry a particular type of data, but some formats
are more specific or more general in their realm of operation.  For
example, the PNG format is excellent for encoding an image, but could
not be easily used to store a 3D computer aided design model. 
Text formats are those where the bytes in the file should be interpreted
as text characters. This means that generic text editors can be used to
view or change the data. This is very useful if your data is small and
can be interpreted by human inspection. There are different ways to
encode text, but most are encoded with
[ASCII](https://en.wikipedia.org/wiki/ASCII) or
[unicode](https://en.wikipedia.org/wiki/Unicode). 
Binary formats are those where the bytes have to be interpreted by the
specific format rules to work out their meaning. This necessitates the
use of specialised programs to read and write the data. 
The CEDA Archive includes a wide range of file formats - some well
supported and other historical ones less so. The table below list some
of the main formats within the CEDA archives with links to tools
supporting the format. For information about which format is used for a
dataset please CEDA data catalogue.
Additionally, how information is stored within or about files (so called
metadata) is key to how the data within files can be used. See
information about  metadata formats in the \"" [Introduction to
metadata](https://help.ceda.ac.uk/article/4428-metadata-basics)\""
article.
",https://help.ceda.ac.uk/article/104-file-formats#introduction,1688,271
Core Supported Formats,"  ------------------------------------------------------------- ---------- ------------------ -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **Format**                                                    **Type**   **File endings**   **Commonly used for**
  [BADC-CSV](https://help.ceda.ac.uk/article/105-badc-csv)      text       .csv               simple \""1-D\"" type of data, e.g. instrument time series data
  [NASA Ames](https://help.ceda.ac.uk/article/4692-nasa-ames)   text       .na                aircraft and older instrument data (older data may have an  [older file-naming convention](https://help.ceda.ac.uk/article/103-filenames))
  [HITRAN](https://help.ceda.ac.uk/article/4427-hitran)         text       various            spectroscopy data
  [JCAMP-DX](http://cedadocs.ceda.ac.uk/999/)                   text       .dx, .jdx          only suitable for spectra from spectroscopy experiments
  [NetCDF](https://help.ceda.ac.uk/article/106-netcdf)          binary     .nc                **CEDA\'s preferred data format.   **Model data and observational data with more than 1 dimension (e.g. time-height data).  Suitable for gridded numeric data such as model output. CF conventions preferred - migration to make CF compliant acceptable.
  [HDF](https://help.ceda.ac.uk/article/4425-hdf)               binary     .hdf               Satellite data. Requires consistent conventions to be followed.
  [PP](https://help.ceda.ac.uk/article/4424-pp-binary-format)   binary     .pp                Met Office model output
  [GRIB](https://help.ceda.ac.uk/article/4426-grib)             binary     .grb               ECMWF model output
  GEOTIFF                                                       image      .tif, .TIFF        Earth observation imagery data
  JPEG2000                                                      image      .jp2               Earth observation imagery data
  JPEG                                                          image      .jpg               For images
  TIFF                                                          image      .tif, .tiff        For images
  ------------------------------------------------------------- ---------- ------------------ -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
",https://help.ceda.ac.uk/article/104-file-formats#core-supported-formats,2652,150
,,https://help.ceda.ac.uk/article/104-file-formats#,0,0
Other Accepted Formats,"A range of other formats have been included to the CEDA Archive over
time. Some of these are historical, whilst others are from third party
sources which CEDA obtains as a facilitation mode. Not all are listed
and the file format information on dataset records in the data catalogue
should be referred to.
  -------------- -------------- -------------- --------------- --------------
  **Format**     **Type**       **File         **Commonly used **Notes**
                                endings**      for**           
  PNG            image          .png                           
  BUFR\          binary                        meteorology     WMO standard
  \                                            data            
  \                                                            
  Nimrod format  binary         .dat           Met Office      To be
                                               NIMROD rain     superseded by
                                               radar data      ODIMS
                                                               compliant HDF5
  BIL            binary         .bil           flat binary     
                                               format used by  
                                               ENVI users -    
                                               produced by     
                                               ARSF processing 
                                               node            
  LAS                                          point cloud for 
                                               EO data         
  PDF(a)                        .pdf           suitable for    
                                               documentation   
                                               only            
  plain text     text           .txt           suitable for    
                                               documentation   
                                               only. Data      
                                               should utilise  
                                               an approved     
                                               format instead. 
  ENVI-HDF                                                     
  -------------- -------------- -------------- --------------- --------------
",https://help.ceda.ac.uk/article/104-file-formats#other-accepted-formats,2312,143
,,https://help.ceda.ac.uk/article/104-file-formats#,0,0
Not Accepted Formats,"These formats have been reviewed by CEDA and deemed not acceptable for
long-term data archival.
  ----------------- ----------------- ----------------- -----------------
  **Format**        **Type**          **File endings**  **Alternative
                                                        format to use**
  csv/tsv\          text              csv, .tsv         BADC-CSV
  Excel             text              .xls              BADC-CSV
  HTML              text              .html             BADC-CSV
  Word              text              .doc, .docx       BADC-CSV, PDF
  ----------------- ----------------- ----------------- -----------------
",https://help.ceda.ac.uk/article/104-file-formats#not-accepted-formats,651,50
Compression and Aggregations,"At times it is desirous for files to be compressed to reduce overall
volumes and also consider aggregation off files together to aid transfer
and storage. These come into play primarily where there are either large
numbers of files or large data volumes to consider, though impacts to
onward use of the data (to uncompress/unpack) should be considered too.
**Note - these should only be applied to files that are already
formatted in a permitted format given above.**
  ---------------------- ------------- ------------------------------ ---------------------------------------------------------------------------------------------------------------
  **Format **            **Type**      **File endings**               **Commonly used for**
  internal compression   compression   (retains main format ending)   Reducing file sizes e.g. HDF5, netCDF
  tar                    aggregation   .tar                           Aggregating a number of files as a \""tar ball\"" allows a set of files to be downloaded together.
  gzip, bzip, zip        compression   .gz, .bz, .zip                 Reduce the volume required for the file to aid transfer and storage. Note, requires uncompressing before use.
  ---------------------- ------------- ------------------------------ ---------------------------------------------------------------------------------------------------------------
",https://help.ceda.ac.uk/article/104-file-formats#compression-and-aggregations,1379,152
CEDA Archive Disaster Recovery Plan,"::: {#wikipage children-count=""0""}
",https://help.ceda.ac.uk/article/4144-disaster-recovery-plan,35,3
Catastrophic Failure of CEDA Archive services {#CatastrophicFailures},"This page contains information about dealing with catastrophic failures
which totally remove access to existing hardware, for example an
uncontrolled fire in the main server room.
Contact is facility manager, Sam Pepler, 01235 446538 or 07823553553,
[sam.pepler\@stfc.ac.uk](mailto:mailto:sam.pepler@stfc.ac.uk){.mail-link}
",https://help.ceda.ac.uk/article/4144-disaster-recovery-plan#catastrophic-failure-of-ceda-archive-services-{#catastrophicfailures},324,37
Procedure {#Overview},"In the event of a catastrophic failure of the CEDA archive that shut
down all services, CEDA management, in consultation with the NERC EDS
management and CEDA would make a plan dependent on the impact of the
catastrophic event. 
The priority would be to recover primary archive data rather than
restarting services.
The order of events would be:
1.  New servers and storage would be purchased or rented ASAP. 
2.  Data from tape backups would be recovered.
3.  Recovery of non-primary archive data from external sources would
    start.
4.  Ingest services restart.
5.  Access Services restart.
6.  Community services restart.
",https://help.ceda.ac.uk/article/4144-disaster-recovery-plan#procedure-{#overview},627,100
Disaster Recovery,"For non-catastrophic instance requiring recovery from back-up see
[standard recovery from back-up
procedure](https://ceda-internal.helpscoutdocs.com/article/4265-storage-d-backups).
:::
",https://help.ceda.ac.uk/article/4144-disaster-recovery-plan#disaster-recovery,186,14
FTP with Python and PERL,"Most programming languages have FTP client libraries. These can be used
to grab data from the CEDA archives programmatically rather than
interactively. Two examples are given below. The first uses Python to
extract ECMWF ERA40 data and the second uses Perl to grab radiosonde
data.
",https://help.ceda.ac.uk/article/281-ftp-with-python-and-perl,282,45
Python example,"Python is available on most platforms and can be downloaded from
[www.python.org](https://www.python.org/). The script below opens an FTP
connection then loops over a number of years, months, days, hours and
variables. It constructs each file name and then retrieves is to a local
directory.
    #!/usr/bin/env python
    # Import required python modules
    import ftplib
    import os
    # Define the local directory name to put data in
    ddir=""C:\\datadir""
    # If directory doesn't exist make it
    if not os.path.isdir(ddir):
        os.mkdir(ddir)
    # Change the local directory to where you want to put the data
    os.chdir(ddir)
    # login to FTP
    f=ftplib.FTP(""ftp.ceda.ac.uk"", """", """")
    # loop through years
    for year in range(1990,2001):
        # loop through months
        for month in range(1,13):
            # get number of days in the month
            if year%4==0 and month==2:
                ndays=29
            else:
                ndays=int(""dummy 31 28 31 30 31 30 31 31 30 31 30 31"".split()[month])
            # loop through days
            for day in range(1, ndays+1):
                # loop through hours
                for hour in range(0, 19, 6):
                    # loop through variables
                    for var in (""10u"", ""10v""):
                        # change the remote directory
                        f.cwd(""/badc/ecmwf-e40/data/gg/as/%.4d/%.2d/%.2d"" % (year, month, day))
                        # define filename
                        file=""ggas%.4d%.2d%.2d%.2d%s.grb"" % (year, month, day, hour, var)
                        # get the remote file to the local directory
                        f.retrbinary(""RETR %s"" % file, open(file, ""wb"").write)
    # Close FTP connection
    f.close()
",https://help.ceda.ac.uk/article/281-ftp-with-python-and-perl#python-example,1763,209
Perl example,"Perl is available on most platforms and can be downloaded from
[www.perl.org](http://www.perl.org/). The script below opens an FTP
connection then downloads all the camborne radiosonde files for the
current year.
    # libraries for FTP and time strings
    use Net::FTP;
    use POSIX qw(strftime);
    # Get this year 
    $thisyear = strftime(""%Y"", localtime);
    # radiosonde directory with thisyears data in
    $dir = ""/badc/ukmo-rad/data/united_kingdom/camborne/$thisyear"";
    # Connect to BADC ftp server
    $ftp = Net::FTP->new(""ftp.ceda.ac.uk"", Debug => 0 ) or die;
    # Login 
    $ftp->login("""",'') or die;
    # change the remote directory
    $ftp->cwd($dir) or die;
    # list dir
    @files = $ftp->ls($dir);
    # get the remote file to the local directory
    for $file  (@files) {
      print ""Getting $file\n"";
      $ftp->get($file) or die;
    }
    # Close FTP connection
    $ftp->quit;
",https://help.ceda.ac.uk/article/281-ftp-with-python-and-perl#perl-example,915,119
Introduction,"::: {#wikipage}
The ingester script has been designed to provide a simple to use,
generic tool for regular ingestions into the CEDA archive. The workflow
of the ingester is:
-   List files either from particular set of directories or from a
    listing file.
-   For each file in the list: 
    -   Work out the destination in the archive using a regular
        expression.
    -   Deposit the file in the archive.
    -   optionally remove the source file.
(Source code is stored here: 
<https://breezy.badc.rl.ac.uk/cedadev/ingest_lib>)
",https://help.ceda.ac.uk/article/4337-ingesterpy#introduction,540,81
Config files {#Filesneeded},"The ingester uses a config file as used by ingest_control. A minimal
example is show below.
[stream-name]
script: ingester
arrivals_users: spepler parton
dirtemplate: /badc/datasetx/data/%(year)s
regex: xxx(?P<year>\d{4})\d{4}.dat
deleterChoice: arrivals
This looks in /datacentre/arrivals/users/spepler and
/datacentre/arrivals/users/parton to create the list of source files.
Files that match the pattern, xxx(?P\<year>\\d{4})\\d{4}.dat, are
deposited in the archive using the directory template to construct the
archive directory. After ingestion the source files are deleted.
Typically, these stream configurations also contain other options for
scheduling by ingest_control.
[cfarr-lidar]
owner: gparton
description: ingestion of lidar data directly from arrivals area (doesn't ingest those unpacked first to processing area)
script: ingester -f
#scheduler details follow:
when: 5,15,25,35,45,55 * * * *
timeout:36
notify_warning: graham.parton@stfc.ac.uk
notify_fail: graham.parton@stfc.ac.uk
# end of scheduler details
arrivals_users: jagnew
dirtemplate: /badc/chilbolton/data/%(instDict)s%(locDict)s/%(year)s/%(month)s/
regex:^(cfarr\-)(?P<instrument>[0-9a-z\-]*)_(?P<location>chilbolton|sparsholt)_(?P<year>\d{4})(?P<month>\d{2})(\d{2})(_[a-zA-Z\-]*)?\.(?P<ext>nc|png|zip)$
deleterChoice: arrivals
force:on
skip_and_clean_up: True
skip: on
",https://help.ceda.ac.uk/article/4337-ingesterpy#config-files-{#filesneeded},1349,134
Config file options {#Usinganexternalscript},"  ----------------------------------- ------------------------------------------------------------------------
  relaxed_names\                      Use relaxed names when depositing. Allows @ and :
  force                               Force overwrite of existing files in archive. The -f command line option
                                      will override this option is used.\
  warn_on_zero_length\                Just warn if zero length files are found in source list, rather that
                                      stopping.
  dirtemplate                         Directory template for the destination archive path. This is in python
                                      format string replacement format. \
  path_template                       An alternative to dirtemplate. This template is used to create the full
                                      path of the destination in the archive, including the filename. This
                                      allows you to rename files as they are deposited.
  regex                               The regular expression to match filename components that are used in the
                                      dirtemplate or path_template. Named groups must be used, for example
                                      (?P\<year>\\d{4}) in the regex will be inserted in the dirtemplate where
                                      %(year)s is found. Note that the whole source path is used as input to
                                      the regex so elements in the arrivals directory structure can be
                                      matched.\
  arrivals_ignore_regex               a regex pattern for files in source area to ignore
  arrivals_dirs \                     A space separated list of directories to search for files to ingest.
                                      **In addition all the options used in the [arrivals
                                      module](https://ceda-internal.helpscoutdocs.com/article/4656-arrivals)
                                      are also used.**
  arrivals_users                      A space seperated list of usernames. This list is mapped to a list of
                                      directories that are appended to the arrivals_dirs list. The directory
                                      mapped to for a user is of the form
                                      /datacentre/arrivals/users/\<username>/\<streamname>.  For example, if
                                      arrivals_users = spepler gparton, and the stream name is xxx then the
                                      list of directories searched are /datacentre/arrivals/users/spepler/xxx
                                      and  /datacentre/arrivals/users/gparton/xxx. **In addition all the
                                      options used in the [arrivals
                                      module](https://ceda-internal.helpscoutdocs.com/article/4656-arrivals)
                                      are also used.**
  nthreads                            Run a multi threaded ingester. defaults to 1. This increases the speed
                                      of the ingester proportional to the number of threads. Note it should be
                                      remembered that the number of deposit servers is finite and so
                                      increasing this number to the number of deposit servers will monopolise
                                      all the servers. One off jobs should keep nthreads \< 15. Regular jobs
                                      nthreads \< 10.  \
  force                               Force overwriting files in the archive with matching paths and
                                      filenames. Defaults to off.
  skip\                               Ignores files that match and are of the same size, but leaves these in
                                      the source area. Defaults to off.
  skip_and_clean_up\                  Ignored files that match and are of the same size AND removes these in
                                      the source area. Defaults to off.
  ----------------------------------- ------------------------------------------------------------------------
",https://help.ceda.ac.uk/article/4337-ingesterpy#config-file-options-{#usinganexternalscript},4258,389
,,https://help.ceda.ac.uk/article/4337-ingesterpy#,0,0
Use at the command line,"The ingester script can be run at the command line using two methods to
associate it with the config file.
Using the -s and -c options at the command line for the stream name and
config file.
    $ ingester -c software/datasets/testdata/test.conf -s test-stream3
Or set the INGEST_CONF environment variable: 
    $ export INGEST_CONF=software/datasets/testdata/test.conf:test-stream3 
    $ ingester
Command line Options
  ---------------- ----------------------------------------------------
  -v               Verbose output
  -t                Trial run. Don\'t deposit or delete source files.
  -n N             Stop after processing N files for ingest. 
  \--max_fails N   Stop after N unsuccessful ingests. 
  -f               Force overwrite of existing files in archive. 
  -l FILE          Listing file used as sources.
  \--skip          Skip files if the same size.
  ---------------- ----------------------------------------------------
",https://help.ceda.ac.uk/article/4337-ingesterpy#use-at-the-command-line,949,111
,,https://help.ceda.ac.uk/article/4337-ingesterpy#,0,0
Usage examples,,https://help.ceda.ac.uk/article/4337-ingesterpy#usage-examples,0,0
Ingester as recursive deposit,"If you are going to keep the directory structure as it appears in the
processing area.
    [recursive_ingest] 
    script: ingester  -v --skip 
    path_template: /badc/dataset1/data/%(path)s 
    regex: /datacentre/processing3/dataset1/(?P<path>.*)$ 
    arrivals_dirs: /datacentre/processing3/dataset1 
    deleterChoice: notArrivals
",https://help.ceda.ac.uk/article/4337-ingesterpy#ingester-as-recursive-deposit,336,29
Multi-threaded deposit,"If you have a big data set.
    [fstone_big_ingest] 
    script: ingester  -v --skip 
    dirtemplate: /badc/flintstone/data/%(load_num)s 
    regex: flintstone_data/yabber_(?P<load_num>\d+)_\d+\.dat$ 
    arrivals_users: fred barny
    deleterChoice: arrivals
    nthreads: 5
",https://help.ceda.ac.uk/article/4337-ingesterpy#multi-threaded-deposit,277,23
Deposit from many sources,"The arrivals_users options are added to the arrivals_dirs option, so you
can use both together to pickup from all over the place.
    [avenger_team] 
    script: ingester  
    path_template: /badc/Avengers/data/%(film)s/%(scene)s 
    regex: commentlog\.(?P<film>)\.(?P<scene>)\.txt$ 
    arrivals_users: hulk ironman cap_marvel thor antman wasp black_panther strange vision banner
    arrivals_dirs: /datacentre/processing3/avengers/hulk_smashed /datacentre/processing3/avengers/thanos
    deleterChoice: keep
    nthreads: 5
",https://help.ceda.ac.uk/article/4337-ingesterpy#deposit-from-many-sources,528,47
Using the ingester as the basis for your own ingest script,"If you need more information than is just contained in the source
filename and path, but your workflow follows the pattern of the
ingester, it is easy to write your script using the ingester libraries.
The example below demonstrates how this can be used.
The first thing to do is to import the Ingester class from the
ingest_lib library and the standard re regular expression library. We
are going to add some extra functionality to the base Ingester class and
run it in the same way as the ingester script. 
    from ingest_lib import Ingester 
    import re
    class DataXIngester(Ingester):
        pass
    i = DataXIngester()
    i.ingest()
This would work exactly like the ingester. Now let\'s add some extra
functionality. We overload the archive_dir method so that it adds
product_type to the other information found in the regex matching.
The archive_dir method should return the archive directory name or None
if the filename does not match.
    from ingest_lib import Ingester
    import re
    class DataXIngester(Ingester):
        def archive_dir(self, filename):
            found = re.search(self.regex, filename)
            if not found:
                return None
            found_dict = found.groupdict()
            # set plots or data
            product_type = {'jpg': 'plots', 'png': 'plots', 'nc': 'data', 'na': 'data', 'tar': 'data'}
            found_dict['product_type'] = product_type[found_dict['ext']]
            completed_template = self.dirtemplate % found_dict
            return completed_template
    def main():
        i = DataXIngester()
        i.ingest()
    if __name__ == ""__main__"":
        main()
If the config file has these options.
    dirtemplate: /badc/datasetX/%(product_type)s/%(year)s
    regex: datax_(?P<year>\d{4})\d{4}\.(?P<ext>)
:::
::: {#wikipage}
Data files will now be filed like so:
datax_20180103.nc -\> /badc/datasetX/data/2018
datax_20170103.nc -\> /badc/datasetX/data/2017
datax_20180103.png -\> /badc/datasetX/plots/2018
:::
",https://help.ceda.ac.uk/article/4337-ingesterpy#using-the-ingester-as-the-basis-for-your-own-ingest-script,1996,245
CRU TS data,"The following guide will assist with the manipulation and visualisation
of the Climatic Research Unit (CRU) gridded time-series (TS) dataset.
For more information on this data, or to understand the ways in which it
can be downloaded, please click here [CRU data user
guide](https://help.ceda.ac.uk/article/4472-cru-data-user-guide).
The following analysis is done using the data in a netCDF format (.nc)
and by running a Python script (.py) to produce 2 different plots, this
will require the data to be downloaded in this specific format. The two
plots that will be created are:
1.  [[Annual mean temperature (1901-2017) - Global
    map](#plot1)]{##plot1}
2.  [[Global average temperature anomaly (1901-2017) - Time series
    plot](#plot2)]{##plot2}
The easiest way to download the data in this format to your local files
is by accessing it here:
<http://data.ceda.ac.uk/badc/cru/data/cru_ts/cru_ts_4.02/data/>
Then select your parameter (the following script is specific to
temperature \""tmp\"") and the 1901-2017 netCDF file
(cru_ts4.02.1901.2017.tmp.dat.nc.gz)
The python script is available to download
[here](http://data.ceda.ac.uk/badc/cru/software/). 
**If you are a JASMIN user,** **the script is located
at /badc/cru/software/plot_cru-ts_examples.py,** **run it with the
existing file path, as it is set to the location of the CRU TS data on
JASMIN. This script is compatible with Python 3, to use this on JASMIN,
load a Python 3 environment by typing \'module load jaspy\' before your
Python command.**
**The data is located on JASMIN in the following
directory: /badc/cru/data/cru_ts/cru_ts_4.02/data/**
**If you would like to explore data analysis of CRU in Jupyter Notebooks
or the **[JASMIN Notebook
Service](https://help.jasmin.ac.uk/article/4851-jasmin-notebook-service)**,
you can find it
here: **https://github.com/cedadev/ceda-notebooks/tree/master/notebooks/data-notebooks/observations/cru
",https://help.ceda.ac.uk#cru-ts-data,1912,247
**Python data visualisation example **,"The following will give a brief explanation of what each part of the
code does in this script and may suggest ways in which this can be
modified to adapt to different scenarios:
The code below is put at the beginning of the script to add the specific
packages (tools) that are needed in python to achieve the outcome.
    import numpy as np
    import matplotlib.pyplot as plt
    from netCDF4 import Dataset
    from  mpl_toolkits.basemap import Basemap
This code reads in the data to the program so it can be viewed,
manipulated and displayed as required. The text within the quotation
marks is the file path. This will need to be changed to the specific
location of where the data is saved.
    filename = ""/badc/cru/data/cru_ts/cru_ts_4.02/data/tmp/cru_ts4.02.1901.2017.tmp.dat.nc""
    data = Dataset(filename)
<div>
If you read in NetCDF data and you don\'t know the names of the
variables you can print out a list of variable names in the file using:
</div>
    print(data.variables.keys())
<div>
The .keys() method provides only the variable names, without this you
will get additional metadata. Each dimension in the file also has a
variable, so you will see a variable for each dimension, in this case:
</div>
-   \'lat\' for latitudes
-   \'lon\' for longitudes
-   \'time\' for time
-   \'tmp\' for near surface temperature
Next, the temperature variable is set. This allows the temperature data
to be used within the script. If another parameter is required from the
data, that variable should be selected. 
    temp = data.variables['tmp'][:]
",https://help.ceda.ac.uk#**python-data-visualisation-example **,1556,246
Plot 1 - Global Map {#plot1},"The temperature variable that has been set as \'temp\' in the code
above, is a function of latitude, longitude and time as it is
3-dimensional. To produce a map plot, the temperature values need to be
averaged across the entire time period of the dataset. This will give 1
average value per grid point. The line below averages the temperature
variable by the time axis.
    temp_av_1901_2017= np.mean(temp[:,:,:],axis = 0)
Now the gridded average has been calculated, the following code shows
how a map can be plotted with this data. This example plot is for the
globe. To create a regional map, change the llcrnrlat, llcrnrlon,
urcrnrlat and urcrnrlon which are the lat/lon values of the lower left
and upper right corners of the map.The Basemap module is used here to
create a map plot. For help using Basemap click
[here](https://matplotlib.org/basemap/).
-   Basemap(projection=\""cyl\"", resolution=\'c\', llcrnrlat=-90,
    urcrnrlat=90, llcrnrlon=-180, urcrnrlon=180) sets the Basemap
    settings. To set up the desired Basemap, the projection, resolution,
    latitude and longitude extent needs to be defined. The latitude and
    longitude extent determine what area will be shown on the map (this
    example is global)
-   \'cmap=plt.cm.viridis\' is the colour used to display the data, this
    can be changed by amending the last word. For colour options, see
    [here](https://matplotlib.org/tutorials/colors/colormaps.html)
-   \'temp_av_1901_2017\' is the data which we defined in the code above
-   plt.title(\""text\"") defines the title which will go on the map plot,
    this can be changed depending on the parameter being plotted
-   cb= map.colorbar(im1,\""bottom\"",size=\""5%\"",pad=\""2%\"",
    extend=\'both\') creates a colour bar scale for the map
-   cb.set_label(u\""Temperature \\u2103\"") adds a label for the colour
    bar, this can be changed dependent on the parameter being plotted
    (adding u before the quote marks and \\u2103 within the quote marks
    adds the symbol for degrees Celsius)
-   plt.savefig(\""text.png\"") is the name and format the map image will
    be saved as. This can be changed.
<!-- -->
    plt.figure()
    map = Basemap(projection=""cyl"", resolution='c', llcrnrlat=-90, urcrnrlat=90, llcrnrlon=-180, urcrnrlon=180) 
    map.drawcoastlines(color=""black"") 
    lons,lats = np.meshgrid(data.variables['lon'][:], data.variables['lat'][:]) 
    x,y = map(lons, lats)
    temp_plot = map.contourf(x, y, temp_av_1901_2017, cmap=plt.cm.viridis) 
    cb = map.colorbar(temp_plot, ""bottom"", size=""5%"", pad=""2%"", extend = 'both')
    cb.set_label(u""Temperature \u2103"")
    plt.title(""Mean Temperature (1901-2017)"")
    plt.annotate('Data - CRU TS v4.02',(-178,-88), fontsize=6)
    plt.show() 
    plt.savefig(""cruts_global.png"")
This code will produce a global map as shown below:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5cdabac72c7d3a6d82bd6c37/file-TSu2t7YgLr.png)
",https://help.ceda.ac.uk#plot-1---global-map-{#plot1},2963,371
Plot 2 - Time series {#plot2},"For the time series graph, the data needs to be averaged in a different
way. To create a time series plot, the data needs to be averaged across
all grid points, so there is 1 global value vs time. The line of code
below does exactly this. NOTE: This script does not account for the
\'pole problem\' eg. no changes have been made to the weighting of the
polar regions grid spacing, hence the values in this graph may appear
slightly higher than reported in other CRU graphs (this graphs max is
nearly 1.5 degrees). 
    global_average= np.mean(temp[:,:,:],axis=(1,2))
To reduce the seasonal noise in this time series graph, an annual
average needs to be calculated from the monthly data. The code below
reshapes the global average into \[117,12\] as there are 117 years in
the dataset, each with 12 months. Then the average is calculated for
each year. These new annual average values are saved as \'annual_temp\'.
    annual_temp = np.mean(np.reshape(global_average, (117,12)), axis = 1)
For this plot, it is useful to look at the temperature values as an
anomaly to a certain temperature period. The following code calculates
the annual temperature anomaly in comparison to the average temperature
in 1961-1990. The first line calculates the average temperature value
for this time period (1961-1990). This is done by slicing the data with
the indices 60:89 as this gives the values from 1961-1990, then
averaging these values. The second line then deducts the average
temperature value between 1961-1990 from each of the annual temperature
values calculated above, saving it as \'temp_anomaly\'.
    av_1961_1990=np.mean(annual_temp[60:89])
    temp_anomaly = annual_temp - av_1961_1990
The following code plots a time series graph. The anomaly data
calculated above is plotted against time, in years. 
-   plt.ylim sets a limit for the y-axis so the data can be viewed more
    clearly
-   plt.title displays a title on the plot, this can be changed
    depending on the parameter being plotted
-   plt.ylabel displays a label for the y-axis, in this case it is
    temperature that is being displayed but this can be changed
    depending on the parameter being plotted
-   The last line is the name and format the graph will be saved as.
    This can be changed. 
<!-- -->
    plt.figure()
    plt.plot(np.arange(1901,2018,1),temp_anomaly)
    plt.ylim(np.floor(min(temp_anomaly)), np.ceil(max(temp_anomaly))) 
    plt.title(""Global Average Temperature Anomaly (1901-2017)"")
    plt.xlabel(""Years"") 
    plt.ylabel(u""Difference from 1961-1990 average (\u2103)"") 
    plt.text(1985, -0.9, ""Data from CRU TS v4.02"", fontsize=8)
    plt.show()
    plt.savefig('temp_anom_ts.png')
This code will produce a time series graph as shown below:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5cdac7d72c7d3a6d82bd6d23/file-2StkQ0puLA.png)
",https://help.ceda.ac.uk#plot-2---time-series-{#plot2},2874,404
How to put MIDAS data into ArcGIS,"The [Met Office MIDAS
datasets](http://catalogue.ceda.ac.uk/uuid/220a65615218d5c9cc9e4785a3234bd0)
can be used in GIS packages. This page will outline the steps needed to
make the MIDAS datasets compatible for ArcGIS.
",https://help.ceda.ac.uk/article/4584-how-to-put-midas-data-into-arcgis,218,26
Step 1,"Follow the [MIDAS quick start
guide](http://cedadocs.ceda.ac.uk/1349/4/midas%20user%20guide_v1.3.pdf)
to extract and prepare your data.
",https://help.ceda.ac.uk/article/4584-how-to-put-midas-data-into-arcgis#step-1,136,12
Step 2,"Gather the longitude and latitude coordinates for your station(s) that
are in your data by using the [Met Office Midas Station
search](http://archive.ceda.ac.uk/midas_stations/).  The longitude and
latitude will appear in a table as shown below.
[![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5a577524042863193800e45e/file-AywGw2Plls.png)](http://archive.ceda.ac.uk/midas_stations/)
",https://help.ceda.ac.uk/article/4584-how-to-put-midas-data-into-arcgis#step-2,424,35
Step 3,"Insert two columns into your data file, one for the longitude and the
other for latitude. Ensure each row with data then has the longitude and
latitude for its station (see below for an example).
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5a57767e042863193800e46b/file-DqveicS1XB.png)
",https://help.ceda.ac.uk/article/4584-how-to-put-midas-data-into-arcgis#step-3,329,36
Step 4,"Save your file as a CSV file.
",https://help.ceda.ac.uk/article/4584-how-to-put-midas-data-into-arcgis#step-4,30,7
Step 5,"Add the CSV file as a base layer using the add icon.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5a57771c2c7d3a194368245e/file-EfBxGL7JFN.png)
",https://help.ceda.ac.uk/article/4584-how-to-put-midas-data-into-arcgis#step-5,186,13
Step 6,"Use the Add XY to data tool by right clicking on the layer. Add XY as
Longitude and Latitude.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5a5777d3042863193800e472/file-EYAtUQCEX8.png)
",https://help.ceda.ac.uk/article/4584-how-to-put-midas-data-into-arcgis#step-6,227,20
Step 7,"Your station will now appear on a map as a dot. Each dot will contain
the measurements for the station.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5a577868042863193800e47b/file-CbkdJnmwlC.png)
",https://help.ceda.ac.uk/article/4584-how-to-put-midas-data-into-arcgis#step-7,237,21
,,https://help.ceda.ac.uk/article/4584-how-to-put-midas-data-into-arcgis#,0,0
Introduction,"For certain projects we run the [[ ]{.icon}
CEDA-CC](http://proj.badc.rl.ac.uk/exarch/wiki/PackageCedaccInstall){.ext-link}
tool to check that the files received are compliant with the format
specification for that project. This can check file names, time ranges
across groups of files and metadata inside files.
This page provides examples of problems identified with files and how
they were tackled.
",https://help.ceda.ac.uk/article/4350-compliance-checking-and-ceda-cc#introduction,402,54
Note about installation of CEDA-CC on ingest1 server {#NoteaboutinstallationofCEDA-CConingest1server},"On ingest1.ceda.ac.uk , CEDA-CC is deployed in the standard virtual
environment (venv27). You can update the CEDA-CC version on this server
by running a single script:
> /usr/local/ingest_software/venv27/bin/update_ceda-cc.sh
",https://help.ceda.ac.uk/article/4350-compliance-checking-and-ceda-cc#note-about-installation-of-ceda-cc-on-ingest1-server-{#noteaboutinstallationofceda-cconingest1server},226,28
Typical workflow {#Typicalworkflow},"The typical compliance checking process is:
1.  Data Providers places a batch of files in arrivals space.
2.  CEDA runs the CEDA-CC tool on that batch.
3.  CEDA runs the CEDA-CC tool in summary mode to examine the logs of
    the main run.
4.  If no errors:
    -   CEDA ingests the data into the archive
    -   CEDA publishes the datasets to ESGF
5.  If errors:
    -   Inform the Data Provider and then go back to step 1.
",https://help.ceda.ac.uk/article/4350-compliance-checking-and-ceda-cc#typical-workflow-{#typicalworkflow},425,77
Example problems identified with files {#Exampleproblemsidentifiedwithfiles},"This section lists a number of real-life example problems that have been
found with input files. It has been written to provide hints on how to
diagnose issues identified by CEDA-CC and how to communicate them back
to the Data Provider - who will be responsible for fixing files.
> > NOTE: CEDA does not undertake to fix problems identified by CEDA-CC
> > - that is the responsibility of the Data Provider (who should also
> > run a local copy of CEDA-CC before sending us data).
",https://help.ceda.ac.uk/article/4350-compliance-checking-and-ceda-cc#example-problems-identified-with-files-{#exampleproblemsidentifiedwithfiles},480,88
1. Units errors in NetCDF files {#a1.UnitserrorsinNetCDFfiles},"In this example, the errors showed up as follows:
$ ceda-cc --sum specs_CNRM-CM5_batch2/
############################ /datacentre/processing/specs/CCCC/trunk/ceda_cc
Summarising error reports from 73018 log file
C4.002.005:  1511  [variable_ncattribute_mipvalues] :Variable [hus] has incorrect attributes: units=""1"" [correct:""kg kg-1""]
               hus_Amon_CNRM-CM_seaIceInit_S19790501_r10i1p1_197905-197911__qclog_20150521.txt
               hus_Amon_CNRM-CM_seaIceInit_S19790501_r10i2p1_197905-197911__qclog_20150521.txt
Number of files with no errors: 71507
I did some grepping and counting to double-checked the errors are all
the same:
$ cd specs_CNRM-CM5_batch2/
$ for i in *_CNRM*.txt ; do grep FAILED $i >> ../FAILED.txt ; done
$ wc -l ../FAILED.txt
1511 ../FAILED.txt
$ sort -u ../FAILED.txt
C4.002.005: [variable_ncattribute_mipvalues]: FAILED:: Variable [hus] has incorrect attributes: units=""1"" [correct: ""kg kg-1""]
This showed that there was a common error across all failures.
Verified the error by looking inside one of the data files:
$ ncdump -h /group_workspaces/jasmin/specs/CNRM-CM5/batch2/CNRM/CNRM-CM/seaIceInit/S19790501/mon/atmos/hus/r1i1p1/hus_Amon_CNRM-CM_seaIceInit_S19790501_r1i1p1_197905-197911.nc | grep hus | grep units
                hus:units = ""1"" ;
Checked the CF standard name table at:
> [[ ]{.icon}
> http://cfconventions.org/Data/cf-standard-names/28/build/cf-standard-name-table.html](http://cfconventions.org/Data/cf-standard-names/28/build/cf-standard-name-table.html){.ext-link}
Said \""hus\"" should have units of \""1\"".
Checked in ceda-cc MIP table:
$ head -1329 /usr/local/ingest_software/venv27/config/specs_vocabs/mip/SPECS_Amon | tail -8
variable_entry:    hus
!============
modeling_realm:    atmos
!----------------------------------
! Variable attributes:
!----------------------------------
standard_name:     specific_humidity
units:             kg kg-1
So, even though the standard name table says the units should be \""1\"",
in SPECS the scientists have decided to use \""kg kg-1\"".
**Outcome**
We asked the Data Provider to change the files.
Recommend that she changes files.
Note that the code they used to fix this was quite simple:
# Find affected files, grab the units of ""hus"" and count them:
$ find /group_workspaces/jasmin/specs/CNRM-CM5/batch2/CNRM/CNRM-CM/seaIceInit -type f -name ""hus_*.nc"" -exec ncdump -h {} \; | grep hus:units | wc -l
   1511 
# Fix all the files using ncatted   
$ find /group_workspaces/jasmin/specs/CNRM-CM5/batch2/CNRM/CNRM-CM/seaIceInit -type f -name ""hus_*.nc"" -exec ncatted -a units,hus,m,c,""kg kg-1"" {} \;
Pierre-Antoine fixed the files himself.
",https://help.ceda.ac.uk/article/4350-compliance-checking-and-ceda-cc#1.-units-errors-in-netcdf-files-{#a1.unitserrorsinnetcdffiles},2641,284
2. Variable not in the MIP Table {#a2.VariablenotintheMIPTable},"For some CORDEX files the summary output reported:
C4.002.002: 22
  --- [variable_in_group] :Variable hurs not in table day: 10
               hurs_AFR-44_ECMWF-ERAINT_evaluation_r1i1p1_MOHC-HadGEM3-RA_v1_day_19900101-19901231__qclog_20150603.txt
               hurs_AFR-44_ECMWF-ERAINT_evaluation_r1i1p1_MOHC-HadGEM3-RA_v1_day_19910101-19951231__qclog_20150603.txt
               hurs_AFR-44_ECMWF-ERAINT_evaluation_r1i1p1_MOHC-HadGEM3-RA_v1_day_19960101-20001231__qclog_20150603.txt
               hurs_AFR-44_ECMWF-ERAINT_evaluation_r1i1p1_MOHC-HadGEM3-RA_v1_day_20010101-20051231__qclog_20150603.txt
               hurs_AFR-44_ECMWF-ERAINT_evaluation_r1i1p1_MOHC-HadGEM3-RA_v1_day_20060101-20081130__qclog_20150603.txt
               hurs_EUR-44_ECMWF-ERAINT_evaluation_r1i1p1_MOHC-HadGEM3-RA_v1_day_19900101-19901231__qclog_20150603.txt
               hurs_EUR-44_ECMWF-ERAINT_evaluation_r1i1p1_MOHC-HadGEM3-RA_v1_day_19910101-19951231__qclog_20150603.txt
               hurs_EUR-44_ECMWF-ERAINT_evaluation_r1i1p1_MOHC-HadGEM3-RA_v1_day_19960101-20001231__qclog_20150603.txt
               hurs_EUR-44_ECMWF-ERAINT_evaluation_r1i1p1_MOHC-HadGEM3-RA_v1_day_20010101-20051231__qclog_20150603.txt
               hurs_EUR-44_ECMWF-ERAINT_evaluation_r1i1p1_MOHC-HadGEM3-RA_v1_day_20060101-20101231__qclog_20150603.txt
This means that the variable represented was not in the MIP Table. A MIP
Table is one of the configuration tables used to drive large
experimental model runs. It is used in CEDA-CC to check the outputs.
**Outcome**
This issue was raised with the Data Providers. They identified that the
project had recently updated the MIP Tables. Contact was made with
Martin Juckes and the new version of the CORDEX MIP Tables was added to
CEDA-CC trunk and rolled out on the ingest1 server.
Once the new MIP Tables were in place this error ceased to be reported
by the checker.
",https://help.ceda.ac.uk/article/4350-compliance-checking-and-ceda-cc#2.-variable-not-in-the-mip-table-{#a2.variablenotinthemiptable},1883,130
3. Incorrect global attribute value {#a3.Incorrectglobalattributevalue},"In one CORDEX case many files had an unknown value for an expected
global variable. The summary said:
C4.002.006: 1830
  --- [global_ncattribute_cv] :Global attributes do not match constraints:[('driving_model_id', 'ERAINT', ""['ECMWF-ERAINT', 'BCC-bcc-csm1-1', 'BCC-bcc-csm1-1-m', 'BNU-BNU-ESM']"")]: 1824
               areacella_CAS-44_ERAINT_evaluation_r0i0p0_MOHC-HadRM3P_v1_fx__qclog_20150603.txt
               areacella_CAS-44i_ERAINT_evaluation_r0i0p0_MOHC-HadRM3P_v1_fx__qclog_20150603.txt
               clivi_CAS-44_ERAINT_evaluation_r1i1p1_MOHC-HadRM3P_v1_6hr_1990010106-1990123118__qclog_20150603.txt
               clivi_CAS-44_ERAINT_evaluation_r1i1p1_MOHC-HadRM3P_v1_6hr_1991010100-1991123118__qclog_20150603.txt
               clivi_CAS-44_ERAINT_evaluation_r1i1p1_MOHC-HadRM3P_v1_6hr_1992010100-1992123118__qclog_20150603.txt
               clivi_CAS-44_ERAINT_evaluation_r1i1p1_MOHC-HadRM3P_v1_6hr_1993010100-1993123118__qclog_20150603.txt
               clivi_CAS-44_ERAINT_evaluation_r1i1p1_MOHC-HadRM3P_v1_6hr_1994010100-1994123118__qclog_20150603.txt
...
From the output it is clear that the value given was \'ERAINT\' and the
list of expected values included \'ECMWF-ERAINT\'.
**Outcome**
The resolution was simply to ask to the data provider to modify the
files (using NCO) so that the global attribute \'driving_model_id\' had
the value \'ECMWF-ERAINT\'. This fixed the problem.
",https://help.ceda.ac.uk/article/4350-compliance-checking-and-ceda-cc#3.-incorrect-global-attribute-value-{#a3.incorrectglobalattributevalue},1404,93
4. Inconsistent file metadata {#a4.Inconsistentfilemetadata},"In one CORDEX case some files had global attributes that did not match
the corresponding filename attributes. The summary said:
C4.002.007:
---  [filename_filemetadata_consistency] :File name segments do not match corresponding global attributes:[(2, 'model_id'), (4, 'startdate'), (5, '@ensemble'), (6, '@forecast_reference_time:4:')]
               zg_day_CNRM-CM5-HRA-LRO_horizlResImpact_S19940501_r1i1p1_19940901-19940930__qclog_20160818.txt
               zg_day_CNRM-CM5-HRA-LRO_horizlResImpact_S19951101_r7i1p1_19960101-19960131__qclog_20160818.txt
Looking at the file attribute model_id:
ncdump -h zg_day_CNRM-CM5-HRA-LRO_horizlResImpact_S19940501_r1i1p1_19940901-19940930.nc | grep model_id
                :model_id = ""CNRM-CM5-LRA-LRO"" ;
We can see that the model_id in the filename and in the attributes are
not the same where \""-LRA-\"" should be \""-HRA-\"". Similarly the
attributes \'startdate\', \'forecast_reference_time\' and
\'associated_experiment\' (which maps to \@ensemble) are checked.
**Outcome**
The resolution was simply to ask to the data provider to modify the
files (using NCO) so that the global attributes \'model_id\',
\'startdate\', \'forecast_reference_time\' and \'associated_experiment\'
have values that are consistent with the filename. This fixed the
problem.
",https://help.ceda.ac.uk/article/4350-compliance-checking-and-ceda-cc#4.-inconsistent-file-metadata-{#a4.inconsistentfilemetadata},1298,131
fileProcessorpy,"::: {#wikipage}
[Original trac
page](http://team.ceda.ac.uk//trac/ceda/wiki/opman/fileProcessor)
::: wiki-toc
1.  [fileProcessor.py](#fileProcessor.py)
    1.  [Introduction](#Introduction)
    2.  [Where source is stored](#Wheresourceisstored)
    3.  [Files needed](#Filesneeded)
    4.  [how to use](#howtouse)
    5.  [Where does it work?](#Wheredoesitwork)
    6.  [Config files](#Configfiles)
        1.  [Generic config settings](#Genericconfigsettings)
    7.  [Example](#Example)
:::
",https://help.ceda.ac.uk/article/4326-fileprocessorpy,493,38
Introduction {#Introduction},"The fileProcessor script has been designed to complement the
[unpacker](http://ceda-internal.helpscoutdocs.com/article/4267-unpackerpy){.wiki}
and
[ingester](http://ceda-internal.helpscoutdocs.com/article/4337-ingesterpy){.wiki}
and takes its inspiration from how fromDeliveries was written. Its aim
to to provide a generic tool that can be used as part of an ingest
stream on the ingest system. It has been designed to:
1.  perform renaming, gzipping and tarring functions according to
    settings held in a configuration file
2.  migrate data that have passed a quarantine period into an \""ingest\""
    area
3.  give options for handling the source files - whether to delete or
    not
4.  utilises the arrivals library to set up the source area as needed
5.  utilises the processLib for common functions such as renaming and
    removal of source files
6.  tracks files all the way through the process to ensure completion
    before removal of the source file is attempted (if a step fails it
    is removed from the delete list)
7.  Ability to call an external, bespoke library to carry out more
    complex renaming tasks
Although designed primarily with the /datacentre/processing area in
mind, in theory this could be used elsewhere on the system.
",https://help.ceda.ac.uk/article/4326-fileprocessorpy#introduction-{#introduction},1257,183
Where source is stored {#Wheresourceisstored},"Source code is stored in the CEDA svn repository here : [[ ]{.icon}
http://proj.badc.rl.ac.uk/badc/browser/ceda_software/fileProcessor/trunk](http://proj.badc.rl.ac.uk/badc/browser/ceda_software/fileProcessor/trunk){.ext-link}
.
",https://help.ceda.ac.uk/article/4326-fileprocessorpy#where-source-is-stored-{#wheresourceisstored},229,15
Files needed {#Filesneeded},"The fileProcessor is driven by settings within a configuration file. How
entries in this config file should be set out is discussed below,
followed by examples.
",https://help.ceda.ac.uk/article/4326-fileprocessorpy#files-needed-{#filesneeded},161,26
how to use {#howtouse},"One off instances can be run as this:
python /usr/local/ingest_software/fileProcessor/fileProcessor.py -c <configfile> -s <stream> [-d|-v]
where :
> -c \<configfile> is the path to the configuration file that you have
> set up your option in for the processing  -s \<stream> is the part of
> the config file that you want the options to be read from -d is the
> dry run - printing what would happen to the screen and not actually
> doing the action.  -v is the \""verbose\"" flag which returns lots of
> logging of what is going on
",https://help.ceda.ac.uk/article/4326-fileprocessorpy#how-to-use-{#howtouse},530,95
Where does it work? {#Wheredoesitwork},"The fileProcessor can cope with files at any source area available on
Ingest1 and can place the output of the operation into any writeable
area too. Primarily, though, it has been designed to source files from
the arrivals area (/datacente/arrivals/users/\<userID>/\<streamId>/) and
output files into a \""quarantine\"" area typically under
/datacentre/processing/\<dataset>/quarantineDir/\<stream>. As a last
step the script will migrate data from the quarantine area to an area
ready for ingesting from with the ingester script, typically this should
be somewhere like
/datacentre/processing/\<dataset>/readyToIngest/\<stream>/ .
",https://help.ceda.ac.uk/article/4326-fileprocessorpy#where-does-it-work?-{#wheredoesitwork},630,82
Config files {#Configfiles},"Within the config file the configuration settings referred to are
delineated from other settings for other runs by a section name given in
square brackets - these are the \""streams\"" within a
Below are the generic options that are required for ALL instances of the
processor, followed by particular sections detailing what is required
for renaming, gzipping and tarring options.
For an explanation of how these map to the process to help you work out
what you need to put in there see this diagram.
",https://help.ceda.ac.uk/article/4326-fileprocessorpy#config-files-{#configfiles},499,85
Generic config settings {#Genericconfigsettings},"[stream-name]
owner: <insert your username here - this is important to help those looking after the system work out who is running jobs>
description: <a short description detailing the job and what it does>
# standard bits for most config files:
script: <command line call for the job, including optional entries>
lockfile: <path and name to a lockfile - standard practice is to pop these under /home/badc/lockfiles/>
lock: <flag for the ingest_control system to stop other instances happening or not>
when: <scheduler times in standard crontab format - e.g. minutes hours day-of-month month year to run the script. Used to schedule recurring tasks under ingest_control>
timeout: <number of hours the script is permitted to continue running for before being terminated - the default is 12>
notify_ok: <space separated list of email addresses to email if the jobs runs ok>
notify_warning: <space separated list of email addresses to email if there are warning messages issued> 
notify_fail: <space separated list of email addresses to email if the job fails>
# end of scheduler details
order: <comma separated list of what functions to do and in which order: reName,zip,tar>
arrivals_users: <either give a space separated list of the users who will contribute to this data stream>
arrivals_dirs: <OR a space separated list of absolute paths to the source directories for the incoming data>
arrivals_wait: <how old the files should be in seconds before being considered for ingestion>
fileAge: <how old the files should be days before being considered for ingestion - note, will be retired in due course>
#if a call to an external library is needed to generate the destination path use the next two lines:
headerclass: <this is the absolute path to the external library to help generate the filename to which the source file will be renamed. Usually under /home/badc/software/datasets/<dataset>>
accessmethod: <name of the init class within the external library - usually leave this as: archive_path_class>
quarantineDir: <the area where the files are written to during the processing stages and where they will remain until they have passed the quarantine period>
fileNameTemplate: <if renmaing then this template is used to map contents from regex to new filenames, can be used in tandem with headerclass if more complex work is needed>
regex: <regular expresion to uniquely identify files to be processed and parts to use for constructing the new filename (see fileNameTemplate)>
archiveDir: <a template that is used to indicate where in the archive to check for existing tarballs which can be used to append contents to based on tarOptions>
tarTemplate: <output tar file file name template built up using parts identified from tarRegex>
tarRegex: <regular expression to identify files to be placed in tar file and ascertain tar file name components used in tarTemplate>
tarOutputRegex: <A regex to return items used in the archiveDir and tarTemplate to spot already existing tarballs in the processingDir, ingestDir or archiveDir areas to possibly append to, depending on the tarOptions setting> 
tarOptions: <option on how to process tar file - options are new|append newFile will ignore existing content in archive/ingest stream and create an entirely new file (which will subsquently replace any that exist down stream during ingest process), append will add new files to existing tar file if it exists and new files are newer versions than that already existing in the downstream tar file>
deleterChoice: <one of arrivals|notArrivals to delete data from /datacentre/arrivals/users/ or /datacentre/processing otherwise the files will be kept>
quarantineCheck: <the type of check to carry out for the quarantine period, options are: fileAge, filename - filename will apply a (?P<year>[0-9])(?P<month>?P<day> regex to the filename to try and get the yyyymmdd string to use, so use with care!)>
quarantinePeriod: <time in days that the file should have passed the quarantine check by before being moved to ingest>
ingestRegex: <used to identify files to test quarantine status and move to ingest area - this is a regular expression which will need to contain group labels to get yyyymmdd info from filenames if fileName is quarantine check selection>
ingestDir: <the directory to which files will be moved once they have passed the quarantine check, where they could be picked up by another script for processing - e.g. ingestion, or some other checking/processing>
",https://help.ceda.ac.uk/article/4326-fileprocessorpy#generic-config-settings-{#genericconfigsettings},4468,700
Example {#Example},"[ukmo-nimrod-composite]
owner: gparton
description: process incoming nimrod composite data (both UK and Europe)
script: python /usr/local/ingest_software/fileProcessor/fileProcessor.py -c /home/badc/software/datasets/ukmo-nimrod/ukmo-nimrod_fileProcessor.cfg -s ukmo-nimrod-composite
order: reName,zip,tar
mode: operational
when: 6,16,26,36,46,56 * * * *
#notify_ok: graham.parton@stfc.ac.uk
#notify_warning: graham.parton@stfc.ac.uk
notify_fail: graham.parton@stfc.ac.uk
timeout: 36
arrivals_users: dartmetoffice
arrivals_wait: 480
arrivals_maxfiles: 80000
fileAge:0
headerclass: /home/badc/software/datasets/ukmo-nimrod/nimrod_mapper
accessmethod: archive_path_class
archiveDir: /badc/ukmo-nimrod/data/composite/%(area)s-%(resolution)skm/%(year)s/
quarantineDir: /datacentre/processing/ukmo-nimrod/quarantine/ukmo-nimrod-composite/
lockfile: /home/badc/lockfiles/ukmo-nimrod-composite-process.lock
lock:yes
fileNameTemplate: metoffice-c-band-rain-radar_%(area)s_%(year)s%(month)s%(day)s%(hour)s%(minute)s_%(resolution)skm-composite.%(type)s
regex: (?P<year>[0-9]{4})(?P<month>[0-9]{2})(?P<day>[0-9]{2})(?P<hour>[0-9]{2})(?P<minute>[0-9]{2})_nimrod_(?P<areaDict>ng_radar|ps_area20)_rainrate_composite_(?P<resolution>1|5)km_(?P<
typeDict>UK_cutout_300X306_correct\.gif|EU_cutout_435X345_uk|EU\.gif|EU)(?!\.gz)(?!\.tmp)$
tarTemplate: metoffice-c-band-rain-radar_%(area)s_%(year)s%(month)s%(day)s_%(resolution)skm-composite.%(type)s.gz.tar
tarRegex: metoffice-c-band-rain-radar_(?P<area>uk|europe)_(?P<year>[0-9]{4})(?P<month>[0-9]{2})(?P<day>[0-9]{2})([0-9]{4})_(?P<resolution>1|5)km-composite.(?P<type>dat|gif).gz
tarOutputRegex: metoffice-c-band-rain-radar_(?P<area>uk|europe)_(?P<year>[0-9]{4})(?P<month>[0-9]{2})(?P<day>[0-9]{2})_(?P<resolution>1|5)km-composite.(?P<type>dat|gif)(\.gz\.tar)$
tarOptions: append
deleteOption: arrivals
quaratinePeriod:1
quarantineCheck: fileName
ingestDir: /datacentre/processing/ukmo-nimrod/readyToIngest/ukmo-nimrod-composite/
ingestRegex: metoffice-c-band-rain-radar_(?P<area>uk|europe)_(?P<year>[0-9]{4})(?P<month>[0-9]{2})(?P<day>[0-9]{2})_(?P<resolution>1|5)km-composite.(?P<type>dat|gif)(\.gz\.tar)$
:::
",https://help.ceda.ac.uk/article/4326-fileprocessorpy#example-{#example},2146,79
Data management helpdesk,"**Helpdesk email address:** data.management\@ceda.ac.uk
**link to
helpdesk:** <https://secure.helpscout.net/mailbox/7b0c55db545d4969/1952553/>
This helpdesk is used for communicating with projects and notification
of data arriving. This helpdesk links to the DMP tool with each query
connecting to a DMP project ID number. 
",https://help.ceda.ac.uk/article/4684-data-management-helpdesk,324,38
Assigning queries,"Kate will assign queries based on the project CEDA officer and
specialist area.
",https://help.ceda.ac.uk/article/4684-data-management-helpdesk#assigning-queries,80,13
DMP project ID,"The project id number can be found on the url link of the project page
in the DMP tool, see example below.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5bc5c83e042863158cc778c5/file-4vQqfw1WWD.png)
This number is then added to the query and the relevant stage (Initial
contact, DMP, Progress, data delivery, other, costing or general help)
is added. This is to help track progress.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5bc5c884042863158cc778c8/file-Vz6Qky8Cgm.png)
By adding the DMP project id number into the query enables a quick
search of conversations relating to that project.
",https://help.ceda.ac.uk/article/4684-data-management-helpdesk#dmp-project-id,674,75
**Saved Replies**,"The helpdesk contains the following saved replies to help with
workflows;
<div>
<div>
[Acknowledgement from
arrivals](https://secure.helpscout.net/settings/saved-replies/143009/#section1076545)
</div>
</div>
<div>
<div>
[Asking for more
information](https://secure.helpscout.net/settings/saved-replies/143009/#section1095165)
</div>
</div>
<div>
<div>
[Chase DMP
acceptance](https://secure.helpscout.net/settings/saved-replies/143009/#section920092)
</div>
</div>
<div>
<div>
[Chase up email with missing info on
DMP](https://secure.helpscout.net/settings/saved-replies/143009/#section920095)
</div>
</div>
<div>
<div>
[Chase up email with
table](https://secure.helpscout.net/settings/saved-replies/143009/#section942322)
</div>
</div>
<div>
<div>
[Chase up progress email
template](https://secure.helpscout.net/settings/saved-replies/143009/#section920093)
</div>
</div>
<div>
<div>
[Depositing data
new](https://secure.helpscout.net/settings/saved-replies/143009/#section1075561)
</div>
</div>
<div>
<div>
[Depositing data
old](https://secure.helpscout.net/settings/saved-replies/143009/#section987713)
</div>
</div>
<div>
<div>
[Draft dmp
email](https://secure.helpscout.net/settings/saved-replies/143009/#section920091)
</div>
</div>
<div>
<div>
[Ingest
stream](https://secure.helpscout.net/settings/saved-replies/143009/#section920101)
</div>
</div>
<div>
<div>
[Initial email but what
data](https://secure.helpscout.net/settings/saved-replies/143009/#section920050)
</div>
</div>
<div>
<div>
[Initial email with known
datasets](https://secure.helpscout.net/settings/saved-replies/143009/#section920032)
</div>
</div>
<div>
<div>
[Initial email with no
data](https://secure.helpscout.net/settings/saved-replies/143009/#section920044)
</div>
</div>
<div>
<div>
[Not responded to initial contact 1st chase up
email](https://secure.helpscout.net/settings/saved-replies/143009/#section920061)
</div>
</div>
<div>
<div>
[Not responded to initial contact 2nd chase up
email](https://secure.helpscout.net/settings/saved-replies/143009/#section920062)
</div>
</div>
<div>
<div>
[Progress email with several no
responces](https://secure.helpscout.net/settings/saved-replies/143009/#section920097)
</div>
</div>
<div>
<div>
[Subsetting email reduce volume for model
data](https://secure.helpscout.net/settings/saved-replies/143009/#section920036)
</div>
</div>
",https://help.ceda.ac.uk/article/4684-data-management-helpdesk#**saved-replies**,2356,164
Getting a citation for your data DOIs for data,"Data producers can now receive due academic credit for the data that
they produce through the assigning of Digital Object Identifiers (DOIs)
to their data when it is held in a recognised repository, such as the
CEDA data centres. 
Publishers are increasingly requesting that researchers ensure that
their data are lodged in a recognised data repository, preferably with
DOIs assigned to the data to aid linking the published article and the
referenced data resource. CEDA can assist with such requests.
DOIs can also be used to formally cite datasets in the same way as a
researcher would cite a journal article.
",https://help.ceda.ac.uk,613,101
Finding out about DOIs,"NERC has guidelines for scientists on how to get a DOI available through
its [website here](https://nerc.ukri.org/research/sites/data/doi/). This
guide has useful answers to commonly asked questions such as: Why should
I get a DOI for my dataset and how does it benefit me?
",https://help.ceda.ac.uk#finding-out-about-dois,274,42
What can CEDA do?,"CEDA is able to assign DOIs to datasets held within its archives. In
order to do this the data must be:
1.  Complete  - the data can be a whole dataset or this can apply to
    part of a continuously produced dataset, e.g. for a year\'s worth of
    data that are complete.
2.  Unchanging - whereby the actual content is fixed and will not be
    further amended to correct for inconsistencies etc.
3.  Properly archived in a persistent location - such as the archives
    held by CEDA.
4.  Have all the necessary information about the dataset to compile a
    suitable CEDA dataset catalogue page. This forms the DOI landing
    page to which the DOI will resolve and bring the person following
    the DOI to the referenced resource.
CEDA data scientists will work with you to ensure that your data meet
these criteria. While this may also necessitate some additional work on
your behalf we\'ll try to guide you through the process and make it as
easy as possible.
",https://help.ceda.ac.uk#what-can-ceda-do?,967,168
How long will it take to get a DOI?,"This will depend on the work needed to ensure the data and associated
dataset catalogue page are suitably in place for the DOI landing page to
be created. 
In all cases we recommend that you contact CEDA at the earliest
available opportunity to discuss your requirements  as we may not be
able to meet short deadlines.
",https://help.ceda.ac.uk#how-long-will-it-take-to-get-a-doi?,319,56
Data already held by CEDA:,"Assuming data are already held in the CEDA archive with a suitable
datasets catalogue entry, a DOI can usually be issued within 1 working
day.
There may, however, be some additional delays to this, typically where
additional background informaiton is required to ensure the dataset
catalogue page is sufficiently populated.
",https://help.ceda.ac.uk#data-already-held-by-ceda:,324,50
Data not held by CEDA/not complete:,"If a complete set of data are not held by CEDA in one of our data
centres then additional work will need to take place. Please see the
other parts of the Archiving Data With CEDA guide for depositors on the
CEDA site for further information. This guide will help you to determine
if we are are a suitable archive for your data, how to prepare a data
mangement plan and the process of archiving data with us. 
",https://help.ceda.ac.uk#data-not-held-by-ceda/not-complete:,409,78
Further Information,"More information about DOIs and on process to obtain a DOI can be found
on the  [NERC DOI
page](https://nerc.ukri.org/research/sites/data/doi.asp).
",https://help.ceda.ac.uk#further-information,148,19
Overview,"1\. Access & permissions
2\. What do data providers see?
3\. Components of a dataset delivery
4\. Data centre requirements for deliveries
5\. Editing an active delivery
6\. Reviewing an active delivery
7. Approving a delivery for ingest
8\. Rejecting a delivery\
\
9. After approval, or after rejection\...
------------------------------------------------------------------------
",https://help.ceda.ac.uk/article/5104-reviewing-data-at-ceda-a-step-by-step-guide#overview,380,50
Introduction,"CEDA accepts data relevant to atmospheric and earth observation fields.
This includes a wide range of instrumental, satellite, aircraft,
observations, analyses and model datasets of interest to the scientific
community. These data arrive frequently and in large volumes at [CEDA\'s
Arrivals Service](https://arrivals.ceda.ac.uk/ceda/review/) (note, you
will need to be signed in with the correct admin permissions/status
before being able to view this page). Submitted deliveries must
currently be manually approved by a reviewer before being ingested to
the arrivals area in the data centre. This step-by-step guide will
outline the process to review a dataset delivery. 
NERC data centres linked with CEDA Arrivals 
are as follows:
-   PDC ([Polar Data Centre](https://www.bas.ac.uk/data/uk-pdc/))
-   BODC ([British Oceanographic Data Centre](https://www.bodc.ac.uk/))
-   EIDC ([Environmental Information Data Centre](https://eidc.ac.uk/))
-   NGDC ([National Geoscience Data
    Centre](https://www.bgs.ac.uk/ngdc/))
-   CEDA ([Centre for Environmental Data
    Analysis](https://www.ceda.ac.uk/))
<div>
CEDA staff, as well as staff from the above data centres, may act as
reviewers for dataset deliveries. The arrivals page for a particular
data centre may be accessed by the following URL:
[<https://arrivals.ceda.ac.uk/>\<datacentre>/intro/]{.ul}. With BODC,
for example: <https://arrivals.ceda.ac.uk/bodc/intro/>.\
</div>
------------------------------------------------------------------------
",https://help.ceda.ac.uk/article/5104-reviewing-data-at-ceda-a-step-by-step-guide#introduction,1504,177
Step-by-step guide,"1.  **Access &
    permissions:**![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/655398fba20843397d51bc52/file-zPqHCCKscF.png)If
    you have admin permissions/privileges/status on the CEDA Archive,
    you will see the \""Reviewer\"" Label on this bar at the top of the
    screen. The two links next to this are \'Deliveries\' **(a)**, which
    brings you to the list of dataset deliveries in all stages, and
    \'Admin\' **(b)**, which brings you to the site administration
    actions. The number next to \'Deliveries\' is the number of
    deliveries which currently require action. \
    \
    **a)** ![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/655b65362defc34db368d229/file-0KpwY6aAnY.png)**b)**
2.  ![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/655b655805db140086d5ade0/file-HEkxeBTE2a.png)**What
    do data providers see?**\
    \
    The arrivals portal is structured like so:
    ![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/655b801f3b683338722d89ec/file-kbZXT6EOev.png)
    When data providers make a dataset delivery, the arrivals interface
    takes them through the following actions/workflow stages/pages:\
    **\
    **a) Deliveries portal - \
    ![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/657c291475eb4a5f70498c28/file-NGZotU8WF6.png)The
    data provider is prompted to ensure their dataset conforms to the
    requirements for archival at CEDA, and agrees to the [deposit
    agreement](https://arrivals.ceda.ac.uk/ceda/agreement/). They then
    create a new delivery (\'+ New delivery\') or choose an existing
    delivery name associated with their user account (e.g. when
    uploading further data for an ongoing project).\
    \
    \
    b) Upload data - \
    [**![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/657c2a70a55b5524fb50d793/file-V2kosGbrRx.png)**Here
    the user chooses the files to attach to the delivery. There are
    several tools available to them at this stage: unzip zipped files,
    fix bad names (this removes unusual characters and spacing), remove
    empty directories, remove zero length files and remove links from
    filenames. They are also given the option to upload data via FTP or
    RSYNC .\
    \
    \
    ]{style=""width: 544.333px;""}c) Review dataset - \
    [**![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/657c2bf4a55b5524fb50d794/file-Z9bWOUffHx.png)**]{style=""width: 536.333px;""}d)
    Submit delivery - \
    [![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/657c2c554773693a6d83e18b/file-UHIwRBV0R3.png)e)
    Finish - \
    ]{style=""width: 533.333px;""}![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/657c2c69e020fe4b2fcaf077/file-WRykYcMtLZ.png)\
3.  **Components of a dataset
    delivery:**![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/655b80aed3a3af43ff7b1796/file-aj6Ma4htQN.png)
    \*\* *Screenshot the delivery summary page and circle/annotate the
    key components as
    below.![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/657c2d44e020fe4b2fcaf078/file-1MFWM0t1R7.png)*
    -   **Owner:** CEDA account username
    -   **Created:** datetime format = D MON YYYY, H:MM a.m./p.m.
    -   **Data centre:** Will be one from the list in the introduction.
    -   **Total size of dataset:** Expressed in GB.
    -   **Total number of files:** \...
    -   **File formats:** Common file formats for deliveries are: netCDF
        (.nc), BADC-CSV (.csv), NASA Ames (.na), HDF (.hdf), tar and
        compressed file types.
4.  **Data centre requirements for deliveries:**\
    \
    Reviewers should check that the following requirements (in the
    deliveries portal (a) section of the arrivals uploader) are
    fulfilled:\
    \
    -   In the **archive remit** - e.g. Atmospheric or Earth Observation
        science for the CEDA Archive.
    -   Available on an **open licence**.
    -   Comprised of a set of files that are laid out in a
        **understandable directory structure**.
    -   A single one-off delivery with **no need update**.
    -   In a **recommended format**, like NetCDF, with **appropriate
        metadata conventions**, like CF.
    -   **Not too voluminous** - needs prior agreement with CEDA if
        uploading more than 5TB.
    -   **Accompanied by a full description** of the dataset so that we
        can describe it in our catalogue.
    Reviewers should check that the catalogue link, given by the data
    provider when making the delivery, is active and the record is
    suitably formatted. This link does not have to be a CEDA catalogue
    record, but should allow metadata to be harvested for such a
    record. If there is not a catalogue link, the reviewer should
    actively check for a metadata.yaml file submission. The contents of
    this YAML file will allow the reviewer to create the CEDA catalogue
    record manually. YAML files might not link the records to its final
    archive path correctly, this should therefore be checked and
    corrected if necessary.
5.  **Editing an active delivery: **\
    \
    The \'Edit\' page allows the reviewer to change elements of the
    delivery: the name which will be given to the directory in the
    archive, the data centre associated with the delivery (and therefore
    which reviewers are responsible for approving it), and catalogue
    link to a draft record. \
    \
    ![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/657c4033a126ca3c882e2cc7/file-JKqcCMr5M6.png)
6.  **Reviewing an active delivery:**\
    \
    This is identical to the \""Upload data\"" page (b) of the data
    provider workflow when delivering data to arrivals. Reviewers are
    able to apply tools to the datasets in the same way with the blue
    buttons, i.e. unzip all, remove empty directories, fix bad names,
    remove zero length files and remove links. Any modifications which
    are made by the reviewer in this stage should be clearly
    communicated to the data provider. \
    \
    Reviewers are reminded that each filename needs to conform to the
    CEDA file-naming convention (more information
    [here](https://help.ceda.ac.uk/article/103-filenames)). Incorrect
    separators in names (i.e. dashes \'-\' instead of underscores
    \'\_\') should be handled by the \""fix bad names\"" tool. Each change
    can be saved by clicking the edit icon next to the delete/remove
    button. \
    ![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/657c404fa126ca3c882e2cc8/file-ctUwtZU7Xy.png)
7.  **Approving a delivery for ingest:**\
    \
    If the requirements of the dataset delivery have been met, all
    components properly checked and the reviewer is satisfied to accept
    the delivery, the green \'Approve\' button in the grey \'Reviewer\'
    tab should be selected. There are several options for ingest stream
    according to the processing which needs to be done to the dataset.\
    \
    ![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/65afa79f87e88924b5fa1ee4/file-x6xQ5fwki9.png)
    -   **Email DC **- manual ingestion - sends message to CEDA helpdesk
        (data.management\@ceda.ac.uk), delivery name may need to be
        changed to a more suitable descriptor for archive.
    -   **CEDA Standard** - automatic ingestion - triggers ingest to
        archive. A job is attempted to archive dataset. The delivery
        name will become the corresponding repository name in the
        archive. The end-point in the archive will be assigned the
        current year (note: when it was ingested, not when it was
        deposited). *\
        *
    -   **NGDC catalogue grant** - an NGDC ingest stream that takes the
        NERC grant number from the BGS catalogue page, provided a link
        and correct formatting. A metadata.yaml file should not be
        included with this delivery, as metadata is harvested from the
        existing BGS catalogue record for the dataset.\
        \
        Note: If the data centre associated with the dataset delivery is
        not CEDA (viewable in the delivery Summary page), only the first
        option in the following dropdown list (\""Email DC\"") will be
        visible. This means the delivery will need to be manually
        reviewed and ingested by datacentre staff.
8.  **Rejecting a delivery:**\
    \
    If the reviewer is not satisfied with the state of the dataset/that
    it meets the criteria/requirements for approval, the dataset can be
    rejected. In this case, it is particularly important to provide
    feedback to the data provider in order to help resolve issues as
    efficiently as possible. This can be done in the message interface
    shown below.\
    \
    If there are minor changes, the reviewer may advise the data
    provider to make these offline, upload the new files and resubmit
    the existing dataset delivery. \
    If there are major changes, the reviewer should explain why the
    dataset does not meet the required standard for the CEDA archive and
    recommend a course of
    action.![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/65afb89911a1ca1c986cd2dd/file-H5mdYyVuwS.png)
    ![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/657339b2900867203f2e8689/file-dBM0AnCEyo.png)
9.  **After approval, or after rejection\...\
    \
    If approved:\
    \
    **Once the dataset is ingested to the archive, an email is sent to
    the data submission email for the associated datacentre - this
    includes the location (end-point) of the delivery in the archive.\
    \
    <div>
    <div>
    CEDA: data.management\@ceda.ac.uk\
    PDC: [polardatacentre\@bas.ac.uk\
    ]{style=""background-color: initial;""}BODC: [data.management\@ceda.ac.uk\
    ]{style=""background-color: initial;""}EIDC: [info\@eidc.ac.uk\
    ]{style=""background-color: initial;""}NGDC: ngdc\@bgs.ac.uk
    </div>
    </div>
    The data provider is sent an email that their delivery has been
    approved. At the moment this does not include more identifying
    information about the delivery other than its name (i.e. no
    catalogue link, creation date, or other delivery components from the
    summary page). *\
    \
    ***If rejected:\
    \
    **The data provider will receive an email from the datacentre
    notifying them that their delivery has been rejected (this will be
    sent to the email address registered with the user who submitted the
    delivery). The datacentre does not receive an email, but the
    delivery will have a \'rejected\' tag on the arrivals deliveries
    inbox.\
    \
    The reviewing process in this guide should be repeated if the
    delivery is resubmitted for approval.*\
    \
    *After the dataset is in the CEDA archive, the reviewer needs to
    conduct a final check on the catalogue record for content and
    completeness, and submit the catalogue record for MOLES review &
    publishing using [this
    form](https://docs.google.com/forms/d/e/1FAIpQLScZXVw4m5e08WL4JFIUGtl9B2ohw3Pn2_a8wFXXmbr1WSDIqA/viewform).
------------------------------------------------------------------------
",https://help.ceda.ac.uk/article/5104-reviewing-data-at-ceda-a-step-by-step-guide#step-by-step-guide,11559,1328
Additional documentation/help pages,"-   [Depositing Data at CEDA: A step by step guide - CEDA Archive and
    Services
    (helpscout.net)](https://secure.helpscout.net/docs/564b4f2490336002f86de436/article/5b5af0c50428631d7a896169/)
-   [CEDA Archive account - CEDA Archive and Services
    (helpscout.net)](https://secure.helpscout.net/docs/564b4f2490336002f86de436/article/56ab392c9033603f7da35ee0/)
-   [CEDA Archive: Quick Start User Guide - CEDA Archive and Services
    (helpscout.net)](https://secure.helpscout.net/docs/564b4f2490336002f86de436/article/56f264e59033604f41b37931/)
-   [CEDA Helpdesk - CEDA Archive and Services
    (helpscout.net)](https://secure.helpscout.net/docs/564b4f2490336002f86de436/article/590c8e962c7d3a057f88d69a/)
[](https://secure.helpscout.net/docs/564b4f2490336002f86de436/article/56e28a53c697911471460f0b)
\
If at any point in this guide you are unsure of the proper process,
please contact the CEDA data management helpdesk
at data.management\@ceda.ac.uk - this deals with communicating with
projects and data arriving via arrivals and they will be able to help
you with your issue.
",https://help.ceda.ac.uk/article/5104-reviewing-data-at-ceda-a-step-by-step-guide#additional-documentation/help-pages,1088,96
Dataset authorisation info,"[Original trac
page](http://team.ceda.ac.uk//trac/ceda/wiki/opman/DatasetAuth)
",https://help.ceda.ac.uk/article/4367-dataset-authorisation-info,79,3
Introduction {#Introduction},"(updated 20230215)
This document describes how the authorisation system works. It covers
public, registered user and restricted resources.
To set up access control fully there are 3 systems that need interacting
with:
-   accessInstructor- this is where the actual access control within the
    archive is set for FTP and web download and JASMIN access
-   userDB - this is where the application system is set up and maps to
    user accounts for actual user access
-   MOLES - to provide the link for people to register/get information
    on access control/licencing that is in place
**For the following \'access group\' refers to the *archive* access
group NOT the Linux group, which will be referred to as the \'jasmin
group\'.**
",https://help.ceda.ac.uk/article/4367-dataset-authorisation-info#introduction-{#introduction},734,118
Important basic principles to note,"-   To proceed you need to know the archive path where you wish to set
    the rules to apply. This does NOT have to be done for every dataset
    within a given point in the archive, but could be higher up the
    directory tree if there is a common access control, licencing and
    JASMIN access that applies to all the datasets within a given part
    of the archive.
-   A \'default\' entry can be set at this higher point which can be
    overwritten/superseded by a more specific rule further down.
-   These more specific rules can be set to expire and then access will
    go to the default option. This is very useful for embargoed data
    where the embargo will expire on a given day.
-   The default rule, if nothing is set, should be to prevent people
    from accessing the data, but beware of what has been set further up
    the directory tree!
-   **for \'restricted data\' a \'group\' can be used to give access to
    a set of resources where the access control and licencing will be in
    common.**
",https://help.ceda.ac.uk/article/4367-dataset-authorisation-info#important-basic-principles to-note,1021,181
1. Licence file,"From the DMP you should have an idea of what licence is needed for the
data you are looking to get up access control for. Ideally, it should be
one of the common licences. The [\'licence
selector\' ](https://docs.google.com/spreadsheets/d/1Z2fvv-L8qRrRqyfQPpmeTBs64uEf_T1zPOyX65e1CbU/edit?usp=sharing)spreadsheet
can help determine this. **NOTE** this also includes two generic
licences specifically designed for \'embargoed/restricted access\' data
(the RUGL and RUNCGL licences), which should remove the need to create
any bespoke licences in most cases!.
However, should you need to have a new, specific licence not already in
the CEDA Artefacts server then :
1.  get a PDF of the licence file 
2.  add it to the [CEDA Artefacts
    server](https://ceda-internal.helpscoutdocs.com/article/4460-service-documentation-for-artefacts-server)
    under the licences/specific_licences folder in the github repository
    (<https://github.com/cedadev/artifacts>). This will then be copied
    to the live artefacts server by a cron job in a few minutes.
3.  Inform Graham that you\'ve added a new licence as this will then
    need to be classified for the types of use the licence permits. 
**2. Set up accessInstructor rules**
This is where the \'.ftpacecs\' and XACML policy files are set, plus the
JASMIN group. The .ftpaccess files control access via FTP and XACML
policy files set the web access control. JASMIN access is *driven* by
this process, though there isn\'t an option to set this directly due to
a mapping that is required\... see note lower down.
1.  login to <https://accessctl.ceda.ac.uk/admin/>
2.  select \'rules\' and do a quick search for the upper most directory
    covering the path you want to set a rule to see what has already
    been set.. if there isn\'t one that meets your needs, then proceed
3.  select \'add rule\', top right
4.  enter the path where the rule should apply (use + to add a new one)
5.  set \'rule type\' - select ONE only of :
6.  1.  \'public\' for fully \'public\' data (non-registered user AND
        registered users will have access). 
    2.  \'reguser\' for those just needing a CEDA user account
    3.  \'group\' if specific restriction is needed.\
        This will then present a Group\' selection list - either use a
        pre-existing group or add a new group. If you need to set up a
        new \'group\' use the green plus symbol **REMEMBER THE SELECTED
        GROUP! This should be lowercase, with no spaces and use
        \'\[a-z0-9\], \'-\' or \'\_\' as needed**
7.  Search for and select a licence or add a new licence (see note
    below)
8.  If required, set expiry date for the rule
9.  Add comment if you wish
10. click Save
",https://help.ceda.ac.uk/article/4367-dataset-authorisation-info#1.-licence-file,2702,415
JASMIN access,"Due to the limited number of linux groups that can be utilised to
control local (JASMIN) access and to enable systems to work properly
with the archive, it is not possible to implement the archive access
groups fully as jasmin groups. Instead there are mappings between some
archive access options and jasmin groups. Changes \_can\_ be made, but
will need to be discussed with the head of curation.
The following table describes the main JASMIN access groups in use:
  ----------------- -------------------------------------------------------------------
  **linux group**   **archive mapping/notes**
  open              \'public\' or \'reguser\'
  bacyl             used when \'groups\' being used and not mapped in category below 
  badcint           no download access
  cmip5_research    Selected CMIP datasets under open research use CMIP licence
  esacat1           ESA Category 1 datasets
  ecmwf             all ECMWF dataset groups
  eurosat           ESA satellite dataset groups 
  ukmo_wx           Met Office weather datasets under \`ukmo_wx\` and \`ukmo_wx_gov\`
  ukmo_clim         Met Office climate datasets - range of access groups
  ----------------- -------------------------------------------------------------------
",https://help.ceda.ac.uk/article/4367-dataset-authorisation-info#jasmin-access,1238,151
Adding a new licence.,"If you need to add a new licence then provide the following
-   Code - a lowercase, short code to be used to speed selection
-   title - a full verbose name for the title
-   url link - link to the licence URL in a reliable location (if not
    external then add to the artefacts server), preferably the link to
    the item on the artefacts server (see top of this page)
-   comment - a free comment if you wish 
-   categories - these are classifications for the types of permitted
    use that the licence permits - speak to Graham about this.
",https://help.ceda.ac.uk/article/4367-dataset-authorisation-info#adding-a-new-licence.,547,103
3. Set up CEDA userDB entries (for \'restricted data\'),"IF you have set up a new group in step 2.6c above you will also need to
set up the access application route in the CEDA UserDB as follows:
1.  log into <http://cedadb.ceda.ac.uk/admin/udbadmin/>
2.  under Datasets select \'add\'
3.  enter the new group as defined for 2.6c above in the \'datasetid\'
    and \'group\' fields. Fill all other fields in as per the table
    below:
      ----------------------------------- -----------------------------------
      datasetid                           Identifier for this entry in the
                                          table. This should normally be the
                                          same as the \'group\'.
      authtype                            Authentication type. The
                                          registration system recognises the
                                          values \'online\' or \'manual\'.
                                          The other values which have been
                                          used are \'public\' if the group
                                          has been made public following a
                                          period of restricted access,
                                          \'none\' if the dataset has been
                                          removed, \'internal\' if this group
                                          will only be assigned manually by
                                          the BADC team, or \'badc\' (?? can
                                          remember what this represents ??).
      group                               This is the group name which is
                                          used to control access to the
                                          dataset. Ideally this should be the
                                          same as the datasetID (to reduce
                                          confusion). This will be the name
                                          that you use in the .ftpaccess
                                          file(s).
      description                         Text description of dataset that
                                          will appear on various forms (some
                                          of which will be seen by users).
                                          This should be short and
                                          meaningfully convey what the
                                          application will get access to
      directory                           not used any more
      conditions                          URL of the PDF file containing the
                                          licence to be used in the
                                          applicaiton process.
      defaultRegLength                    Default number of months before
                                          users dataset registration will
                                          expire. Value can be modified when
                                          dataset is approved.
      dataCentre                          \'badc\' or \'neodc\' (not used)
      infourl                             This is a url which points to a
                                          page that gives information about
                                          what this group provides access to.
                                          This will appear as a link in the
                                          \'my groups\' section of MyCEDA and
                                          may be used in other places in the
                                          future. If there is a one-to-one
                                          relationship between this group and
                                          a dataset then this page could be
                                          the dataset page. However, if there
                                          is a more complicated relationship
                                          (eg. only providing access to part
                                          of a dataset) then you should
                                          create a page explaining how this
                                          group works and set infourl to
                                          point to it.
      Associated linux group id           not used anymore
      Check for public key                not used any more
      comments                            Any comments, for internal
                                          documentation purposes only.\
                                          \
      ----------------------------------- -----------------------------------
4.  Select \'save\' 
5.  If authtype was set as \'Manual\' then go to
    <http://cedadb.ceda.ac.uk/admin/udbadmin/privilege/>. Here is where
    to set the external authoriser for the resource - for this you will
    need to know the CEDA user account of the authorisers for the
    data\.... or CEDA helpdesk if this is to be used
6.  Select \'add privilege\'
7.  look up the user for the \'userkey\' using the looking glass icon to
    the right of the box and searching in the pop-up dialogue
8.  select type to be \'authorise\'
9.  Select the dataset ID that you have set up as per step 3 above
10. add a comment and save
This will then enable you to test the application system. To do this use
the following URL (replace the \<datasetid> with the one you\'ve set up:
[https://services.ceda.ac.uk/cedasite/resreg/application?attributeid=](https://services.ceda.ac.uk/cedasite/resreg/application?attributeid=metop_iasi)\<datasetid>
Fill in the application and submit. This will send the first
\'authorisation\' email off to the external authoriser.
",https://help.ceda.ac.uk/article/4367-dataset-authorisation-info#3.-set-up-ceda-userdb-entries-(for-\'restricted-data\'),5771,541
4. Set up Catalogue (MOLES) \'permission/Constraint\' for datasets,"The final step is to then make sure that the access control and
licencing is conveyed to the user and provide the link to gain access if
required. This is done via the CEDA data catalogue (MOLES) using an
record called a \'Constraint\' object\... these are linked to Dataset
records (Observations) via the \'permission\' field.  
NOTE - Where possible, pre-existing \'Constraints\' should be used. For
example, for \'public\' access under the Open Goverment licence then the
Constraint object to use is:
[https://catalogue.ceda.ac.uk/admin/cedamoles_app/constraints/129/](https://catalogue.ceda.ac.uk/admin/cedamoles_app/constraints/129/change/?_changelist_filters=q%3Dpublic). 
If you\'re not sure of the licence at this point, don\'t worry - there
is a constraint object for that too!
",https://help.ceda.ac.uk/article/4367-dataset-authorisation-info#4.-set-up-catalogue-(moles) \'permission/constraint\'-for-datasets,787,103
4.1 Setting up a new Constraint object,"1.  go to
    <https://catalogue.ceda.ac.uk/admin/cedamoles_app/constraints/> and
    check for Constraints that already meet your requirements and re-use
2.  else click the \'add constraints\' button
3.  give a label that is meaningful to help search for the constraint
    object
4.  select access category to match that set in the securityDB
5.  IF access category is \'restricted\' then add in the Access Roles -
    i.e. the \'group\' set in step 2.6c
6.  Add a link to the licence file in the CEDA Artefacts service
7.  Select Save
",https://help.ceda.ac.uk/article/4367-dataset-authorisation-info#4.1-setting-up-a-new-constraint-object,538,83
4.2 Linking to Observation/Dataset record,,https://help.ceda.ac.uk/article/4367-dataset-authorisation-info#4.2-linking-to-observation/dataset-record,0,0
{#a3Adddatasetauthorisors},"1.  Open up Observation record to add \'constraint\' object to
2.  find the \'permission\' box and search for the constraint you
    require and select it\...
3.  save the Observation record
4.  check the user-view.. if record is not in \'preview\' there should
    be an appropriate button displayed - e.g. Apply for Access.
    Informaiton should also be under the \'details\' tab.
5.  If able to - check that the \'apply for access button does work 
",https://help.ceda.ac.uk/article/4367-dataset-authorisation-info#{#a3adddatasetauthorisors},453,74
Handling Erroneous Data and Data Versioning,"::: {#wikipage children-count=""0""}
[Original trac
page](http://team.ceda.ac.uk//trac/ceda/wiki/opman/dataErrorsVersioning)
::: wiki-toc
1.  [Handling Erronous Data and Data
    Versioning](#HandlingErronousDataandDataVersioning)
    1.  [Definitions](#Definitions)
    2.  [Handling data Corrections](#HandlingdataCorrections)
    3.  [Why keep the files?](#Whykeepthefiles)
    4.  [How to archive the old data?](#Howtoarchivetheolddata)
    5.  [How to deal with subsequent supply of corrected
        data](#Howtodealwithsubsequentsupplyofcorrecteddata)
:::
The notes that follow should be used for a common approach to coping
with minor and major updates to archived data following publication.
Using a common approach will allow for user familiarity with how we
handle such data in the archive and give us the potential for automating
checks in the archive and perhaps ingestion processes to cope with these
amendments from data providers.
",https://help.ceda.ac.uk/article/4364-handling-erronous-data-and-data-versioning,945,108
Definitions {#Definitions},"**When to use correction and versioning terminology** When a change has
occurred in the processing of the incoming data to warrant the creation
of a new Observation record (MOLES3 speak, MOLES2=new Deployment) then
this should be treated as a new **VERSION** - and thus the data in the
archive are held distinctly separate from earlier versions of the data.
However, when an error in the data already provided has been identified
(e.g. erroneously produced due to processing hiccup) and only a portion
of the dataset are to be re-issued then a **CORRECTION** is issued. The
following copes with how to handle corrections.
",https://help.ceda.ac.uk/article/4364-handling-erronous-data-and-data-versioning#definitions-{#definitions},622,102
Handling data Corrections {#HandlingdataCorrections},"Once alerted that erroneous data have been provided some initial steps
need to be taken:
-   establish if the data are already in the archive. If not then
    prevent these from being ingested.
-   If already in the archive, determine if these data have already been
    used or not. If not then consider removing access to these files
    (BUT RETAIN them!)
-   Liaise with the data provider to establish the nature of the error
    and how these will be corrected and over what time scale.
",https://help.ceda.ac.uk/article/4364-handling-erronous-data-and-data-versioning#handling-data-corrections-{#handlingdatacorrections},492,85
Why keep the files? {#Whykeepthefiles},"The files need to be kept as they may have been already used-in-anger,
thus the data are already in circulation and results may have been made
available. Therefore the data must be retained to allow results to be
reproduced.
",https://help.ceda.ac.uk/article/4364-handling-erronous-data-and-data-versioning#why-keep-the-files?-{#whykeepthefiles},225,39
How to archive the old data? {#Howtoarchivetheolddata},"While the data are to be retained they must be demarcated to ensure that
users are aware that the data are subject to some error. To do this I
employed the following method:
-   Create a sub-directory called ""erroneous_files"" at a suitable
    location.\
> > for MST radar data products I created the following:
/badc/mst/data/mst-products-v3/st-mode/radial/erroneous_files
/badc/mst/data/mst-products-v3/st-mode/cartesian/erroneous_files
-   Within the erroneous_files sub-folder create a readme file to inform
    the user of the reason for the folder and to act as an index of
    sub-folders created therein, e.g.
  00README_error_details
  00README_error_details.
> > This file provides details of the contents of this directory. The
> > contents have been moved from the main directory structure to demark
> > these files as those known to suffer from a particular error.
> > These files are maintained within the archive to preserve file
> > traceability and provenance where these have already been made use
> > of within the user community and thus permit reproducibility of
> > reported results based on these erroneous files.
> > In addition, where corrections to these erroneous files have been
> > determined details are provided. Where possibile corrected versions
> > of these files have been produced by the data provider and placed
> > within the archive. Such files will be duly marked as corrections
> > with an appropriate suffix matching
the directory names used within this archive and detailed below.
Index of folders containing erroneous files.
*******************************************
correction_c1
*************
date range - 2007/02/06 - 2008/04/08 inclusive
see mst.nerc.ac.ukj/announce_20080416.html for further details
Owing to a bug in the new MST radar control and data acquisition system, there have been errors in the 
reported ranges and altitudes of all MST radar data products for the period from 13:44:03 UT on 6th 
February 2007 until 14:37:00 UT on 8th April 2008, inclusive.
3)  Create sub-directories to contain the data that need moving from the archive – including any sub-folders needed to reflect the archive structure. 
E.g. for the two sets of data above I have:
/badc/mst/data/mst-products-v3/st-mode/radial/erroneous_files/correction_c1/YYYY/MM/<files>
/badc/mst/data/mst-products-v3/st-mode/cartesian/erroneous_files/correction_c1/YYYY/MM/<files>
                A readme file can contain additional information about this correction, e.g. :
                                correction_c1
*************
date range - 2007/02/06 - 2008/04/08 inclusive
see mst.nerc.ac.ukj/announce_20080416.html for further details
Owing to a bug in the new MST radar control and data acquisition system, there have been errors in t
he reported ranges and altitudes of all MST radar data products for the period from 13:44:03 UT on 6
th February 2007 until 14:37:00 UT on 8th April 2008, inclusive.
A compensation for this problem was introduced into the signal processing software starting with the
                               cycle/dwell beginning at 12:48:15 UT on 7th February 2008. However, it turns out that this over-cor
rected the problem. Moreover, this meant that the data at the lowest three range gates were unreliab
le.
<and so on..>
-   Move files from the archive proper to the corrections sub-folder
    just created (and thus remove the data from the main part of the
    archive).
",https://help.ceda.ac.uk/article/4364-handling-erronous-data-and-data-versioning#how-to-archive-the-old-data?-{#howtoarchivetheolddata},3435,495
How to deal with subsequent supply of corrected data {#Howtodealwithsubsequentsupplyofcorrecteddata},"Hopefully the data supplier will provide new data to replace the ones
that have just been demarcated as having some issue. These should be
ingested into the archive proper, but with a suffix to clearly mark
these files out as being a) new, b) different and c) related to the
holding in the erroneous_files directory. E.g. for the above files a
suffix of ""\_c1"" will be added to the new files.
Using a standard approach we should then be able to script some of the
steps required to deal with such files and also when a data provider
sends through replacement data without informing us (I propose that the
ingest step checks the archive for data that may already exist and takes
appropriate action -- e.g. move the newly arrived data to a holding
directory and informing the data scientist of the new data).
:::
",https://help.ceda.ac.uk/article/4364-handling-erronous-data-and-data-versioning#how-to-deal-with-subsequent-supply-of-corrected-data-{#howtodealwithsubsequentsupplyofcorrecteddata},811,145
Arrivals module,"In ingest lib there is an arrivals module to list files from the
arrivals area or elsewhere. This list is used as input for ingest
processes, for example by the ingester script.
Arrivals options are:
  ----------------------------------- --------------------------------------------------------
  arrivals_dirs\                      A space separated list of directories to search for
                                      files to ingest.
  arrivals_users\                     A space seperated list of usernames. This list is mapped
                                      to a list of directories that are appended to the
                                      arrivals_dirs list. The directory mapped to for a user
                                      is of the form
                                      /datacentre/arrivals/users/\<username>/\<streamname>. 
                                      For example, if arrivals_users = spepler gparton, and
                                      the stream name is xxx then the list of directories
                                      searched are /datacentre/arrivals/users/spepler/xxx and
                                       /datacentre/arrivals/users/gparton/xxx.
  arrivals_wait\                      Don\'t add files that are modified less than this number
                                      of seconds ago. This is a simple measure used to not
                                      include any file that is still being uploaded. Defaults
                                      to 500 (\~8 mins) . 
  arrivals_maxfiles\                  If the number of files exceeds this number then stop
                                      adding to the list. Defaults to 10000
  arrivals_ignore_regex\              Skip files that match this regex. For
                                      example, arrivals_ignore_regex = \\.tmp would ignore all
                                      files with .tmp extentions.
  arrivals_include_regex\             Only include files that match this regex. For
                                      example, arrivals_include_regex = \\.nc would only
                                      include files with .nc extentions.\
  ----------------------------------- --------------------------------------------------------
",https://help.ceda.ac.uk/article/4656-arrivals,2302,203
Using Arrivals in scripts,"There are two pertinent  methods in the arrivals module. One to make the
arrivals directories if they do not already exist, and another to make
the list of files. If the config contains this option:
    [streamX]
    arrivals_users =spepler
Then code using this would be of this form:
    from ingest_lib import Arrivals 
    A = Arrivals()             # should pick up options from config
    A.make_arrivals_dirs()     # make /datacentre/arrivals/users/spepler/streamX<br>
    file_listing = A.arrivals_files()
    for file_path in filelisting:
        ...
",https://help.ceda.ac.uk/article/4656-arrivals#using-arrivals-in-scripts,559,73
"Using tape for archive data {#using-tape-for-archive-data children-count=""0""}","For infrequently used, large data continual access online may not be the
best option, but use of tape for archive purpose should be considered
instead. For this we can explore the use of the Near Line Archive (NLA).
NOTE - see link at the end for external archive users wishing to access
data that have been archived in this way.
","https://help.ceda.ac.uk/article/4987-nla#using-tape-for-archive-data-{#using-tape-for-archive-data-children-count=""0""}",330,60
"What is the NLA {#what-is-the-nla children-count=""0""}","The Near-Line Archive (NLA) is where data are stored when they cannot be
kept on disk for the CEDA archive. The main users for this system are
Sentinel datasets which are too large to all be kept on disk, other
datasets which are not used frequently have been added as well. 
","https://help.ceda.ac.uk/article/4987-nla#what-is-the-nla-{#what-is-the-nla-children-count=""0""}",276,51
"When to use it {#when-to-use-it children-count=""0""}","If the data isn\'t frequently used and large in size is the best time to
use the NLA system. This is usually a decision made between data
scientists and Sam. 
","https://help.ceda.ac.uk/article/4987-nla#when-to-use-it-{#when-to-use-it-children-count=""0""}",159,30
"How to use it {#how-to-use-it children-count=""0""}","To move data to tape you can move over whole filesets which have been
created in the cedaarchiveapp (
<https://cedaarchiveapp.ceda.ac.uk/admin/cedaarchiveapp/fileset/>). You
have to ensure that the fileset that you want to be moved to the NLA is
ticked for \'Sd backup\' as well as \'primary on tape\'. Only files over
a certain size (find file size) will be moved from disk to tape. Once a
fileset has been ticked for tape it will be verified that the file on
disk is the same as the file on tape before the NLA system will remove
it from disk to only be on tape. There are some exceptions such as
Sentinel 1 products which are not fully verified, these just make sure
there is a record of the file on tape before removing it.
The admin interface for the NLA system can be found here:
<https://nla.ceda.ac.uk/admin/>
","https://help.ceda.ac.uk/article/4987-nla#how-to-use-it-{#how-to-use-it-children-count=""0""}",818,144
Guidance for Users,"The users guide is at <https://help.ceda.ac.uk/article/265-nla>
This will run through how you can pull data back from tape. Currently,
only JASMIN users can pull data back from tape. 
",https://help.ceda.ac.uk/article/4987-nla#guidance-for-users,184,28
Data Management Plans Overview,"::: {#wikipage children-count=""0""}
\
The purpose of a data management plan (DMP) is to set up a coherent
approach to data issues pertaining to a particular programme of work and
act as a living document bringing clarity of roles and scope of data
management work undertaken by CEDA Archive to support the data providing
project. The data management objectives are to ensure that :
-   A high quality documented data archive is created.
-   Appropriate data support is provided to the data users and creators.
-   Data are made available to users in a timely fashion.
-   Academic credit for data creation is given.
-   Conditions of use, access and deposit are clearly stated and do not
    infringe on the data creators' rights.
-   Potentially scientifically valuable data are kept for reuse in the
    long-term and by other disciplines.
-   Results can be checked and validated.
In order to fulfil this objective the DMP needs to:
-   be an agreed record of the data management needs and issues within a
    programme of work.
-   define who is responsible for data management activities both within
    data centres and by the data creators.
-   list the expected data products that will need management and
    provide a mechanism for recording/agreeing changes to the final
    archived products
-   list the expected data inputs of the programme
-   lay out the expected timetable and key dates.
-   list particular issues, such as volume of large volumes or sensitive
    information
-   clearly state the conditions of use, access and deposit
-   briefly describe the context of the programme
The DMP will need be tailored for different types of data and to
encompass any unusual aspects of particular datasets. Information may
need to be gathered from several sources including the data providers,
project PIs and CEDA support staff. 
",https://help.ceda.ac.uk/article/4383-data-management-plans,1846,301
DMPs and the Data Lifecycle,"For NERC Grants applicants will submit an Outline Data Management Plan
(ODMP) as part of the grant application. If successful details of the
grant award will be made available in the DataMAD system and assigned to
the most relevant NERC Data Centre. Details of likely data products from
the grant award are also recorded within the DataMAD system. From the
details in the DataMAD system a draft Data Management Plan (DMP) can
then be generated.  See DataMAD workflows for further information.
For non-NERC Grant awards ODMPs are not produced, nor details entered
into the DataMAD system. In such cases the first point of contact is via
the Data Management Helpdesk with subsequent details entered into the
JIRA data management tracking tool. Details are use from these entries
to produce the required DMP.\
\
The project PI/steering committee will review the initial DMP template
",https://help.ceda.ac.uk/article/4383-data-management-plans#dmps-and-the-data-lifecycle,880,143
DMP Templates,"Outline DMPs are often required to be submitted to project steering
committees or planning groups in order to outline the role played by the
CEDA Archive in the project. A paragraph/couple of lines covering most
or all of the following sections will be needed.
To ensure that all aspects of data management plans are covered, and to
reduce unnecessary administrative burden, DMP templates exist to aid
production of DMPs. The DataMAD tool will allow production of draft DMPs
from available templates for data producing projects from NERC grant
awards. For other data obtain a copy of the required DMP template and
edit as needed (there is not automated workflow to produce these from
JIRA at present).
:::
",https://help.ceda.ac.uk/article/4383-data-management-plans#dmp-templates,706,117
Ingest Servers,"::: {#wikipage}
\
::: wiki-toc
1.  [Ingest Servers](#IngestServers)
    1.  [Data being PUSHED to CEDA](#DatabeingPUSHEDtoCEDA)
    2.  [Data being PULLED to CEDA](#DatabeingPULLEDtoCEDA)
    3.  [Scheduling ingestions
        processes](#Schedulingingestionsprocesses)
2.  [General ingest software](#Generalingestsoftware)
    1.  [fileProcessor.py](#fileProcessor.py)
    2.  [fromdeliveries.py](#fromdeliveries.py)
    3.  [](#a)
    4.  [The deposit client](#Thedepositclient)
    5.  [Arrivals deleter](#Arrivalsdeleter)
:::
This page outlines the Ingest Servers and the ingest processes to be
used.
Presently the ingest servers are: ingest1.ceda.ac.uk (on JASMIN/CEMS)
",https://help.ceda.ac.uk/article/4245-ingest-servers,675,63
Data being PUSHED to CEDA {#DatabeingPUSHEDtoCEDA},"As detailed on
[opman/arrivals](http://ceda-internal.helpscoutdocs.com/article/4354-arrivals-machine){.wiki}
there is an Arrivals machine to which all data being pushed to CEDA
arrives. On that server data is put into dataset subfolders which appear
under the /datacentre/arrivals top level folder on the Ingest servers.
",https://help.ceda.ac.uk/article/4245-ingest-servers#data-being-pushed-to-ceda-{#databeingpushedtoceda},321,39
Data being PULLED to CEDA {#DatabeingPULLEDtoCEDA},"When datasets are updated by CEDA staff pulling data into CEDA and/or
placing onto the system from another media (e.g. external hard disk)
then these should be initially placed within dataset\'s subfolder of
/datacentre/processing.
Under /datacentre/processing3  the dataset directory name should match
those used within the archive itself or the stream name, e.g. use
ukmo-rad instead of radios for UK Met Office global radiosonde data.
Beyond the /datacentre/processing3/\<dataset dir>/ directory the data
scientist may structure the directory as desired.
Add a 00INFO.txt file to the processing directory so that people know
the owner and purpose of the directory.
",https://help.ceda.ac.uk/article/4245-ingest-servers#data-being-pulled-to-ceda-{#databeingpulledtoceda},668,98
Scheduling ingestions processes {#Schedulingingestionsprocesses},"A command line utility is provided to control all ingest scripts -
[ingest_control](http://ceda-internal.helpscoutdocs.com/article/4272-ingest-control){.wiki}
. This is the only method to schedule ingest jobs, which enabling us to
have a complete ingest list.
",https://help.ceda.ac.uk/article/4245-ingest-servers#scheduling-ingestions-processes-{#schedulingingestionsprocesses},260,32
General ingest software {#Generalingestsoftware},"Many ingest operations are common across ingest streams.
",https://help.ceda.ac.uk/article/4245-ingest-servers#general-ingest-software-{#generalingestsoftware},57,8
fileProcessor.py {#fileProcessor.py},"If renaming, zipping and/or tarring operations need to happen during the
ingest process then the fileProcessor.py script may be of use. This is
set up to work with the new ingest server and /processingCache set up in
mind, but could be applied to other file processing operations
elsewhere.
For details of how to use this script see [these
pages](http://ceda-internal.helpscoutdocs.com/article/4326-fileprocessorpy){.wiki}
",https://help.ceda.ac.uk/article/4245-ingest-servers#fileprocessor.py-{#fileprocessor.py},423,59
fromdeliveries.py,"*[ Deprecated ]{.underline}* : Still available on glacial. For ingesting
files into the archive use the fromdeliveries.py script. For more
details see [these
pages?](http://team.ceda.ac.uk//trac/ceda/wiki/opman/fromDeliveries){.missing
.wiki} .
",https://help.ceda.ac.uk/article/4245-ingest-servers#fromdeliveries.py,245,26
{#a},"On ingest1 a new deposit service is in place to separate archive deposit
actions from ingest processing actions. This ensures consistency of the
archhive and carries out some simple QC checks.
There are 2 ways to use this service: via a client library or via a
command line tool.
",https://help.ceda.ac.uk/article/4245-ingest-servers#{#a},280,49
The deposit client {#Thedepositclient},"Data should copied to the archive via the deposit server. This maintains
the correct permissions, ownership and performs logging. To use this
service you can use a command line tool or import a deposit client
library into an ingest script.
Use these lines to import the client and make an instance to use.
from deposit_client import DepositClient
D = DepositClient()
use a deposit client like this:
D.deposit(rpath, readmepath)           # add a file to the archive
D.rmdir(self,d)                        # remove a directory in the archive
D.remove(self,f)                       # remove a file in the archive
D.deposit(self, src, dst, force=True)  # make a deposit with forced overwrite in archive.
D.symlink(self, linkto, linkname)      # make a sym link in the archive
D.makedirs(self,d)                     # recursively make directories
D.recursive_deposit(self,src, dst)     # recursive deposit from src to dst
Command line tool
(venv27)[badc@ingest1 ~]$ cedaarchive -h
CEDA Archive deposit command line script for interactive interaction with the archive.
usage: /usr/local/ingest_software/venv27/bin/cedaarchive [options] mkdir <dir>
           <dir> = directory to make in the archive
usage: /usr/local/ingest_software/venv27/bin/cedaarchive [options] makedirs <dir>
           <dir> = directory to make in the archive (including all parent directories)
usage: /usr/local/ingest_software/venv27/bin/cedaarchive [options] rmdir <dir>
           <dir> = directory to remove from the archive
usage: /usr/local/ingest_software/venv27/bin/cedaarchive [options] remove <file>
           <file> = file or symlink to remove from the archive
usage: /usr/local/ingest_software/venv27/bin/cedaarchive [options] deposit <src> <dst>
           <src> = file to deposit
           <dst> = target destination in the archive for the file
usage: /usr/local/ingest_software/venv27/bin/cedaarchive [options] rdeposit <src> <dst>
           <src> = directory to deposit recursively
           <dst> = target destination in the archive for the directory
usage: /usr/local/ingest_software/venv27/bin/cedaarchive [options] symlink <linkto> <linkname>
           <linkto> = path in the archive to link to
           <linkname> = link name in the archive
-v verbose
-h this help
-t test
-f force overwiting of files that exist in archive
",https://help.ceda.ac.uk/article/4245-ingest-servers#the-deposit-client-{#thedepositclient},2322,287
Arrivals deleter {#Arrivalsdeleter},"-   from Python:
> > import sys
> > sys.path.append(\""/home/badc/software/infrastructure/arrivals-deleter/\"")
> > import arrivalsDeleter d = arrivalsDeleter.
> > [ArrivalsDeleter?](http://team.ceda.ac.uk//trac/ceda/wiki/opman/ArrivalsDeleter){.missing
> > .wiki} () d.delete(filename1) d.delete(filename2) \... d.close()
-   from the command line
<!-- -->
-   (CAUTION: run this as user \""badc\"", otherwise the file
    /home/badc/.fileOpsLogin won\'t be read and the program exits)
> > /home/badc/software/infrastructure/arrivals-deleter/arrivalsDeleter.py
> > filename1 \[filename2 \...\]
-   from python, an argument to the constructor: d =
    arrivalsDeleter.ArrivalsDeleter(login_file_name)
<!-- -->
-   from the command line, the flag -l login_file_name
> > import sys
> > sys.path.append(\""/home/badc/software/infrastructure/arrivals-deleter/\"")
> > import arrivalsDeleter d = arrivalsDeleter.
> > [ArrivalsDeleter?](http://team.ceda.ac.uk//trac/ceda/wiki/opman/ArrivalsDeleter){.missing
> > .wiki} () d.rename(oldfilename,newfilename) \... d.close()
Arrived files can be read on machines in group \@badc_ingest (currently,
glacial) as /arrivals is mounted there (as well as /.arrivals and
/disks/tropical/\* so that the symlinks work as expected). However, the
directories are quite deliberately exported read-only. This is to ensure
that if the ingest machines do any processing of the files before
ingest, they do *not* store it back on the arrivals machine, but should
do so locally instead. This means that there needs to be a way for
ingest processes to delete files which are finished with. This is
provided by means of a python module which connects to the file
operations server.
The arrivals deleter lives
[here](http://team.ceda.ac.uk/trac/ceda/browser/ceda_software/arrivals-deleter/trunk)
in subversion, and is installed at
/home/badc/software/infrastructure/arrivals-deleter/ .
To use it:
(If you specify multiple filenames, these are all deleted in a single
session, which is slightly more efficient.)
Note that the module uses the file /home/badc/.fileOpsLogin which
contains login details for the file operations server. This is only
readable by user badc . If you want to run the deleter from another
username, then make an alternative copy of this file (similarly
protected please), and override the default filename by using:
Note also that the command line interface supports the flag \"" -i \"" to
make it prompt interactively for each file.
The arrivals deleter has now been extended to include a rename function.
This now means that files on the arrivals area on tropical can be moved
to different directories, provided that the destination directory has
been created in advance. The syntax is :
:::
",https://help.ceda.ac.uk/article/4245-ingest-servers#arrivals-deleter-{#arrivalsdeleter},2730,365
Earth Observation Data at CEDA,"CEDA hold a wide range of Earth observation data, from satellite to
airborne data and digital terrain model data. This article will
hopefully direct most people to relevant data that we hold and a few
hints and tips along the way. However, if you still can\'t find the data
that you need, please free to get in touch and we\'ll do what we can to
assist.
",https://help.ceda.ac.uk/article/236-earth-observation-data-at-ceda,354,66
,,https://help.ceda.ac.uk/article/236-earth-observation-data-at-ceda#,0,0
Satellite Data,"[Sentinel](http://catalogue.ceda.ac.uk/search/?return_obj=by_search_obj&search_term=sentinel&search_obj=coll)
[](http://catalogue.ceda.ac.uk/search/?return_obj=by_search_obj&search_term=sentinel&search_obj=coll)[Landsat](http://catalogue.ceda.ac.uk/search/?return_obj=by_search_obj&search_term=landsat&search_obj=coll)
[AATSR](http://catalogue.ceda.ac.uk/search/?return_obj=by_search_obj&search_term=aatsr&search_obj=coll)
",https://help.ceda.ac.uk/article/236-earth-observation-data-at-ceda#satellite-data,423,3
Airborne Data,"[ARSF](http://catalogue.ceda.ac.uk/uuid/55d1c9b6e7a4ce41b7a6f8416b7b6261)
[NEXTMap](http://catalogue.ceda.ac.uk/uuid/8f6e1598372c058f07b0aeac2442366d)
",https://help.ceda.ac.uk/article/236-earth-observation-data-at-ceda#airborne-data,151,2
Surface Orography Data,"[NEXTMap](http://catalogue.ceda.ac.uk/uuid/8f6e1598372c058f07b0aeac2442366d)
[Landmap](http://catalogue.ceda.ac.uk/search/?return_obj=by_search_obj&search_term=landmap&search_obj=coll)
",https://help.ceda.ac.uk/article/236-earth-observation-data-at-ceda#surface-orography-data,185,2
Find Satellite data,"[Satellite Data finder](http://geo-search.ceda.ac.uk/)
",https://help.ceda.ac.uk/article/236-earth-observation-data-at-ceda#find-satellite-data,55,3
When records are reviewed (needs editing for the new version of the review form!),"A record should be reviewed prior to it being \""published\"" (Obs, Obs
Collections, Projects) or as soon as possible after it has been worked
up (Instrument, Platform, Computation).
",https://help.ceda.ac.uk/article/4545-catalogue-reviews#when-records-are-reviewed-(needs-editing-for-the-new-version-of-the-review-form!),181,28
Requesting a review,"Send an email to 
[data.management\@ceda.ac.uk](mailto:data.management@ceda.ac.uk?subject=MOLES%20Review%20Request)
stating:
-   the record type (model\|atmos\|satellite\|eo\|other)
-   URL to the user view of the record to review
-   what to review (e.g. just that record or connected ones)
-   deadline if urgent
",https://help.ceda.ac.uk/article/4545-catalogue-reviews#requesting-a-review,315,37
Who Reviews what?,"Don\'t review your own record! It\'s very easy to miss things and make
assumptions. Ideally the review should be done by a data scientist
covering a different type of data, e.g. someone responsible for model
data could look at satellite data or observation data; aircraft data
should be reviewed by someone looking after model/satellite data etc.
",https://help.ceda.ac.uk/article/4545-catalogue-reviews#who-reviews-what?,347,56
Reviewing a record,"There will mostly likely be a series of records to review. Start with a
sample Observation record, then review connected Project and Observation
Collection records before looking at the Procedure. If the sample passes
OK, review other observation records - if common issues arise then
there\'s no need to repeat that for all the records! Just flag that up
to the requester for amendment before reviewing further records.
Keep in mind that the records should be understandable by someone at
undergraduate level with a reasonable awareness of the field.
[Watch a video on reviewing records](http://somup.com/cbXjl5V39c)
-   Open up the record to review
-   Open up the [MOLES Review Google
    Form](https://docs.google.com/forms/d/e/1FAIpQLScZXVw4m5e08WL4JFIUGtl9B2ohw3Pn2_a8wFXXmbr1WSDIqA/viewform)
    -   Add in the user view link to the record
    -   Add in the title
    -   Select the record type
    -   Review fields requested
        -   If Observation record also check the archive location (is it
            at the right point? does it resolve?)
        -   Note, feel free to make small amendments (e.g. typos, adding
            in missing Priority numbers for Parties)
        -   Parties - ask for full names if possible, check it\'s the
            right party type
    -   Make a decision on the next step for the record owner
        -   Return for review - if substantial changes are needed (e.g.
            title, abstract fields need overhaul, links to other records
            needed)
        -   Minor changes and self-publish - small changes (typos,
            update to temporal/geographic areas)
",https://help.ceda.ac.uk/article/4545-catalogue-reviews#reviewing-a-record,1626,230
What next?,"-   When you receive the review email follow the link in the email.
-   Address issues raised, updating the review with initialled comments 
-   Re-submit for review if requested or publish the record
",https://help.ceda.ac.uk/article/4545-catalogue-reviews#what-next?,201,33
Common Issues and what to do,"-   duplicate Party used
    -   Making sure the one to keep is as complete as possible
    -   Flag to Graham which to keep, which to destroy (!), he has a
        script to \""merge\"" party records
-   Other duplicate record used (e.g. instrument, platform)
    -   Flag to Graham - we need to carefully manage the migration of
        content here before destroying the duplicate record!
-   Linked record is re-used by other records (e.g. a Project record
    from an Observation, or a geographic extent) - can we change it?
    -   Check which other records it is linked to and the potential
        impact of your desired change! 
    -   If they would all benefit fro the change and it won\'t affect
        fields such as the citation string (i.e. publisher, author
        names) then make the change
    -   Otherwise consider \""cloning\"" the record in question, modifying
        it and making use of that to isolate the change
",https://help.ceda.ac.uk/article/4545-catalogue-reviews#common-issues-and-what-to-do,938,150
HDF,"![](https://www.hdfgroup.org/wp-content/uploads/2016/09/logo-3-small.png)
",https://help.ceda.ac.uk/article/4425-hdf,74,1
HDF: Hierarchical Data Format,"*This is intended as a short introduction to the HDF data format. The
complete HDF documentation can be found at
[http:/www.hdfgroup.org](https://www.hdfgroup.org/).*
------------------------------------------------------------------------
",https://help.ceda.ac.uk/article/4425-hdf#hdf:-hierarchical-data-format,240,22
Background,"HDF (Hierarchical Data Format) is a file format for sharing data in a
distributed environment. It is orientated towards scientific data. HDF
was developed at the National Center for Supercomputing Applications
[NCSA](http://www.ncsa.illinois.edu/) at the University of Illinois at
Urbana Champaign.
In 2006, the HDF Group ([THG](https://www.hdfgroup.org/)) was formed as
a not-for-profit corporation whose mission is to sustain the HDF
technologies and to support worldwide HDF user communities with
production-level software and services. It is a spin-off from the HDF
group at NCSA. Their aim is to maintain the HDF libraries and tools. The
project is open source, meaning that the libraries and utilities are
freely available.
",https://help.ceda.ac.uk/article/4425-hdf#background,730,106
**What is HDF?**,"At its lowest level, HDF is a physical file format for storing
scientific data. At its highest level, HDF is a collection of utilities
and applications for manipulating, viewing, and analyzing data in HDF
files. Between these levels, HDF is a software library that provides
high-level APIs and a low-level data interface.
The HDF package provides utilities for a non-sequential access to
various records within that data base. HDF files are completely portable
across computer architectures and operating systems. Integer and
floating point numbers are stored within HDF files as binary streams of
data, which shrinks the size of the file very significantly and which
speeds reading and writing of such files. Also, the data are stored in
an architecture independent manner, while at the same time remaining
fully compliant with IEEE specifications, and HDF provides its own
facilities for data compression, allowing it to be automatically
compressed if required (this compression is also architecture and
operating system independent).
The latest version (HDF5) had been developed to address the growing
issues associated with storing, accessing and sharing complex and
volumous data from a diverse range of backgrounds. In particular, it
offers:
-   Unlimited size, extensibility, and portability
-   General data model
-   Flexible, efficient I/O
-   Flexible data storage
-   Unlimited variety of datatypes
-   Data transformation and complex subsetting
-   Supported on many platforms with API\'s for the most common
    languages.
HDF5 was created to address the data management needs of scientists and
engineers working in high performance, data intensive computing
environments. As a result, the HDF5 library and format emphasize storage
and I/O efficiency. For instance, the HDF5 format can accommodate data
in a variety of ways, such as compressed or chunked. HDF5 has a number
of advantages over other common data formats, and is widely used.
",https://help.ceda.ac.uk/article/4425-hdf#**what-is-hdf?**,1954,299
Why use HDF?,"-   To share data in heterogeneous computing environments
-   To share scientific data with other scientists
-   To use available software that understands HDF
-   To improve I/O performance, data can be read and written efficiently
    on parallel computing systems
-   To improve storage efficiency, data can be compressed or chunked
-   HDF software is written in a number of languages including C and
    FORTRAN, with high-level APIs using IDL, XML and Java.
-   NCSA maintains a suite of free, open source software.
The [HDF-EOS](http://hdfeos.org/) project: In 1993 NASA chose HDF to be
the standard file format for storing data from the Earth Observing
System (EOS), which is a data gathering system of sensors (mainly
satellites) supporting the Global Change Research Program. This is still
supported and maintained by NCSA.
",https://help.ceda.ac.uk/article/4425-hdf#why-use-hdf?,834,130
What\'s in the HDF file?,"There are several types of HDF object; rasters, palettes, tables, and
multi-dimensional arrays. The latest version of HDF (HDF5) can store two
primary objects; datasets and groups. A dataset is essentially a
multi-dimensional array of data elements, and a group is a structure for
organising objects in an HDF5 file.
",https://help.ceda.ac.uk/article/4425-hdf#what\'s-in-the-hdf-file?,317,50
How to read HDF?,"Some pointers to HDF tools are available from the HDF Group website at:
> <https://support.hdfgroup.org/tools/>
The [NCAR Command Language](http://www.ncl.ucar.edu/) can read and write
HDF4. More details available at:
> <http://www.ncl.ucar.edu//Document/Manuals/Ref_Manual/NclFormatSupport.shtml#HDF>
",https://help.ceda.ac.uk/article/4425-hdf#how-to-read-hdf?,302,30
Supported Formats,"The CEDA Archive cannot support a multitude of file formats because we
need to be able to have systems that can parse both the internal
metadata and data so as to make them available to services (e.g.
catalogues and visualisation/subsetting services). We are long past the
point where we can operate as a big file store and just rely on a
filename convention and a file hierarchy. So there is a very real and
major cost with every new format. For this reason it is important
clearly identify our supported formats, list the reasons why they have
been selected, and give guidance on metadata standards. Our default
policy should be that new formats are not acceptable unless:
-   it is not possible for the data creators to encode their data
    satisfactorily in one of the formats we already support, or
-   it is not possible for the data users to easily manipulate the data
    in one of the formats we already support, or
-   the data are being provided in a specific format because it conforms
    to the requirements of a major international programme.
-   The format is becoming a common community standard. 
CEDA should take a leadership responsibility to make it easy to create
and use existing supported formats (e.g. by development of software like
`nappy`), and to encourage, cajole, and demand that existing supported
formats are used wherever possible. In the case of NERC programmes we
will insist supported standards are adopted, but even in programmes
outside our direct control we should advise and seek to influence the
choice of format. CEDA should actively review the formats its supports
for ingest, storage and access. The current policy is to ingest, store
and access data in the same format. A supported format should satisfy
all the requirements below: 
-   R1: Producers can make the files on most platforms with available
    tools. \[Ingest\] 
-   R2: format should be platform neutral. \[Ingest and use\] 
-   R3: Required file level metadata can be encoded. \[Ingest,
    preservation and use\] 
-   R4: Format compliance can be checked. \[Ingest\] 
-   R5: Format should be stable (changes are managed). \[Preservation\] 
-   R6: Format should be open (if not one has any software to read the
    data, user can write there own). \[Preservation\] 
-   R7: Format should be migratable (as improved formats appear the
    significant properties of the files can be transferred to new
    formats). \[Preservation and use\] 
-   R8: Format is in use by the data centre user community \[Use\]
",https://help.ceda.ac.uk/article/4953-format-review-procedure#supported-formats,2521,414
Methodology for examining a format,"All formats should be critically reviewed before they are accepted as a
format that is used in the archive. The methodology used in the review
is to form a panel of data scientists to examine the format. This panel
must have at least 3 people. One of the panel members is tasked with
presenting the case for and against the formats introduction to the list
of recommended formats. This person will have prepared the answers to
the questions below in advance of the meeting and have tried to create
and read data in the format. The requirements are examined in turn. The
result of the review is an indication of its suitability for particular
feature types and for which communities of users and producers. Also
recommendation on conventions and metadata encoding that should be
adopted.
To aid this process either a CEDA offiicer or the data provider should
first complete the [Format Review
Form](https://forms.gle/Dc9h81qiLUazzuuv6).
Question to be considered in the review:
R1: Producers can make the files on most platforms with available tools.
-   Who are the foreseen creators that would produce data in this
    format?
-   What are the foreseen feature types for the data?
-   What are their preferred computing platforms and tools for this
    community?
-   Are there existing tools to create files in this environment?
-   If not can they create the files with other tools available to them?
R2: format should be platform neutral.
-   Do the files have a big-endian, little-endian options?
-   If text format or the format has text fields, do they use a
    specified character set?
-   Is common software for reading the data available on common
    platforms?
R3: Required file level metadata can be encoded.
-   What metadata standards are used by the user community?
-   What are the requirements for file level metadata for ingest and
    use?
-   Are all of these metadata elements encodable into the format?
-   Are there established conventions for encoding the metadata into the
    format?
R4: Format compliance can be checked.
-   Are there tools to check file format compliance?
-   Are there tools to check metadata conventions compliance?
-   Is it feasible to develop checking tools?
R5: Format should be stable and managed (changes are managed).
-   Are changes to the format managed?
-   Historically what is the frequency of changes to the format?
-   Have changes been backwards compatible?
-   Have tools to read old version been preserved?
-   Who is actively managing the format\'s development?
R6: Format should be open (if not one has any software to read the data,
user can write there own).
-   Are there public domain document describing the format?
-   Are the documents sufficient to engineer software to read data in
    the file format?
-   How much effort would it be to engineer read software from scratch?
R7: Format should be migratable (as improved formats appear the
significant properties of the files can be transferred to new formats).
-   Are there any format specific encodings that would be hard to move
    to other formats. (e.g. coloured cells in excel)
R8: Format is in use by the data centre user community
-   What is the designated user community for this feature type and use
    metadata?
",https://help.ceda.ac.uk/article/4953-format-review-procedure#methodology-for-examining-a-format,3253,538
Review results,"The review results should be a document with the answers to the above
questions and a conclusion section which lays out if and in what context
the format is suitable (example below).
-   The format is suitable the following feature types: Gridded data
    such as model data.
-   The format is suitable the following producers: EO and Atmospheric.
    The tools available make this data only producible by technically
    literate.
-   The format is suitable the following users: Reads into common tools
    like Grads, xconv and IDL. Not suitable for spreadsheet users.
-   Recommended conventions: Use of already established CF conventions
-   The format should be a recommended format: YES
-   Comments:
-   (If applicable) When should this be review again? 2011
Once a format has been accepted it should be documented on the
appropriate data centre website with reasons for suitability, links to
external resources and, if not available suitably elsewhere, a
description of the format with details on production and use tools.
------------------------------------------------------------------------
",https://help.ceda.ac.uk/article/4953-format-review-procedure#review-results,1104,164
Current formats (under review),"A list of preferred formats for data deposit is given on this page:
<https://help.ceda.ac.uk/article/104-file-formats>.
Formats that have either been accepted as core formats or additional
formats that have undergone formal review, with the review outcome, are
given. Where a change in the format has been identified or there is a
reason for its deprecation a subsequent review should take place. Note,
some formats are required due to 3rd party suppliers as noted. These
correspond to data pulled in by CEDA as a facilitation mode with no
recourse for reformatting at this stage. Other potential common formats
(including ones identified from historic datasets predating the format
review procedure) have also been listed.
",https://help.ceda.ac.uk/article/4953-format-review-procedure#current-formats-(under-review),724,111
**Available Delivery Tools**,"Data providers can upload data to CEDA using a variety of tools. The
data scientist liaising with the provider will be happy to advise on the
most appropriate mechanism to use and will help to set the data
provision route.
  -------------------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- --------------- ------------------
  Delivery Route (click for details)           Types of data transfer is suitable for                                                                                                                                                                                            For Archiving   For GWS delivery
  [Via HTTP - the CEDA File uploader](#http)   Suitable for small scale data providers and short lived projects                                                                                                                                                                  y               n
  [FTP](#ftp)                                  Suitable for small - medium scale data uploads for suitable projects where RSYNC is not an option                                                                                                                                 y               y
  [RSYNC](#rsync)                              Our preferred delivery mechanism which is suitable for all types of data provision. This route is particularly suited to regular, automated data uploading and is especially useful for very large files and dataset transfers.   y               y
  -------------------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- --------------- ------------------
",https://help.ceda.ac.uk/article/142-sending-data-to-ceda#**available-delivery-tools**,2018,141
HTTP,"[]{#http .CMY_Link .CMY_Valid}
The file uploader service allows single files to be transferred to CEDA
for archive ingestion - <https://arrivals.ceda.ac.uk/>. Please ask your
CEDA liaison officer if you need to know which ingest stream to use.
",https://help.ceda.ac.uk/article/142-sending-data-to-ceda#http,244,36
FTP,"Any ftp client can be used to connect to CEDA\'s ftp arrivals server:
    ftp <userId>@arrivals.ceda.ac.uk
After logging in with your CEDA account credentials you will arrive at
your delivery area which will contain sub-directory for each data
\""stream\"" to which you are permitted to deposit data.  If you are
unable to locate the required sub-directory or are unsure which one to
use please contact your CEDA team liaison.
If possible, please use a temporary filename during transfer and rename
the file once the file has completed transferring to ensure that we are
able to distinguish partially and completely transferred files.
Please note - we ask depositors not to deposit any files in to their top
folder, but to always use one of the available sub-folders (Please see
the note above about  ingestion streams)
Details of how to use ftp are available
[here](https://help.ceda.ac.uk/article/280-ftp) (note, the general ftp
guide has been written using the download ftp server, as opposed to the
upload ftp server). 
",https://help.ceda.ac.uk/article/142-sending-data-to-ceda#ftp,1022,162
RSYNC,"Find your rsync password on the arrivals server (
<https://arrivals.ceda.ac.uk/>) by going to your deliver page. There is
a button labelled \""other upload methods\"" with details of the rsync
command to use. 
    rsync -av --password-file=<path to password file> <path to source> <ceda account id>@arrivals.ceda.ac.uk::<ceda_account id>/<TARGET_DIR>
For example
    rsync -av --password-file=mypasswordfile.txt data_dir fbloggs@arrivals.ceda.ac.uk::fbloggs/upload_dir
Notes:
1.  the double colon between the arrivals.ceda.ac.uk URL and the users
    account ID.
2.  The option to use a a file to hold the rsync account password is
    shown above - recommended for routine deliveries, but may not be
    needed for rsync transfers by hand.
3.  If the password file approach is used (see note 2) then the password
    file needs to be set as only user r/w privileges - i.e. it should
    show as having permissions: \""r-w\-\-\-\-\--\"". Otherwise a
    \""password file must not be other-accessible\"" error will occur.
4.  other rsync options can also be used e.g. --a for recursing down
    through the source
5.  If you want to rsync contents of a source directory over you need to
    add a trailing slash (\""/\"") to the source path. No trailing slash
    will be needed for the target directory path though
Please be aware, however, that RSYNC will carry out a full comparison
between the source and destination. Thus, if you wish to send only a few
files from your source to update the CEDA archive holdings then care is
needed to avoid unnecessarily transferring large parts of the source to
the CEDA system
Please note - we ask depositors not to deposit any files in to their top
folder, but to always use one of the available delivery sub-folders.
",https://help.ceda.ac.uk/article/142-sending-data-to-ceda#rsync,1752,273
Ingest from Group-workspaces/Project-spaces,"CEDA supports projects through shared storage spaces such as JASMIN
group workspaces or FTP project spaces. Users of these services should
understand that:
-   this is NOT the archive - placing data into these areas will not
    constitute having deposited data in the CEDA archive
-   the group-workspaces/project-spaces are NOT managed by CEDA and so
    content should be considered at risk
However, it is possible to prepare a dataset in these areas for eventual
ingestion into the archive. If you wish to do this please contact your
CEDA support officer in the first instance to discuss ingestion into the
archive as it may be possible to ingest directly from these areas.
",https://help.ceda.ac.uk/article/142-sending-data-to-ceda#ingest-from-group-workspaces/project-spaces,678,111
PP file format,"*A brief description of the PP format with links to software utilities
and code tables.*
",https://help.ceda.ac.uk/article/4424-pp-binary-format#pp-file-format,89,15
Introduction,"PP-format is a record-based binary format used in a number of datasets
archived in the CEDA archives. It is a Met Office proprietary format
mainly associated with Met Office products, though not exclusively.
CEDA holds a number of software utilities available for handling PP.
These including decoding, unpacking, subsetting, converting and
byte-swapping. Please see the [XCONV/CONVSH
pages](https://cms.ncas.ac.uk/tools-and-utilities/) for details on the
software available.
If you are familiar with Python then you could use either the Python
package [IRIS](https://scitools.org.uk/iris/), developed by the UK Met
Office to read PP files or
[CF-Python](https://cms.ncas.ac.uk/tools-and-utilities/) (and the
accompanying [CF-Plot](http://ajheaps.github.io/cf-plot/)) to read and
analyse PP files. 
",https://help.ceda.ac.uk/article/4424-pp-binary-format#introduction,799,100
Description of PP-format,"The official Met Office [Unified Model Documentation Paper
F3](https://artefacts.ceda.ac.uk/badc_datadocs/um/umdp_F3-UMDPF3.pdf)
describes the PP-format in some detail.
More information about tools and the pp-format can be found on the [NCAS
CMS site](https://cms.ncas.ac.uk/tools-and-utilities/).
",https://help.ceda.ac.uk/article/4424-pp-binary-format#description-of-pp-format,298,30
"NOTE - DO NOT PUBLISH THESE ARTICLES {#note---do-not-publish-these-articles children-count=""0""}","just click on the tick to save them  
","https://help.ceda.ac.uk/article/4624-00-template-more-detailed-ingestion-workflows#note---do-not-publish-these-articles-{#note---do-not-publish-these-articles-children-count=""0""}",38,8
"What are the data {#what-are-the-data children-count=""0""}","  ----------------------------------- -----------------------------------
  Collection Title/description:       
  MOLES collection page:              
  DMP link:                           \
  ----------------------------------- -----------------------------------
","https://help.ceda.ac.uk/article/4624-00-template-more-detailed-ingestion-workflows#what-are-the-data-{#what-are-the-data-children-count=""0""}",266,12
"Who is involved {#who-is-involved children-count=""0""}","::: {children-count=""0""}
  ----------------------- ---------------
  CEDA officer            Graham Parton
  Data provider contact   See DMP links
  ----------------------- ---------------
:::
","https://help.ceda.ac.uk/article/4624-00-template-more-detailed-ingestion-workflows#who-is-involved-{#who-is-involved-children-count=""0""}",193,17
"Ingestion process {#ingestion-process children-count=""0""}",,"https://help.ceda.ac.uk/article/4624-00-template-more-detailed-ingestion-workflows#ingestion-process-{#ingestion-process-children-count=""0""}",0,0
"Data arrivals route {#data-arrivals-route children-count=""0""}","Data are prepared by ##\# staff which they push to their arrivals areas
into various streams. Arrivals areas are:
 - . 
\- . 
Arrivals stream name(s): 
-   cfarr-radar : for all radar related products
","https://help.ceda.ac.uk/article/4624-00-template-more-detailed-ingestion-workflows#data-arrivals-route-{#data-arrivals-route-children-count=""0""}",201,34
"Data pre-ingestion handling {#data-pre-ingestion-handling children-count=""0""}","::: {children-count=""0""}
These use file-processor as automatic processes on the crontab.
:::
::: {children-count=""0""}
config details in:  `/home/badc/software/datasets//_fileProcessor.cfg`
:::
::: {children-count=""0""}
steam: \[cfarr-cloud-camera-unpacker-2\]
:::
::: {children-count=""0""}
description: unpacks cloud-camera-2 images ready for ingestion
:::
::: {children-count=""0""}
script: unpacker -c
/home/badc/software/datasets/chilbolton/chilbolton_fileProcessor.cfg -s
cfarr-cloud-camera-unpacker
:::
","https://help.ceda.ac.uk/article/4624-00-template-more-detailed-ingestion-workflows#data-pre-ingestion-handling-{#data-pre-ingestion-handling-children-count=""0""}",504,43
"Data Ingestion {#data-ingestion children-count=""0""}","Ingestion processes have to map between the filename components and
directory locations. To do this the ingestion is done via calls to the 
`cfarr_ingester.py` script. This script has a class that inherits from
the standard Ingester class. `cfarr_ingester.py` is then called for each
ingest stream as defined in
the `/home/badc/software/datasets/chilbolton/chilbolton_fromDeliveries.cfg` config
file
Runs on crontab.
","https://help.ceda.ac.uk/article/4624-00-template-more-detailed-ingestion-workflows#data-ingestion-{#data-ingestion-children-count=""0""}",417,55
"Post Ingestion work {#post-ingestion-work children-count=""0""}","None
","https://help.ceda.ac.uk/article/4624-00-template-more-detailed-ingestion-workflows#post-ingestion-work-{#post-ingestion-work-children-count=""0""}",5,1
"QC checks: {#qc-checks children-count=""0""}","Files are only checked by CFARR staff. No other checking done at this
stage. Format of files was confirmed to be CF-compliant ahead of setting
up automatic processes.
","https://help.ceda.ac.uk/article/4624-00-template-more-detailed-ingestion-workflows#qc-checks:-{#qc-checks-children-count=""0""}",167,28
"Common problems: {#common-problems children-count=""0""}","Standard issues around ingest only. 
","https://help.ceda.ac.uk/article/4624-00-template-more-detailed-ingestion-workflows#common-problems:-{#common-problems-children-count=""0""}",37,5
"Any other useful information need to know: {#any-other-useful-information-need-to-know children-count=""0""}","None.
","https://help.ceda.ac.uk/article/4624-00-template-more-detailed-ingestion-workflows#any-other-useful-information-need-to-know:-{#any-other-useful-information-need-to-know-children-count=""0""}",6,1
Observation Records,"::: {#wikipage}
[Original trac
page](http://team.ceda.ac.uk//trac/ceda/wiki/opman/MOLESEditorUserGuide/observation)
::: wiki-toc
",https://help.ceda.ac.uk/article/4187-observation-records,129,7
Page Contents,"1.  [Observation Records](#ObservationRecords)
:::
**Each Observation should describe a particular discrete part of the
archive, which in turn may be part of one or more data collections
described by an Observation Collection.**
**Also note: the publication of the dataset and the Observation record
should go hand-in-hand**
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------
  Field ( **bold =  Description            Checklist/\*Controlled vocab explanations/ *Notes*                                                      Y/N
  mandatory field**                                                                                                                                
  )                                                                                                                                                
  ----------------- ---------------------- ------------------------------------------------------------------------------------------------------- -----------------
  **Title**         Short text title for   Do you understand the title?\                                                                           
                    the dataset.           Is the title brief and simple?\                                                                         
                                           Does the title contain unexplained technical terms or acronyms?\                                        
                                           Does the title describe the resource rather than the activity/project which produced it?\               
                                           Is the title in sentence case?\                                                                         
                                           Is the title in English?\                                                                               
                                           Does the title contain non-standard characters (e.g. © ° ± ² ³ µ)?                                      
  **Abstract**      Short text only entry  Do you understand the abstract?\                                                                        
                    that will be the first Is the abstract written in plain English?\                                                              
                    information that users Does the abstract describe the resource rather than an activity/project which produced it?\             
                    will see about the     Do the first few sentences summarise the contents of the resource?\                                     
                    text.                  Does the abstract also explain \'Where\', \'When\', \'How\', \'Why\' and \'Who\' (if appropriate)?\     
                                           Does the abstract contain unexplained technical terms or acronyms?\                                     
                                           Does the abstract contain non-standard characters (e.g. © ° ± ² ³ µ)?                                   
  **Data Status**   Controlled vocab to    \* Completed\                                                                                           
                    indicate the status of \* Historic archive\                                                                                    
                    the data production    \* Obsolete\                                                                                            
                                           \* Ongoing\                                                                                             
                                           \* Planned\                                                                                             
                                           \* Required\                                                                                            
                                           \* Under development                                                                                    
  Publication State There is a drop down   \* Old\                                                                                                 
                    box for the            \* working - use this while preparing the record prior to dataset release\                              
                    publication state of   \* published - dataset details are available to users, but the content of the dataset and/or metadata   
                    the data in the        may still be changing\                                                                                  
                    archive (and also the  \* Citable - this is ONLY to be used for dataset that are static and thus have a DOI assigned to them   
                    Observation record).                                                                                                           
                    Options are                                                                                                                    
  Data publication  Date-Time of the data  *should be set once and NOT changed. Not to be used for DOI date!*                                      
  date              publication - i.e.                                                                                                             
                    moved publication                                                                                                              
                    status to                                                                                                                      
                    \""published\"" for the                                                                                                          
                    first time                                                                                                                     
  Latest Data       Date-time when the     *in due course this should hopefully be automated. Ignore this for the time being*                      
  Update            data were last updated                                                                                                         
                    in the archive                                                                                                                 
  Data update       Controlled vocab to                                                                                                            
  frequency         indicate the frequency                                                                                                         
                    by which the data are                                                                                                          
                    updated for this                                                                                                               
                    dataset.                                                                                                                       
  Permissions       Used to specify if the *Simply click through to edit the permissions. The drop down list will allow you to set the access type 
                    access constraint and  as: public, reguser or restricted. If it is a restricted dataset then you\'ll also need to give the     
                    usage permissions      access groups in the box underneath. \[hopefully we can drive this from the security DB and userDB in   
                    (licence) details      due course\]\                                                                                           
                                           NOTE - if you re-use one of the existing permissions be VERY careful about changing it as it may also   
                                           be used by other datasets!*                                                                             
  Keyword           freetext field to                                                                                                              
                    enter csv keywords                                                                                                             
  Input parameter                                                                                                                                  
  Language          Language of the        *default is English, but we could expand this to a controlled iso list in due course (ISO 639)*         
                    dataset                                                                                                                        
  Image details     link to an image to be                                                                                                         
                    used as the logo                                                                                                               
  GEMINI2 related   For exporting to NERC                                                                                                          
  domain/feature    DCS we need to specify                                                                                                         
                    the related                                                                                                                    
                    domain/feature. This                                                                                                           
                    needs to be a                                                                                                                  
                    controlled list and we                                                                                                         
                    will utilise defaults                                                                                                          
                    based on our MOLES2                                                                                                            
                    -\> NERC DCS export to                                                                                                         
                    fill this in                                                                                                                   
  Result            This is where to       *This should be unique so don\'t re-use any of the existing options. Instead either edit the existing   
                    specify the connection entry or create a new one if there isn\'t an existing entry. Specify the internal path to the point in  
                    to the data            the archive that is [ uniquely specified by this Observation record and this one alone! ]{.underline}   
                                           For resources not in the badc or neodc archives ask Graham for advice (there is work needed here)*      
  Data quality      A free text entry to   *Create a new one only. Most fields are pre-filled to conform to the standard - leave those as given.   
  statement         give a description of  Simply provide the required free text in the top box*                                                   
                    the quality of the                                                                                                             
                    data                                                                                                                           
  Data valid        This is for forecast                                                                                                           
  date/time         data to show the                                                                                                               
                    period for which the                                                                                                           
                    data are valid. Rarely                                                                                                         
                    (if at all?) used                                                                                                              
  Data Temporal     temporal range of the  *State date is required, but end date is optional. Leave the end date blank if the dataset is being     
  Extent            dataset in the archive added to. This should reflect the true date range in the archive, not the anticipated end date. \[note  
                                           - we should hopefully be able to drive this with FAtCat in due course\]. DON\'T reuse existing options  
                                           - create a new entry if needed*                                                                         
  Geographic Extent geographic bounding    *DON\'T reuse existing options - create a new entry if needed*                                          
                    box encompassing all                                                                                                           
                    the available data in                                                                                                          
                    the archived dataset                                                                                                           
                    to date                                                                                                                        
  Horizontal                                                                                                                                       
  Resolution                                                                                                                                       
  Vertical Extent                                                                                                                                  
  **Data Lineage**  Free text field to                                                                                                             
                    explain the providence                                                                                                         
                    of the data that we                                                                                                            
                    hold. E.g. produced by                                                                                                         
                    X, processed by Y who                                                                                                          
                    delivered to BADC for                                                                                                          
                    ingestion into the                                                                                                             
                    archive following                                                                                                              
                    standard checks.                                                                                                               
  Procedure         Select the appropriate *Below Acquisition there are Instruments and Platform (and details of mobile platform operations)       
                    type of procedure for  given. For Computations there is a way to link to external descriptors of the algorithm/model - e.g.    
                    this dataset:          CIM record, so only outline details are needed there.*                                                  
                    observational data are                                                                                                         
                    covered by                                                                                                                     
                    *Acquisitions* ;                                                                                                               
                    simulations covered by                                                                                                         
                    *Computations.                                                                                                                 
                    Composite* processes                                                                                                           
                    are used when there                                                                                                            
                    are 2 or more                                                                                                                  
                    computation or                                                                                                                 
                    acquisitions, or a                                                                                                             
                    combination thereof to                                                                                                         
                    be described (e.g.                                                                                                             
                    satellite data                                                                                                                 
                    collection described                                                                                                           
                    by acquisition, but                                                                                                            
                    processing algorithm                                                                                                           
                    described by                                                                                                                   
                    computation)                                                                                                                   
  Projects that     Select (or create) the *Only select more than one project if the data were produced for either:\                               
  data were         appropriate Project    i) multiple, unconnected projects (e.g. FAAM flights operated for two concurrent, unrelated projects)\  
  collected in      record(s) for which    ii) change in funding line for ongoing facility\                                                        
  support of        these data were        **Please use the Project\'s parent-child relationship to capture relationships such as                  
                    principally            programme-project-campaign and ONLY link to the relevant record!\                                       
                    collected/generated.   ** See this video for more info: [[ ]{.icon}                                                            
                                           https://www.youtube.com/watch?v=pfCVVXmLwPI](https://www.youtube.com/watch?v=pfCVVXmLwPI){.ext-link}*   
  Phenomena         This is the parameter  *for each entry there should not only be a title, but also a link to the controlled vocab list from     
  Measured or       list! This needs to be which this comes. E.g. air_temperature from the CF names list.*                                         
  Simulated         driven from FAtCat to                                                                                                          
                    complete this.                                                                                                                 
  Related           Other observations can *Note - this needs more work to allow the relationship to be specified, to cover: \""X superceeds Y\"";   
  Observation       be selected that this  \""B is a continuation of A\"".*                                                                          
                    one related to, apart                                                                                                          
                    from being in the same                                                                                                         
                    collection (that is                                                                                                            
                    covered by the                                                                                                                 
                    Observation                                                                                                                    
                    Collections referring                                                                                                          
                    to the files)                                                                                                                  
  Identifiers       Unless adding a DOI,   *the moles2 url is to ensure that users who find an reference in a paper to a MOLES2 record using the   
                    or a new abbreviation, URLs given then can confirm to themselves that they are on the equivalent MOLES3 page.*                 
                    please leave these as                                                                                                          
                    given.                                                                                                                         
  Parties           A list of              \* **Author** - party who contributed to the dataset authoring\                                         
                    people/organisations   \* **CEDA Officer** - internal CEDA person responsible for the record/resource in question\             
                    (called collectively a \* Curator - party responsible for curating the content (data centre)\                                  
                    \""party\"") involved    \* Custodian - party responsible for maintaining the resource (data centre)\                            
                    with the data creation \* Distributor - party responsible for distributing/providing access to the resource (data centre)\     
                    or curation.\          \* **Metadata Owner** - party maintaining this metadata record (data centre)\                           
                    Select (or create) a   \* Point of Contact - default is the data centre, but may be used in due course for a named person from 
                    named party from the   the author list too\                                                                                    
                    list, select their     \* **Publisher** - organisation that published the data first (most of the time one of our data         
                    role from the          centres, but could be external group where we are mirroring the data only)\                             
                    controlled vocabulary  \                                                                                                       
                    and, if more than one                                                                                                          
                    party carrying out                                                                                                             
                    that role, use the                                                                                                             
                    priority number to                                                                                                             
                    sequence them in this                                                                                                          
                    role.                                                                                                                          
  Notes             present and past news  *latest news item will be at the top of the user view, with past items listed in the main body of the   
                    items particular to    record*                                                                                                 
                    the record                                                                                                                     
  Online Resources  Online link to         Do resource locators include titles which are concise, accurate accounts of the resource in question?\  
                    relevant resource -    Do resource locators include descriptions which fully explain their purpose?\                           
                    e.g. documentation,    \                                                                                                       
                    WPS, TOOLS\            Permitted options to use here:\                                                                         
                    There are set          \                                                                                                       
                    categories that should \* documentation - any grey literature that is relevant to support the use of these data\               
                    be used.\              \* data service - any tool to help users make use of the data, e.g. link to WPS, station search NOT for 
                    Do NOT use: DMP,       data provider tools! those should be on the Project page!!!\                                            
                    Download, Apply for    \* Metadata - link to folder with relevant metadata files, e.g. MIDAS station metadata\                 
                    Access - these are     \* search - link to some deeper search tool, e.g. Elastic search, EUFAR flight tool\                    
                    handled elsewhere.\    \                                                                                                       
                    \                      \*image and logo - at present use the image details to record these. Eventually we need to refine       
                    Internal resource type things to allow a specific logo to be selected and use the same mechanism to also link to other         
                    - used to tag a        relevant links\                                                                                         
                    specific entry with a  \                                                                                                       
                    given tag: DMP should  \                                                                                                       
                    only be used on        \                                                                                                       
                    Project pages! FAT Cat                                                                                                         
                    link will be dropped;                                                                                                          
                    Sample data link -                                                                                                             
                    should be added in in                                                                                                          
                    due course, along with                                                                                                         
                    link to sample plots,                                                                                                          
                    but not implemented                                                                                                            
                    properly within CEDA.                                                                                                          
  Reviews           For carrying out       *For the time being please refer all new records to Anabelle or Graham for a review before they are     
                    metadata content       published. This is to ensure consistent quality checks are carries out*                                 
                    reviews - not in                                                                                                               
                    proper operation just                                                                                                          
                    yet.                                                                                                                           
  Migration         All the old legacy     *Please leave the \""moles2citation\"" at present, but look to move other items into other parts of MOLES 
  Properties        content from the       if possible - e.g. add items under the links section to the online resources section. Once migrated     
                    MOLES2 \""Content\""     feel free to delete content*                                                                            
                    section is in here -                                                                                                           
                    split into the various                                                                                                         
                    div tags.                                                                                                                      
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------
:::
",https://help.ceda.ac.uk/article/4187-observation-records#page-contents,30871,1759
"Introduction {#introduction children-count=""0""}","The [CEDA Archive](http://archive.ceda.ac.uk/) is the UK's national data
centre for atmospheric physics, atmospheric composition, climate change
and earth observation sciences funded through the UK [Natural
Environment Research Council](http://nerc.ukri.org/). The CEDA Archive
accepts digital environmental data in re-usable formats for curation,
including long-term storage and public dissemination. This policy
document covers the nature and scope of the CEDA Archive\'s acquisition
of digital resources for long-term curation and dissemination and the
decision-making process for acceptance. Access and use specifies who can
access digital resources held by the CEDA Archive and how access is
enabled. This acquisition policy is under periodic review and will
develop throughout the lifetime of the data centre.
","https://help.ceda.ac.uk/article/4857-acquisition-policy#introduction-{#introduction-children-count=""0""}",816,109
"Nature and scope of digital resources {#nature-and-scope-of-digital-resources children-count=""0""}","This section outlines the range of digital resources accepted for
deposit and curation at the CEDA Archive. The evaluation of resources
offered for deposit at the CEDA Archive is also described.
","https://help.ceda.ac.uk/article/4857-acquisition-policy#nature-and-scope-of-digital-resources-{#nature-and-scope-of-digital-resources-children-count=""0""}",195,31
"Scope of digital resources accepted {#scope-of-digital-resources-accepted children-count=""0""}","The CEDA Archive holds digital resources within the atmospheric physics,
atmospheric composition, climate change and earth observation science
domains. However, digital resources concerned with other areas of the
environmental sciences will also be considered for curation on a
case-by-case basis. The CEDA Archive prioritises ingestion of digital
resources generated as a result of NERC funding. Digital resources
generated via alternative funding avenues may also be considered if the
deposit in question is perceived to be of importance to the
environmental research community. There is no limit on the geographical
or temporal range of digital resources the CEDA Archive accepts. The
data are generally either:
-   NERC owned data or data generated by NERC funded grants, where we
    aim to provide long-term preservation. 
-   Third party data, where the aim is to facilitate research. 
-   Data generated by non-NERC funded grants that is deemed to be of
    significant long-term value to our core research communities.
","https://help.ceda.ac.uk/article/4857-acquisition-policy#scope-of-digital-resources-accepted-{#scope-of-digital-resources-accepted-children-count=""0""}",1028,154
"Type of digital resources accepted {#type-of-digital-resources-accepted children-count=""0""}","The CEDA Archive holds nationally and internationally important digital
resources concerned with the atmospheric physics, atmospheric
composition, climate change and earth observation communities. Files,
images, moving images and model outputs are all acceptable for deposit
and curation where it is deemed feasible to maintain them and they are
considered to be capable of re-use. In all cases, digital resources must
be provided with sufficient supporting documentation to enable
understanding of the resource without the need to contact the depositor
directly.
The CEDA Archive does not accept data where there is a disclosure risk
and ask that any data containing sensitive information be anonymised or
be of a sufficiently broad scale that sensitive location information is
not made publicly available.
","https://help.ceda.ac.uk/article/4857-acquisition-policy#type-of-digital-resources-accepted-{#type-of-digital-resources-accepted-children-count=""0""}",808,119
"Criteria for evaluating digital resources {#criteria-for-evaluating-digital-resources children-count=""0""}","The CEDA Archive will identify those types of resource that have
potential re-use value as part of its ingestion process. Each deposit
request will be checked against a number of criteria based on the NERC
data value checklist.
These criteria include:
-   Are related resources already held by the CEDA Archive or other data
    centre?
-   Was the research funded by NERC?
-   Is the CEDA Archive the most appropriate data centre for that
    resource?
-   Are data unique, repeatable or does the cost of regeneration exceeds
    the cost of curation?
-   Have the resources been used in a peer-reviewed publication?
-   Can the resource be described sufficiently to support re-use
-   Can the resource be stored in a re-usable (and preferably
    non-proprietary) format?
-   Will ingestion incur excessive cost due to the size of the resource
    and has this has been budgeted for by the project?
","https://help.ceda.ac.uk/article/4857-acquisition-policy#criteria-for-evaluating-digital-resources-{#criteria-for-evaluating-digital-resources-children-count=""0""}",901,147
"{#section children-count=""0""}","Depositors are provided with guidance as to what is expected from them
including:
-   Accepted [file
    formats](https://help.ceda.ac.uk/article/104-file-formats) and [file
    naming conventions](https://help.ceda.ac.uk/article/103-filenames)
-   [Accepted supporting
    documentation](https://help.ceda.ac.uk/article/143-supplementary-info)
-   [Metadata
    required](https://help.ceda.ac.uk/article/143-supplementary-info)
","https://help.ceda.ac.uk/article/4857-acquisition-policy#{#section-children-count=""0""}",429,28
"Payment for acquisitions {#payment-for-acquisitions children-count=""0""}","Charges for acquisition services to NERC-funded researchers or
researchers wishing to deposit nationally important digital resources
will usually be waived. However, the CEDA Archive recognises that in
extraordinary circumstances, where digital archiving entails significant
expenditure, costs may need to be recovered from the depositor. Charging
may therefore be necessary for certain classes of depositor or for
excessive volumes of data. The charging policy states that charging will
be based on the time and materials used when ingesting a digital
resource.
","https://help.ceda.ac.uk/article/4857-acquisition-policy#payment-for-acquisitions-{#payment-for-acquisitions-children-count=""0""}",563,80
"Access and use {#access-and-use children-count=""0""}","All data in the CEDA Archive will be listed in the
[catalogue](https://catalogue.ceda.ac.uk). This publicly accessible
catalogue enables searching of the entire CEDA Archive holdings and
provides view and download services. Information on citation, licensing,
embargo periods and access to supporting documentation for re-use is
also provided via the catalogue which holds discovery level standards
compliant metadata for each digital resource. Public access to the
digital resources held by the CEDA Archive is conditional on acceptance
of any terms and conditions under which the resources are made
available. Access to the holdings is in most cases free at the point of
use. Data held by the CEDA Archive are available under a range of
different licences, including open licence such as the UK's Open
Government Licence (OGL). The licence applicable for each dataset is
listed on the associated data catalogue page. The CEDA Archive supports
the depositor's right of first publication and, in line with NERC data
policy, permits an agreed embargo period of up to two years from the
point of data collection or generation. Digital resources may be
ingested into the CEDA Archive, however public access to those data
would be restricted until such time as the embargo period expires.
It is the policy of the CEDA Archive to:
-   provide data management advice and guidance to researchers working
    on NERC-funded atmospheric physics, atmospheric composition, climate
    change and earth observation projects
-   store a copy of the data \'as supplied\' in a secure archive
    location in an agreed format
-   ensure the data are discoverable and provide access to data held in
    the archive under appropriate access control
-   create Digital Object Identifiers (DOIs) for digital datasets,
    facilitating data citation and display these alongside the data
    where requested by the data provider
-   encourage the use of the datasets held by the CEDA Archive for a
    full range of purposes including future scientific projects and
    within information products or decision support systems
","https://help.ceda.ac.uk/article/4857-acquisition-policy#access-and-use-{#access-and-use-children-count=""0""}",2104,323
CEDA Helpdesk,"CEDA aims to support all members of our user community accessing and
using its archives and services. Some of this support is covered by the
[JASMIN support team](https://accounts.jasmin.ac.uk/account/support/)
who are responsible for JASMIN system administration and \""tenancy\""
administration. The CEDA team additionally provides support for all
CEDA\'s services and archives.
",https://help.ceda.ac.uk/article/4470-ceda-helpdesk,379,50
Support Hours,"CEDA and JASMIN services are supported on a best-efforts basis. Support
will be provided during normal working hours, defined as between 0900
and 1700 on Monday to Thursday and between 0900 and 1630 on Friday,
excluding Public Holidays and STFC Privilege Days. Note that times are
given in UK time.
",https://help.ceda.ac.uk/article/4470-ceda-helpdesk#support-hours,299,50
Help Documentation and User Support,"CEDA are constantly seeking to improve our user support ahead of any
issues, but sometimes things don\'t always go to plan. To help users as
much as possible we have the following resources available:
-   CEDA help documentation site:
    [help.ceda.ac.uk](https://help.ceda.ac.uk/) - covers all aspects of
    data services and archive
-   If you think a service is broken or having issues you can view its
    uptime on the [status
    page](https://stats.uptimerobot.com/vZPgQt7YnO). This should give
    you a clue about whether your problem is specific to you or if there
    is a more general problem with a service being down.
-   Help Beacons - these appear at the bottom right of CEDA services and
    allow you to search our help documentation or get in touch quickly
    and easily. They also have a handy list of the most relevant help
    pages listed for you. 
-   Dataset documentation and news - each dataset is covered by a
    Dataset record in CEDA\'s [data
    catalogue](https://catalogue.ceda.ac.uk/). On there you\'ll find
    links to documentation (under the \""docs\"" tab), background
    information (e.g. data quality information) and news items
    specifically about the data in question (e.g. news of issues,
    updates, other superseding datasets)
-   Dedicated staffed help desk. If you still can\'t find the
    information you need via the methods detailed above, you can contact
    us at <support@ceda.ac.uk>
",https://help.ceda.ac.uk/article/4470-ceda-helpdesk#help-documentation-and-user-support,1446,216
News and Social Media,"This is where you may find news items detailing new datasets/service,
upcoming scheduled work or known live service issues. The latest CEDA
news headlines are on a banner at the top of the [CEDA home
page](http://www.ceda.ac.uk/), but all past news items are available
[here](http://www.ceda.ac.uk/blog/). We also provide [RSS
feed](https://www.ceda.ac.uk/blog/feeds/rss/) and [Atom
feeds](https://www.ceda.ac.uk/blog/feeds/atom/). 
Follow us on [Twitter](https://twitter.com/cedanews) for updates and
news.
",https://help.ceda.ac.uk/article/4470-ceda-helpdesk#news-and-social-media,508,60
Our commitment,"The CEDA and JASMIN teams together undertake to:
1.  Attempt to provide an initial response to all support queries within
    one working day.
    -   When responding to support queries, the CEDA Team will take
        reasonable steps to ensure that the information we provide is
        accurate and useful, but we can\'t guarantee this. You are
        responsible for your use of any advice or information we may
        give you.
2.  Be proactive in communicating important service updates and issues
    to users via the CEDA news channels and JASMIN mailing lists.
    Advance warning of known service interruptions will be made where
    possible.
    -   Users are encouraged to sign up to these services to remain
        abreast of these news announcements.
3.  Maintain online help documentation to aid users to find answers to
    common queries ahead of contacting the CEDA support desk for further
    assistance.
    -   Online documentation is a ""best efforts"" activity. Users are
        encouraged to provide positive suggestions which will improve
        documentation.
For further specific information on JASMIN support please see the
[JASMIN Support page](https://accounts.jasmin.ac.uk/account/support/).
",https://help.ceda.ac.uk/article/4470-ceda-helpdesk#our-commitment,1228,172
**General Enquiries **,"**Email address:** <support@ceda.ac.uk> (preferred)
**Phone number:** +44 (0)1235 446432 (Note: this phone number is not
fully supported. Emailing the helpdesk is the preferred and most
reliable method for contacting the team)
",https://help.ceda.ac.uk/article/4470-ceda-helpdesk#**general-enquiries **,227,31
Elasticsearch at CEDA: Making data more discoverable,"CEDA maintains an Elasticsearch cluster to index information which
enables us to improve the searchability of our data holdings. This is
both for us to improve the services we provide and allows external users
to query our data holdings and build services using the response.
",https://help.ceda.ac.uk/article/4694-ceda-elasticsearch#elasticsearch-at-ceda:-making-data-more-discoverable,276,45
What is available?,"  -----------------------------------------------------------------------
  Name                                Description
  ----------------------------------- -----------------------------------
  ceda-fbi                            Main index containing file level
                                      metadata about all files in the
                                      CEDA archive
  ceda-dirs                           Index of all the directories in the
                                      CEDA archive. Includes information
                                      about the CEDA catalogue and
                                      00README if there is one.
  ceda-eo                             Earth observation index containing
                                      metadata about satellite scenes
                                      from Sentinel 1,2,3, Sentinel ARD
                                      and Landsat 5,7 and 8.
  faam                                Metadata from flights flown by the
                                      Facility for Airborne Atmospheric
                                      Research
  eufar                               Metadata from flights flown by the
                                      European Facility for Airborne
                                      Research\
  arsf                                Metadata from flights flown by the
                                      Airborne Research and Survey
                                      Facility
  bas-masin                           Metadata from flights flown by the
                                      British Antarctic Survey --
                                      Meteorological Airborne Science
                                      Instrumentation\
  -----------------------------------------------------------------------
You can access the search and mappings API to get information.
The Elasticsearch API is extensively and thoroughly documented at:
<https://www.elastic.co/guide/en/elasticsearch/reference/current/search-your-data.html>
",https://help.ceda.ac.uk/article/4694-ceda-elasticsearch#what-is-available?,2069,133
401 Unauthorised,"Access to our Elasticsearch cluster is restricted. Unauthenticated
requests will only allow you to perform search and get mapping
operations on the indices specified above. 
",https://help.ceda.ac.uk/article/4694-ceda-elasticsearch#401-unauthorised,174,25
How to search the indexes?,,https://help.ceda.ac.uk/article/4694-ceda-elasticsearch#how-to-search-the-indexes?,0,0
Python,"There is a very useful python client that will enable you to interact
easily with elasticsearch at CEDA.
It can be installed using:
    pip install python-elasticsearch
A basic example script which will return some results from the ceda-fbi
index:
from elasticsearch import Elasticsearch
query = {
    ""query"": {""match_all"": {}}
}
es = Elasticsearch([""https://elasticsearch.ceda.ac.uk""])
es.search(index=""ceda-fbi"", body=query)
es.indices.get_mapping(index=""ceda-fbi"")
",https://help.ceda.ac.uk/article/4694-ceda-elasticsearch#python,469,56
Access via command-line tools: wget and curl,"Here is a link to the URI search documentation from elasticsearch which
describes all the allowed keywords.
<https://www.elastic.co/guide/en/elasticsearch/reference/current/search-uri-request.html>
Some examples using the *wget* and *curl* command-line tools:
    wget https://elasticsearch.ceda.ac.uk/ceda-fbi/_search?size=1
    wget https://elasticsearch.ceda.ac.uk/ceda-fbi/_mapping
    curl https://elasticsearch.ceda.ac.uk/ceda-fbi/_search?size=1
    curl https://elasticsearch.ceda.ac.uk/ceda-fbi/_mapping
",https://help.ceda.ac.uk/article/4694-ceda-elasticsearch#access-via-command-line-tools:-wget-and-curl,512,35
Curl with body,"    curl https://elasticsearch.ceda.ac.uk/ceda-fbi/_search -H 'Content-Type: application/json' -d '{""query"": {""match_all"": {}}}'
",https://help.ceda.ac.uk/article/4694-ceda-elasticsearch#curl-with-body,129,9
Introduction,"This policy documents the digital preservation policy of the Centre for
Environmental Data Analysis Archive (CEDA Archive). The CEDA Archive is
a Natural Environment Research Council (NERC) data centre hosted by the
Science and Technology Facilities Council (STFC). The aim is to ensure
the longevity of the digital information assets held by the Data Centre
in a sustainable way by addressing the factors which risk making them
unusable and/or inaccessible.
",https://help.ceda.ac.uk/article/4860-data-preservation-policy#introduction,459,70
,,https://help.ceda.ac.uk/article/4860-data-preservation-policy#,0,0
Preservation Policy,"We aim to:
-   Maintain the integrity of the data by regularly auditing checksums
    of data files. 
-   Ensure all data access is secured for the relevant level of
    information security.
-   Ensure data are accompanied by sufficient documentation to enable
    their re-use for analytical and research purposes.
-   Ensure data are checked and validated according to appropriate data
    and documentation ingestion procedures.
-   Ensure data are catalogued according to appropriate metadata
    standards.
-   Provide suitable storage media for long-term data management,
    migrating data to new media as needed.
-   Ensure we have sufficient rights to preserve data and distribute it
    under an appropriate licence. However, we don\'t own the data.
",https://help.ceda.ac.uk/article/4860-data-preservation-policy#preservation-policy,761,111
,,https://help.ceda.ac.uk/article/4860-data-preservation-policy#,0,0
Retention,"Retention policy varies for different data sets. Generally:  
-   Observations, as a unrepeatable measurement of the Earth system, are
    retained indefinitely.
-   Model data, which can have a limited shelf-life and may be withdrawn
-   Third party data, where there is other primary archive, may be kept
    as a rolling archive of the most recent data, or reviewed if usage
    falls. 
-   Data that takes significant resource to keep, such as very large
    datasets, may need special consideration.
-   Exceptionally, data may be withdrawn for a number of reasons, see
    the [CEDA withdrawal
    policy](https://help.ceda.ac.uk/article/4730-withdrawal-policy) for
    further details.
",https://help.ceda.ac.uk/article/4860-data-preservation-policy#retention,693,97
External Policy Considerations,"As a core activity in the CEDA Archive, preservation does not exist in
isolation. It needs to take account of:
-   The [NERC Data
    Policy](https://nerc.ukri.org/research/sites/data/policy/); NERC and
    STFC Information Security policy; User needs.
-   Legislation that applies particularly to data repositories: [Data
    Protection Act 2018
    (DPA 2018)](http://www.legislation.gov.uk/ukpga/2018/12/pdfs/ukpga_20180012_en.pdf),
    [General Data Protection Regulation (GDPR)](https://gdpr-info.eu/),
    [Freedom of Information Act
    2000](http://www.legislation.gov.uk/ukpga/2000/36/contents), and
    [Environmental Information Regulations
    2004](http://www.legislation.gov.uk/uksi/2004/3391/contents/made)
<!-- -->
-   Good practice Learnt from other repositories for example, [Core
    Trust Seal](https://www.coretrustseal.org/) certification (presently
    under submission) and the [Open Archival Information System (OAIS)
    reference model](https://www.iso.org/standard/57284.html)
",https://help.ceda.ac.uk/article/4860-data-preservation-policy#external-policy-considerations,1005,89
,,https://help.ceda.ac.uk/article/4860-data-preservation-policy#,0,0
,,https://help.ceda.ac.uk/article/4860-data-preservation-policy#,0,0
Preservation Strategy,"The preservation strategy of the CEDA Archive aims to maintain a
flexible preservation system that can evolve to meet the demands of
changing technology and developing user expectations. The CEDA Archive
has chosen to implement a preservation strategy based upon open and
available file formats. The same ingestion procedure is used for all
data resources and no judgement is made on the scholarly value of the
datasets once they have been identified as suitable for deposit with the
CEDA Archive. All datasets accepted for deposit must be accompanied by
supporting documentation of sufficient quality to enable re-use over the
long-term. To reduce the risk of obsolescence, files are only accepted
in a non-proprietary formats.
Migration to new media is performed as technologies progress, but the
data files themselves are unaltered whenever possible. Checksumming of
files is performed to verify that nothing has changed. 
Online storage for the data centre repository is administered by a
dedicated IT infrastructure team. The environmental parameters which
control the storage media are tightly controlled to reduce
vulnerability. Data are backed up to Tape Libraries continuously. A copy
of the archive is kept securely off site and forms a key component of
the CEDA Archive\'s disaster recovery and business continuity
procedures, providing for recovery of data and infrastructure under
commonly anticipated threats (e.g. technical failure, human error). The
system also ensures the safety of the data in the event of a more
serious incident if, for example, the buildings housing the data centre
and/or major IT infrastructure were to be rendered inoperable.
The preservation of the CEDA Archive\'s data relies on servers and
networks it uses. Currently we use the NERC
[JASMIN](http://www.jasmin.ac.uk/) system as our infrastructure. Servers
are continually monitored and periodically reviewed to ensure timely
upgrades in both hardware and software. JASMIN is run from secure
premises with industry standard measures to ensure its physical
integrity, such as fire suppression and card entry. Access to JASMIN
is secured by public-key ssh and external access is only available via a
Secured VPN with multi-factor authentication using digital tokens.
",https://help.ceda.ac.uk/article/4860-data-preservation-policy#preservation-strategy,2259,342
Funding and resource planning,"The CEDA Archive is, and always has been, dependant on funding from NERC
to carry out its activities. The CEDA Archive is currently funded as
part of the NERC Environmental Data Service (NERC EDS), now in it\'s 2nd
phase of 5 years of funding running from April 2023. There is a close
cooperation with the other four NERC data centres that, alongside CEDA,
are commissioned as part of the NERC EDS: British Oceanographic Data
Centre, Environmental Information Data Centre, National Geoscience Data
Centre and Polar Data Centre. 
Resource management for preservation of digital resources includes:
-   technical infrastructure, including equipment purchases, maintenance
    and upgrades, software/hardware obsolescence monitoring, network
    connectivity etc.
-   financial plan, including strategy and financing the CEDA Archive
    and commitment to long-term funding
-   staffing infrastructure, including recruitment, induction and
    ongoing staff training
The CEDA Archive makes every effort to remain up-to-date with any
relevant technological advances to ensure continued access to its
collections. The CEDA Archive also implements a programme of continual
improvement in how users interact with the data centre, for example,
improved deposit and request functions for users.
",https://help.ceda.ac.uk/article/4860-data-preservation-policy#funding-and-resource-planning,1286,183
Bulk download WGET,"![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6317252a7164226be0c8348f/file-YUMXmqnefh.png)
There are a number of methods for undertaking bulk downloads of data
from the CEDA archive. Details on how to use them at any given point in
the archive are displayed in the
[data.ceda.ac.uk](https://data.ceda.ac.uk) service by clicking on the
shopping trolley icon at the top of the directory view.
One option is to use WGET\... however, to use this service a valid CEDA
user account in order to obtain a client certificate to use with the
request on the command line.
",https://help.ceda.ac.uk/article/5061-bulk-download-wget,603,82
Getting your client certificate,"You can generate a short-lived client certificate by following [this
guide on the CEDA help
site](https://help.ceda.ac.uk/article/4442-ceda-opendap-scripted-interactions),
particularly the \""Getting a Security Certificate\"" section. The guide
uses cURL as an example, but once you have a client certificate, you can
provide it with your wget request using the \""\--certificate\"" option of
the wget command line tool.\
\
`wget --certificate $PWD/creds.pem -e robots=off --mirror --no-parent -r <some data URL>`
Note the data URL will be to access the data via our OpenDAP service,
for example:
\
`wget --certificate $PWD/creds.pem -e robots=off --mirror --no-parent -r https://dap.ceda.ac.uk/badc/ukmo-midas-open/data/uk-radiation-obs/dataset-version-202107/aberdeenshire/00145_cairnwell`
",https://help.ceda.ac.uk/article/5061-bulk-download-wget#getting-your-client-certificate,788,93
"Introduction {#introduction children-count=""0""}","CEDA holds a wide range of instrumental, satellite, aircraft and model
datasets of interest to the scientific community. From the point of view
of data access, it is highly desirable to adhere to common file formats
and file-naming conventions for all the data produced under the various
projects. A well thought out and organised file-naming convention allows
quick data access and avoids the user having to read the file in order
to enquire as to its contents. Using this convention will save time and
resources when setting up data management for each individual project,
it will also allow greater analysis and manipulation of the data by
software within CEDA and beyond.
There may be specific conventions followed for particular types of data
or communities, such as the NWP data from met agencies, or the GHRSST
satellite data standards. CEDA encourages the use of these standards
where possible. CEDA has its own file naming convention given below if
no community standard exists. Where alternative file naming conventions
have been used in the archive the nomenclature used should be explained
on the relevant dataset catalogue page.
","https://help.ceda.ac.uk/article/103-filenames#introduction-{#introduction-children-count=""0""}",1142,184
"Basic Character set {#basic-character-set children-count=""0""}","Files and directories in the CEDA archive are named using a
restricted character set to avoid problems when the files are used. For
example, plus(+) in a filename may produce an error in a web service
that interprets it as a space. Any character that has a special meaning
within a URL, Windows or the Unix shell are avoided. This leaves the
following plain ASCII characters:
a-z  A-Z  0-9  -  _  .
Older data and data where there is a clear, long-established naming
pattern may break this rule, but generally we will demand that new data
deposits follow this convention.
","https://help.ceda.ac.uk/article/103-filenames#basic-character-set-{#basic-character-set-children-count=""0""}",572,100
"CEDA File-naming convention {#ceda-file-naming-convention children-count=""0""}","The file-naming convention for instrumental (and other) datasets uses
long file names since these indicate significant information about the
contents of the file without having to read the file or refer to the
directory structure. Important attributes in a file name include
INSTRUMENT, LOCATION/PLATFORM and TIME.
  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  Please note that FAAM file names expand the convention below by allowing three \[\_extra\] fields, two of which are mandatory for data collected on board the FAAM aircraft (for details, please refer to the  [FAAM Filename Convention](https://help.ceda.ac.uk/article/3796-faam-flight-data-file-names)). Participants to FAAM campaigns may feel free to generalise this rule to all data collected during FAAM campaigns, and use up to 3 extra fields separated by underscore signs, if they wish to do so.
  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
The chosen convention is as follows:
instrument_[location|platform]_YYYYMMDD[hh][mm][ss][_extra][_cor#].ext
Where:
`instrument` - is the instrument name (full or shortened) or model name.
When the same instrument is used by a number of groups, the instrument
name should be prefixed with the institute name/code and a hyphen, for
example uea-ptrms and york-ptrms. See [current
list](http://catalogue.ceda.ac.uk/listings/instr/).
`location|platform` - is the location name (full or shortened) or the
name of the platform on which the instrument is deployed. This refers to
the location/platform for the observation and not the institute or
location of the participating scientist/group. This field could be used
for a range of items such as a site, a station, a platform (e.g. an
aircraft), an institute or a university. See [current
list](http://catalogue.ceda.ac.uk/listings/plat/).
`YYYYMMDD` - is the date on which measurements were taken. If a data
file spans more than one day then this field should represent the first
day during which data was recorded. The year is given as four digits
with month and day as two digits each.
`[hh][mm][ss]` - is the time of day specified (optional). Hours, minutes
and seconds can be represented as two digits each. Hours can be used
alone, only hours and minutes used or all three fields can be included.
However, minutes or seconds cannot be used without the preceding time
unit (i.e. no minute field allowed unit without the hour field).
`[_extra]` - this section allows additional code to define such things
as different range resolutions and so forth. It could also be used for
Version numbers etc,.
\[\_cor#\] - this section denotes that the file is a corrected version
of a previously released file that the data provider has released. The
number indicated at the end of the \""cor\"" is linked to the details of
the correction noted in the dataset catalogue page and the appropriate
readme file for the dataset.
`.ext` - will normally be .nc (NetCDF) or .na (NASA Ames) although
occasionally other formats will be used, in particular .png and .gif for
Image files. See [current list](#ext).
Filenames should contain only the characters  `[-_.a-z0-9]`. Spaces are
forbidden and upper case characters should be avoided. The underscore
\""\_\"" character should only be used as a separator between fields.
","https://help.ceda.ac.uk/article/103-filenames#ceda-file-naming-convention-{#ceda-file-naming-convention-children-count=""0""}",4170,486
"File-naming for non-standard data (e.g. model, trajectory data) {#file-naming-for-non-standard-data-e.g.-model-trajectory-data children-count=""0""}","Some projects will also generate model data, flight data, data recorded
at sea (stationery and in transit), trajectories and other non-standard
data types. It is suggested that the above format be adapted in the
following ways:
1.  Data recorded by on board moving craft\
    When data is recorded on a moving craft the varying spatial location
    should not be recorded in the filename. Instead, the location field
    in the filename should include a name (or code) for the vessel and
    optionally the flight/voyage code/number.
2.  Trajectory data\
    Calculated trajectory data is similar to data recorded on a moving
    craft. The varying spatial location should not be recorded in the
    filename. Instead, the  `location` field in the filename should
    include a relevant code for the trajectory type/model/number.
3.  Model data\
    In the case of the model data, the  `instrument` field in the
    filename should instead be used for a model code (indicating the
    type, version etc., of the model). For box models running at one
    location only the `location` field should include this. However,
    models that output data over a grid can use appropriate codes to
    represent this.
4.  Use of the `[_extra]` additional information field\
    The  `[_extra]` field is unlikely to be used in most cases but is
    provided as an option for exceptional cases where the data producer
    wishes to include some additional information not otherwise catered
    for. Suitable warning should be used against overloading this field.
    Such a use might be in forecast files where the date and time
    provide the start time whilst the `[_extra]` field provides the time
    of the actual forecast.
5.  Use of the `[hh][mm][ss]` time options\
    The  `[hh][mm][ss]` options are included or occasions where data is
    produced at such a high frequency that storing it in multiple files
    per day, hour or minute becomes appropriate. This is unlikely to be
    commonplace but is available for special cases.
6.  Image files\
    Text files ( `.txt`) may be included to describe image data. Apart
    from the file name extension (last field), files containing images
    and their associated metadata should have the same name. When data
    exist both in the form of NASA Ames formatted fields and
    images,files also have the same name, except for the file name
    extension.
","https://help.ceda.ac.uk/article/103-filenames#file-naming-for-non-standard-data-(e.g.-model,-trajectory-data)-{#file-naming-for-non-standard-data-e.g.-model-trajectory-data-children-count=""0""}",2403,373
"Standardising common names in the naming convention - adding new names {#standardising-common-names-in-the-naming-convention---adding-new-names children-count=""0""}","In order to standardise the names used within the file-naming convention
CEDA will need to collate those currently used by the community and
publish them via our website. This can be regularly extended to include
new locations, instruments, models, etc. Interaction with instrument
scientists and modellers will be essential to achieve this aim
successfully. Should you need to extend the lists indicated above
please  [contact us](http://www.ceda.ac.uk/contact/) to discuss your
requirements.
","https://help.ceda.ac.uk/article/103-filenames#standardising-common-names-in-the-naming-convention---adding-new-names-{#standardising-common-names-in-the-naming-convention---adding-new-names-children-count=""0""}",494,70
"Old CEDA \""8.3\"" filenames {#8.3-filenames children-count=""0""}","Older datasets within the CEDA archive pre-date the CEDA file naming
convention and were constructed to conform to the limits of filename
sizes present in MS-Dos: namely an \""8.3\"" character string, i.e. 8
characters, a period and then 3 more characters. This file naming
convention was often, therefore, restrictive in the way that the
necessary filename parts could be encoded. However, many such files in
the CEDA archive will typically follow this pattern:
ppYYMMDD.eee
where:
-   `pp` - typically used for a location or platform identifier
-   `YY` - two digit year
-   `MM` - month
-   `DD` - day
-   `eee` - the file extension was typically used to encode some
    information about he parameter or instrument data covered by the
    file. 
For example in the ACSOE EAE-96 campaign at Mace Head we find files such
as:
mh960703.cn1  mh960703.cn3  mh960703.epi  mh960703.nx2  mh960703.o31
the `mh` was used for the Mace Head site while the various extensions
(`cn1, cn3, epi , nx2 and o31`) correspond to different data outputs
from the various suites of instruments deployed at the site during the
campaign. Such platform and instrument code have, where possible, been
added as CEDA abbreviations onto relevant records in the CEDA data
catalogue.
","https://help.ceda.ac.uk/article/103-filenames#old-ceda-\""8.3\""-filenames-{#8.3-filenames-children-count=""0""}",1253,202
"File Extensions {#ext children-count=""0""}","Extensions may give an indication to the format of the data. The
following list are the file extensions commonly used within the CEDA
Archive.
","https://help.ceda.ac.uk/article/103-filenames#file-extensions-{#ext-children-count=""0""}",143,24
"Allowed values: {#allowed-values children-count=""0""}","  ------- -----------------------------------------
  Value   Description
  na      Nasa-Ames file
  csv     BADC-CSV file
  ict     ICARTT ASCII format (updated Nasa-Ames)
  nc      NetCDF file
  txt     free form metadata file
  jpg     JPEG image file
  gif     GIF image file
  png     PNG image file
  tar     TAR archive file
  ------- -----------------------------------------
","https://help.ceda.ac.uk/article/103-filenames#allowed-values:-{#allowed-values-children-count=""0""}",384,42
"{#section children-count=""0""}",,"https://help.ceda.ac.uk/article/103-filenames#{#section-children-count=""0""}",0,0
Dataset plans,"This help page describes CEDA\'s generic dataset plans for the CEDA
archive. Once CEDA has decided the data is suitable for deposit, one of
the following categories will be assigned depending on the level of
interest to the scientific community. The categories are reference,
structured, compatible, in-project, other repository, other NERC data
centre or other DMP. These categories are described below.
",https://help.ceda.ac.uk/article/4561-dataset-plans,405,61
Reference,"These are data that needs to be discoverable and downloadable, but is
left to the user to work out some of the usability issues. CEDA will
make a catalogue entry and add the data files to the archive. This is a
suitable solution if there is not likely to be mass interest in the data
and its principal use will be to provide evidence for publication.
Minimum qualification: a paper referencing the dataset. 
",https://help.ceda.ac.uk/article/4561-dataset-plans#reference,408,73
Structured,"As well as the reference, these data are in a community supported
format, with a defined file and directory naming convention. It may also
have specified file level metadata attribute conventions. The data are
more usable by third parties and is suitable for a dataset where there
is an intention to make the data more reusable.
Minimum qualification: evidence of use of similar datasets by CEDA core
communities. 
",https://help.ceda.ac.uk/article/4561-dataset-plans#structured,415,68
Interoperable,"In addition to being structured, these data are connected to specified
community tools or systems that enable better discovery or processing.
For example, climate model data in ESGF, MIDAS data in the WPS or
aircraft data in the Flight Finder.
Minimum qualification: evidence of use of similar datasets by CEDA core
communities and community tool specifications. Some evidence that the
data will fit the tools.
",https://help.ceda.ac.uk/article/4561-dataset-plans#interoperable,411,65
In-project,"Data that are not required to be shared outside of the project. These
are data we don't plan to see in the archive at all.
",https://help.ceda.ac.uk/article/4561-dataset-plans#in-project,123,25
Other repositories,"Data that are more suited to being archived in a different repository.
This may be an action to check the data are in the external archive?
",https://help.ceda.ac.uk/article/4561-dataset-plans#other-repositories,140,26
Other NERC DC,"Data to go to other NERC data centre; 
-   BODC - British Oceanographic Data Centre
-   EIDC - Environmental Information Data Centre 
-   PDC - Polar Data Centre
-   NGDC - National Geoscience Data Centre
",https://help.ceda.ac.uk/article/4561-dataset-plans#other-nerc-dc,205,35
Other DMP,"The Data is planned for in another DMP. For example, a campaign may
reference FAAM core data, but we will plan for this data under the FAAM
DMP. 
",https://help.ceda.ac.uk/article/4561-dataset-plans#other-dmp,146,28
Where to put data docs,"Creating archive content and catalogue records for datasets also
uncovers documentation that is important to capture. However, knowing
where to place this and how to link to it is sometimes and art in
itself. This flow chart might help deciding how to handle this:
",https://help.ceda.ac.uk/article/4827-where-to-put-data-docs,265,44
"How to decide where items reside {#how-to-decide-where-items-reside children-count=""0""}","To determine where to place an item the following flow chart should be
used:
**Start:**
1.  Is the material already in a stable, external location (e.g.
    recognised document repository, journal or well established site)?
    Yes -\> **Link to existing item from the catalogue page.**\
    Else:
2.  Is the object dynamically generated when requested by a client call?
    Yes -\> Object is **out of scope** (i.e. CGI script, service, etc
    etc)\
    Else:
3.  Is access control needed for the Object?Yes -\> **place object in
    the archive **in appropriate location (i..e.
    /\<datacenetre>/\<primary
    collection>/\<images\|plots\|docs\|software>/\
    Else:
4.  Does BOTH the URL and the OBJECT need to be persistent? Yes -\>
    Recommend the user **places it into suitable long-term repository**
    such as
    [Zenodo](https://zenodo.org/deposit/new?c=ceda-document-repository)
    and to inform you of the link to the object.\
    Else:
5.  Does the URL need to be persistent, but the OBJECT is changeable
    (that we will manage)? Yes -\> place object in the **CEDA Artefact
    server ([CEDA Artefacts
    guide](https://help.ceda.ac.uk/article/5066-ceda-artefacts-service))\
    **Else:
**Out of scope and you\'re on your own\... (too bespoke\... but check
with someone first (e.g. curation manager or main person for catalogue
questions!)**
","https://help.ceda.ac.uk/article/4827-where-to-put-data-docs#how-to-decide-where-items-reside-{#how-to-decide-where-items-reside-children-count=""0""}",1364,183
Project records,"::: {#wikipage}
",https://help.ceda.ac.uk/article/4154-opmanmoleseditoruserguideproject-ceda,16,2
Project Records {#ProjectRecords},"The Project records should describe the background reason **WHY** data
we hold were collected - this could tell you about a particular field
campaign, a funding program or a project - and may link to other related
project records to show a programme-project-field-campaign type of
hierarchy if relevant.
There are lots of cross over between the DMP tool and these records
which may afford us some shortcuts in work here in due course!
These records may also hold links to relevant project resources that we
or others supply - e.g. project spaces, GWS info etc.
  ----------------- ---------------------- ------------------------ -----------------
  Field ( **bold =  Description            Checklist/\*Controlled   Y/N
  mandatory field**                        vocab explanations/      
  )                                        *Notes*                  
  **Title**         Short text title for   Do you understand the    
                    the dataset            title?\                  
                    collection.            Is the title brief and   
                                           simple?\                 
                                           Does the title contain   
                                           unexplained technical    
                                           terms or acronyms?\      
                                           Does the title describe  
                                           the resource rather than 
                                           the activity/project     
                                           which produced it?\      
                                           Is the title in sentence 
                                           case?\                   
                                           Is the title in          
                                           English?\                
                                           Does the title contain   
                                           non-standard characters  
                                           (e.g. © ° ± ² ³ µ)?      
  **Abstract**      Short text only entry  Do you understand the    
                    that will be the first abstract?\               
                    information that users Is the abstract written  
                    will see about the     in plain English?\       
                    text.                  Does the abstract        
                                           describe the resource    
                                           rather than an           
                                           activity/project which   
                                           produced it?\            
                                           Do the first few         
                                           sentences summarise the  
                                           contents of the          
                                           resource?\               
                                           Does the abstract also   
                                           explain \'Where\',       
                                           \'When\', \'How\',       
                                           \'Why\' and \'Who\' (if  
                                           appropriate)?\           
                                           Does the abstract        
                                           contain unexplained      
                                           technical terms or       
                                           acronyms?\               
                                           Does the abstract        
                                           contain non-standard     
                                           characters (e.g. © ° ± ² 
                                           ³ µ)?                    
  Image details     link to an image to be                          
                    used as the logo                                
  Publication State There is a drop down   \* Old\                  
                    box for the            \* working - use this    
                    publication state of   while preparing the      
                    the data in the        record prior to dataset  
                    archive (and also the  release\                 
                    Observation Collection \* published - dataset   
                    record). Options are   details are available to 
                                           users, but the content   
                                           of the dataset and/or    
                                           metadata may still be    
                                           changing\                
                                           \* Citable - this is     
                                           ONLY to be used for      
                                           dataset that are static  
                                           and thus have a DOI      
                                           assigned to them         
  Observation       A filter and select                             
  Collections       option to choose which                          
                    Observation                                     
                    Collections hold the                            
                    resulting data                                  
                    produced by this                                
                    project. If the                                 
                    desired Observation                             
                    Collection doesn\'t                             
                    exist then use the +                            
                    button to start                                 
                    creating a new one                              
                    (which is immediately                           
                    added to the selection                          
                    too)                                            
  Keywords          A list of comma                                 
                    separated keywords                              
  Project Status    Controlled vocab to    \* Completed\            
                    indicate the status of \* Historical Archive\   
                    the project - should   \* Obsolete\             
                    be revised to rule out \* Ongoing\              
                    ones that aren\'t      \* Planned\              
                    relevant and to line   \* Required\             
                    up wit those in the    \* Under development\    
                    DMP tool where                                  
                    possible.                                       
  Parent Project    Select another Project                          
                    record that acts as an                          
                    parent to this                                  
                    record.. e.g. a                                 
                    Programme record for                            
                    which this Project                              
                    record is one of the                            
                    child projects                                  
  Sub-projects      This shows a list of                            
  (read only)       other project records                           
                    treating this record                            
                    as it\'s parent                                 
  Identifiers       Unless adding a DOI,   *the moles2 url is to    
                    or a new abbreviation, ensure that users who    
                    please leave these as  find an reference in a   
                    given.                 paper to a MOLES2 record 
                                           using the URLs given     
                                           then can confirm to      
                                           themselves that they are 
                                           on the equivalent MOLES3 
                                           page.*                   
  Parties           A list of              \* **CEDA Officer** -    
                    people/organisations   internal CEDA person     
                    (called collectively a responsible for the      
                    \""party\"") involved    record/resource in       
                    with the data creation question\                
                    or curation.\          \* **Metadata Owner** -  
                    Select (or create) a   party maintaining this   
                    named party from the   metadata record (data    
                    list, select their     centre)\                 
                    role from the          \* Point of Contact -    
                    controlled vocabulary  default is the data      
                    and, if more than one  centre, but may be used  
                    party carrying out     in due course for a      
                    that role, use the     named person from the    
                    priority number to     author list too\         
                    sequence them in this  \* Principle             
                    role.                  Investigator - party     
                                           leading the Project\     
                                           \* Co-Investigator -     
                                           person co-leading the    
                                           project\                 
                                           \* Funder - the party    
                                           funding the Project      
  Online Resources  Online link to         Do resource locators     
                    relevant resource -    include titles which are 
                    e.g. documentation,    concise, accurate        
                    WPS.\                  accounts of the resource 
                    There are set          in question?\            
                    categories that should Do resource locators     
                    be used.\              include descriptions     
                    Do NOT use: DMP,       which fully explain      
                    Download, Apply for    their purpose?           
                    Access - these are                              
                    handled elsewhere.                              
  Reviews           For carrying out       *For the time being      
                    metadata content       please refer all new     
                    reviews - not in       records to Anabelle or   
                    proper operation just  Graham for a review      
                    yet.                   before they are          
                                           published. This is to    
                                           ensure consistent        
                                           quality checks are       
                                           carries out*             
  Migration         All the old legacy     *Please leave the        
  Properties        content from the       \""moles2citation\"" at    
                    MOLES2 \""Content\""     present, but look to     
                    section is in here -   move other items into    
                    split into the various other parts of MOLES if  
                    div tags.              possible - e.g. add      
                                           items under the links    
                                           section to the online    
                                           resources section. Once  
                                           migrated feel free to    
                                           delete content*          
  ----------------- ---------------------- ------------------------ -----------------
",https://help.ceda.ac.uk/article/4154-opmanmoleseditoruserguideproject-ceda#project-records-{#projectrecords},12190,852
{#a},":::
",https://help.ceda.ac.uk/article/4154-opmanmoleseditoruserguideproject-ceda#{#a},4,1
CEDA CF Conventions examples of global attributes,"The [CF
conventions](https://help.ceda.ac.uk/article/4507-the-cf-metadata-convention)
are guidelines and recommendations as to where to put information, and
they provide advice as to what information you might want to include in
your ([NetCDF](https://help.ceda.ac.uk/article/106-netcdf)) file. Extra
attributes are not outlawed and CEDA **strongly** recommends that you
include as much information as possible.
CF conventions make one global attribute mandatory: **Conventions**
\""CF-1.0\""
For the **source**, if the data are model generated, source should name
the model and the version number. If the data are observational, source
should characterise them, e.g. surface observation, radiosonde,
satellite.
With the **history** attribute, well-behaved generic software will
automatically append its name, input parameters, and a timestamp.
Here we give three examples of datasets using the CF conventions, with
the recommended global attributes:
-   [instrumental data](#instrument)
-   [gridded model data](#model), and
-   [surface data](#surface).
",https://help.ceda.ac.uk/article/4432-ceda-cf-examples,1054,132
Instrumental Data {#instrument},"  ----------------------- ----------------------- ---------------------------------------------------------------------------------------------------------------------------
  Global attributes       Succinct definition     Instrumental data example
  **Conventions**                                 \""CF-1.0\"";
  **title**               A succinct description  \""Chilbolton 3-GHz Advanced Meteorological Radar: Doppler Radar\"";
                          of what is in the       
                          dataset.                
  **institution**         Specifies where the     \""Original data recorded at Chilbolton Observatory (BN_Grid_Ref=\\\""SU 394 386\\\""), part of the Radio Communications
                          original data were      Research Unit, RAL, UK.\\n\""\
                          produced.               \""Data processed at University of Reading.\\n\""\
                                                  \""Data held at the Centre for Environmental Data Analysis (CEDA),UK.\"";
  **source**              The method of           \""Chilbolton 3-GHz Advanced Meteorological Radar (CAMRa)\\n\""\
                          production of the       \""Frequency: 3.075 GHz\\n\""\
                          original data.          \""Antenna diameter: 25m\\n\""\
                                                  \""Peak power: 560 kW\\n\""\
                                                  \""Pulse width: 0.5 us\\n\""\
                                                  \""Pulse repetition frequency: 610Hz\\n\""\
                                                  \""Beam width: 0.28 degrees\\n\""\
                                                  \""Cross-polar isolation: -34 dB\"";
  **history**             Provides an audit trail \""16/08/02 10:47:54 - netCDF generated from original data using softare version 1.1.7 by \"" *user*\"" \""*data file* \""\\n\""\
                          for modifications to    \""16/08/02 10:47:55 - unfolded data using chilunfold -noisyPHIDP\"" -user \"" *user*\"" \""*data file*\""\"";
                          the original data.      
  **references**          Published or web-based  \""Information on the data is available at
                          references which        [https://artefacts.ceda.ac.uk/badc_datadocs/chilbolton%5cn\""](https://artefacts.ceda.ac.uk/badc_datadocs/chilbolton%5cn)\
                          describe the data, or   \""Further information on the data is available at
                          the methods used to     [http://www.met.rdg.ac.uk/radar/doc/camra.html\"";](http://www.met.rdg.ac.uk/radar/doc/camra.html)
                          produce them.           
  **comment**             Miscellaneous           \""In calculating the height of each data point, the curvature of the earth should be accounted for.\\n\""\
                          information about the   \""Data restrictions: for academic research use only.\"";
                          data or the methods     
                          used to produce them.   
  ----------------------- ----------------------- ---------------------------------------------------------------------------------------------------------------------------
",https://help.ceda.ac.uk/article/4432-ceda-cf-examples#instrumental-data-{#instrument},3167,227
Gridded Model Data {#model},"  ----------------------- ----------------------- -----------------------------------------------------------------------------------------------------------------
  Global attributes       Succinct definition     Gridded model data example
  **Conventions**                                 \""CF-1.0\"";
  **title**               A succinct description  \""ECMWF ERA-40 Re-analysis data: Model Level Parameters\"";
                          of what is in the       
                          dataset.                
  **institution**         Specifies where the     \""Data owned by ECMWF.\\n\""\
                          original data were      \""Data held by the Centre for Environmental Data Analysis (CEDA),UK\"";
                          produced.               
  **source**              The method of           \""Data generated by the European Centre for Medium-Range Weather Forecasts (ECMWF) T159L60 version of the
                          production of the       Integrated Forecast System ERA-40 model.\"";
                          original data.          
  **history**             Provides an audit trail \""07/08/01 - Data originally extracted from the ECMWF MARS archive.\\n\""\
                          for modifications to    \""21/03/02 - Conversion from GRIB to netCDF format at CEDA using XCONV V1.05\"";
                          the original data.      
  **references**          Published or web-based  \""Information on the data is available at
                          references which        [http://archive.ceda.ac.uk/ecmwf-e40%5cn\""](http://archive.ceda.ac.uk/ecmwf-e40%5cn)\
                          describe the data, or   \""Further information is available from the ECMWF web pages at
                          the methods used to     [https://www.ecmwf.int/research/era/Project/index.htm\"";](https://www.ecmwf.int/research/era/Project/index.htm)
                          produce them.           
  **comment**             Miscellaneous           \""Data restrictions: for academic research use only.\"";
                          information about the   
                          data or the methods     
                          used to produce them.   
  ----------------------- ----------------------- -----------------------------------------------------------------------------------------------------------------
",https://help.ceda.ac.uk/article/4432-ceda-cf-examples#gridded-model-data-{#model},2348,173
Surface Data {#surface},"  ----------------------- ----------------------- ---------------------------------------------------------------------------------------------------------------
  Global attributes       Succinct definition     Surface station data example
  **Conventions**                                 \""CF-1.0\"";
  **title**               A succinct description  \""UK Met Office Surface data: Rainfall Amount\"";
                          of what is in the       
                          dataset.                
  **institution**         Specifies where the     \""Data owned by UK Met Office.\\n\""\
                          original data were      \""Data held by the Centre for Environmental Data Analysis (CEDA),UK.\"";
                          produced.               
  **source**              The method of           \""UK Met Office surface station data\\n\""\
                          production of the       \""MET_DOM: WADRAIN\\n\""\
                          original data.          \""STATION_ID: 261316\"";
  **history**             Provides an audit trail \""Data collected on day of measurement.\\n\""\
                          for modifications to    \""30/11/97 - Stored in Met Office MIDAS database.\\n\""\
                          the original data.      \""12/05/00 - Extracted from MIDAS by CEDA.\"";
  **references**          Published or web-based  \""Information on the data is available at
                          references which        [https://artefacts.ceda.ac.uk/badc_datadocs/surface/\"";](https://artefacts.ceda.ac.uk/badc_datadocs/surface/)
                          describe the data, or   
                          the methods used to     
                          produce them.           
  **comment**             Miscellaneous           \""Data restrictions: for academic research use only.\"";
                          information about the   
                          data or the methods     
                          used to produce them.   
  ----------------------- ----------------------- ---------------------------------------------------------------------------------------------------------------
",https://help.ceda.ac.uk/article/4432-ceda-cf-examples#surface-data-{#surface},2130,152
Instrument Records,"::: {#wikipage}
[Original trac
page](http://team.ceda.ac.uk//trac/ceda/wiki/opman/MOLESEditorUserGuide/instrument)
::: wiki-toc
",https://help.ceda.ac.uk/article/4185-instrument-records,128,7
Page Contents,"1.  [Instrument Records](#InstrumentRecords)
:::
These records give background information on the Instrument and NOT the
data in the archive from the instrument - this is accessed through the
links to the relevant Observation (aka Dataset in the user view).
  -----------------------------------------------------------------------------------
  Field ( **bold =  Description            Checklist/\*Controlled   Y/N
  mandatory field**                        vocab explanations/      
  )                                        *Notes*                  
  ----------------- ---------------------- ------------------------ -----------------
  **Title**         Short text title for   Do you understand the    
                    the dataset.           title?\                  
                                           Is the title brief and   
                                           simple?\                 
                                           Does the title contain   
                                           unexplained technical    
                                           terms or acronyms?\      
                                           Does the title describe  
                                           the resource rather than 
                                           the activity/project     
                                           which produced it?\      
                                           Is the title in sentence 
                                           case?\                   
                                           Is the title in          
                                           English?\                
                                           Does the title contain   
                                           non-standard characters  
                                           (e.g. © ° ± ² ³ µ)?      
  **Abstract**      Short text only entry  Do you understand the    
                    that will be the first abstract?\               
                    information that users Is the abstract written  
                    will see about the     in plain English?\       
                    text.                  Does the abstract        
                                           describe the resource    
                                           rather than an           
                                           activity/project which   
                                           produced it?\            
                                           Do the first few         
                                           sentences summarise the  
                                           contents of the          
                                           resource?\               
                                           Does the abstract also   
                                           explain \'Where\',       
                                           \'When\', \'How\',       
                                           \'Why\' and \'Who\' (if  
                                           appropriate)?\           
                                           Does the abstract        
                                           contain unexplained      
                                           technical terms or       
                                           acronyms?\               
                                           Does the abstract        
                                           contain non-standard     
                                           characters (e.g. © ° ± ² 
                                           ³ µ)?                    
  Image details     link to an image to be                          
                    used as the logo                                
  Instrument Type   Controlled vocab to    *more can be added -     
                    categorise the         please suggest \[work    
                    instrument type        also needed to link this 
                                           to a vocab in due        
                                           course\]*                
  Identifiers       Unless adding a new    *the moles2 url is to    
                    abbreviation, please   ensure that users who    
                    leave these as given.  find an reference in a   
                                           paper to a MOLES2 record 
                                           using the URLs given     
                                           then can confirm to      
                                           themselves that they are 
                                           on the equivalent MOLES3 
                                           page.*                   
  Parties           A list of              \* Operator - party who  
                    people/organisations   operates the instrument\ 
                    (called collectively a \* **CEDA Officer** -    
                    \""party\"") involved    internal CEDA person     
                    with the instrument or responsible for the      
                    the metadata record.\  record\                  
                    Select (or create) a   \* Funder - party who    
                    named party from the   funds the instrument\    
                    list, select their     \* **Metadata Owner** -  
                    role from the          party maintaining this   
                    controlled vocabulary  metadata record (data    
                    and, if more than one  centre)\                 
                    party carrying out     \* Point of Contact -    
                    that role, use the     default is the data      
                    priority number to     centre, but may be used  
                    sequence them in this  in due course for a      
                    role.                  named person from the    
                                           author list too\         
                                           \                        
  Online Resources  Online link to         Do resource locators     
                    relevant resource -    include titles which are 
                    e.g. documentation.\   concise, accurate        
                    There are set          accounts of the resource 
                    categories that should in question?\            
                    be used.\              Do resource locators     
                    Do NOT use: DMP,       include descriptions     
                    Download, Apply for    which fully explain      
                    Access - these are     their purpose?           
                    handled elsewhere.                              
  Reviews           For carrying out       *For the time being      
                    metadata content       please refer all new     
                    reviews - not in       records to Anabelle or   
                    proper operation just  Graham for a review      
                    yet.                   before they are          
                                           published. This is to    
                                           ensure consistent        
                                           quality checks are       
                                           carries out*             
  Migration         All the old legacy     *Please leave the        
  Properties        content from the       \""moles2citation\"" at    
                    MOLES2 \""Content\""     present, but look to     
                    section is in here -   move other items into    
                    split into the various other parts of MOLES if  
                    div tags.              possible - e.g. add      
                                           items under the links    
                                           section to the online    
                                           resources section. Once  
                                           migrated feel free to    
                                           delete content*          
  -----------------------------------------------------------------------------------
:::
",https://help.ceda.ac.uk/article/4185-instrument-records#page-contents,8251,578
Outline,"Over time we need to refind the catalogue content, especially with older
information that remains in a poor state for historical reasons. These
notes indicate how to safely tidy up content to ensure that:
-   information is not lost 
-   existing URLs that may have been used externally resolve to an
    equivalent, meaningful resources
-   duplications/variations are avoided
The notes will be in two parts:
1.  General principals and workflows
2.  Specific record types and workflows
NOTE - these comments below refer to the main record types in the
catalogue, ie. Observations, Observation Collections, Projects,
Platforms, Computations and Instruments records. Party records are also
covered, but as these are a different type of object in the catalogue
they have a dedicated section that needs to be referred to. This does
NOT touch on other types of record within the catalogue which are often
sub-components of the main record types. For guidance on those please
contact the catalogue manager.
",https://help.ceda.ac.uk/article/5103-catalogue-content-tidying#outline,1002,158
General Principals,,https://help.ceda.ac.uk/article/5103-catalogue-content-tidying#general-principals,0,0
,,https://help.ceda.ac.uk/article/5103-catalogue-content-tidying#,0,0
0. Archive content \'retirement\' has a set procedure to follow\... (inc empty directories),"These notes are for catalogue content that isn\'t related to
removal/migration of archive content as well\... if you are tidying up
the archive then see the [Remove Data
Procedure](https://help.ceda.ac.uk/article/4543-remove-data-procedure)
",https://help.ceda.ac.uk/article/5103-catalogue-content-tidying#0.-archive-content-\'retirement\'-has-a-set-procedure-to-follow\...-(inc-empty-directories),241,29
1. Before deleting anything be aware of the knock-on effects!,"As the catalogue is a relational database any given record may be linked
to from other records. For the principal record types there  *should* be
a banner at the top of the record indicating any connections that may be
present:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/654dfa81687c016dc15b7512/file-FRYrqlFtnm.png)Some
of the connections are a little hidden - e.g. the link between an
Observation and an Instrument/Platform record will go through the
intermediately \'Acquisition\' object\... and there may also be a
Composite Process object that sits between an Observation and the
Acquisition too.
",https://help.ceda.ac.uk/article/5103-catalogue-content-tidying#1.-before-deleting-anything-be-aware-of-the-knock-on-effects!,646,84
2. Identify an equivalent end-point/record,"Often tidying up content is due to poorly structured content or
duplicates, meaning that we need to re-work the way that the cataloguing
is done to cover a part of the archive. However, whilst this is good we
need to find what will the be replacement record that takes the place of
the content we\'re aiming to remove. Once you\'ve done that you can then
move on to step 3.
It might also be possible that the record is just an orphaned record
with no connection to anything else in the catalogue or, more
importantly, the archive and so there\'s no equivalent record to replace
it and it will be a simple case of deletion. In this case also check if
there is a need to record this step back from publishing something -
e.g. record the fact that data were not delivered on a JIRA ticket for
the project so that we have a record that X was expected but not
delivered (it has been practice at times to spin up catalogue content in
anticipation of content being delivered which has then failed to
materialise).
",https://help.ceda.ac.uk/article/5103-catalogue-content-tidying#2.-identify-an-equivalent-end-point/record,1007,186
3. There\'s a tool for that\...,"For cases where there\'s duplicate records or a bulk (say more than 10
records) operation to be done there may be an existing tool to do the
\'merger\' or can be written to make the process quicker. Check out the
Tools section below to see what has been listed and/or speak the the
catalogue manager about these.
",https://help.ceda.ac.uk/article/5103-catalogue-content-tidying#3.-there\'s-a-tool-for-that\...,313,57
4. Switch your record linking,"If you have other records linking to this record that you wish to remove
(e.g. an obsolete party record) then it\'s important to reconnect those
other records to your equivalent record. For some cases we may already
have a tool to help with this (see Tools section below), but before that
is done there may be a need to complete information transfer\...
",https://help.ceda.ac.uk/article/5103-catalogue-content-tidying#4.-switch-your-record-linking,354,62
5. Retain information and identifiers,"The record that you want to retire may have various metadata elements
that contain useful information that need to be ported over to your
equivalent record. In particular:
-   any previously used \'identifiers\' 
-   the present \'MOLES 3\' url for the record
This is because we have redirect services in place that do a catalogue
look-up to resolve those URLs to the equivalent records. Add these to
the \'identifiers\' section of the record you are retaining.\
Other items to retain include details stored in other fields on the
record, especially the Abstract and *Migrated Properties*. See specific
records below for other fields and things to note.
",https://help.ceda.ac.uk/article/5103-catalogue-content-tidying#5.-retain-information-and-identifiers,654,106
A word on \'Migrated Properties\',"\... aka the \""More Info (under review)\"" content at the bottom of the
\'Details\' tab.
Moving to the present catalogue from the previous instantiation was
mostly a smooth operation mapping content over to equivalent
records/fields in the latest catalogue instantiation. However, there
were various fields that could not be directly mapped over and so were
stored in the \'migrated properties\' section, requiring additional work
to migrate the information to some equivalent place in due course. In
most cases this is things like moving documentation to pages in the
[CEDA Artefact Service](https://artefacts.ceda.ac.uk/) (for content that
may be changed), [CEDA Document
Repository](https://zenodo.org/communities/ceda-document-repository?q=&l=list&p=1&s=10&sort=newest)
(for fixed content) or moving links to the \'online resource\' section
of your replacement record.
More needs to be done on this generally, so speak to the
catalogue manager about this to prompt further work!
",https://help.ceda.ac.uk/article/5103-catalogue-content-tidying#a-word-on-\'migrated-properties\',982,135
5. Removal of the record,"If you\'re not using a \'merger\' tool as noted below, once you\'ve
moved content from the record and you\'re happy to have it removed then
you should be good to go!
",https://help.ceda.ac.uk/article/5103-catalogue-content-tidying#5.-removal-of-the-record,166,31
General Workflows,,https://help.ceda.ac.uk/article/5103-catalogue-content-tidying#general-workflows,0,0
,"Observation Records
",https://help.ceda.ac.uk/article/5103-catalogue-content-tidying#,20,2
Party Records," - see the tools section
",https://help.ceda.ac.uk/article/5103-catalogue-content-tidying#party-records,25,5
Observation/Observation Collection," - Results. If there was a Result linked to the record then don\'t
delete the Result when coming to remove your Observation record. Instead
add this as an \'old data path\' on your new Observation record\'s
Result object.
",https://help.ceda.ac.uk/article/5103-catalogue-content-tidying#observation/observation-collection,222,38
There\'s a Tool For That\... (perhaps),"For duplicate entries that need \'merging\' - i.e. switching connections
to one record to link up with another record you want to retain and the
remove the obsolete record (note, these merger tools will also handle
any existing links to records that are to be de-duplicated):
-   party merger - removes duplicate Party record instances
-   duplicate image remover - removed duplicate image instances
-   add_sam - add the archive manager as the latest CEDA Officer where
    needed on records
-   new_ceda_officer - adds a given person as the latest CEDA Officer
    when a previous staff member moves on
-   composite merger - merges composite record content
-   update docs urls - update online resource links
-   tag for export - tags records for export into the NERC Data
    Catalogue Service
-   add online resource tool - adds new online resources to a set of
    records defined by a list of UUIDs
-   tag_obs_cols - need to check 
-   procedure merger  - need to check
-   id_merer  - need to check
No suitable tool but you\'re doing bulk changes? The catalogue manager
can help script some of this work.\
",https://help.ceda.ac.uk/article/5103-catalogue-content-tidying#there\'s-a-tool-for-that\...-(perhaps),1115,189
opmanDataPushTool CEDA,"::: {#wikipage}
[Original trac
page](http://team.ceda.ac.uk//trac/ceda/wiki/opman/DataPushTool)
",https://help.ceda.ac.uk,96,5
Description {#Description},"The Data Push Tool is a perl script used for pushing data to the BADC in
a hopefully efficient and robust way from remote sites. Features
include:
-   Support for ftp or
    [bbftp](http://ceda-internal.helpscoutdocs.com/article/4405-opmanbbftpserver-ceda){.wiki}
    transfer methods
-   Concurrent transfers
-   Checksumming of files after transfer, with:
    -   Automatic retries on bad checksum
    -   Email notification on repeated bad checksum
-   Logging
It has two basic modes of operation:
-   One-off mode: transfers a specified set of files (read from standard
    input) and then exits.
-   Daemon mode: keeps running, and watches for ASCII files to appear in
    a directory containing lists of data files to transfer.
The main limitation at present is that it makes a separate ftp or bbftp
session for each file to be transferred; it is therefore appropriate in
situations where the startup overhead of a session is small compared to
the data transfer itself.
For each file to be transferred, it will launch a process to do the
following:
-   Compute a local checksum.
-   Transfer the file to a temporary upload directory using ftp or
    bbftp.
-   Connect to the [checksum
    service](http://ceda-internal.helpscoutdocs.com/article/4280-checksum-service){.wiki}
    (CGI script) to obtain the checksum.
-   If successful (checksums match), connect to the related CGI script
    which moves the file to another directory at the BADC end
    (effectively tagging the file as ready for ingest), and then delete
    the original file.
-   If unsuccessful, retry up to some maximum number of times.
-   If still unsuccessful, send an email (up to some maximum number of
    emails per instance of the script.)
A number of the above processes will be launched in parallel, up to some
maximum.
Note that for I/O performance reasons, the tool uses a lockfile
mechanism to ensure sequential checksumming files at both ends, despite
concurrent transfers. (This is separate from the similar mechanism
imposed by the checksum service at the server end, and so in practice
the latter is redundant where this tool is the client.)
",https://help.ceda.ac.uk#description-{#description},2136,326
Installation {#Installation},"The data push tool is located
[here](http://team.ceda.ac.uk//trac/ceda/browser/ceda_software/data-push-tool){.source}
. Start by checking it out and copying to the remote site. For example:
-   SROOT = svn+ssh://glue.badc.rl.ac.uk/svn/badc
-   svn co \$SROOT/data-push-tool/trunk
-   tar cvfz data-push-tool.tar.gz \--exclude=.svn trunk
-   copy tarball to remote site
-   unpack tarball in a scratch directory
To install, then:
-   set the environment variables:
    -   \$EMAIL (email address to receive notifications from the script)
    -   \$BASEDIR (desired base directory of installation; should differ
        from where you unpacked the tarball) if you do not set these,
        then the installation will prompt you for the email address
        and/or default to installing into \$HOME/data-push-tool
-   run the Install script
-   customise the installation by editing the variables in the file
    script/lib/globals.pm of the installation, according to the
    description in the comment lines
-   ensure that permissions are set correctly; in particular, if the
    tool itself, or processes which provide input file lists for the
    tool running in daemon mode, are to run under different user IDs,
    then you should ensure that they have write permission on the
    relevant directories under log/ and run/ .
",https://help.ceda.ac.uk#installation-{#installation},1329,183
Running the data push tool {#Runningthedatapushtool},"-   First check the datafile permissions. Ensure that the tool runs with
    permission to read the files and to delete them (i.e. write
    permission on the directory). Provided that these are met, the tool
    does not actually need to run with ownership of the files. Also
    ensure that the files to be transferred are group-readable if bbftp
    is to be used. This is because bbftp preserves file permissions, and
    the checksum service will fail if the files are not group-readable
    at the BADC end.
<!-- -->
-   The main executable is called transfers.pl .
<!-- -->
-   To run the tool in one-off mode:
    -   Invoke the tool with a list of files on standard input. For
        example, filelist contains:
        ``` wiki
        /path/to/datafile1 
        /path/to/datafile2
        /path/to/datafile3
        ```
        and you do:
        ``` wiki
        /path/to/transfers.pl < filelist
        ```
<!-- -->
-   To run the tool in daemon mode:
    -   Start the daemon by:
        -   *either:* invoking the tool with the -d flag.
        -   *or better:* using the daemonctl script (in the same
            directory), which takes one of the following command line
            arguments: start , stop , restart , status , startmail .
            Most of these options are self-explanatory; startmail will
            start the daemon if not already running, and send an email
            if it does so; this is useful for cron jobs to ensure the
            daemon is running.
-   You then provide ascii files containing lists of data files for
    transfer. The ascii files can have arbitrary names, but should be
    put in the directory which you configured in globals.pm , the
    default being run/daemon/input_lists . These ascii files must
    contain the line \*\*END\*\* at the end, so that the daemon knows
    when the ascii file is completely written. For example,
    /path/to/data-push-tool/run/daemon/input_lists/my_arbitrary_file_name
    may contain:
    ``` wiki
    /path/to/datafile1 
    /path/to/datafile2
    /path/to/datafile3
    **END**
    ```
    The daemon will move these ascii files to the output directory when
    it has read the list of files from them and added them to its
    in-memory queue for transfer.
<!-- -->
-   The data push tool will inform of its progress by the following
    means:
    -   The main log file is at log/transfers.log.\<timestamp> or
        log/transfers-daemon.log.\<timestamp> \-- the timestamp is based
        on when the tool was launched
    -   The directory log/ftplogs contains separate output files
        containing the output of each individual ftp session.
    -   Serious errors will be communicated by email, e.g. if a file
        still fails to transfer after retries.
        -   The maximum number of failed transfers to advise by email is
            configurable; if this number is exceeded, an email will be
            sent warning that further failures will not be notified by
            email, although will continue to appear in the logs. The
            count of emails sent is per instance of the script, e.g. if
            running in daemon mode it is reset by restarting the daemon.
<!-- -->
-   Signals:
    -   \""kill -USR1\"" the top-level process in order to reset the count
        of emails sent. This means that after the script has stopped
        sending emails because of hitting the maximum number, it is
        possible to make it again send emails in case of future errors,
        without having to restart the script.
    -   \""kill -USR2\"" the top-level process in order to make it write
        to a file in the log directory the lists of datafiles being
        transferred and awaiting transfer. This means for example that
        if you have to stop the script and launch a new instance, then
        it is easier to construct a list of files as input for the new
        instance to continue where the old one left off. Also if you
        \""kill -TERM\"" the top-level script, it will produce these lists
        just before it terminates.
",https://help.ceda.ac.uk#running-the-data-push-tool-{#runningthedatapushtool},4084,604
Wishlist of future improvements {#Wishlistoffutureimprovements},"-   Warning on files not group readable if using bbftp
-   Option for emails on successful batch transfer (though maybe just
    use the [arrival
    monitor](http://ceda-internal.helpscoutdocs.com/article/4407-opmanfilearrivalmonitor-ceda){.wiki}
    ) instead.
-   Suspend and terminate signals that propagate to children.
:::
",https://help.ceda.ac.uk#wishlist-of-future-improvements-{#wishlistoffutureimprovements},329,37
CEDA Artefacts Service,"The CEDA Artefacts Service ( <https://artefacts.ceda.ac.uk/>) is
designed as a store for grey literature items related to the archive
which:
-   are transient in nature and 
-   can be open
If they are fixed items then first consider adding them to the CEDA
Document Repository in Zenodo as that is a much better option for open
grey literature items. Items that need access control will need to be
added to the archive under a suitable \'docs\' directory with access
control set in the access instructor system.
If you\'re not sure where an item would be best held see
<https://help.ceda.ac.uk/article/4827-where-to-put-data-docs>
",https://help.ceda.ac.uk/article/5066-ceda-artefacts-service,632,99
Adding/amending items to CEDA Artefacts Service.,"The artefacts service content is populated by a script that runs
regularly pulling content from a git repository:
<https://github.com/cedadev/artifacts>
Thus, to add items to the artefacts service:
1.  log into the cedadev git repo
2.  find the required directory you need to  add/amend content to 
3.  use the github tools to change/amend the item as needed
4.  commit your change with a suitable comment to aid traceability of
    what has been added/amended and why
The changed content ought to be live within a short space of time.
",https://help.ceda.ac.uk/article/5066-ceda-artefacts-service#adding/amending-items-to-ceda-artefacts-service.,536,87
What will be the URL for my item?,"Once you\'ve added your item to the github repo then you can find the
public URL to use for the item by either:
-   finding the item on the [Artefacts
    service](https://artefacts.ceda.ac.uk) and using the URL
-   replacing the github path
    (<https://github.com/cedadev/artifacts/tree/master/>) with with
    <https://artefacts.ceda.ac.uk/> .. e.g. \
    <https://github.com/cedadev/artifacts/blob/master/licences/specific_licences/Terms-and-Conditions-for-the-use-of-ESA-Data.pdf>\
    becomes\
    <https://artefacts.ceda.ac.uk/licences/specific_licences/Terms-and-Conditions-for-the-use-of-ESA-Data.pdf>
",https://help.ceda.ac.uk/article/5066-ceda-artefacts-service#what-will-be-the-url-for-my-item?,612,50
CEDA Archive account,"::: {.section .callout-yellow}
",https://help.ceda.ac.uk/article/39-ceda-account,31,3
The accounts system is currently being redeveloped.,"There is currently ongoing work to upgrade our CEDA accounts management
systems. If you have an older CEDA account (made more than 2 years ago),
please make sure you\'ve updated your password or you may not be able to
log in. Newer users or user\'s who have updated their password in the
last 2 years should not need to do anything.
:::
Access to CEDA Archive datasets and other resources sometimes requires
you to have a CEDA Archive account. While much of our data are openly
available to non-registered users, there are two principle reasons for
having CEDA Archive accounts:
1.  Some data has restricted use and hence access control. 
2.  Some data, while open for any use, may still require usage
    statistics to be gathered. For example, data from research facility
    may need usage metrics to justify the data collection. 
",https://help.ceda.ac.uk/article/39-ceda-account#the-accounts-system-is-currently-being-redeveloped.,834,141
Setting up a CEDA Archive account,"Anyone can register for a CEDA user account. We only use your details
for the purpose of access control and analysing usage - see our [CEDA
Privacy
Statement](https://help.ceda.ac.uk/article/4639-privacy-and-cookies).
Please note:  accounts are for individual use only and passing on your
account to others is not permitted as this could invalidate any access
licenses already granted.
[Register for a CEDA
account](https://services.ceda.ac.uk/cedasite/register/info/)
",https://help.ceda.ac.uk/article/39-ceda-account#setting-up-a-ceda-archive-account,469,60
Logging in,"You will be prompted to login at various places around the CEDA Archive,
for example, when trying to access restricted datasets. Follow the
onscreen instructions
",https://help.ceda.ac.uk/article/39-ceda-account#logging-in,162,25
Administering your CEDA user account,"To help us maintain our services, please keep your account details up to
date.
[Update and view your account
details](https://services.ceda.ac.uk/cedasite/myceda/user/)
This is important as such changes as change of country or institution
type may effect your access to some restricted resources (for example,
access to ECMWF data is only available to UK based researcher, so
someone either moving abroad and/or out of an academic institution would
no longer be able to make use of those data).
",https://help.ceda.ac.uk/article/39-ceda-account#administering-your-ceda-user-account,495,76
"NOTE - DO NOT PUBLISH THESE ARTICLES {#note---do-not-publish-these-articles children-count=""0""}","just click on the tick to save them  
","https://help.ceda.ac.uk/article/4587-template-specific-dataset-process#note---do-not-publish-these-articles-{#note---do-not-publish-these-articles-children-count=""0""}",38,8
What are the data,"**MOLES collection page: **
",https://help.ceda.ac.uk/article/4587-template-specific-dataset-process#what-are-the-data,28,4
"Who is involved {#who-is-involved children-count=""0""}","**CEDA officer: **
**Contact:**
**[Ingestion process]{.ul}**
**Ingest stream name(s):**
**Manual cut and paste commands:**
-   Are there any scripts? What are they?
\<\>
-   Is it a manual or automatic process?
\<\>
-   How to run scripts if manual process?
\<\>
-   How often should this be checked?
\<\>
**QC checks:**
Are files checked? How to check the files?
**Common problems:**
**Any other useful information need to know:**
","https://help.ceda.ac.uk/article/4587-template-specific-dataset-process#who-is-involved-{#who-is-involved-children-count=""0""}",432,68
FileSet Allocation and other storage procedures,"::: {#wikipage children-count=""0""}
",https://help.ceda.ac.uk/article/4224-fileset-allocation-procedure,35,3
Filesets,"A storage fileset is an allocation of storage space for a chunk of the
archive. It may house a particular dataset, a portion of a dataset or
even several datasets.  A fileset may be manifested as a directory on
disk volume, or portion of a tape system, or as a bucket in an object
store. All these storage systems have practical limits in the volume of
data that can be stored and the number of files that they can contain.
Regardless of the logical datasets boundaries it is important to divide
the storage space into filesets so that storage operations, such as
migration and audit can be carried out. Clearly as storage media evolve
the practice limits of the storage may change and any advice on filesets
will need to evolve. Examples of current constraints are:
-   Disk partition size
-   Practical object store bucket sizes
-   Number of objects handled by file metadata servers for parallel disk
    volumes
-   Tape sizes
-   Audit job run times
Filesets divide the storage in terms of volume and number of files. Each
fileset has an estimate of its final size. If the dataset is going to
grow indefinitely then estimate the size for 4 years ahead. This allows
volume planning, which stops unneeded migration actions. The fileset
associates a hierarchical identifier for the logical dataset with a
storage system; In its simplest form, a storage directory pointed to via
a symbolic link.
    /badc/cira -> /datacentre/archive/storage-234-cira
",https://help.ceda.ac.uk/article/4224-fileset-allocation-procedure#filesets,1452,243
Best practice for creating filesets,"-   Keeping a dataset together on storage media is preferable as this
    leads to better retrieval performance and cleaner migration, audit
    and backup processes. For this reason it is best to allocate a
    single fileset per dataset and only if it exceeds practical limits
    to break it down further.
-   The volume of a single disk volume is currently the driver for
    limiting fileset volumes. Practically in order to aid disk
    management they should not exceed 20% of the size of a partition
    currently this gives a 40TB limit. If the archive is reaching
    capacity, it can be hard to find space for on disk so splitting
    anything larger than 10TB is best practice. If space is tight then
    even small volumes may be required.
-   File numbers are limited by the performance of parallel file system
    metadata servers, which can be overwhelmed if an audit of the data
    is requested of all files in quick succession. Best practice is to
    split filessets with more than 1M files.
",https://help.ceda.ac.uk/article/4224-fileset-allocation-procedure#best-practice-for-creating-filesets,1012,168
{#Partitions},,https://help.ceda.ac.uk/article/4224-fileset-allocation-procedure#{#partitions},0,0
Partitions {#Partitions},"*Partition administration should be done by the Storage Coordinator.*
Partitions are disk storage volumes for the archive. Partitions are
added through the admin interface of the cedaarchiveapp
<http://cedaarchiveapp.ceda.ac.uk/admin/cedaarchiveapp/fileset/> tool. 
The mount point is the directory under which the disk is mounted. The
status field should in the first instance be set to \""blank\"". The table
below explains the meanings of the partition status field.
  ------------ --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **Status**   **meaning**
  Blank        A new partition with nothing on it. It is not yet available for use as archive storage.
  Allocating   Moving a partition from Blank to Allocating labels the partition to accept new allocations of Filesets.
  Closed       Closed partitions are no longer available for new allocations. Partitions are marked as closed if they are deemed to be fully allocated. Closed partitions will still be receiving data and if volume prediction are right they will fill to around their capacity. If the predicted volume is under estimated then closed partitions may be marked as Allocating again.
  Migrate      Partitions marked as migrating have been scheduled for retirement. The data should be reallocated to \""Allocating\"" partitions and then migrated - This state is currently now used in the migration workflow.
  Retired      Partitions that have had all their data migrated are marked as retired.
  ------------ --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> In the Allocations process it is assumed that the Storage Coordinator
> will maintain a reasonable number of \""Allocating\"" partitions so that
> filesets are always allocatable.
",https://help.ceda.ac.uk/article/4224-fileset-allocation-procedure#partitions-{#partitions},2338,237
Adding new partitions {#CreatingaFileSetfromtheadmininterface},"Go to the [partition
list](https://cedaarchiveapp.ceda.ac.uk/admin/cedaarchiveapp/partition/). To
add a partition click the \""add Partition button\"". 
An alternative to manually adding partitions is to write a Django script
to add a large batch of partitions. 
",https://help.ceda.ac.uk/article/4224-fileset-allocation-procedure#adding-new-partitions-{#creatingafilesetfromtheadmininterface},261,33
Creating a FileSet from the admin interface {#CreatingaFileSetfromtheadmininterface},"*FileSet administration should be done by the Data Scientist (with
guidance from the storage coordinator)*
FileSets are added through the admin interface of the cedainfodb tool
<http://cedaarchiveapp.ceda.ac.uk/admin/login/?next=/admin/cedaarchiveapp/fileset/>.
To add a FileSet click the \""add fileset button\"". The fileset is
defined by the logical path and the estimated size that it is estimated
to grow to.
",https://help.ceda.ac.uk/article/4224-fileset-allocation-procedure#creating-a-fileset-from-the-admin-interface-{#creatingafilesetfromtheadmininterface},412,55
Creating FileSets via ingest_lib function {#CreatingFileSetsviaaCSVfile},"    (ingest) [badc@ingest1 ~]$ python
    Python 2.7.15 |Anaconda, Inc.| (default, Oct 23 2018, 18:31:10) 
    [GCC 7.3.0] on linux2
    Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
    >>> import ingest_lib
    >>> ingest_lib.make_fileset(""/badc/.testing/testfileset2"", ""5GB"")
    >>>
",https://help.ceda.ac.uk/article/4224-fileset-allocation-procedure#creating-filesets-via-ingest_lib-function-{#creatingfilesetsviaacsvfile},312,33
Creating via command line tool,"    (ingest) [badc@ingest1 ~]$ make_fileset /badc/.testing/testfileset 4GB
    (ingest) [badc@ingest1 ~]$ ls -l /badc/.testing/testfileset/
    total 0
    (ingest) [badc@ingest1 ~]$ ls -l /badc/.testing/testfileset
    lrwxrwxrwx 1 badc badcint 57 Jan 14 10:45 /badc/.testing/testfileset -> /datacentre/archvol3/pan102/archive/spot-9739-testfileset
    (ingest) [badc@ingest1 ~]$
",https://help.ceda.ac.uk/article/4224-fileset-allocation-procedure#creating-via-command-line-tool,381,34
Scripting fileset and partition operations,"The storage coordinator may need to perform various ad hoc operations to
update the content of the cedaarchiveapp. If the operations need to
apply to large batchs or are repeated then the best way may be to write
a simple script. The scripts are run as badc on
cedaarchiveapp.ceda.ac.uk. A typical script is show below.
    import sys
    sys.path.append('/usr/local/cedaarchiveapp_site')
    import django
    django.setup()
    from cedaarchiveapp.models import FileSet, Partition
    # find relevant partitions
    qb208 = Partition.objects.get(mountpoint=""/datacentre/archvol5/qb208"")
    qb209 = Partition.objects.get(mountpoint=""/datacentre/archvol5/qb209"")
    fss = FileSet.objects.filter(migrate_to=qb208)
    print(qb208)
    print(qb209)
    print(fss)
    for fs in fss:
        fs.migrate_to = qb209
        fs.save()
",https://help.ceda.ac.uk/article/4224-fileset-allocation-procedure#scripting-fileset-and-partition-operations,831,90
{#CreatingstorageassociatedwithFileSets},":::
",https://help.ceda.ac.uk/article/4224-fileset-allocation-procedure#{#creatingstorageassociatedwithfilesets},4,1
Accessing CEDA ElasticSearch Services from the Met Office,"<div>
These instructions provide a simple overview for querying the CEDA
ElasticSearch (ES) service from Met Office Linux servers. There are
three stages:
</div>
1.  Create a Python virtual environment and install the
    \""elasticsearch\"" library (only once)
2.  Activate the virtual environment (each time you want to query ES)
3.  Run an example query
",https://help.ceda.ac.uk/article/4672-ceda-elasticsearch-from-mo,355,54
"Step 1: Create a Python virtual environment and install the \""elasticsearch\"" library (only once)","<div>
Create a new working directory and create a virtual environment with:
</div>
    cd ~/
    mkdir es-test
    cd es-test/
    module load scitools
    python -m venv venv --system-site-packages
    source venv/bin/activate
    pip install elasticsearch
","https://help.ceda.ac.uk/article/4672-ceda-elasticsearch-from-mo#step-1:-create-a-python-virtual-environment-and-install-the-\""elasticsearch\""-library-(only-once)",258,32
Step 2: Activate the virtual environment (each time you want to query ES),"<div>
Each time you start a new terminal session, you will need to re-activate
the environment:
</div>
    module load scitools
    source venv/bin/activate
",https://help.ceda.ac.uk/article/4672-ceda-elasticsearch-from-mo#step-2:-activate-the-virtual-environment-(each-time-you-want-to-query-es),157,22
Step 3: Run an example query,"<div>
Save the following code to a file called \`test_query.py\`:
</div>
    from elasticsearch import Elasticsearch
    from elasticsearch import RequestsHttpConnection
    # `proxy_pac` in the linux desktop environment
    PROXY = ""webgate/proxy.pac""
    # Create a Connection class that allows you to specify a web-proxy setting
    class MyConnection(RequestsHttpConnection):
        def __init__(self, *args, **kwargs):
            proxies = kwargs.pop('proxies', {})
            super(MyConnection, self).__init__(*args, **kwargs)
            self.session.proxies = proxies
    # Create a test query and run it
    def run_query():
        es_url = ""https://jasmin-es1.ceda.ac.uk""
        # Set up the connection and run a query
        es = Elasticsearch([es_url], connection_class=MyConnection, proxies = {""http"":PROXY})
        query = {
                ""query"": {
                    ""match_phrase_prefix"": {
                        ""info.directory.analyzed"":""/badc/ukmo-nimrod/data/single-site/munduff-hill""
                                            }
                        },
                ""aggs"": {
                    ""total_vol"": {
                                  ""sum"": {
                                          ""field"": ""info.size""
                                          }
                                 }
                        }
                    }
        # Run query to get results object
        results = es.search(index=""ceda-fbi"", body=query)
        # Get some content from the results
        data = results[""hits""][""hits""]
        # Print some outputs
        print(f'Total Results: {results[""hits""][""total""]}')
        print(f'JSON returned for {len(data)} objects.')
        print(f'Example content: {data}')
        return data
    run_query()
Run the code using:
    python test_query.py
",https://help.ceda.ac.uk/article/4672-ceda-elasticsearch-from-mo#step-3:-run-an-example-query,1838,154
Metadata Basics,"::: {.section .callout-blue}
",https://help.ceda.ac.uk/article/4428-metadata-basics,29,3
Contents,"1.  [What are metadata? Why are they essential?](#What)
2.  [Elements of metadata attached to specific types of data](#Elem)
    -   [2.1 Metadata for tables of numbers (observations or model
        output)](#Nume)
    -   [2.2 Metadata for images of the Earth surface](#Pict)
    -   [2.3 Metadata for software](#Soft)
3.  [Metadata standards and data formats](#Stan)
    -   [3.1 Metadata in NASA Ames format](#NASA)
    -   [3.2 Metadata in NetCDF and the CF standard](#NetC)
4.  [Complementary metadata](#Comp)
:::
------------------------------------------------------------------------
*Please address your comments on -- or suggested contributions to --
this page to * [CEDA](http://www.ceda.ac.uk/contact/) *.*
",https://help.ceda.ac.uk/article/4428-metadata-basics#contents,720,87
1. What are metadata? Why are they essential? {#what},"The term *metadata* encompasses all the information necessary to
interpret, understand and use a given dataset. *Discovery metadata* more
particularly apply to information (keywords) that can be used to
identify and locate the data that meet the user\'s requirements ( *via*
a Web browser, a Web based catalogue, etc). *Detailed metadata* include
the additional information necessary for a user to work with the data
without reference back to the data provider (although one element of the
detailed metadata may be the data provider\'s contact!).
Metadata pertaining to observational data, for example, include details
about how (with which instrument or technique), when, where and with
which accuracy (or error bars) the data were collected, by whom
(including affiliation and contact address or telephone number) and in
the framework of which research project. In the case of processed data,
the nature of the initial raw data and the derivation process must be
stated. The nature and units of the recorded variables are of course
essential, as well as the grid or the reference system. Metadata
pertaining to model output should include the name of the model, the
conditions of the calculation, the type of constraint applied, the
length of the integration, the nature of the output, the geographical
domain over which the output is defined (when applicable), etc. Specific
conditions applying to the model or the experiment may be mentioned.
Metadata also obviously include information on the format in which the
data are stored, the order of the variables, etc, to allow potential
users to read them. Metadata pertaining to software models include the
key points of the theory on which the model is based, the techniques and
computational language used, references, etc.
Metadata relative to a specific data set can be provided as a separate
document or as a piece of the data set itself. For digital data sets,
this means that the metadata can sit in separate files (for example text
files) or be integrated into the data file(s), as a header or at
specified locations in the file. Some data formats provide room and
rules for metadata (see [Section 3](#Stan)).
As far as possible, metadata of data held by CEDA follow the guidelines
laid below. Data providers are encouraged to comply with the [CEDA
implementation of the Climate and Forecast (CF) Metadata
Convention](https://help.ceda.ac.uk/article/106-netcdf) (see also
[Section 3](#Stan)).
",https://help.ceda.ac.uk/article/4428-metadata-basics#1.-what-are-metadata?-why-are-they-essential?-{#what},2452,388
2. Elements of metadata attached to specific types of data {#Elem},"The following sub-sections list the minimal information that should
ideally accompany certain types of data commonly archived for the use of
atmospheric scientists.
",https://help.ceda.ac.uk/article/4428-metadata-basics#2.-elements-of-metadata-attached-to-specific-types-of-data-{#elem},165,23
"2.1 Metadata for tables of numbers (observations or model output) {#metadata-for-tables-of-numbers-observations-or-model-output id\""nume\""=""""}","Metadata should include the following overall information. Some
information in this list may be applicable in specific cases only.
-   **Information about the experiment.**\
    Date when experiment or model simulation started.\
    Site or trajectory bounding box or domain limits.\
    Platform, instrumentation.\
    Model name.\
-   **Information about the experimenter(s).**\
    Names, affiliation, contact address including e-mail, telephone
    number.\
    Research programme name, research project code.
-   **Information about the independent variables (usually
    spatio-temporal grid).**\
    Names, units, domain of definition of independent variables.\
    Interval values when appropriate.
-   **Information about the data, including processing level.**\
    Version number.\
    Date of last revision.\
    Processing level (nature of raw data, derivation method).\
    Nature, name, units, scaling factors, accuracy of dependent
    variables.
-   **Information about data storage.**\
    Number of files of the entire dataset.\
    File sizes.\
    File number of current file.
-   **Information about data format.**\
    Archive structure.\
    File structure.\
    Number of lines in file header if any.\
    Record structure.
-   **Additional information.**\
    May include particular conditions of experiment or model run, model
    boundary conditions, article reference, source of further
    information, or other comments.
","https://help.ceda.ac.uk/article/4428-metadata-basics#2.1-metadata-for-tables-of-numbers-(observations-or-model-output)-{#metadata-for-tables-of-numbers-observations-or-model-output-id\""nume\""=""""}",1452,176
[]{#Pict},"2.2 Metadata for images of the Earth surface
[]{#Pict}
[]{#Pict}
[]{#Pict}
[]{#Pict}
[]{#Pict}
[]{#Pict}
[]{#Pict}
[]{#Pict}
Elements of metadata of maps and (photographic, satellite,\...) images
of the Earth surface should include the following.
-   **Information about the picture.**\
    Date when picture was taken.\
    Date of last revision, if any.\
    Geographical resolution and coverage.\
    Orientation. Platform, technique used, wavelength channel.\
    Picture resolution (real size corresponding to pixel).
-   **Information about the experimenter(s).**\
    Names, affiliation, contact address including e-mail, telephone
    number.\
    Research programme name, research project code.
-   **Information about picture storage.**\
    Number of files of the entire images set.\
    File sizes.\
    File number and name of image file.
-   **Additional information.**\
    Photographic treatment.\
    Experiment associated with current image.\
    Any relevant information regarding the conditions when the picture
    was taken (e.g. meteorological conditions).\
    Any relevant information on the way the map was produced or the
    image derived.
",https://help.ceda.ac.uk/article/4428-metadata-basics#[]{#pict},1168,145
2.3 Metadata for software {#Soft},"Metadata pertaining to a model should include the following.
-   **Information on the model**\
    Brief description of model general aim.\
    Model structure.\
    Physical processes involved, including equation set.\
    Parameterisations.\
    Algorithmic implementation techniques used.\
    Spatio-temporal coverage when applying.\
    Boundary conditions, including reference(s).\
    Initial conditions, including reference(s).\
    Program language.\
    Input nature and format.\
    Output nature and format.\
    Summary of model validation, or appropriate reference(s).\
    Summary of results from former studies conducted with the model, or
    appropriate reference(s).\
-   **Information on the author(s)**\
    Names, affiliation, contact address including e-mail, telephone
    number.\
    Research programme name, research project code.
-   **Information on how to run the model**\
    Platforms, operation language, script.\
    Input files.
**N.B.** Metadata relative to software are commonly included as
comments, either in the top section of the source file, or at various
places of the code.
",https://help.ceda.ac.uk/article/4428-metadata-basics#2.3-metadata-for-software-{#soft},1118,134
3. Metadata standards and data formats {#Stan},"Since the evaluation of information relevance may vary widely with
individuals, some metadata standards have been -- and are still
currently being -- developed with the aim of standardising and unifying
metadata presentation. The other advantage of metadata standards is that
they ensure the transmission of the information contained in the
metadata (and hence the ability to use the data), in some predefined
generic way, to remote and future users, provided that the latter will
know the adopted conventions. Which in turn requires the existence,
maintenance and transmission of manuals describing the set of
conventions relevant to a particular metadata standard -- some kind of
*meta-metadata*.
Since a crucial section of the metadata pertains to the data format,
different metadata standards have been developed in conjunction with the
various data formats. (To know about the formats supported by CEDA,
please refer to the CEDA   [File Formats
Demystified](https://help.ceda.ac.uk/article/104-file-formats)).
Existing data format standards, and metadata standards alike, are based
both on the specific needs of confined scientific communities and on
habits already in use within these communities. All of them regularly
undergo updates and are susceptible of further evolution. In geosciences
and among disciplines where 2-dimensional Earth surface reference
systems play an important role (like archæology), the most popular data
formats seem to belong to the [GIS](http://www.gistandards.org.uk/)
family (*Geographic Information Systems*). In the atmospheric research
community, however, the third spatial (vertical) dimension obviously
plays a crucial role, along with time. Sections [3.1](#NASA) and
[3.2](#NetC) below respectively give a brief outline of two formats
widely used in the atmospheric sphere, namely the *NASA Ames Format for
Data Exchange*, applying to data coded in ASCII, and *NetCDF* (network
Common Data Form), applying to data coded in binary language and hence
better adapted to voluminous data sets such as 3- or 4-dimensional
fields, satellite data, etc. Both data formats include some metadata
rules.
Standard rules can be mandatory, conditional or optional. They apply to
three aspects of the metadata:
-   **Content**. Which elements of information must/should/may be
    recorded.\
    \
    Section 2 above is an attempt to answer this issue.\
    \
-   **Vocabulary**. Standard terms that must/should/may be used to
    describe the elements of information (i.e. allowed units, allowed
    variable names, etc.).\
    \
    A standard thesaurus was developed to address this issue, based on
    the [Climate and Forecast (CF) Metadata
    Convention](http://cfconventions.org/) and the specific needs of the
    CEDA and of atmospheric scientists.\
    Meanwhile, [data providers are strongly encouraged to comply with
    the CEDA[implementation of the
    CFconvention](https://help.ceda.ac.uk/article/106-netcdf)]{.ul} (see
    [Section 3.2](#NetC) below), [even when using a data format other
    than NetCDF]{.ul} (such as NASA Ames).\
    \
-   **Layout**. Order and syntax of the recorded elements (i.e. the
    format).\
    \
    This issue is addressed by each specific data format. It is
    essential to allow the retrieval of the information by a piece of
    software.\
",https://help.ceda.ac.uk/article/4428-metadata-basics#3.-metadata-standards-and-data-formats-{#stan},3323,465
3.1 Metadata in NASA Ames format {#NASA},"The [NASA Ames Format for Data
Exchange](http://cedadocs.ceda.ac.uk/73/4/index.html) has been developed
by S. Gaines and S. Hipskind at the NASA Ames Laboratory, for the
benefit of instrument scientists operating atmospheric probe apparatus
onboard balloons and aircrafts, and its straightforwardness and
portability serve this purpose perfectly. It is in principle able to
deal with 3- and 4-dimensional data sets, although the data layout
within a file, which shows its original aim (i.e. the storage of time
series), does not optimise the representation of fields on a 3-D or 4-D
gridded domain. NASA Ames formatted data are coded in ASCII, which
presents the noticeable advantage of being directly readable by (English
speaking) humans, but the drawback of producing cumbersome files, which
again is not optimal for 3-D or 4-D variables. Each NASA Ames file is
divided into a header and a body, the latter containing the data, the
former the metadata. The required metadata include both discovery and
detailed metadata.
NASA Ames rules include some statements about the metadata
[content]{.ul}. Any additional information (for example, elements listed
in [Section 2.1](#Nume) that would not fit into the provided rules) can
still be inserted in dedicated comment lines at the end of the header.
The metadata [layout]{.ul} is strictly defined in the NASA Ames format,
but for the comment lines, which are loosely constrained. A complete
description of the NASA Ames data and metadata format (including
[content]{.ul} and [layout]{.ul} rules) is available from the CEDA   
[NASA Ames Format Page](http://cedadocs.ceda.ac.uk/73/4/index.html).
The NASA Ames format makes no statement on any mandatory or suggested
[vocabulary]{.ul}. As mentioned earlier, data providers using NASA Ames
are strongly encouraged to follow the CEDA [guidelines on CF
conventions](https://secure.helpscout.net/members/authorize/?jump=http%3A%2F%2Fhelp.ceda.ac.uk%2Fauthorize%3FsiteId%3D564b4bd3c697910ae05f445d)
(see also [Section 3.2](#NetC) below).
",https://help.ceda.ac.uk/article/4428-metadata-basics#3.1-metadata-in-nasa-ames-format-{#nasa},2030,281
3.2 Metadata in NetCDF and the CF standard {#NetC},"[NetCDF](https://help.ceda.ac.uk/article/106-netcdf) is the binary data
format underlying the [Network Common
DataForm](https://www.unidata.ucar.edu/software/netcdf/) supported by
[Unidata](https://www.unidata.ucar.edu/). It allows the user to insert
metadata in the data files.
The [NetCDF Climate and Forecast (CF) Metadata
Convention](http://cfconventions.org/) has developed a standard dealing
mainly with [vocabulary]{.ul} rules. Although this standard was
developed with the NetCDF format in mind, it can be applied to any set
of geophysical data, and probably extended to cover a much broader range
of disciplines as well.
With the aim of providing a consistent way of describing atmospheric
data sets, CEDA has developed its own implementation of CF metadata
rules. If you are about to submit metadata to the CEDA, whether you use
NetCDF or not, please refer to the [CEDA implementation of the
CFConvention](https://help.ceda.ac.uk/article/4432-ceda-cf-examples).
",https://help.ceda.ac.uk/article/4428-metadata-basics#3.2-metadata-in-netcdf-and-the-cf-standard-{#netc},972,124
4. Complementary metadata {#Comp},"Any additional documentation on recorded data or images, whether
pertaining to a single data file or a whole dataset, that would not find
its place into the structures described above (because it does not fall
into any described category or because it is too voluminous) may be
attached to the data, for example in the form of text files. These
documents may for example include technique description, possible use of
the data, study conclusions, etc.
",https://help.ceda.ac.uk/article/4428-metadata-basics#4.-complementary-metadata-{#comp},452,75
"Outline {#outline children-count=""0""}","Data sometime come to the end of usefulness. E.g. they are depreciated
or need to be removed.
Note - for empty directories which were created in anticipation of data,
but data were not delivered please see the additional note at the end of
this page.
","https://help.ceda.ac.uk/article/4543-remove-data-procedure#outline-{#outline-children-count=""0""}",251,45
"Purpose {#purpose children-count=""0""}","This process outlines the necessary steps to carry out when removing a
dataset from the archive in terms of :
1.  Pre-removal checks
2.  Updating the associated MOLES Catalogue record
3.  Removing content from the archive
","https://help.ceda.ac.uk/article/4543-remove-data-procedure#purpose-{#purpose-children-count=""0""}",222,36
"Pre-removal checks {#pre-removal-checks children-count=""0""}","-   Is the dataset a popular one? May need to inform users ahead of the
    removal
-   What happens to back-up copies
","https://help.ceda.ac.uk/article/4543-remove-data-procedure#pre-removal-checks-{#pre-removal-checks-children-count=""0""}",119,22
"Catalogue Record {#catalogue-record children-count=""0""}","-   **Don\'t delete the record!\
    **
-   Update the dataset record so that:
    -   the Publication status = Removed
    -   Data Status is one of the \""old\"" status - with the exception of
        Historical Archive - that has a specific meaning
    -   Add a Removed Data date
    -   Add a \""Reason for Data Removal\"" in the box next to the Lineage
        statement (this will be added with the date of removal to the
        full Lineage statement in the user view and on record export
        should that happen)
    -   Add a news item if necessary
-   If possible add a link to a relevant dataset. This can be done as
    follows
    -   If superceded by (i.e. a newer version) another dataset - on the
        Superceding dataset record use the \""related observation\"" to
        indicate that this newer record supercedes the one associated
        with the removed data
    -   If external either consider creating an external dataset record
        and showing the appropriate relationship with this record OR
        (this is a stronger, more obvious link and has specific
        meaning!)
    -   simply add in a link to the other records as an online resource
-   Issue news item/email to users
","https://help.ceda.ac.uk/article/4543-remove-data-procedure#catalogue-record-{#catalogue-record-children-count=""0""}",1214,194
"Removing Archive Content {#removing-archive-content children-count=""0""}","-   leave the directory associated with the \""data path\"" of the MOLES
    Catalogue record in place - this will act as a placeholder in case
    users use a link that bypasses the catalogue. A script will keep the
    00README_catalogue_and_licence.txt file up to date with a note that
    the data have been removed and why
","https://help.ceda.ac.uk/article/4543-remove-data-procedure#removing-archive-content-{#removing-archive-content-children-count=""0""}",326,54
"Coping with empty directories (non-delivery of data) {#coping-with-empty-directories-non-delivery-of-data children-count=""0""}","Where directories have been created in the archive for data that then
fail to materialise, and no published MOLES record exists (otherwise
follow above procedure), the workflow is to:
1.  Ensure that a data product entry is created for the associated DMP
    entry in Datamad
2.  Mark that as not delivered adding a reason for non-delivery if
    possible. If the reason is not known (e.g. clearing up historic
    archive) also state this. It is best to record definitively what is
    and what is not known to avoid future questions.
3.  Remove archive directory using the appropriate tool
4.  If removal of archive directory other than the standard tools on
    Ingest machines (e.g. via loss or via cedaarchiveapp) then follow up
    work will be required to also get content removed from the FBI and
    DBI. Contact the Elasticsearch developer for assistance if required.
5.  If a MOLES record was created review the content and migrate as
    needed. For example, if a paper was produced instead of data then
    ensure the reference is recorded on an appropriate Project
    record/Collection record if possible.
","https://help.ceda.ac.uk/article/4543-remove-data-procedure#coping-with-empty-directories-(non-delivery-of-data)-{#coping-with-empty-directories-non-delivery-of-data-children-count=""0""}",1121,180
"Moved data {#moved-data children-count=""0""}","Where data are to be moved to a new location in the archive the workflow
is:
1.  Re-ingest a copy of the data using the Deposit Client based tool as
    appropriate to the new location
2.  Update associated catalogue record\'s Result object\'s data path
3.  Create a new Result object to record the old data path and link to
    the dataset\'s Result object as \'old data path\'
4.  Run necessary [FBI re-scanning on old data
    paths](https://ceda-internal.helpscoutdocs.com/article/4829-filling-in-missing-datasets#gsc.tab=0)
    to ensure that old content is removed from FBI/DBI listing (if
    directories don\'t disappear contact the person responsible for the
    FBI/DBI indexes in Elasticsearch and ask them to update as needed)
5.  Once you are happy that the process has been completed remove the
    old data
**NOTE - take care with regards to filesets. If the old path contains
one or more filesets in their entity discuss this with the archive
manager before proceeding**
","https://help.ceda.ac.uk/article/4543-remove-data-procedure#moved-data-{#moved-data-children-count=""0""}",987,150
Ingest script writing conventions,"The working versions of ingest scripts are generally in the badc home
area under software/datasets.
The scripts should be in a git repository on gitlab
(<https://breezy.badc.rl.ac.uk/>) to record changes and act as a
preserved copy of the scripts. Ingest scripts could contain passwords
for external services so the internal gitlab repository is most
suitable.
The scripts used for ingest are generally small scale and hard to
separate from configuration used in the deployment. Configuration and
scripts are changed very frequently and so should be kept in the same
package. 
",https://help.ceda.ac.uk/article/4687-ingest-script-writing-conventions,577,89
"Creating a ingest script package {#creating-a-ingest-script-package children-count=""0""}","Use the gitlab web interface to make a package.
git clone package into the software/datasets dir
","https://help.ceda.ac.uk/article/4687-ingest-script-writing-conventions#creating-a-ingest-script-package-{#creating-a-ingest-script-package-children-count=""0""}",97,16
"Updating scripts and configuration {#updating-scripts-and-configuration children-count=""0""}","Change files and commit to git
git status
git add
git commit -m \'a commit message\'
Push the commits to gitlab 
git push
","https://help.ceda.ac.uk/article/4687-ingest-script-writing-conventions#updating-scripts-and-configuration-{#updating-scripts-and-configuration-children-count=""0""}",122,23
"Scheduling and running {#scheduling-and-running children-count=""0""}","If there is a chance that the script will be run again, either manually
or as a scheduled job, then it should be added to ingest_control
([https://ceda-internal.helpscoutdocs.com/article/4272-ingest-control](https://ceda-internal.helpscoutdocs.com/article/4272-ingest-control#gsc.tab=0)).
Adding script to ingest control allows other users to control the
dataset workflow if the original creator of the script is not there, and
also documents the setup and environment for the script.
","https://help.ceda.ac.uk/article/4687-ingest-script-writing-conventions#scheduling-and-running-{#scheduling-and-running-children-count=""0""}",485,60
Terms and Conditions for data and information provided by the NERC or STFC through the Centre for Environmental Data Analysis (CEDA),"::: section
<div>
<div>
",https://help.ceda.ac.uk/article/3846-policies#terms-and-conditions-for-data-and-information-provided-by-the-nerc-or-stfc-through-the-centre-for-environmental-data-analysis-(ceda),24,4
Exclusion of Liability,"Changing circumstances may cause the STFC or NERC to have to change the
information and contents of CEDA\'s pages at any time. Whilst every
effort has been made to ensure the accuracy of information presented,
both STFC and NERC disclaim all responsibility for and accept no
liability for any errors or losses caused by any inaccuracies in such
information or the consequences of any person acting or refraining from
acting or otherwise relying on such information.
Your use of information provided by STFC or NERC through the Centre for
Environmental Data Analysis is at your own risk. Please read any
warnings given about the limitations of the information.
Neither NERC nor STFC gives any warranty as to the quality or accuracy
of the information or its suitability for any use. All implied
conditions relating to the quality or suitability of the information,
and all liabilities arising from the supply of the information
(including any liability arising in negligence) are excluded to the
fullest extent permitted by law.
Neither NERC nor STFC gives any warranty as to the accuracy or
completeness of data or images in the form in which they are cached or
downloaded to your computer, as they may be affected by on-line
conditions over which NERC or STFC has no control.
</div>
<div>
",https://help.ceda.ac.uk/article/3846-policies#exclusion-of-liability,1290,215
Notes on Limitations,"-   Scientific observations are made according to the prevailing
    understanding of the subject at the time. The quality of such
    observations may be affected by subsequent advances in knowledge,
    improved methods of interpretation, and better access to sampling
    locations.
-   Raw data may have been transcribed from analogue to digital format,
    or may have been acquired by means of automated measuring
    techniques. Although such processes are subjected to quality control
    to ensure reliability where possible, some raw data may have been
    processed without human intervention and may in consequence contain
    undetected errors.
-   Detail clearly defined and accurately depicted on large-scale maps
    may be lost when small-scale maps are derived from them.
-   Although samples and records are maintained with all reasonable
    care, there may be some deterioration in the long term. The most
    appropriate techniques for copying original records are used, but
    there may be some loss of detail and dimensional distortion when
    such records are copied.
-   Data may be compiled from the disparate sources of information at
    STFC's or NERC\'s disposal, including material donated to STFC or
    NERC by third parties, and may not have been subject to any
    verification or other quality control process.
-   Data, information and related records which have been donated to
    STFC or NERC have been produced for a specific purpose, and that may
    affect the type and completeness of the data recorded and any
    interpretation. The nature and purpose of data collection, and the
    age of the resultant material may render it unsuitable for certain
    applications/uses. You must verify the suitability of the material
    for your intended usage.
-   The data, information and related records supplied by STFC or NERC
    should not be taken as a substitute for specialist interpretations,
    professional advice and/or detailed site investigations. You must
    seek professional advice before making technical interpretations on
    the basis of the materials provided.
-   If a report or other output is produced for you on the basis of data
    you have provided to STFC or NERC, or your own data input into a
    STFC or a NERC system, please do not rely on it as a source of
    information about other areas or features.""
</div>
</div>
<div>
",https://help.ceda.ac.uk/article/3846-policies#notes-on-limitations,2403,365
Accessibility,"This page states our intention to provide a website that is usable and
accessible to all and details some of the measures taken. This
website\'s objective on accessibility is to conform to the guidelines
for UK Government Websites, which support the W3C\'s Web Content
Accessibility Guidelines at Level AA. These are to ensure that we
achieve and maintain an appropriate Web accessibility standard and that
our web sites are inclusive.
This accessibility statement applies only to the Centre for
Environmental Data Analysis (CEDA) websites and relates to all new and
reviewed public facing websites, but does not apply to any other site,
including:
-   Legacy websites operated by or for CEDA
-   Other websites operated by partner organisations
-   Any site that is linked from one of our pages
Please note, some sections of this web site are controlled by third
parties and so it has not been possible for the same standards of
accessibility to be applied to them.
We are making every effort to make CEDA\'s websites accessible,
including layout, and easy to use for everyone, no matter what browser
you choose to use, and whether or not you have any disabilities.
<div>
",https://help.ceda.ac.uk/article/3846-policies#accessibility,1173,195
Recruitment,"CEDA recruitment is handled by our parent organisation the Science and
Technology Facilities Council (STFC) which currently recruits through
the i-recruitment Portal. Please refer to the STFC Accessibility
Statement regarding that recruitment site.
</div>
<div>
",https://help.ceda.ac.uk/article/3846-policies#recruitment,262,35
Media formats,"Most complex Adobe® Acrobat® Portable Document Format (PDF) files on
this site added after November 2008 are tagged to allow basic
accessibility pre-existing PDFs will be tagged on request.
For more information and help about accessibility and changing your
browser settings please visit the BBC\'s My Web, My Way pages. More
information about PDF accessibility can be found on the Adobe website
accessibility section. To download a copy of Acrobat reader, please
visit the acrobat download page on the Adobe website. If you require any
further help and advice regarding assistive software and hardware,
please contact the Shaw Trust. Font size can be changed by setting your
browser settings to your own preference - this option is usually
available from the \""view\"" option in your browser toolbar.
CEDA videos are hosted on YouTube and presented with captions. To play a
video, users can click the triangle button on the player or the image.
Alternatively, keyboard-only and assistive software users can use the
""\[Video Autoplay\]"" link for this purpose.
</div>
<div>
",https://help.ceda.ac.uk/article/3846-policies#media-formats,1072,170
Tell us if its broken,"Maintaining an accessible site is an ongoing process and we are
continually working to offer a user friendly experience. However, if you
have any problems using this web site please contact us.
</div>
</div>
:::
",https://help.ceda.ac.uk/article/3846-policies#tell-us-if-its-broken,212,35
Introduction,"In keeping with the  [filenaming
convention](https://help.ceda.ac.uk/article/103-filenames) for
Instrumental (and other) datafiles, many of the names given in
the [current list of valid instrument
names](http://catalogue.ceda.ac.uk/listings/instr/) include
abbreviations or shortened versions of the chemical species that are
measured.
",https://help.ceda.ac.uk/article/3797-chemicalspeciesabbreviations#introduction,336,36
Chemical species abbreviations,"This list provides standards for use with chemical and particulate data.
  ----------- ----------------------------------------
  **c**       Carbon
  **co**      Carbon monoxide
  **co2**     Carbon dioxide
  **hc**      Hydrocarbons
  **hno2**    Nitrous acid
  **hno3**    Nitric acid
  **nh3**     Ammonium
  **no**      Nitrogen monoxide
  **nox**     NO + NO ~2~
  **no2**     Nitrogen dioxide
  **n2o**     Nitrous oxide
  **o3**      Ozone
  **pm**      Particulate matter
  **pm1**     PM ~1~
  **pm10**    PM ~10~
  **pm2p5**   PM ~2.5~
  **pmch**    Perfluoromethylcyclohexane C ~7~F~14~
  **pmcp**    Perfluoromethylcyclopentane C ~6~F~12~
  **so2**     Sulfur dioxide
  ----------- ----------------------------------------
",https://help.ceda.ac.uk/article/3797-chemicalspeciesabbreviations#chemical-species-abbreviations,736,72
Elastic Search Aircraft Indices,"::: {#wikipage}
[Original trac
page](http://team.ceda.ac.uk//trac/ceda/wiki/ElasticSearch/Indices)
::: wiki-toc
1.  [Elastic Search - Indices](#ElasticSearch-Indices)
    1.  [Naming Conventions](#NamingConventions)
    2.  [Overview of indices](#Overviewofindices)
        1.  [aerial-photo (aliased to \""ap\"")](#aerial-photoaliasedtoap)
        2.  [arsf-2015-05-27 (aliased to
            \""arsf\"")](#arsf-2015-05-27aliasedtoarsf)
        3.  [eufar-2015-04-27 (aliased to
            \""eufar\"")](#eufar-2015-04-27aliasedtoeufar)
        4.  [faam-2015-04-27 (aliased to
            \""faam\"")](#faam-2015-04-27aliasedtofaam)
        5.  [um-2015-05-15 (aliased to
            \""um\"")](#um-2015-05-15aliasedtoum)
    3.  [Querying Elasticsearch for the
        indices](#QueryingElasticsearchfortheindices)
:::
",https://help.ceda.ac.uk/article/4287-elastic-search---indices,813,51
Naming Conventions {#NamingConventions},"The indices in Elasticsearch are named according to the following
convention:
\[dataset_name\]-\[year\]-\[month\]-\[day\], where the date is the date
of processing.
For example, If one were indexing ARSF data from May 27th 2015, the
formal index name would be \""arsf-2015-05-27\"".
These index names can then be aliased to the dataset name using the
\""elasticsearch-head\"" tool (which is available
[here](http://jasmin-es1.ceda.ac.uk:9100){.ext-link} ) or as a chrome
addon.
For example, one could create an alias for the \""arsf-2015-05-27\"" at
\""arsf\"".
This is to allow multiple versions of the same index to exist on the
same Elasticsearch cluster.
Haystack indices are created using the pattern:
    cedamoles-haystack-prod_[date formatted using %d%m%Y_%H%M%S_%f]<br>
",https://help.ceda.ac.uk/article/4287-elastic-search---indices#naming-conventions-{#namingconventions},771,104
Overview of Production Indices {#Overviewofindices},"CEDA currently maintains several production indexes. The true name of
these indices, created as above, is likely to change as indices evolve
and is less important than the alias which is coded into services built
on the indices. The names listed below are aliases.
",https://help.ceda.ac.uk/article/4287-elastic-search---indices#overview-of-production-indices-{#overviewofindices},265,44
ceda-eo  {#aerial-photoaliasedtoap},"Geospatial and temporal information about satellite observations from
the Sentinel and Landsat missions.
",https://help.ceda.ac.uk/article/4287-elastic-search---indices#ceda-eo -{#aerial-photoaliasedtoap},105,13
arsf {#arsf-2015-05-27aliasedtoarsf},"Geospatial, temporal, and parameter metadata scanned from files in the
CEDA archive under the \""/neodc/arsf\"" directory.
",https://help.ceda.ac.uk/article/4287-elastic-search---indices#arsf-{#arsf-2015-05-27aliasedtoarsf},121,16
eufar {#eufar-2015-04-27aliasedtoeufar},"Geospatial, temporal, and parameter metadata scanned from files in the
CEDA archive under the \""/badc/eufar\"" directory.
",https://help.ceda.ac.uk/article/4287-elastic-search---indices#eufar-{#eufar-2015-04-27aliasedtoeufar},121,16
faam {#faam-2015-04-27aliasedtofaam},"Geospatial, temporal, and parameter metadata scanned from files in the
CEDA archive under the \""/badc/faam\"" directory.
",https://help.ceda.ac.uk/article/4287-elastic-search---indices#faam-{#faam-2015-04-27aliasedtofaam},120,16
ceda-level-1 {#um-2015-05-15aliasedtoum},"Basic file level metadata for all files in the Archive.
",https://help.ceda.ac.uk/article/4287-elastic-search---indices#ceda-level-1-{#um-2015-05-15aliasedtoum},56,10
ceda-level-2,"File and parameter metadata scanned from files in the CEDA archive.
",https://help.ceda.ac.uk/article/4287-elastic-search---indices#ceda-level-2,68,11
ceda-level-3,"Geospatial, temporal, parameter and file level metadata for as many
files as possible in the CEDA archive.
",https://help.ceda.ac.uk/article/4287-elastic-search---indices#ceda-level-3,107,17
cedamoles-haystack-prod,"Index created hourly from the MOLES database and is used in the MOLES
catalogue search.
",https://help.ceda.ac.uk/article/4287-elastic-search---indices#cedamoles-haystack-prod,88,15
Querying Elasticsearch for the indices,"We can ask Elasticsearch about the indices:
> [[ ]{.icon}
> http://jasmin-es1.ceda.ac.uk:9200/\_cat/indices](http://jasmin-es1.ceda.ac.uk:9200/_cat/indices){.ext-link}
It reports something like:
green open um-2015-05-15    5 1 797435 0 103.8mb  51.9mb 
green open arsf-2015-05-27  5 1  49565 0 269.4mb 134.7mb 
green open faam-2015-04-27  5 1   2838 0  78.6mb  39.3mb 
green open aerial-photo     5 1  10899 0    10mb     5mb 
green open eufar-2015-04-27 5 1  13491 0  42.5mb  21.2mb
:::
",https://help.ceda.ac.uk/article/4287-elastic-search---indices#querying-elasticsearch-for-the-indices,488,62
NASA Ames,"The  [NASA Ames](http://cedadocs.ceda.ac.uk/73/4/index.html) Format for
Data Exchange, often referred to as NASA Ames Format, grew out of NASA
aircraft campaigns and was first formalised at the Ames Research Centre,
California.  It is still used by the atmospheric observation community
although it has largely been replaced by the
binary [NetCDF](https://help.ceda.ac.uk/article/106-netcdf) format for
aircraft data.  
NASA-Ames is an ASCII text file format with a defined header followed by
columns of space separated variables.
",https://help.ceda.ac.uk/article/4692-nasa-ames,531,71
Which type of data?,"<div>
Any set of functions of 1 to 4 independent variables *can* be recorded
using the NASA Ames format, however, it is most useful for simple
datasets with just one independent variable (eg time series or
profiles). Other formats, such as 
[NetCDF](https://help.ceda.ac.uk/article/106-netcdf), may be more
suitable for more complex data structures, or for very large datasets.
</div>
",https://help.ceda.ac.uk/article/4692-nasa-ames#which-type-of-data?,385,57
File structure,"<div>
Each file is made of two parts. At the top of the file, the file header
includes information on the data (
[metadata](https://help.ceda.ac.uk/article/4428-metadata-basics?preview=5880d49b2c7d3a4a60b94cc7)).
The actual data are recorded in the lines that follow the header. In
many cases, some of the independent variables are defined in the header
and are not repeated in the data section (e.g. for a regular
grid). Below is a brief description, but you can find extra details
here  [CEDA NASA Ames FFI
Summary](http://cedadocs.ceda.ac.uk/73/4/FFI-summary.html). 
</div>
",https://help.ceda.ac.uk/article/4692-nasa-ames#file-structure,577,80
Header or Metadata section,"<div>
The header includes, in a strictly defined order and format, all the
information needed to read and understand the data. The image below
describes each line of metadata for the most commonly used version
(index 1001).
</div>
<div>
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5beae9e32c7d3a31944de975/file-4qe0EA12eR.png)
</div>
<div>
Information on the data and source of the data (instrumentation, model
used, data processing, data quality, location, date, revision date,
etc.), is usually included in the comment lines. 
</div>
<div>
Many data files contain the variable  *time* which can be expressed in
several ways, [guidelines on the recommended format of the time-variable
header lines, units and the data values are
available](http://cedadocs.ceda.ac.uk/73/4/nasa-ames-time.htm).
</div>
",https://help.ceda.ac.uk/article/4692-nasa-ames#header-or-metadata-section,844,104
Data section,"<div>
The data section starts directly after the header starting with the
independent variable in the first column. The primary variables follow
in subsequent columns in the order listed in the header. The columns are
separated by spaces (usually 4) and **not** tabs (see example below).
</div>
<div>
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5beaee682c7d3a31944de9f8/file-a9qccQSmGd.png)
</div>
",https://help.ceda.ac.uk/article/4692-nasa-ames#data-section,441,50
File names,"At CEDA, NASA Ames files generally follow the CEDA file naming
convention given below, with the ending .na. This enables quick access
to pertinent metadata and avoids the need to open and read the file in
order to assess its contents.
For further information please see the  [File Names
explained](https://help.ceda.ac.uk/article/103-filenames) documentation.
",https://help.ceda.ac.uk/article/4692-nasa-ames#file-names,360,51
Examples,"<div>
Examples of NASA Ames formats can be found below;
</div>
<div>
-   [Example 1001-a](http://cedadocs.ceda.ac.uk/73/4/example-1001-a.na).
-   [Example 1001-b](http://cedadocs.ceda.ac.uk/73/4/example-1001-b.na).
Examples are also included in the guides to
[1D](http://cedadocs.ceda.ac.uk/73/4/na-for-dummies-1D.html)
and [2D](http://cedadocs.ceda.ac.uk/73/4/na-for-dummies-2D.html)
formats.
</div>
",https://help.ceda.ac.uk/article/4692-nasa-ames#examples,401,31
Checking your data files,"<div>
A NASA Ames Format checker is [available at
CEDA](http://archive.ceda.ac.uk/nachecker).  Please contact
support\@ceda.ac.uk for your file to be checked. The checker is based on
a program written by S. Gaines, NASA Ames Research Center.
</div>
",https://help.ceda.ac.uk/article/4692-nasa-ames#checking-your-data-files,249,35
Uploading NASA Ames files to CEDA,"<div>
To upload your data please use our  [HTTP:](http://arrivals.ceda.ac.uk/)
File uploader service. A step by step guide can be found 
[here.](https://help.ceda.ac.uk/article/4660-depositing-data-at-ceda-a-step-by-step-guide?preview=5b5af0c50428631d7a896169)\
</div>
<div>
",https://help.ceda.ac.uk/article/4692-nasa-ames#uploading-nasa-ames-files-to-ceda,275,23
Further reading,"<div>
Format Specification for Data Exchange, Version 1.3 ( [Gaines and
Hipskind, 1998](http://cedadocs.ceda.ac.uk/73/3/G-and-H-June-1998.html))
is the primary reference for NASA Ames formatting. As far as possible,
the CEDA documentation keeps the same nomenclature and notation system
as in this original document.
</div>
</div>
<div>
Every File Format Index is explained line by line in the  [NASA Ames FFI
Summary](http://cedadocs.ceda.ac.uk/73/4/FFI-summary.html) provided by
CEDA.
</div>
",https://help.ceda.ac.uk/article/4692-nasa-ames#further-reading,494,62
SNAP and snappy installation instructions,"\[This content is replicated on the file system at:
/apps/jasmin/supported/snap/INSTALLATION/README-snap-on-jasmin\]
These step by step instructions will enable you to create an updated
version of snap / snappy on JASMIN when required.
A fully scripted install has not been provided. This is partly because
the interactive installer uses a GUI, and partly because of the need to
do things as different users and on different machines, but the
advantage is that the only commands run as root are to create writable
directories and then fix up ownership, and the installer itself, which
does potentially unknown things, is then run under a normal username
without write access to the OS \-- this prevents it causing
inconsistencies between sci machines.
It might be possible to use the terminal-based equivalent of the
installer (unset the DISPLAY variable) to script this, but the questions
which it asks might not be completely repeatable, so it would probably
require an \""expect\"" script - probably not worth the effort given how
often it would be run.
Alan Iwi 20/8/2021
",https://help.ceda.ac.uk/article/5006-installing-snap-and-snappy#snap-and-snappy-installation-instructions,1074,171
SNAP installation,"-   Go to a clean temporary directory
-   get the interactive installer
<!-- -->
    wget https://step.esa.int/downloads/8.0/installers/esa-snap_all_unix_8_0.sh
(a copy of this is also in the directory that contains the README
file: /apps/jasmin/supported/snap/INSTALLATION)
-   As root, create a writable directory for the new installation You
    will run the install as non-root and then fix up the permissions
    afterwards.
<!-- -->
    # mkdir /apps/jasmin/supported/snap/8.0 # chown /apps/jasmin/supported/snap/8.0
-   Move any \~/.snap directory out of the way from your home directory
    (if you have it)
<!-- -->
    mv ~/.snap ~/OLD.snap
-   Run the installer \*\*\* AS YOUR USERNAME NOT AS ROOT. \*\*\*
<!-- -->
    umask 022 # ensure files will be created with sensible permissions so we will 
              # only need to change the ownership
    sh esa-snap_all_unix_8_0.sh     # DO NOT DO THIS AS ROOT.
\[NOTE - if preferred you can use terminal-based installer if you unset
the DISPLAY variable when doing this i.e.: env DISPLAY=\""\"" ; sh
esa-snap_all_unix_8\_0.sh \]
-   Answer the prompts as follows:
    -   accept agreement
    -   if asked, say yes to deleting all user snap data \[not sure
        where it finds this, I have tried very hard to find out where it
        is looking and what it is referring to, but if you say no then
        when you run the desktop it seems to attempt some Netbeans
        update, not sure why\]
    ```{=html}
    <!-- -->
    ```
    -   tell it destination folder \[e.g.
        /apps/jasmin/supported/snap/8.0\]
    -   if asked, confirm okay to install to existing directory \[this
        is empty one you just created\]
    -   install all components
    -   do not create symlinks \[module file will be used for that
        instead\]
    -   don\'t configure for use with python \[you will do this
        afterwards from the command line\]
    -   say no to running snap desktop
-   As root, take back all of the files that were installed.
<!-- -->
    # chown -R root.root /apps/jasmin/supported/snap/8.0
-   As your user, run the app (using its full path for now) and check
    that it starts okay
<!-- -->
    /apps/jasmin/supported/snap/8.0/bin/snap
-   If the RPM mesa-dri-drivers is not installed on the local machine,
    then you will get a bunch of warnings on the terminal output and you
    will also get a window saying \""Unable to initialize
    WWWorldMapToolView\"" but apart from that, the application ought to
    open and run.
",https://help.ceda.ac.uk/article/5006-installing-snap-and-snappy#snap-installation,2515,376
SNAPPY installation:,,https://help.ceda.ac.uk/article/5006-installing-snap-and-snappy#snappy-installation:,0,0
Building jpy wheel,"During this installation, you will need a \""wheel\"" file for jpy with a
filename such as jpy-0.10.0.dev1-cp37-cp37m-linux_x86_64.whl.
This should be built for the correct python version but apart from that,
it is not specific to any particular snappy installation, and if you are
providing a later version of snappy it is okay to copy the one from the
previous version, otherwise, you will need to build it as follows. If
you don\'t need to build it, skip down to \""configuring snappy\"".
The build will need to be on a machine that has the JDK of a version
that matches the version of JRE on the sci machines (the sci machines
don\'t have JDK, but on a sci machine do \""rpm -qa \| grep openjdk\"" to
see the JRE version). To avoid installing software on the sci machines,
sci2-test was used for this purpose, and JDK was installed (as root)
using:
    # yum install java-1.8.0-openjdk-devel.x86_64
Then (as non-root on the development machine in question), build as
follows:
First, you create a conda env and install maven into it - this is
because Jaspy doesn\'t have Maven, which is needed for the build, so the
aim is to create a conda environment that contains the same version of
Python as is in Jaspy and also contains maven.
    wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh 
    sh Miniconda3-latest-Linux-x86_64.sh 
    PATH=/path/to/my_conda_base/bin/:$PATH    # substitute path based on wherever you just put it
    conda create -n jpy_build python=3.7.9=hffdb5ce_0_cpython # this based on ""conda list"" inside jaspy 
    source activate jpy_build 
    conda install maven
Then the jpy wheel build itself - you will need to set the JAVA_HOME
environment variable to point to the jdk environment (if unsure do e.g.
\""rpm -ql java-1.8.0-openjdk-devel\"" to see where the files live).
You will have to make sure that you are using the correct version of
python, so note the module load jaspy in the instructions here.
    git clone https://github.com/bcdev/jpy.git
    cd jpy
    module load jaspy/3.7/r20210320  # or whichever jaspy version you want to use with snappy
    JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.292.b10-1.el7_9.x86_64/ 
    python setup.py bdist_wheel
This will produce a wheel file in the dist directory. If you are working
under a local /tmp directory then copy it somewhere that you\'ll have
access on the sci machines (e.g. your home directory)
    cp dist/jpy-0.10.0.dev1-cp37-cp37m-linux_x86_64.whl ~/
",https://help.ceda.ac.uk/article/5006-installing-snap-and-snappy#building-jpy-wheel,2469,370
Configuring snappy,"\[This is done on an ordinary sci machine, so if you used a development
machine to build the jpy wheel, then log out of that. You probably
won\'t have write permission on /apps.\]
You will be running snappy-conf, telling it where to find python and
where to put snappy.
First of all, as root, create a writable directory for yourself, for
example as follows. (The versioning here is based on the SNAP version
and the JASPY module version. snappy is provided as part of SNAP so
doesn\'t seem to have an independent version number of its own.)
      # mkdir -p /apps/jasmin/supported/snap/snappy/8.0_jaspy-3.7-r20210320
      # chown <your_username> <path_to_directory_you_just_made>
Now in a NON-ROOT shell, do the following (adapt version numbers as
required):
    snap_dir=/apps/jasmin/supported/snap/8.0
    snappy_dir=/apps/jasmin/supported/snap/snappy/8.0_jaspy-3.7-r20210320
    module load jaspy/3.7/r20210320
    mkdir -p $snappy_dir/snappy/
    wheel_file=~/jpy-0.10.0.dev1-cp37-cp37m-linux_x86_64.whl   # or wherever you have it
    cp $wheel_file $snappy_dir/snappy/
    # now you are ready to run snappy-conf
    python=$(which python)
    echo $python  # check that this is from Jaspy
    $snap_dir/bin/snappy-conf $python $snappy_dir
It should say \""Configuring SNAP-Python interface\...\"" After it says
\""Done.\"" it does not immediately return to the command prompt but
should do within less than a minute.
    # should have some files installed under here...
    ls -lR $snappy_dir/
As root, now take back the files that you created\...
    # chown -R root.root /apps/jasmin/supported/snap/snappy/8.0_jaspy-3.7-r20210320
",https://help.ceda.ac.uk/article/5006-installing-snap-and-snappy#configuring-snappy,1636,211
SNAPPY testing,"Use the test_snappy.py file in the same directory as this readme.
Make a copy and change snappy_path variable near the top.
Then do:
    module load jaspy <br>python /path/to/test_snappy_copy.py
It will set the python path to where you selected and also find under
there a test data file which is provided as part of the distribution.
The test will try to import the module, read the test data file and
check some values.
You might get warnings about GDAL versions but it should get to \""test
completed\"", e.g.:
        testing SNAPPY import...
        INFO: org.esa.s2tbx.dataio.gdal.GDALVersion: Incompatible GDAL 3.1.4 found on system. Internal GDAL 3.0.0 from distribution will be used.
        INFO: org.esa.s2tbx.dataio.gdal.GDALVersion: Internal GDAL 3.0.0 set to be used by SNAP.
        INFO: org.esa.snap.core.gpf.operators.tooladapter.ToolAdapterIO: Initializing external tool adapters
        INFO: org.esa.snap.core.util.EngineVersionCheckActivator: Please check regularly for new updates for the best SNAP experience.
        testing SNAPPY file read...
        INFO: org.esa.s2tbx.dataio.gdal.GDALVersion: Internal GDAL 3.0.0 set to be used by SNAP.
        INFO: org.hsqldb.persist.Logger: dataFileCache open start
        test completed
",https://help.ceda.ac.uk/article/5006-installing-snap-and-snappy#snappy-testing,1254,158
Providing module files,"Separate module files for snap and snappy are provided. The snap one
does not depend on a particular version of Jaspy, but the snappy one
does so loads the relevant Jaspy and snap modules as well as setting up
the PYTHONPATH variable
Using root access, copy the previous module files from under:
    /apps/jasmin/modulefiles/snap/ 
    # and
    /apps/jasmin/modulefiles/snappy/
to the new version numbers in the filenames and also edit the version
numbers inside these.
",https://help.ceda.ac.uk/article/5006-installing-snap-and-snappy#providing-module-files,471,72
Testing module files,"Do \""module load\"" on the new module files. Type \""snap\"" (or at least,
\""which snap\"") - check that it can be found without supplying a full
path.
Also edit the test_snappy program that you used above, to comment out
the sys.path.append line and check that it will still run (i.e.
PYTHONPATH set up correctly).
",https://help.ceda.ac.uk/article/5006-installing-snap-and-snappy#testing-module-files,312,54
Depositing Data at CEDA FTP and RSYNC,"There are 3 ways you can send data to CEDA (HTTP, FTP and RSYNC). This
help document will take you through steps to upload your data via FTP
and RSYNC recommend for large amounts of data. To upload data via HTTP
please refer to [Depositing Data at CEDA: A step by step
guide](https://help.ceda.ac.uk/article/4660-depositing-data-at-ceda-a-step-by-step-guide?auth=true)
",https://help.ceda.ac.uk/article/4991-depositing-data-at-ceda-ftp-and-rsync,369,53
FTP,"Once you have collected/produced your atmospheric or earth observation
data; processed, calibrated, validated and quality checked it and
prepared it for archiving using correctly applied [standard data
formats](https://help.ceda.ac.uk/category/4423-formats) (for example,
[NetCDF](https://help.ceda.ac.uk/article/106-netcdf) should adhere to
the [CF
conventions](https://help.ceda.ac.uk/article/4507-the-cf-metadata-convention)
and have good quality
[metadata](https://help.ceda.ac.uk/article/4428-metadata-basics)
including [global
attributes](https://help.ceda.ac.uk/article/4432-ceda-cf-examples)) and
[meaningful filenames](https://help.ceda.ac.uk/article/103-filenames),
you are now ready to upload it to the CEDA archive, using the following
steps. A video tutorial can be found
[here](https://www.youtube.com/watch?v=KiZ_2xRlVPY).
1. Register for a  [CEDA
account](https://services.ceda.ac.uk/cedasite/register/info/).
2\. Go to [arrivals.ceda.ac.uk](http://arrivals.ceda.ac.uk/) and login
with your CEDA username and password.
3\. Click begin. You need to agree to the [deposit
agreement](https://artefacts.ceda.ac.uk/licences/depositors_agreement)
the first time you use the service.  
4\. Click browse deliveries.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5b5ed2cd0428631d7a89700a/file-GDZHNBN57G.png)
5\. Choose an existing delivery directory or create a new delivery
directory by clicking on \'new directory\'. (Add a suitable title e.g.
the project, instrument name, model name or what the data contains,
etc).
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5b5ed3540428631d7a897010/file-xLBilyUBmB.png)
7\. Click on \'other upload methods\' and take note of the URL string
8\. Download FileZilla
[https://filezilla-project.org](https://filezilla-project.org/)
9. Paste the URL connection string, e.g.
**ftp://username:password\@arrivals.ceda.ac.uk/mydata ** into the
\""Host\"" field and hit the \""Quickconnect\"" button (the other fields
will be filled in automatically, or you can fill them in yourself)
10.Browse and upload files using the explorer at the bottom of the
screen
<div>
This will be similar for any desktop FTP client\
With the  `ftp` command bundled with many Linux/UNIX operating systems
and Windows:
1.  Type `ftp``arrivals.ceda.ac.uk`\` to connect directly to the
    arrivals server
2.  Enter your FTP username and password as found on your data upload
    page
3.  When connected, you will be in your Arrivals home area,
    use `cd mydata` navigate to the subdirectory with the same name as
    your data delivery
4.  Start uploading data with the `put` command
11\.
[Create](http://artefacts.ceda.ac.uk/tools/dataset_ingest_labeler/dataset_labeler.html)
and upload a metadata.yaml text file along with the data.  The
metadata.yaml file needs to contain the details of the dataset, project
and instrument or model. Adding the information this way helps keep the
record and the data together. To create the metadata file use this
[utility](http://artefacts.ceda.ac.uk/tools/dataset_ingest_labeler/dataset_labeler.html)
or edit one of the examples below.
[station-data_metadata_example.yaml](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/attachments/5ba24c0a2c7d3a16370f4656/station-data_metadata_example.yaml)
[instrument_metadata_example.yaml](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/attachments/5ba24bf82c7d3a16370f4654/instrument_metadata_example.yaml)
[model_metadata_example.yaml](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/attachments/5ba24bef2c7d3a16370f4653/model_metadata_example.yaml)
12\. Click check delivery button on arrivals service.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5b5ed4102c7d3a03f89d1f24/file-P13PnIYdFX.png)
13\. Confirm Submission. It s really important to confirm delivery in
the arrivals service so that CEDA  can be notified.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5b5ed45c0428631d7a897020/file-9RmwVVSW2A.png)
",https://help.ceda.ac.uk/article/4991-depositing-data-at-ceda-ftp-and-rsync#ftp,4154,378
RSYNC,"</div>
Once you have collected/produced your atmospheric or earth observation
data; processed, calibrated, validated and quality checked it and
prepared it for archiving using correctly applied [standard data
formats](https://help.ceda.ac.uk/category/4423-formats) (for example,
[NetCDF](https://help.ceda.ac.uk/article/106-netcdf) should adhere to
the [CF
conventions](https://help.ceda.ac.uk/article/4507-the-cf-metadata-convention)
and have good quality
[metadata](https://help.ceda.ac.uk/article/4428-metadata-basics)
including [global
attributes](https://help.ceda.ac.uk/article/4432-ceda-cf-examples)) and
[meaningful filenames](https://help.ceda.ac.uk/article/103-filenames),
you are now ready to upload it to the CEDA archive, using the following
steps. A video tutorial can be found
[here](https://www.youtube.com/watch?v=RFSDyt88ICg).
1. Register for a  [CEDA
account](https://services.ceda.ac.uk/cedasite/register/info/).
2\. Go to [arrivals.ceda.ac.uk](http://arrivals.ceda.ac.uk/) and login
with your CEDA username and password.
3\. Click begin. You need to agree to the [deposit
agreement](https://artefacts.ceda.ac.uk/licences/depositors_agreement)
the first time you use the service.  
4\. Click browse deliveries.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5b5ed2cd0428631d7a89700a/file-GDZHNBN57G.png)
5\. Choose an existing delivery directory or create a new delivery
directory by clicking on \'new directory\'. (Add a suitable title e.g.
the project, instrument name, model name or what the data contains,
etc).
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5b5ed3540428631d7a897010/file-xLBilyUBmB.png)
7\. Click on \'other upload methods\' and take note of the command to
edit
8. You need to have the  `rsync` command installed (which comes by
default on Linux/UNIX systems, Windows users will have to install it
themselves)
9.Edit the command given on the Rsync instructions of the upload page
e.g. **rsync -av /path/to/source
username\@arrivals.ceda.ac.uk::username/mydata (**change
\""/path/to/source\"" to the location of your data files, username to your
CEDA username and mydata to the folder to upload data to)
10\. Run the command, enter your Rsync password when prompted and your
data will be synced with the upload directory
<div>
As mentioned on the upload page, after generating a new password for
Rsync you may need to wait up to an hour before you can use it
</div>
11\.
[Create](http://artefacts.ceda.ac.uk/tools/dataset_ingest_labeler/dataset_labeler.html)
and upload a metadata.yaml text file along with the data.  The
metadata.yaml file needs to contain the details of the dataset, project
and instrument or model. Adding the information this way helps keep the
record and the data together. To create the metadata file use this
[utility](http://artefacts.ceda.ac.uk/tools/dataset_ingest_labeler/dataset_labeler.html)
or edit one of the examples below.
<div>
[station-data_metadata_example.yaml](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/attachments/5ba24c0a2c7d3a16370f4656/station-data_metadata_example.yaml)
[instrument_metadata_example.yaml](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/attachments/5ba24bf82c7d3a16370f4654/instrument_metadata_example.yaml)
[model_metadata_example.yaml](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/attachments/5ba24bef2c7d3a16370f4653/model_metadata_example.yaml)
12\. Click check delivery button on arrivals service.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5b5ed4102c7d3a03f89d1f24/file-P13PnIYdFX.png)
13\. Confirm Submission. It s really important to confirm delivery in
the arrivals service so that CEDA  can be notified.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5b5ed45c0428631d7a897020/file-9RmwVVSW2A.png)
</div>
",https://help.ceda.ac.uk/article/4991-depositing-data-at-ceda-ftp-and-rsync#rsync,3971,365
Introduction,"The Centre for Environmental Data Analysis (CEDA) hosts a range of
activities associated with environmental data archives.The CEDA Document
Repository is for grey literature primarily concerning Earth Observation
and atmospheric sciences.  As of 24th November 2022 this is now hosted
within the CERN operated \'Zenodo\' service where users can upload their
own items and submit to be part of the [CEDA Document Repository
Community](https://zenodo.org/communities/ceda-document-repository/).
",https://help.ceda.ac.uk/article/297-ceda-document-repository#introduction,492,64
As a visitor\...,"As a visitor to the repository the content is publicly available
enabling visitors to browse the documents within the repository without
the need to register. You can search our document repository content on
the Zenodo site by visiting the [search page of our
community](https://zenodo.org/communities/ceda-document-repository?q=&l=list&p=1&s=10&sort=newest).
Note, you will find the text search box by clicking on the menu icon,
top right of the screen to open the side bar:
![Where to find the text search box in
Zenodo](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6538d3a65929497d15ca1213/file-I5XbhaUEhM.png)
You can then use the filter button to display other fitters to help
refine your search results.
![Where to find the search
filter](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6538d413c5ebc545bcfe0550/file-48xckeenvb.png)
",https://help.ceda.ac.uk/article/297-ceda-document-repository#as-a-visitor\...,913,100
As a depositor\... (but not of data please!),"To submit items to the CEDA Document Repository Community you will need
to:
1.  Create your own Zenodo user account
2.  Upload your content using this link to [submit to our Zenodo
    community](https://zenodo.org/deposit/new?c=ceda-document-repository).
    (Note you can also submit your item to any other Zenodo community
    you wish to be listed within too.)
We\'ll review the submission to make sure that your items meet the
criteria for inclusion in our Documentation community. See our [curation
policy for the CEDA Document
Repository](https://zenodo.org/communities/ceda-document-repository/curation-policy)
for further details on it\'s scope and what we\'ll accept.
",https://help.ceda.ac.uk/article/297-ceda-document-repository#as-a-depositor\...-(but-not-of-data-please!),678,90
What about data?,"For all NERC funded data these must be offered to a NERC funded data
centre. Zenodo lacks some aspects (e.g. geo-temporal fields to aid
compliance with standards such as GEMINI2.3 records etc.) that permit
this to be a suitable repository for NERC funded data.
",https://help.ceda.ac.uk/article/297-ceda-document-repository#what-about-data?,261,44
CEDA Archive Web Download Service,"::: {.section .callout-green}
Access the Download Service here:   <http://data.ceda.ac.uk/>
:::
[](http://data.ceda.ac.uk/)This service allows you to access the data
held in the CEDA Archive:
-   browse the archive directory structure
-   log into myCEDA service for access to restricted datasets
-   download single and multiple files
-   view and subset [NetCDF](https://help.ceda.ac.uk/article/106-netcdf)
    file contents
-   see in-browser plotting of
    [NASA-Ames ](http://cedadocs.ceda.ac.uk/73/4/index.html)files
Additionally, the  OPeNDAP technology underpinning this service allows
additional functionality such as accessing file contents directly via
the internet from within data processing scripts written within Python,
Matlab IDL and other scripting languages. 
",https://help.ceda.ac.uk/article/4431-ceda-archive-web-download-and-services,780,92
Viewing and downloading individual files,"-   **Text files** - You may find that some text based files will
    automatically open in your web browser when selected. You can then
    download these files by using the S*ave as\...* facility on your
    browser (usually found under the *File* drop-down menu). Others will
    start to download directly.
-   **Binary and Compressed files** - these should automatically start
    to download to your computer.
    [WinZip](http://www.winzip.com/win/en/) on Windows or
    [gzip](http://www.gzip.org/) and tar utilities on Linux machine may
    be needed to unpack the contents.
",https://help.ceda.ac.uk/article/4431-ceda-archive-web-download-and-services#viewing-and-downloading-individual-files,584,82
,,https://help.ceda.ac.uk/article/4431-ceda-archive-web-download-and-services#,0,0
**Download multiple files**,"\*\*\*At present this function is unavailable through the web-download
service. Users may wish to use one of the options listed below in the
meanwhile.\*\*\*
::: {.section .callout-yellow}
Note that attempting to download many files at once is likely to result
in long download times and possible failure.
**Non-JASMIN users** may download via the CEDA archive
[FTP](https://help.ceda.ac.uk/article/280-ftp) service. 
**JASMIN USERS:*** *Remember also that if you are doing your data
analysis on JASMIN, you do not normally need to download the data as the
CEDA archive data is available as a read-only file system. However, not
all data are available for all users to directly access. See the JASMIN
\'[CEDA Archive](https://help.jasmin.ac.uk/article/3838-ceda-archive)\'
help article for further details.
:::
",https://help.ceda.ac.uk/article/4431-ceda-archive-web-download-and-services#**download-multiple-files**,811,115
,,https://help.ceda.ac.uk/article/4431-ceda-archive-web-download-and-services#,0,0
Plotting NASA-Ames files,"[NASA-Ames](http://cedadocs.ceda.ac.uk/73/4/index.html) format files
with File Format Index (FFI) 1001 can be plotted from the data browser.
Any files which are plottable will have a \'plot\' link next to them.
Following this link will open a new window showing a plot of the first
dependent variable against the independent variable (which in most cases
should be time). Below the plot is a form which allows you to select
another dependent variable to plot. You can also change the scales of
the plot and change the plot symbol. The box labelled  *Omit points with
value* allows you to exclude any points from the plot which have a
specified value. This can be used to remove any points which are marked
with a special value. (Note that the
[NASA-Ames ](http://cedadocs.ceda.ac.uk/73/4/index.html)format itself
requires that a \'fill\' value is specified for each variable. These
values are automatically excluded from the plot).
Once you have made your selection, press the \'Plot\' button on the
bottom right hand side of the form to generate the plot.
",https://help.ceda.ac.uk/article/4431-ceda-archive-web-download-and-services#plotting-nasa-ames-files,1057,169
Scripted Interaction with archive via OPeNDAP,"OPeNDAP stands for \""Open-source Project for a Network Data Access
Protocol\"", and is a data transport architecture and protocol widely
used by earth scientists. There are a variety of libraries available to
interact with OPeNDAP services. The following are a few example sites
detailing libraries for common languages used within the CEDA data
community.
  ----------------------------------- ------------------------------------------------------------------------
  python                              <http://www.pydap.org/en/latest/>
  matlab                              <https://www.opendap.org/index.php/deprecated-software/matlab-loaddap>
  IDL                                 <https://www.opendap.org/index.php/deprecated-software/IDLClient>\
  ----------------------------------- ------------------------------------------------------------------------
Our OpenDAP interface is located here: 
<http://dap.ceda.ac.uk/thredds/fileServer/>
Whist some of these tools may work for public datasets held in the CEDA
Archives, users wishing to use data available to registered CEDA users
will need to follow additional steps to use security certificates as
detailed on the CEDA OPeNDAP:  [Scripted
interactions](https://help.ceda.ac.uk/article/4442-ceda-opendap-scripted-interactions)
help page. 
",https://help.ceda.ac.uk/article/4431-ceda-archive-web-download-and-services#scripted-interaction-with-archive-via-opendap,1300,116
Finding Data The CEDA Data Catalogue,"There are thousands of datasets held in the CEDA archives and it can be
difficult to find the data most suited to your requirements. To help
with this, CEDA\'s Data Catalogue allows users to easily find data
matching their search terms and other related data.
",https://help.ceda.ac.uk/article/137-ceda-data-catalogue,260,45
Searching for data,"The data catalogue contains large numbers of datasets and data
collections as well as records covering background information about
projects, instruments, locations and algorithms used in data production.
We\'ve aimed to keep the catalogue as intuitive as possible to aid
getting to relevant data as quickly as possible.
You can access the full catalogue here:
",https://help.ceda.ac.uk/article/137-ceda-data-catalogue#searching-for-data,361,55
"[https://catalogue.ceda.ac.uk](https://catalogue.ceda.ac.uk/) {#httpscatalogue.ceda.ac.uk style=""text-align: center;""}","On the front page you\'ll see a simple search box which will carry out a
free-text search across the thousands of datasets and other record types
in our catalogue:
::: {#fixed_header_wrapper}
<div>
**Search catalogue records for**
<https://catalogue.ceda.ac.uk/?q=cheese&results_per_page=5&sort_by=relevance>
</div>
<div>
::: {#id_record_types style=""display; table;""}
::: {style=""display: table-row;""}
::: {style=""display: table-cell;""}
On the same page, you can view special record selections where a beacon
pop up bar gives you the option to search for public datasets,
registered CEDA user datasets, restricted datasets, Datasets with a DOI,
instrument records, projects, collections and platforms etc.\
\
","https://help.ceda.ac.uk/article/137-ceda-data-catalogue#[https://catalogue.ceda.ac.uk](https://catalogue.ceda.ac.uk/)-{#httpscatalogue.ceda.ac.uk-style=""text-align:-center;""}",710,92
*What happens when I search?*,"When you search the CEDA Catalogue, it will search against the title,
abstract, keywords, identifiers and, for Datasets, variables recorded
across all the records types. The records can then be filtered by the
record type to help refine the searches.
A summary of matching records will be displayed with links to the full
catalogue entry which are ordered according to relevance ranking. These
can also be re-ordered using the \""sort by\"" option to list the records
alphabetically if desired.
:::
::: {style=""display: table-cell;""}
:::
::: {style=""display: table-cell;""}
:::
:::
::: {style=""display: table-row;""}
",https://help.ceda.ac.uk/article/137-ceda-data-catalogue#*what-happens-when-i-search?*,613,92
"Search Results, Refining Your Search and Exploring Connections","Once you have submitted your search text you will be presented with a
list of returned items in our catalogue, such as shown in the example
below which then provide a range of options to help refine your search
or to explore connections between records:
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6425644891efbc3c62649dca/file-9FW4YMdvQW.png)
From this view you can use a range of search facets on the left hand
side to refine your search in various ways including: record type,
geographic area, data temporal range, access type, publication/DOI date
and limit selections to DOied resource, non-geographic data and latest
versions of datasets.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6425643b7bf4bb61c0116746/file-6EaSsiPC0h.png)
Further details of each search option are available by hovering over the
blue \'i\' for more information in a tooltip and also further down this
help page. 
","https://help.ceda.ac.uk/article/137-ceda-data-catalogue#search-results,-refining-your-search-and-exploring-connections",978,122
Searching via related records,"Next to each returned record in the catalogue search result you will see
a \'Related records\' button:\
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6425659b7bf4bb61c0116749/file-eUKIVXAfxS.png)
Clicking on this will take you through to the record in question and to
a dedicated search interface related specifically to records connected
to that record\... such as in the following example for the FAAM dataset
collection. These related records are ones which have been directly
connected to the selected record and so give more contextual search.
Here you can further refine your search within just these related
records. To get back to the main search you can use the red backward
arrow.
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/655e20e4d8c4315d1df5105b/file-L21P5wxJGT.png)
::: {style=""display: table-cell;""}
\
:::
::: {style=""display: table-cell;""}
:::
:::
:::
</div>
:::
",https://help.ceda.ac.uk/article/137-ceda-data-catalogue#searching-via-related-records,964,115
*Browsing from one record to another*,,https://help.ceda.ac.uk/article/137-ceda-data-catalogue#*browsing-from-one-record-to-another*,0,0
,"<div>
<div>
If you wish to explore more in depth then you can click through to
record using the More Info button. From there you can use the \'related
records\' button down the page to see the various record types the
record is connected to in the catalogue. For example, a Dataset
Collection may consist of many Datasets which can both be related to a
particular Project. Dataset pages may also show connections between
various versions of a dataset.
</div>
</div>
",https://help.ceda.ac.uk/article/137-ceda-data-catalogue#,466,81
Catalogue structure - what the records mean,,https://help.ceda.ac.uk/article/137-ceda-data-catalogue#catalogue-structure---what-the-records-mean,0,0
(searching by the \'Record Type\' facet),"Underpinning CEDA\'s data catalogue are some key concepts that are
useful to understand. The catalogue is centred on the following key
types of record:
-   ***Dataset**** - these records describe and link to the actual data
    in our archive. They also provide spatial and temporal information,
    access and usage information and link to background information on
    why and how the data were collected. NOTE. this overall category
    includes both geographic and non-geographic datasets (such as lab
    data, survey data).*
-   **Dataset Collection** - a collection of Datasets that share some
    common purpose, theme or association. These collections link to one
    or more Dataset records.
-   **Project** - background information about a project, program or
    other major activity that provides the purpose for producing the
    data. Navigate from this record to the related Dataset record(s)
    (via the relevant Dataset Collection) to access the data.
-   **Instrument** - describes an instrument that is used to collect
    data via observations. Follow the link to the associated Dataset
    records to access the relevant data.
-   **Platform**- describes a platform (e.g. sites, stations, aircraft)
    on which instruments were mounted/located during the data collected
    during a project. Follow the link to the associated Dataset records
    to access the relevant data.
-   **Computation** - describes computational stages in producing data -
    e.g. models, processing algorithms, used to produce data such as NWP
    and climate model output and Earth Observation higher level
    processed products. Follow the link to the associated Dataset
    records to access the relevant data.
",https://help.ceda.ac.uk/article/137-ceda-data-catalogue#(searching-by-the-\'record-type\'-facet),1716,249
Non-Geographic Datasets {#non_geo},"Not all datasets within the CEDA archive are geographically related and
include data from laboratories (e.g. cloud chambers, sample analysis),
wind tunnels and surveys. To help find such datasets first ensure the
geographic bounding box selections are clear and the record selection is
either reset or has the \'Datasets\' record type selected, then select
the \'Non-Geographic Datasets\' search option. 
",https://help.ceda.ac.uk/article/137-ceda-data-catalogue#non-geographic-datasets-{#non_geo},405,59
*Geographic Searches*,"![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/6425683ca3236b1edbce8a8d/file-9sFigWBZMq.png)
With the majority of data being geographical the \'Bounding box\' search
options allow for specific areas around the world to be selected to find
data which either intersects with (i.e. at least overlaps partially) or
is *entirely* within the selected region.
These searches are based on a -180 to +180 longitude grid, i.e. East and
West of the Greenwich meridian\... this *may* differ from how the data
are actually stored within the files themselves, but for consistency of
cataloging these have all been recorded in the catalogue on this
standard grid.
Many datasets are given as having global coverage and will appear within
search results with the \'intersects with\' option, but will not appear
with the \'within\' option selected from the drop down box.
At this stage the actual bounding boxes for the returned results are not
given on a map display together, but each dataset record (and platform
records too) will show the geographic bounding box both graphically and
as text on the individual record pages.
",https://help.ceda.ac.uk/article/137-ceda-data-catalogue#*geographic-searches*,1150,168
Temporal searches,"There are two ways to limit your searches bases on date. The first
related to the data themselves. Here specifying either or both a data
\'from\' or \'to\' date will allow datasets with data that fall within
the period to returned. Selecting \'entire range\' here will result in
finding datasets with data temporal ranges that cover the entire period
(including datasets that may start and/or end before/after the period
selected).\
![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/655e21f288e8fb00a8f81f31/file-3uHlzmsAMB.png)
The second option allows for searches for when datasets either by their
date of publication or when they were ascribed DOIs. Though these
options may not be the most useful for most users these are helpful for
data providers and other interested parties wishing to find out more
about the overall publication of data through CEDA.
",https://help.ceda.ac.uk/article/137-ceda-data-catalogue#temporal-searches,897,127
*Access Type*,"Within the CEDA Archive there are three broad categories of how data
access is controlled. The most open are those under the Public access
type - these do not require registration of logging in to access the
resource. \'Registered\' are for those datasets that require a user to
register as a CEDA user - mainly to provide more detailed user
demographics on dataset usage. Finally, the \'Restricted\' datasets are
those that have further restrictions on access and thus require specific
applications for access to be submitted.
**NOTE: these access types are not the same as the permitted use for the
data. For details on what can and can\'t be done with specific data
please refer to the licence information for the dataset in quesion.**
*Other NERC Data*
*You can search for data held in any of the NERC data centres through
the central NERC Data Catalogue Service:*
*[https://data-search.nerc.ac.uk](https://data-search.nerc.ac.uk/)*
*The NERC DCS covers data from us and the following NERC data centres:*
-   [British Oceanographic Data Centre](https://www.bodc.ac.uk/)
-   [National Geoscience Data
    Centre](http://www.bgs.ac.uk/services/ngdc/home.html)
-   [Polar Data Centre](https://www.bas.ac.uk/data/uk-pdc/)
-   [UK Centre for Ecology and Hydrology](https://www.ceh.ac.uk/)
-   [UK Solar System Data Centre](https://www.ukssdc.ac.uk/)
-   [Archaeology Data Service](http://archaeologydataservice.ac.uk/)
",https://help.ceda.ac.uk/article/137-ceda-data-catalogue#*access-type*,1418,192
,"The CEDA Archive seeks to offer long-term archiving for data, but the
level of curation needed will vary from dataset to dataset based on how
likely onward re-use is expected to be. Generally, data with a higher
level of preparation will be more widely accessible to a wider pool of
future users, but CEDA also recognises there are resource limitations
limiting this.
The table below shows three levels of curation offered by the CEDA
Archive, what level of re-use that can be expected for each type, and
the level of data standard and conventions the data will need to meet. 
  -----------------------------------------------------------------------
  \                 Reference\        Structured\       Interoperable\
  ----------------- ----------------- ----------------- -----------------
  Suitable for      Complete          Key or ongoing    Core community
                    datasets\         datasets\         datasets
  Anticipated data  Low\              medium-high       high
  re-use level                                          
  Discoverable in   ✓                 ✓                 ✓
  CEDA data                                             
  catalogue, Google                                     
  Scholar, NERC                                         
  Data Catalogue,                                       
  Data.gov.uk etc.                                      
  DOI-able dataset  ✓                 ✓                 ✓
  (citable in                                           
  papers)                                               
  Web, FTP download ✓                 ✓                 ✓
  Direct JASMIN     if permitted      if permitted\     if permitted\
  access                                                
  Community         encouraged        ✓                 ✓
  wide/archive                                          
  quality format                                        
  (e.g. netCDF)                                         
  File metadata     encouraged\       ✓                 ✓
  follows                                               
  conventions (e.g.                                     
  CF)                                                   
  Extra data tools                                      ✓
  (e.g. subsetting)                                     
  -----------------------------------------------------------------------
",https://help.ceda.ac.uk/article/4691-levels-of-data-curation#,2397,190
Reference,"These are data that are discoverable and downloadable, but it is left to
the user to work out some of the usability issues. CEDA will make a
catalogue entry and add the data files to the archive. This is a
suitable solution if there is not likely to be mass interest in the data
and their principal objective is to provide evidence to support a
publication. Data in this category should be small volume (\< 1TB). 
<div>
Minimum qualification: a paper/documentation referencing the dataset. 
</div>
",https://help.ceda.ac.uk/article/4691-levels-of-data-curation#reference,498,85
Structured,"<div>
As well as being Reference ready, these data are in a community
supported format, with a defined file and directory naming convention.
They may also have specified file level metadata attribute conventions.
This level is suitable for a dataset where there is an intention to make
the data more reusable.
</div>
Minimum qualification: evidence of use of similar datasets by CEDA core
communities.
",https://help.ceda.ac.uk/article/4691-levels-of-data-curation#structured,402,64
Interoperable,"In addition to being Referenced and Structured, these data are connected
to specific community tools or systems that enable better discovery or
processing. For example, climate model data in ESGF, MIDAS land surface
station data in the CEDA WPS or aircraft data in the Flight Finder tool.
Minimum qualification: evidence of use of similar datasets by CEDA core
communities and community tool specifications. Some evidence that the
data will fit the tools.
",https://help.ceda.ac.uk/article/4691-levels-of-data-curation#interoperable,456,72
CEDA Satellite Data Finder Troubleshooting,"Here are some of the known issues with possible solutions. If you are
still experiencing issues, please contact us either through leaving
feedback via the [Feedback
Form](https://docs.google.com/forms/d/e/1FAIpQLSfHCCGwi7DJg9KEEJ-bxCp94kMNbyJZefBmeyvrH9Jjr-6zEA/viewform?usp=send_form)
or contact us [here](#contactModal){#sbContact .contactUs}.
::: {.section .callout-blue .dashed}
-   [Why aren\'t there many results?](#low-results)
-   [Why is the quicklook image the wrong orientation?](#orientation)
-   [Why can\'t I see the quicklook image?](#quicklook-hide)
-   [Why is it taking so long to load?](#long-load)
-   [Where is the rectangle tool?](#rectangle)
-   [Related Articles](#related-articles)
:::
",https://help.ceda.ac.uk/article/4497-cedageosearch-troubleshooting,711,78
Why aren\'t there many results? {#low-results},"To keep the search interface lightweight and fast, only the first 1000
results are drawn on the screen. There may be more hits in a given area,
but they may not be drawn by the interface - this is to improve
performance and keep the searches working in real-time. The total number
of results is displayed below the **Export Results** button. It is best
to refine the search to get fewer than 1000 results.
",https://help.ceda.ac.uk/article/4497-cedageosearch-troubleshooting#why-aren\'t-there-many-results?-{#low-results},406,74
Why is the quicklook image the wrong orientation? {#orientation},"This is a known issue and has been brought from the ESA (European Space
Agency) archive. There is no fix at this time. In the future a solution
may be explored.
",https://help.ceda.ac.uk/article/4497-cedageosearch-troubleshooting#why-is-the-quicklook-image-the-wrong-orientation?-{#orientation},161,31
Why can\'t I see the quicklook image? {#quicklook-hide},"There are two options if the quicklook is not loading.
1.  For the Sentinel 3 dataset, there are currently no quicklook images.
    This will show the image below:
    ![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5981e29d2c7d3a73488b9292/file-KhL6UVn8W8.png){width="" 200""}\
2.  If the image below is displayed, you need to be signed in at
    [data.ceda.ac.uk](https://auth.ceda.ac.uk/account/signin/?r=http%3A//data.ceda.ac.uk/)
    ![](https://s3.amazonaws.com/helpscout.net/docs/assets/564b4bd3c697910ae05f445c/images/5981e2ae042863033a1b92fd/file-g4sr9lLwM9.png){width=""200""}
",https://help.ceda.ac.uk/article/4497-cedageosearch-troubleshooting#why-can\'t-i-see-the-quicklook-image?-{#quicklook-hide},622,46
Platform Records,"::: {#wikipage}
[Original trac
page](http://team.ceda.ac.uk//trac/ceda/wiki/opman/MOLESEditorUserGuide/platform)
::: wiki-toc
",https://help.ceda.ac.uk/article/4333-platform-records,126,7
Page Contents,"1.  [Platform Records](#PlatformRecords)
:::
Platform records cover things such as: facility sites (e.g. MST radar
site), field locations (e.g. Davidstow airfield), mobile vehicles
carrying an instrument (e.g. aircraft, ship). They give background
information on the Platform and NOT the data in the archive from the
platform - this is accessed through the links to the relevant
Observation (aka Dataset in the user view).
  -----------------------------------------------------------------------------------
  Field ( **bold =  Description            Checklist/\*Controlled   Y/N
  mandatory field**                        vocab explanations/      
  )                                        *Notes*                  
  ----------------- ---------------------- ------------------------ -----------------
  **Title**         Short text title for   Do you understand the    
                    the dataset.           title?\                  
                                           Is the title brief and   
                                           simple?\                 
                                           Does the title contain   
                                           unexplained technical    
                                           terms or acronyms?\      
                                           Does the title describe  
                                           the resource rather than 
                                           the activity/project     
                                           which produced it?\      
                                           Is the title in sentence 
                                           case?\                   
                                           Is the title in          
                                           English?\                
                                           Does the title contain   
                                           non-standard characters  
                                           (e.g. © ° ± ² ³ µ)?      
  **Abstract**      Short text only entry  Do you understand the    
                    that will be the first abstract?\               
                    information that users Is the abstract written  
                    will see about the     in plain English?\       
                    text.                  Does the abstract        
                                           describe the resource    
                                           rather than an           
                                           activity/project which   
                                           produced it?\            
                                           Do the first few         
                                           sentences summarise the  
                                           contents of the          
                                           resource?\               
                                           Does the abstract also   
                                           explain \'Where\',       
                                           \'When\', \'How\',       
                                           \'Why\' and \'Who\' (if  
                                           appropriate)?\           
                                           Does the abstract        
                                           contain unexplained      
                                           technical terms or       
                                           acronyms?\               
                                           Does the abstract        
                                           contain non-standard     
                                           characters (e.g. © ° ± ² 
                                           ³ µ)?                    
  Child platform    To cover where there   *will be rarely used,    
                    would be a platform of but useful when there    
                    one type deployed      are generic set-ups      
                    at/on another platform moved around or deployed 
                    - e.g. a 10m mast at   at multiple locations.*  
                    Cardington. the 10m                             
                    mast would be the                               
                    child of the                                    
                    Cardington site                                 
  Platform Type     Controlled vocab to    *more can be added -     
                    categorise the         please suggest \[work    
                    platform type          also needed to link this 
                                           to a vocab in due        
                                           course\]*                
  Platform location free text field for    *needs more refinement.  
                    lat, long etc.         This should be a         
                                           bounding box/point - or  
                                           blank where not relevant 
                                           (e.g. aircraft). For     
                                           mobile platforms the     
                                           location information is  
                                           handled by the Operation 
                                           in the Acquisition for   
                                           the given dataset.*      
  Identifiers       Unless adding a new    *the moles2 url is to    
                    abbreviation, please   ensure that users who    
                    leave these as given.  find an reference in a   
                                           paper to a MOLES2 record 
                                           using the URLs given     
                                           then can confirm to      
                                           themselves that they are 
                                           on the equivalent MOLES3 
                                           page.*                   
  Parties           A list of              \* Operator - party who  
                    people/organisations   operates the instrument\ 
                    (called collectively a \* **CEDA Officer** -    
                    \""party\"") involved    internal CEDA person     
                    with the instrument or responsible for the      
                    the metadata record.\  record\                  
                    Select (or create) a   \* Funder - party who    
                    named party from the   funds the instrument\    
                    list, select their     \* **Metadata Owner** -  
                    role from the          party maintaining this   
                    controlled vocabulary  metadata record (data    
                    and, if more than one  centre)\                 
                    party carrying out     \* Point of Contact -    
                    that role, use the     default is the data      
                    priority number to     centre, but may be used  
                    sequence them in this  in due course for a      
                    role.                  named person from the    
                                           author list too\         
                                           \                        
  Online Resources  Online link to         Do resource locators     
                    relevant resource -    include titles which are 
                    e.g. documentation.\   concise, accurate        
                    There are set          accounts of the resource 
                    categories that should in question?\            
                    be used.\              Do resource locators     
                    Do NOT use: DMP,       include descriptions     
                    Download, Apply for    which fully explain      
                    Access - these are     their purpose?           
                    handled elsewhere.                              
  Reviews           For carrying out       *For the time being      
                    metadata content       please refer all new     
                    reviews - not in       records to Anabelle or   
                    proper operation just  Graham for a review      
                    yet.                   before they are          
                                           published. This is to    
                                           ensure consistent        
                                           quality checks are       
                                           carries out*             
  Migration         All the old legacy     *Please leave the        
  Properties        content from the       \""moles2citation\"" at    
                    MOLES2 \""Content\""     present, but look to     
                    section is in here -   move other items into    
                    split into the various other parts of MOLES if  
                    div tags.              possible - e.g. add      
                                           items under the links    
                                           section to the online    
                                           resources section. Once  
                                           migrated feel free to    
                                           delete content*          
  -----------------------------------------------------------------------------------
:::
",https://help.ceda.ac.uk/article/4333-platform-records#page-contents,9589,687
MIDAS Meteorological Data FAQ,"The Met Office MIDAS dataset collection is one of the most popular
dataset collections in CEDA\'s archives and often causes a number of
questions for users.
Whilst answers to most questions can be found in the [MIDAS User
Guide](http://cedadocs.ceda.ac.uk/1492/), the following Frequently Asked
Questions (FAQs) have been put together to hopefully answer some common
issues:
1.  [When are station data updates available?](#updates)
2.  [Am I using the right station ID?](#station_ids)
3.  [Why doesn\'t this station have X type of data?](#types)
4.  [My station says it was operational for my time period, but the data
    aren\'t available!](#op_station)
5.  [Looks like my station should report the data I want, but I can\'t
    find them!](#incorrect_station_info)
6.  [Why does a site only report daily/monthly rain values and not
    both?](#raindata)
7.  [Can I get a complete station listing?](#station_metadata)
8.  [Where else can I get station data?](#other_station_data)
9.  [How to access the latest MIDAS Open Data](#recent_midas)
",https://help.ceda.ac.uk/article/269-midas-faq,1044,150
"1. When are station data updates available? {# updates children-count=""0""}","MIDAS data are collected by the Met Office as part of their ongoing long
term database. Some of the data messages they receive in the MIDAS
system may arrive sometime after they were originally observed due to
the mechanism the information may be submitted and/or digitised. Such
delays may mean that data may be a month or two behind, depending on the
practices being followed.
In addition, CEDA only obtains a copy of the data from the MIDAS system
once a month and usually covering the previous 6 months to pick up older
data. Thus, in some cases there may be upto 2 months delay before the
bulk of missing data are within the MIDAS collection at CEDA .
","https://help.ceda.ac.uk/article/269-midas-faq#1.-when-are-station-data-updates-available?-{#-updates-children-count=""0""}",657,119
"2.  Am I using the right station ID? {#station_ids children-count=""0""}","[](#recent_midas)
[](#raindata%3EWhy%20does%20a%20site%20only%20report%20daily/monthly%20rain%20values%20and%20not%20both?%3C/a%3E%3C/li%3E%0A%3C/ol%3E%0A%3Ch2%20id=)
[](#raindata%3EWhy%20does%20a%20site%20only%20report%20daily/monthly%20rain%20values%20and%20not%20both?%3C/a%3E%3C/li%3E%0A%3C/ol%3E%0A%3Ch2%20id=)
[](#raindata%3EWhy%20does%20a%20site%20only%20report%20daily/monthly%20rain%20values%20and%20not%20both?%3C/a%3E%3C/li%3E%0A%3C/ol%3E%0A%3Ch2%20id=)
A meteorological station may operate a suite of instruments each with
their own instrument identifiers and even reporting within different
networks. As such a site will typically have more than one identifier
associated with it.\
\
For example,
[Camborne](https://archive2.ceda.ac.uk/cgi-bin/midas_stations/station_details.cgi.py?id=1395&db=midas_stations)
has a MIDAS src_id \'1395\' which helps to \""join\"" all the data from
this station together in the MIDAS collection. However, for it also has
a range of station codes for the wide range of message types that it
issues, as shown in this sample below:
  -------------- -------------- -------------------- ------------------
  Station code   Message type   Message start date   Message end date
  DCNN 8927      SYNOP          05-03-2002           Current
  DCNN 8927      DRADR35        01-01-1995           31-12-1996
  DCNN 8927      MODLERAD       01-10-1981           27-04-2002
  DCNN 8927      NCM            01-09-1978           Current
  RAIN 382432    NCM            01-08-1994           Current
  RAIN 382430    WAHRAIN        01-01-1980           30-09-1994
  RAIN 382432    SREW           01-11-1994           Current
  RAIN 382430    NCM            01-09-1978           31-07-1994
  WIND 892701    HWND6910       01-09-1978           30-09-1982
  WIND 892702    HWND6910       01-08-1994           28-04-2001
  WIND 892702    HWND6910       01-10-1982           31-07-1994
  WMO 03808      CLM71          01-01-1961           31-12-1990
  WMO 03808      UATMP          01-01-1948           Current
  WMO 03808      SYNOP          01-09-1978           Current
  WMO 03808      CLM71-06       01-01-1995           Current
  WMO 03808      CLM75TMP       01-01-1949           Current
  -------------- -------------- -------------------- ------------------
","https://help.ceda.ac.uk/article/269-midas-faq#2.- am-i-using-the-right-station-id?-{#station_ids-children-count=""0""}",2286,187
"3. Why doesn\'t this station have X type of data? {#types children-count=""0""}","Different station covered by the MIDAS collection have differing
operational uses - some are part of networks of rain gauges used by
water companies to monitor rainfall levels, others are at airfields
whilst others are part of meteorological agencies\' monitoring networks.
As such, the types of data that they collect and, therefore, report via
\""messages\"" they issue will vary. In the Camborne example above many
message types can be seen, from \""UATMP\"" which indicates that the site
issues TEMP messages as part of an upper air network to \""MODLERAD\""
which indicates that it also collects and reports irradiance data at the
surface. As such attention to the different *message types* is also
needed to ensure that the station is *likely* to be reporting the
required data that you want. (e.g. if it only reports \""WADRAIN\"" - i.e.
daily rain - then don\'t expect to find wind data from the site).
","https://help.ceda.ac.uk/article/269-midas-faq#3.-why-doesn\'t-this-station-have-x-type-of-data?-{#types-children-count=""0""}",903,150
"4. My station says it is operational for my time period, but the data aren\'t available! {#op_station children-count=""0""}","Though a station\'s start and end date may be a good indicator when data
may be available, a station\'s reporting characteristics will often vary
over time, so it\'s important that you also check the individual start
and end dates for the message types related to the data that you are
after for the station. For example, in the Camborne example above you
can see that SREW, SYNOP messages are still ongoing, but CLM71 and
HWND6910 messages have ceased. In other cases the site itself may have
been operational for some time, but there may not be data available from
that site until a later time.
Earlier data may not yet have been added to MIDAS, but could be
available via the [National Meteorological Library and
Archive](https://www.metoffice.gov.uk/research/library-and-archive)
operated by the Met Office. See FAQ 8: [Where else can I get station
data?](#other_station_data) for more information.
Additionally, 2020 data delivery may have been affected due to COVID-19
restrictions at some sites and will either be delivered at a later date
as the backlog of data are added to the MIDAS system or will remain
unavailable.
","https://help.ceda.ac.uk/article/269-midas-faq#4.-my-station-says-it-is-operational-for-my-time-period,-but-the-data-aren\'t-available!-{#op_station-children-count=""0""}",1128,183
"5. Looks like my station should report the data I want, but I can\'t find them! {#incorrect_station_info children-count=""0""}","Sometimes the station information would suggest that you *should* be
able to get the data that you want from the station, but it still
doesn\'t appear to be available within MIDAS. This could be due to a
variety of reasons such as: 
-   they are fairly recent data and they haven\'t been added to the
    MIDAS collection yet
-   they are recent data that have arrived at the Met Office in a paper
    form, but haven\'t been digitised yet
-   they are old data, but no digitisation has happened (see below)
-   the station information may not be correct - these are hand crafted
    by a team at the Met Office, based on available information and
    sometimes this doesn\'t reflect the actual MIDAS holdings
For all the above cases CEDA have very limited ability to get any
missing data, except for recent data that is still in the process of
being digitised (for these, CEDA still have to wait for that process to
be complete and have no way of determining when those data will be
available).  Where we are aware of station information being incorrect,
though, we do try and let the Met Office MIDAS team know about these so
get this information checked and updated in due course.
Earlier data may not yet have been added to MIDAS, but could be
available via the [National Meteorological Library and
Archive](https://www.metoffice.gov.uk/research/library-and-archive)
operated by the Met Office. See FAQ 8: [Where else can I get station
data?](#other_station_data) for more information.
","https://help.ceda.ac.uk/article/269-midas-faq#5. looks-like-my-station-should-report-the-data-i-want,-but-i-can\'t-find-them!-{#incorrect_station_info-children-count=""0""}",1490,249
"6. Why does a site only report daily/monthly rain values and not both? {#raindata children-count=""0""}","Within MIDAS all data from a given site can be found though search via
its \""src_id\"" - a unique identifier that the Met Office gives for the
site itself. A site may, however, operate one or more raingauges - for
example, it may operate a raingauge to record daily values or one to
record monthly values, or one for both types of record. Each rain gauge
will also have its own identifier (id, as opposed to src_id in the MIDAS
data) and the Met Office stores the data all gauges for the site. The
gauge IDs are listed on the station details page available through the
MIDAS Station Search tool. Thus, by looking by \""src_id\"" you will get
data from all gauges on site, and specific rain gauge data can be
further identified by its specific \""id\"". However, whilst daily data
may be recorded at a given site without an additional monthly rain gauge
the Met Office does not derive monthly values from daily values reported
from that site within MIDAS. 
","https://help.ceda.ac.uk/article/269-midas-faq#6.-why-does-a-site-only-report-daily/monthly-rain-values-and-not-both?-{#raindata-children-count=""0""}",951,171
"7. Can I get a complete station listing? {#station_metadata children-count=""0""}","The [MIDAS Station Search
tool](https://archive.ceda.ac.uk/tools/midas_stations) is a great way to
find specific stations and access their metadata, but if you need access
to station metadata from many sites (e.g. all the UK sites) to pull into
a database, for example, then you can access the site station details a
series of \""metadata\"" files in the CEDA archive. You can access these
files at: <https://data.ceda.ac.uk/badc/ukmo-midas/metadata/>. Here you
will find a readme file detailing the contents of each sub-folder and
the metadata files they contain. These provide details such as a
station\'s name, location and river basin, as well as specific notes
(\""remarks\"") given for data from the sites and details on the
geographic locations (e.g. county names). Note, these metadata files are
updated periodically when CEDA obtains a new version of them from the
Met Office, roughly monthly, so these files may change from time to
time.
","https://help.ceda.ac.uk/article/269-midas-faq#7.-can-i-get-a-complete-station-listing?-{#station_metadata-children-count=""0""}",944,146
"8. Where else can I get station data? {#other_station_data children-count=""0""}","The [National Meteorological Library and
Archive](https://www.metoffice.gov.uk/research/library-and-archive),
operated by the Met Office, have large stores of old weather reports
stretching back to the 19th century. Whilst some of these records are in
the process of being digitised and values converted into computer files
for eventual inclusion in MIDAS, there are may records yet to go through
this process. The NMLA archives include items such as:
-   ship logs
-   weather report from stations operated in the UK by both the Met
    Office and, prior to that, various meteorological societies or other
    organisations
-   weather charts
These may be held purely in paper archives or may also have been
digitised to be viewable online, but not yet \'keyed\' (the process to
convert the written text visible in the digital image of the original
form to values stored in a database/file.
To explore what is available the National Meteorological Library and
Archive visit the NMLA catalogue at:
<https://library.metoffice.gov.uk/Portal/Default/en-GB/Search/SimpleSearch>.
","https://help.ceda.ac.uk/article/269-midas-faq#8.-where-else-can-i-get-station-data?-{#other_station_data-children-count=""0""}",1075,155
"9. How to access the latest MIDAS Open Data {#recent_midas children-count=""0""}","The MIDAS Open data are release annually where the latest version holds
data up to the end of the previous year. Whilst more recent data are
available in the full MIDAS collection for users, up to the end of the
previous month when CEDA gets an update, these aren\'t available to all
users. However, limited extractions for recent data are available via
the [Met Offce\'s National Meteorological Library and Archive enquiry
service](mailto:metlib@metoffice.gov.uk?subject=CEDA%20related%20query:%20Access%20to%20latest%20MIDAS%20Open%20Data%20request&body=Dear%20nmla%20enquiries%20desk,%0d%0a%0d%0ai%20wish%20to%20obtain%20data%20for%20the%20following%20stations%20from%20midas:%20%3Cinsert%20midas%20src_ids%20and/or%20full%20station%20names%3E%0D%0A%0D%0AFor%20the%20time%20period:%20%3Cstart%20date-time%3E%20to%20%3Cend%20date-time%3E%0D%0A%0D%0ARequired%20temporal%20resolution%20(e.g.%20hourly,%20daily,%20monthly)%0D%0A%0D%0ARequired%20weather%20variable%20(e.g.%20temperature,%20rainfall,%20wind):%0D%0A%0D%0AKind%20Regards%0D%0A).
For details of the levels of enquiry they are able to support see the
[NMLA data
policy](https://www.metoffice.gov.uk/binaries/content/assets/metofficegovuk/pdf/research/library-and-archive/archive/nmla-policies/nmla_data_policy_2019.pdf).
","https://help.ceda.ac.uk/article/269-midas-faq#9.-how-to-access-the-latest-midas-open-data-{#recent_midas-children-count=""0""}",1281,89
Reading a NetCDF file from a Python Script using OpenDAP,"***This article is now obsolete, and we recommend that you refer to the
Python NetCDF4 subsetting example at the bottom of our [Archive Access
Tokens](https://help.ceda.ac.uk/article/5100-archive-access-tokens#netcdf4)**** article.***
This page provides an example of how you can connect remotely to a
NetCDF file in the CEDA Archive. This uses the OpenDap protocol that
allows you to query and subset a NetCDF file that is stored on a remote
server. For other ways to connect to OpenDap please see the [scripted
interactions](https://help.ceda.ac.uk/article/4442-ceda-opendap-scripted-interactions)
page.
-   [The Python script](#python)
-   [Setting up an environment to run the python script (Python
    2.7)](#env2)
-   [Setting up an environment to run the python script (Python 3.7 and
    Jaspy)](#env3)
-   [Using the script](#script)
-   [Explanation of the script](#explanation)
-   [Finding the OpenDAP URL](#findurl)
",https://help.ceda.ac.uk/article/4712-reading-netcdf-with-python-opendap,929,124
The Python script {#python},"The example script can be found at
<https://github.com/cedadev/opendap-python-example>. Download the
repository and follow the instructions to use remote_nc_reader.py
\""remote_nc_reader.py\"", provides an example of using a Python2.7 script
to connect to, and read both metadata and data from a file in the CEDA
Archive. The example given requires that the user is registered with
CEDA and the \""setup_credentials()\"" function is used to download and
write the relevant certificate files required by the \""netCDF4\""
library.
",https://help.ceda.ac.uk/article/4712-reading-netcdf-with-python-opendap#the-python-script-{#python},524,73
Setting up an environment to run the python script (Python 2.7) {#env2},"**Note**: This process can be a bit sensitive to the versions of the
NetCDF library. We have found the most success when using conda to
create the virutal environment and install netcdf4.
The above script requires that a few python packages are installed. To
ensure that you can use it you may need to create your own \""virtual
environment\"" and install the required packages as follows:
    $ virtualenv venv 
    $ source venv/bin/activate 
    $ pip install ContrailOnlineCAClient netCDF4
Once the virtual environment has been created you can re-use it next
time you login by simply activating it with:
    $ source venv/bin/activate
",https://help.ceda.ac.uk/article/4712-reading-netcdf-with-python-opendap#setting-up-an-environment-to-run-the python-script-(python-2.7)-{#env2},637,100
Setting up an environment to run the python script (Python 3.7 / Jaspy) {#env3},"To run the script using the Jaspy software environment, a virtual
environment which inherits the packages from the Jaspy Conda
installation can be used:
    $module load jaspy
    $python -m venv --system-site-packages ~/pyvenv3
    $source ~/pyvenv3/bin/activate
    $pip install ContrailOnlineCAClient
Once the virtual environment has been created you can re-use it next
time you login by simply activating it with:
    $source ~/pyvenv3/bin/activate
",https://help.ceda.ac.uk/article/4712-reading-netcdf-with-python-opendap#setting-up-an-environment-to-run-the python-script-(python-3.7-/-jaspy)-{#env3},453,59
Using the script {#script},"In order the use the script you will need to ensure that Python and the
required dependencies (see below) are available, and that you have a
valid CEDA account with access to the URL that you intend to connect
to. The script could be used as follows:
    $ export CEDA_USERNAME=some_user
    $ export CEDA_PASSWORD=some_password 
    $ python remote_nc_reader.py http://dap.ceda.ac.uk/thredds/dodsC/badc/ukcp18/data/marine-sim/skew-trend/rcp85/skewSurgeTrend/latest/skewSurgeTrend_marine-sim_rcp85_trend_2007-2099.nc skewSurgeTrend
The first two lines set the user details that are picked up by the
\""remote_nc_reader.py\"" script. The two arguments provided at the
command-line are:
1.  The URL to a remote NetCDF file (on an OpenDAP server).
2.  The variable ID in that NetCDF file that you want to interrogate. 
",https://help.ceda.ac.uk/article/4712-reading-netcdf-with-python-opendap#using-the-script-{#script},814,107
Explanation of the script {#explanation},"The above script includes a number of stages that deserve some
explanation. This section shows each function and explains how it works.
The \""main()\"" function is where the script starts:
<div>
    def main(nc_file_url, var_id):
        """"""
        Main controller function.
        :param nc_file_url: URL to a NetCDF4 opendap end-point.
        :param var_id: Variable ID [String]
        :return: None
        """"""
        setup_credentials(force=False)
        ds = get_nc_dataset(nc_file_url, var_id)
</div>
The \""main()\"" function takes two arguments from the command-line: (1)
the URL to a remote NetCDF file (on an OpenDAP server) and (2) the
variable ID in that NetCDF file that you want to interrogate. 
Before the \""main()\"" function attempts to contact the remote OpenDAP
server it calls the function \""setup_credentials()\"":
    def setup_credentials(force=False):
        """"""
        Download and create required credentials files.
        Return True if credentials were set up.
        Return False is credentials were already set up.
        :param force: boolean
        :return: boolean
        """"""
        # Test for DODS_FILE and only re-get credentials if it doesn't
        # exist AND `force` is True AND certificate is in-date.
        if os.path.isfile(DODS_FILE_PATH) and not force and cert_is_valid(CREDENTIALS_FILE_PATH):
            print('[INFO] Security credentials already set up.')
            return False
        onlineca_client = OnlineCaClient()
        onlineca_client.ca_cert_dir = TRUSTROOTS_DIR
        # Set up trust roots
        trustroots = onlineca_client.get_trustroots(
            TRUSTROOTS_SERVICE,
            bootstrap=True,
            write_to_ca_cert_dir=True)
        # Write certificate credentials file
        key_pair, certs = onlineca_client.get_certificate(
            username,
            password,
            CERT_SERVICE,
            pem_out_filepath=CREDENTIALS_FILE_PATH)
        # Write the dodsrc credentials file
        write_dods_file_contents()
        print('[INFO] Security credentials set up.')
        return True<br>
The \""setup_credentials()\"" function makes sure that the appropriate
certificate files have been downloaded and saved to your local \$HOME
directory. As part of this process, a check is done to ensure that the
credentials file contains a valid certificate. This is done using the
\""cert_is_valid()\"" function, as follows:
    def cert_is_valid(cert_file, min_lifetime=0):
        """"""
        Returns boolean - True if the certificate is in date.
        Optional argument min_lifetime is the number of seconds
        which must remain.
        :param cert_file: certificate file path.
        :param min_lifetime: minimum lifetime (seconds)
        :return: boolean
        """"""
        try:
            with open(cert_file) as f:
                crt_data = f.read()
        except IOError:
            return False
        try:
            cert = x509.load_pem_x509_certificate(crt_data, default_backend())
        except ValueError:
            return False
        now = datetime.datetime.now()
        return (cert.not_valid_before <= now
                and cert.not_valid_after > now + datetime.timedelta(0, min_lifetime))
The \""cert_is_valid()\"" function loads the existing credentials file (if
there is one) and checks it has not expired. It returns \""True\"" if the
the certificate is still within the minimum lifetime (seconds) specified
by the second argument.
Once the credentials files are all in place and valid then the
\""main()\"" function calls the \""get_nc_dataset()\"" function as follows:
    def get_nc_dataset(url, var_id):
        """"""
        Open a remote connection to a NetCDF4 Dataset at `url`.
        Show information about variable `var_id`.
        Print metadata / data in the file and return the Dataset object.
        :param url: URL to a NetCDF OpenDAP end-point.
        :param var_id: Variable ID in NetCDF file [string]
        :return: netCDF4 Dataset object
        """"""
        dataset = Dataset(url)
        print('\n[INFO] Global attributes:')
        for attr in dataset.ncattrs():
            print('\t{}: {}'.format(attr, dataset.getncattr(attr)))
        print('\n[INFO] Variables:\n{}'.format(dataset.variables))
        print('\n[INFO] Dimensions:\n{}'.format(dataset.dimensions))
        print('\n[INFO] Max and min variable: {}'.format(var_id))
        variable = dataset.variables[var_id][:]
        units = dataset.variables[var_id].units
        print('\tMin: {:.6f} {}; Max: {:.6f} {}'.format(variable.min(), units, variable.max(), units))
        return dataset
The \""get_nc_dataset()\"" function is provided to demonstrate that you
can interact with a secured NetCDF file on a remote OpenDAP server using
the same interface (netCDF4-python) that you would use on a local file.
In this example the following information is printed:
-   Global attributes
-   Variables
-   Dimensions
-   Min and max of requested variable
The \""get_nc_dataset()\"" function returns the [NetCDF4 Dataset python
object](http://unidata.github.io/netcdf4-python/#netCDF4.Dataset) that
you could interrogate further as required.
",https://help.ceda.ac.uk/article/4712-reading-netcdf-with-python-opendap#explanation-of-the-script-{#explanation},5156,563
Finding the OpenDAP URL {#findurl},"To discover the URL to use with the netCDF4 Dataset object above:
-   Use the [CEDA archive browser](http://data.ceda.ac.uk/) to navigate
    to the dataset you wish to open (for example [CRU TS temperature
    data](http://data.ceda.ac.uk/badc/cru/data/cru_ts/cru_ts_3.24.01/data/tmp)).
-   On the right hand side there is a column of download icons.  These
    are for downloading the data directly, and are not for use with
    OpenDAP.
-   For datasets that support OpenDAP access there will be an additional
    icon, which looks like cogs.
-   Click the cog icon.
-   This takes you to a page where you could subset the data and
    download it.
-   However, we are interested in the URL contained in the box marked
    \""Data URL\"".
-   Copy this URL from the box by triple-clicking in the box, clicking
    the right mouse button on the highlighted text and selecting
    \""Copy\"".
-   You can now paste the URL into your code.  It will start with:
",https://help.ceda.ac.uk/article/4712-reading-netcdf-with-python-opendap#finding-the-opendap-url-{#findurl},957,151
CFChecker commandline tool,"::: {#wikipage}
[Original trac
page](http://team.ceda.ac.uk//trac/ceda/wiki/opman/cfchecker)
::: wiki-toc
1.  [CF-Checker command-line tool](#CF-Checkercommand-linetool)
    1.  [Installation Procedure](#InstallationProcedure)
        1.  [1. Create a virtualenv on top of Python
            2.6+](#a1.CreateavirtualenvontopofPython2.6)
        2.  [2. Install cdat_lite
            prerequisites](#a2.Installcdat_liteprerequisites)
        3.  [3. Install cfchecker
            prerequisites](#a3.Installcfcheckerprerequisites)
        4.  [4. Install cfchecker](#a4.Installcfchecker)
    2.  [Example setup script](#Examplesetupscript)
:::
The cfchecks tool is installed in the JASMIN Analysis Platform so it is
available on ingest1, jasmin-sci\*, cems-sci\* and LOTUS.
You can run the checker as follows:
$ cfchecks <path-to-netcdf>
By default the script is configured to download recent versions of the
CF standard name table and area types table from the CF site at PCMDI.
These locations can be configured as other URLs or local files on the
command-line or through environment variables. See setup.sh for details.
$ cfchecks --help
 cfchecker [-a|--area_types area_types.xml] [-s|--cf_standard_names standard_names.xml] [-u|--udunits udunits.dat] [-v|--version CFVersion] file1 [file2...]
Description:
 The cfchecker checks NetCDF files for compliance to the CF standard.
Options:
 -a or --area_types:
       the location of the CF area types table (xml)
 -s or --cf_standard_names:
       the location of the CF standard name table (xml)
 -u or --udunits:
       the location of the udunits.dat file
 -h or --help: Prints this help text.
 -v or --version: CF version to check against.
",https://help.ceda.ac.uk/article/4160-cf-checker-command-line-tool,1693,196
Installation Procedure {#InstallationProcedure},"> NOTE: The following procedure includes installation of dependencies.
> This is not needed when using on JASMIN systems.
Brief notes follow on how to install the cf-checker in its own
directory. It is assumed that you have a suitable python interpreter
(Python 2.5.1+) with virtualenv installed.
",https://help.ceda.ac.uk/article/4160-cf-checker-command-line-tool#installation-procedure-{#installationprocedure},297,47
1. Create a virtualenv on top of Python 2.6+ {#a1.CreateavirtualenvontopofPython2.6},"$ cd /usr/local/cf-checker
$ virtualenv --no-site-packages $PWD
> Create \$VIRTUALENV/setup.sh which activates the environment and sets
> other environment variables. (See bottom of this page for an example)
",https://help.ceda.ac.uk/article/4160-cf-checker-command-line-tool#1.-create-a-virtualenv-on-top-of-python-2.6+-{#a1.createavirtualenvontopofpython2.6},208,28
2. Install cdat_lite prerequisites {#a2.Installcdat_liteprerequisites},"HDF5
:   Check the required version of HDF5 that is compatible with the
    NetCDF4 install you are preparing. Download that version of HDF5
    (currently HDF5-1.8.8)
> Configure to install into its own sub-directory
>
> ``` wiki
> $ ./configure --prefix=$VIRTUALENV/hdf5-1.8.8
> $ make
> $ make check
> $ make install
> ```
> Note: checks take a long time
> Link \$VIRTUALENV/hdf5-1.8.8 to \$VIRTUALENV/hdf5 Link
> \$VIRTUALENV/hdf5/bin/\* to \$VIRTUALENV/bin
> In setup.sh set LDFLAGS and CPPFLAGS to reflect the header and library
> locations of hdf5 (neede to compile netcdf4)
NetCDF4
:   Check you have curl headers TODO: check version
> configure to install into its own sub-directory
>
> ``` wiki
> $ ./configure --prefix=$VIRTUALENV/netcdf4-$VERSION --enable-netcdf4 --enable-dap
> $ make
> $ make check
> $ make install
> ```
Numpy
:   ``` wiki
    $ . setup.sh
    $ pip install numpy
    ```
<!-- -->
cdat_lite
:   ``` wiki
    $ pip install cdat_lite
    ```
",https://help.ceda.ac.uk/article/4160-cf-checker-command-line-tool#2.-install-cdat_lite-prerequisites-{#a2.installcdat_liteprerequisites},972,156
3. Install cfchecker prerequisites {#a3.Installcfcheckerprerequisites},"UDUNITS2
:   Download udunits2 from unidata
    ``` wiki
    $ ./configure --prefix=/usr/local/cf-checker/udunits-$VERSION
    $ make
    $ make install
    ```
",https://help.ceda.ac.uk/article/4160-cf-checker-command-line-tool#3.-install-cfchecker-prerequisites-{#a3.installcfcheckerprerequisites},161,17
4. Install cfchecker {#a4.Installcfchecker},"$ pip install cfchecker
Copy the following file to
\$VIRTUALENV/udunits/share/udunits/udunits2_local.xml
<?xml version=""1.0"" encoding=""US-ASCII""?>
<!--                                                                            
  Include patch for bit units and import the defaults.                        
-->
<unit-system>
  <import>udunits2.xml</import>
  <unit>
    <def>1</def>
    <aliases>
      <name> <singular>bit</singular> </name>
    </aliases>
  </unit>
</unit-system>
",https://help.ceda.ac.uk/article/4160-cf-checker-command-line-tool#4.-install-cfchecker-{#a4.installcfchecker},483,35
Example setup script {#Examplesetupscript},"This is the setup.py script on glacial. Fair has a slightly different
script because it is installed into its own Python interpreter.
#
# Please execute this script as ""source setup.sh""
#
CFCHECKER_HOME=/usr/local/cf-checker
# Source virtual environment
source $CFCHECKER_HOME/bin/activate
# Configure HDF5 location for compilation
export LDFLAGS=""-L$CFCHECKER_HOME/hdf5/lib""
export CPPFLAGS=""-I$CFCHECKER_HOME/hdf5/include""
# Configure dynamic library locations
export LD_LIBRARY_PATH=$CFCHECKER_HOME/lib/hdf5:$CFCHECKER_HOME/netcdf/lib:$CFCHECKER_HOME/udunits/lib
# Configure cf-checker dependencies
# By default the checker downloads standard names and area types from the web.
export CF_STANDARD_NAMES='http://cf-pcmdi.llnl.gov/documents/cf-standard-names/standard-name-table/current/cf-stand\
ard-name-table.xml'
export CF_AREA_TYPES='http://cf-pcmdi.llnl.gov/documents/cf-standard-names/area-type-table/current/area-type-table.\
xml'
export UDUNITS=$CFCHECKER_HOME/udunits/share/udunits/udunits2_local.xml
:::
",https://help.ceda.ac.uk/article/4160-cf-checker-command-line-tool#example-setup-script-{#examplesetupscript},1016,83
Ingest Scripts and Git,"When creating a new ingest script or config file it is important to make
sure that this is stored in CEDA\'s Git Lab repo. However, it\'s not
obvious how to do this. Here\'s some instructions.
**ALWAYS USE CEDA\'s Git Lab instance as this is a private repo and
never the public CEDA git repo!**
",https://help.ceda.ac.uk/article/4837-ingest-scripts-and-git,295,54
Check if your software dir is covered by Git,"    cd <dir you wish to check>
    git status
This will tell you if your directory is covered by Git or not
",https://help.ceda.ac.uk/article/4837-ingest-scripts-and-git#check-if-your-software-dir-is-covered-by-git,108,21
Check your code into Git Lab,"1.  log into CEDA\'s Git Lab instance at the Ingest project level:
    <https://breezy.badc.rl.ac.uk/ingest>
2.  Select New Project
3.  Give this the name of the directory you are working in
4.  follow the instructions under the **Push an existing Git
    repository** section
",https://help.ceda.ac.uk/article/4837-ingest-scripts-and-git#check-your-code-into-git-lab,277,41
Ingest control,"::: {#wikipage}
\
Cron is used to manage ingestion jobs. This enables data scientists to
schedule jobs to run regularly at certain times, days or dates.
To make sure ingestion processes have a common way of reporting if they
have failed, and to keep track of output, a standard script is used to
control each run. This script is called icwrapper. This script also
allows the process to be locked so that only one instance of the script
runs at any one time and can email messages summarising the errors and
output from the job.
The icwrapper script is controlled by configuration files with each
section defining an ingest stream. See below for options used by the
script.
A command line tool \""ingest_control\"" is used to simplify frequent
tasks, such as listing ingest streams and loading the cron schedule. It
can also be used to interrogate the output and status of the previous
run.
",https://help.ceda.ac.uk/article/4272-ingest-control,888,154
Example configuration file {#Exampleconfigurationfile},"Ingest is controlled via simple configuration files. These are usually
under /home/badc/software/datasets, although they can be in other places
if there is a particular reason to have them somewhere else. These look
like this
[DEFAULT]
notify_warning= sam.pepler@stfc.ac.uk
notify_fail= sam.pepler@stfc.ac.uk
working_dir = /home/badc/software/datasets/testdata
[test-stream1]
when = 30 8 * * *
script =./test.py test.conf test-stream1
#script = sleep 300                                                                                                                           
notify_ok= sam.pepler@stfc.ac.uk
arrivals_users = spepler wgarland
arrivals_wait = 30
[test-stream2]
when = 32 1 * * *
script = /usr/local/ingest_software/ingest_to_archive/trunk/arrivals_monitor.py
notify_ok = sam.pepler@stfc.ac.uk
arrivals_users = wgarland
arrivals_wait = 5
working_dir = /home/badc/software/datasets/testdata
arrivals_monitor_file = am_test.txt
Each section defines an ingest stream. The DEFAULT section allow default
attributes to be given to all the streams in one config file.
The icwrapper script knows the following stream options:
  ----------------- ------------------------------------------------------------------- -------------------- -----------------
  **Option**        **Example**                                                         **Default**          **Comments**
  owner             spepler                                                                                  Who to contact
                                                                                                             about this job
  when              \* \* \* \* \*                                                      With no when option  cron time syntax.
                                                                                        the stream will be   
                                                                                        marked as for        
                                                                                        running manually.    
  do_not_run        on                                                                  off                  Stops the job
                                                                                                             being scheduled
  errors_ok         on                                                                  off                  Marks process as
                                                                                                             ok-errors rather
                                                                                                             than warn is
                                                                                                             there is any
                                                                                                             standard errors.
  notify_warning    [[ ]{.icon}                                                                              
                    s.peplr\@stfc.ac.uk](mailto:s.peplr@stfc.ac.uk){.mail-link}                              
  notify_fail       [[ ]{.icon} badc\@rl.ac.uk](mailto:badc@rl.ac.uk){.mail-link} [[                         
                    ]{.icon}                                                                                 
                    sam.pepler\@stfc.ac.uk](mailto:sam.pepler@stfc.ac.uk){.mail-link}                        
  notify_ok         [[ ]{.icon}                                                                              
                    s.peplr\@stfc.ac.uk](mailto:s.peplr@stfc.ac.uk){.mail-link}                              
  lock              on                                                                  on                   only one process
                                                                                                             can run
  script            /x/y/script.py my.conf stream3                                                           This is the only
                                                                                                             mandatory option.
                                                                                                             ingest_control
                                                                                                             will ignore
                                                                                                             sections without
                                                                                                             this option.
  retry             3                                                                   0                    number of times
                                                                                                             to retry on fail
  timeout           2                                                                   12                   hours until the
                                                                                                             process is killed
  working_dir       /home/badc/software/datasets/omi-toms                               cron scripts will    working
                                                                                        run in the home dir. directory. Allows
                                                                                        manual scripts will  relative paths to
                                                                                        run in the users     be used for the
                                                                                        current directory.   script and config
                                                                                                             file arguments.\
  cron_host         ingest2.ceda.ac.uk                                                  ingest1.ceda.ac.uk   Run this ingest
                                                                                                             script on a
                                                                                                             different ingest
                                                                                                             host. Requires
                                                                                                             the full host
                                                                                                             name. \
  conda_env\        ingest_py                                                           ingest               Switch to this
                                                                                                             conda environment
                                                                                                             to run the job.
  ----------------- ------------------------------------------------------------------- -------------------- -----------------
",https://help.ceda.ac.uk/article/4272-ingest-control#example-configuration-file-{#exampleconfigurationfile},7209,313
{#Runningingestcontrol},,https://help.ceda.ac.uk/article/4272-ingest-control#{#runningingestcontrol},0,0
{#Runningingestcontrol},,https://help.ceda.ac.uk/article/4272-ingest-control#{#runningingestcontrol},0,0
Running ingest control {#Runningingestcontrol},"Ingest control is installed on ingest1.ceda.ac.uk. The tool is only used
by the user badc.
SSTDMSJP01$ ssh badc@ingest1.ceda.ac.uk
(venv27)[badc@ingest1 ~]$ ingest_control 
Ingest control> help
Documented commands (type help <topic>):
========================================
ALLSTART  a           edit  grep  list      q         reload  tailerr   w    
ALLSTOP   crontab     err   kill  listconf  quit      run     tailout   watch
EOF       deregister  f     l     out       register  show    timeline
Undocumented commands:
======================
help
Ingest control>
Used the help command to find out what the commands do.
",https://help.ceda.ac.uk/article/4272-ingest-control#running-ingest-control-{#runningingestcontrol},626,73
{#a},":::
",https://help.ceda.ac.uk/article/4272-ingest-control#{#a},4,1
Data Access public and restricted data,"While each dataset in the CEDA archive is covered by its own usage
licence, access to data is split into the following access types:
",https://help.ceda.ac.uk/article/98-accessing-data,133,24
Public data,"These datasets can be accessed via the main CEDA archives without
needing to log in (though a CEDA account is needed for FTP access to the
CEDA Archive).
You can see a full list of our public datasets here:
> [CEDA Data Catalogue listing of \'Public\' access
> datasets](https://catalogue.ceda.ac.uk/?q=&results_per_page=20&sort_by=relevance&objects_related_to_uuid=&geo_option=True&north_bound=&east_bound=&west_bound=&south_bound=&start_date=&end_date=&date_option=publication_date&start_date_pub=&end_date_pub=&permission=public)
",https://help.ceda.ac.uk/article/98-accessing-data#public-data,533,49
Registered user data,"These data are available to all registered users and simply require the
user to log into their account when prompted to do so. This restriction
has typically been applied to permit anonymised usage and download
statistics to be reported to the data provider and to provide a means
for CEDA to contact users if an issue regarding the dataset arises.
In addition to those datasets made publically available (see above) you
can see a full list of datasets available to all registered users here:
> [CEDA Data Catalogue listing of \'Registered User\' access
> datasets](https://catalogue.ceda.ac.uk/?q=&results_per_page=20&sort_by=relevance&objects_related_to_uuid=&geo_option=True&north_bound=&west_bound=&east_bound=&south_bound=&start_date=&end_date=&date_option=publication_date&start_date_pub=&end_date_pub=&permission=registered)
",https://help.ceda.ac.uk/article/98-accessing-data#registered-user-data,832,95
Restricted data,"At times data need to be restricted either to a limited user group for
some reason (e.g. field campaign participants) or where access
applications from users need to be first checked and authorised to
ensure the proposed usage lies within the licence for the data (e.g. for
Met Office data access). Further details about accessing these
restricted data are given below.  Access to some of these data may
subsequently be altered to a more permissive access type, e.g. for NERC
funded data access should be opened up no later than 2 years after the
date of collection/production. However, other datasets  - typically
third party datasets where CEDA is acting to assist the community to
access these data - will remain restricted (e.g. Met Office data and
ECMWF data).
",https://help.ceda.ac.uk/article/98-accessing-data#restricted-data,766,128
Off-line content,"In addition to online content, some data held by CEDA, mainly within the
UKSSDC, are only available as either as a physical artefact (e.g.
photographic plates of sun spot activity) or have been digitised but
require processing before they can be made available. In such cases
users are requested to contact CEDA to discuss access to such content.
",https://help.ceda.ac.uk/article/98-accessing-data#off-line-content,347,58
Applying for access to restricted datasets,"Find the data you need by searching the  [CEDA Metadata
Catalogue](http://catalogue.ceda.ac.uk/).
Please read the  [Catalogue Intro](http://catalogue.ceda.ac.uk/intro) to
efficiently find the data you are looking for.
Once you will have found the Dataset of interest, click on the \""Apply
for access\"" button on the dataset\'s catalogue page to apply for access
. This will take you through the dataset application process where you
will be asked to provide details of your intended use of the restricted
data. 
It is important to provide sufficient information in your application
for it to be assessed by the person authorising your request - this may
be someone within CEDA for third party data or someone external for
project related data. Thus, there may be a delay in access being
approved, but CEDA aims to process applications as quickly as possible
(within 1 working day for applications we can process and within 1 month
where external authorisers are involved to allow for annual leave etc.).
Occasionally we may need to contact you for further information about
your application or to ensure that the application is just for yourself
(the dataset usage licence is only applicable to yourself  - others
wishing to use the data must submit their own application for access).
Once your application has been processed you will receive an email
notifying you of the decision - either telling you that you have access
to the restricted dataset or that it has been rejected for some reason. 
",https://help.ceda.ac.uk/article/98-accessing-data#applying-for-access-to-restricted-datasets,1497,242
"Change of use, Extending access and Renewing access","Access to restricted data is only granted for the purpose under which
the application was submitted and for the duration of the licence.
Therefore, if you wish to use the data for an alternative use or to
either renew your access or have it extended then a fresh application
should be submitted to ensure that a new data usage license is granted.
This can be easily done from your \""myCEDA\"" page by clicking on the
plus symbol next to your existing data access listed under the \""My
datasets and services\"" tab.
Each application will be judged on its own merit to ensure that it is
compatible with the usage restrictions applied to the dataset. 
","https://help.ceda.ac.uk/article/98-accessing-data#change-of-use,-extending-access-and-renewing-access",647,114
unpackerpy,"::: {#wikipage}
[Original trac
page](http://team.ceda.ac.uk//trac/ceda/wiki/opman/unpacker)
::: wiki-toc
1.  [unpacker.py](#unpacker.py)
    1.  [Introduction](#Introduction)
    2.  [Where source is stored](#Wheresourceisstored)
    3.  [Limitations](#Limitations)
    4.  [Files needed](#Filesneeded)
    5.  [Config options](#Configoptions)
    6.  [Additional, un-desired files](#Additionalun-desiredfiles)
:::
",https://help.ceda.ac.uk/article/4267-unpackerpy,415,29
Introduction {#Introduction},"A generic script that can be used to unpack an incoming archive ball
(one with various levels of tarring, gzipping and/or zipping).
It allows:
-   unpacking of an incoming archive balls matching a regex
    (extractRegex)
-   extraction of selected items according to a match with a given regex
-   optional handling of other found files in the incoming archive balls
    not matching the regex
-   optional quarantine check and migration to another area - e.g. an
    ingestion area or an area where further processing could happen
-   can set up an arrivals area if this is not already in existence
-   it makes use of the arrivals library to determine incoming files etc
    from multiple sources
-   as it is a stand alone operation, such operations can be scheduled
    to daisy-chained them together as an ingestion stream with
    fileProcessor and ingester, plus other actions
",https://help.ceda.ac.uk/article/4267-unpackerpy#introduction-{#introduction},885,144
Where source is stored {#Wheresourceisstored},"Source code is stored in the CEDA svn repository - presently just here :
[[ ]{.icon}
http://proj.badc.rl.ac.uk/badc/browser/ceda_software/unpacker/trunk](http://proj.badc.rl.ac.uk/badc/browser/ceda_software/fileProcessor/trunk){.ext-link}
",https://help.ceda.ac.uk/article/4267-unpackerpy#where-source-is-stored-{#wheresourceisstored},239,17
Limitations {#Limitations},"At present the unpacker has the following limitations:
1.  You can\'t specify the unpack depth
2.  On unpacking it will ignore any internal directory structure within
    the tar ball
3.  It doesn\'t cope with bzip - only tar, gzip and zip
4.  You can\'t specify it to extract down to a matching gzip, tar or zip
    file within the archive ball.. it will unpack past this. (which can
    be a pain if you want to do something like: unpack to a gzip file,
    rename that gzip file with fileProcessor and then ingest - instead
    you\'ll need to add in the zipping step into fileProcessor)
5.  It is presently hardwired to process in batches of 1000 unpacked
    files
",https://help.ceda.ac.uk/article/4267-unpackerpy#limitations-{#limitations},670,117
Files needed {#Filesneeded},"This only requires a configuration file with an appropriate entry for
the \""stream\"" to be processed.
",https://help.ceda.ac.uk/article/4267-unpackerpy#files-needed-{#filesneeded},102,16
Config options {#Configoptions},"[stream-name]
owner: <insert your username here - this is important to help those looking after the system work out who is running jobs>
description: <a short description detailing the job and what it does>
# standard bits for most config files:
script: <command line call for the job, including optional entries>
lockfile: <path and name to a lockfile - standard practice is to pop these under /home/badc/lockfiles/>
when: <scheduler times in standard crontab format - e.g. minutes hours day-of-month month year to run the script. Used to schedule recurring tasks under ingest_control>
timeout: <number of hours the script is permitted to continue running for before being terminated - the default is 12>
notify_ok: <space separated list of email addresses to email if the jobs runs ok>
notify_warning: <space separated list of email addresses to email if there are warning messages issued> 
notify_fail: <space separated list of email addresses to email if the job fails>
# end of scheduler details
arrivals_users: <either give a space separated list of the users who will contribute to this data stream>
arrivals_dirs: <OR a space separated list of absolute paths to the source directories for the incoming data>
arrivals_wait: <how old the files should be in seconds before being considered for ingestion>
fileAge: <how old the files should be days before being considered for ingestion - note, will be retired in due course>
extractRegex: <a regex used to identify the incoming files to ingest and then to supply parameters for the dirtemplate/headerclass library call>
regex: <regular expression to uniquely identify files within a tarball (which would also be decompressed if zipped) to prepare a list for subsequent operations. (this should be the first in the operations order if required).>
extraFiles: < option for handling extra files in the incoming archive ball not matched by the regex. Options are: split - unpacks these to a date-timed directory within the quarantine area and allows optional removal of source archive ball; keep - keeps the original archive ball (overwrites the deleterChoice setting to ""keep""); ignore - doesn't do anything with these extra files and follows the deleterChoice/ Finally the argument can be followed by "",notify"" for optionally reporting this back as a warning message 
deleterChoice: <one of arrivals|notArrivals to delete data from /datacentre/arrivals/users/ or /datacentre/processing otherwise the files will be kept>
For example:
[cfarr-lidar-ct75k-unpacker]
owner:gparton
description: unpacks ct75k archive balls ready for ingestion
script: python /usr/local/ingest_software/unpacker/unpacker.py -c /home/badc/software/datasets/chilbolton/chilbolton_fileProcessor.cfg -s cfarr-lidar-ct75k-unpacker
arrivals_dirs: /datacentre/arrivals/users/jagnew/cfarr-lidar/
quarantineDir: /datacentre/processing/chilbolton/quarantine/
ingestDir: /datacentre/processing/chilbolton/readyToIngest/
lockfile: /home/badc/lockfiles/chilbolton-lidar-ct75k-unpacker.lock
extractRegex: (.*/)?ct75k_(?P<year>[0-9]{4})(?P<month>[0-9]{2})(\.tar\.gz)$
regex: (.*/)?cfarr-lidar-ct75k_chilbolton_(?P<year>[0-9]{4})(?P<month>[0-9]{2})(?P<day>[0-9]{2}).(?P<product>png|nc)$
extraFiles: split,notify
notify_warning: graham.parton@stfc.ac.uk
fileAge:0
quaratinePeriod:0
quarantineCheck: fileAge
ingestRegex: cfarr-lidar-ct75k_chilbolton_(?P<year>[0-9]{4})(?P<month>[0-9]{2})(?P<day>[0-9]{2})(\.)(?P<product>nc|png)$
order: extract,moveToIngest
deleteOption: arrivals
mode: operational
",https://help.ceda.ac.uk/article/4267-unpackerpy#config-options-{#configoptions},3509,430
"Additional, un-desired files {#Additionalun-desiredfiles}","If the unpacker spots that there are other files that have not matched
the regex then it will follow the options set by the \""extraFiles\""
field of the configuration settings. Options are:
-   ignore  - don\'t worry about their existence and continue to handle
    the source balls as normal under the deleterOption setting
-   keep - keep the incoming source balls and prevent these from being
    deleted - i.e. override the deleterOption to \""keep\""
-   split - splits out the files in to a date-timed sub-directory within
    the quarantine area. This will also maintain the directory structure
    that the extrafiles were found under within the archive ball, but
    will have unpacked them too, thus loosing any compression etc.
There is also an option as to whether or not to raise a notification
about this issue with the \""notify\"" option.  This is strongly
recommended for the *keep* and *split* options, and also recommended for
the *ignore* option too in order to ensure that nothing nasty is
happening, or that you\'re not missing files due to an issue with your
regex setting.
:::
","https://help.ceda.ac.uk/article/4267-unpackerpy#additional,-un-desired-files-{#additionalun-desiredfiles}",1096,181
Climate Data in CEDA Archives,"CEDA hold a wide range of climate observation and climate model data.
This article will hopefully direct most people to relevant data that we
hold and a few hints and tips along the way. However, if you still
can\'t find the data that you need, please feel free to get in touch and
we\'ll do what we can to assist.
",https://help.ceda.ac.uk/article/235-climate-data-in-ceda-archives,315,60
Climate Observational Data,"The Climatic Research Unit at the University of East Anglia
[CRU](http://catalogue.ceda.ac.uk/uuid/b6c783922d1ce68c4293d90caede5bb9)
data is an extremely popular high resolution gridded observational
climate record. The
[CRU TS](http://catalogue.ceda.ac.uk/uuid/3f8944800cc48e1cbc29a5ee12d8542d)
(time-series) data provide month-by-month variations in climate over the
last century and the [CRU
CY](http://catalogue.ceda.ac.uk/uuid/116aed45b5f0d15ddc3b0e753837e8c9)
data consists of country averages at a monthly, seasonal and annual
frequency. The [CRU
JRA](https://catalogue.ceda.ac.uk/uuid/863a47a6d8414b6982e1396c69a9efe8)
(Japanese reanalysis) data are 6-hourly, land surface, gridded time
series of ten meteorological variables. Variables within these datasets
include cloud cover, diurnal temperature range, frost day frequency,
precipitation, daily mean temperature, monthly average daily maximum
temperature, vapour pressure, Potential Evapo-transpiration and wet day
frequency. 
The Met Office Hadley Centre (MOHC) provides a number of observational
climate records that are available from CEDA including the Central
England Temperature (
[CET](http://catalogue.ceda.ac.uk/uuid/a946415f9345f6da9bf4c475c19477b6))
record, the Hadley Centre Global sea-Ice coverage and Sea Surface
Temperature
([HadISST1.1](http://catalogue.ceda.ac.uk/uuid/facafa2ae494597166217a9121a62d3c))
and a gridded dataset of global historical surface temperature anomalies
([HadCRUT4](http://catalogue.ceda.ac.uk/uuid/f7189fabb084452c9818ba41e59ccabd))
to name a few. The Met Office
[HadUK-Grid](https://catalogue.ceda.ac.uk/uuid/4dc8450d889a491ebb20e724debe2dfb)
collection provides gridded surface data derived from a range of
observations inputs. A full list of available datasets can be found
here: [MOHC climate observational
data](http://catalogue.ceda.ac.uk/uuid/ce252c81a7bd4717834055e31716b265).
",https://help.ceda.ac.uk/article/235-climate-data-in-ceda-archives#climate-observational-data,1888,181
Climate Model Data,,https://help.ceda.ac.uk/article/235-climate-data-in-ceda-archives#climate-model-data,0,0
*Coupled Model Intercomparison Projects (CMIP)*,"The World Climate Research Programme\'s
([WCRP](https://www.wcrp-climate.org/)) has coordinated a number of
iterations of the Coupled Model Intercomparison (CMIP) Projects. Within
the CMIP program leading climate modelling centres around the world
performed a set of pre-defined experiments. The climate model output
from the simulations of the past, present and future climate is
collected and distributed to institutes and individuals outside of the
major modelling centres. This allows scientists to perform research of
relevance to climate science for the Intergovernmental Panel on Climate
Change ([IPCC](http://www.ipcc.ch/)) assessment reports. Currently at
CEDA data is available for: 
-   [CMIP3](http://catalogue.ceda.ac.uk/uuid/5b9dbe341d2fb169922d36e7c0cf8805)
    (Coupled Model Intercomparison Project Phase 3) performed during the
    years 2005 and 2006 and informed the IPCC Fourth Assessment Report
    ([AR4](http://www.ipcc.ch/publications_and_data/ar4/syr/en/contents.html)).
-   [CMIP5](http://catalogue.ceda.ac.uk/uuid/d2d8f982d66cce55bb59fc769ca39264)
    (Coupled Model Intercomparison Project Phase 5) performed during the
    years 2009 and 2012 and informed the IPCC Fourth Assessment Report
    ([AR5](https://www.ipcc.ch/report/ar5/)).
    -   If you are a JASMIN user see the [CMIP5 example
        analysis](https://help.jasmin.ac.uk/article/167-example-job-1)
        article which gives an example of how to access and perform
        analysis on CMIP5 data.
    -   For details on how to find CMIP5 data in the CEDA data catalogue
        see: [CEDA CMIP5
        data](https://help.ceda.ac.uk/article/4465-cmip5-data)
-   CMIP6 (Coupled Model Intercomparison Project Phase 6) performed
    during the years 2019 - present and will inform the IPCC Sixth
    Assessment Report (AR6). This project is in progress and data are
    still being actively retrieved by CEDA, not all datasets are
    complete. At present data can not be found through the CEDA
    catalogue but can be obtained from the [CEDA
    data](http://data.ceda.ac.uk/badc/cmip6/data/CMIP6) browser
    or directly from JASMIN at \'/badc/cmip6/data/\`.  
",https://help.ceda.ac.uk/article/235-climate-data-in-ceda-archives#*coupled-model-intercomparison-projects-(cmip)*,2157,252
*UK Climate Projections*,"The UK Climate Projections (UKCP) project provides climate modelling
information on plausible changes in 21st century climate for the United
Kingdom.
-   [UKCIP02](http://catalogue.ceda.ac.uk/uuid/27d315060f7c29609a5a01d0a72a7a3a): 
    The UK Climate Impacts Programme 2002 (UKCIP02) are a set of climate
    projections derived from a series of climate modelling experiments
    commissioned and funded by Department for Environment, Food
    and Rural Affairs (DEFRA), performed by the Hadley Centre and
    analysed by the Tyndall Centre.
    The UKCIP02 data are comprised of four scenarios of future climate
    change for the UK based on the understanding of the science of
    climate change in 2002. The climate change scenarios provide a
    common starting point for assessing climate change vulnerability,
    impacts and adaptation in the UK. 
-   [UKCP09](http://catalogue.ceda.ac.uk/uuid/077fd790439c44b99962552af8d37a22):
    The UKCP09 data provides users with access to information on
    plausible changes in 21st century climate for the United Kingdom.
    UKCP09 provides future climate projections for land and marine
    regions as well as observed (past) climate data for the UK. UKCP09
    was produced in 2009, funded by a number of agencies led by Defra.
    It is based on sophisticated scientific methods provided by the Met
    Office, with input from over 30 contributing organisations. UKCP09
    can be used to help organisations assess potential impacts of the
    projected future climate and to explore adaptation options
    to address those impacts.
-   [UKCP18](https://catalogue.ceda.ac.uk/uuid/c700e47ca45d4c43b213fe879863d589):
    The government-funded UK Climate Projections (or UKCP18) provide the
    most up-to-date assessment of how the UK climate may change during
    this century. Developed by the Met Office, the projections include
    information to support scientific studies, climate change risk
    assessments and adaptation plans. UKCP18 uses cutting-edge climate
    science to deliver updates to land and marine climate scenarios out
    to 2100, including: probabilistic land projections, high-resolution
    spatially-coherent land simulations and coastal projections related
    to sea-level rise and storm surge. CEDA plays a pivotal role in
    UKCP18 by providing both the data archive and tools for interfacing
    with the data sets. All data is provided in the CF-netCDF format
    distributed via an Open Government Licence (OGL). CEDA also runs a
    web-tool to support the projections needed to provide an intuitive
    interface for selecting data products, tailoring the outputs and
    enabling batch extractions where required. This solution, the [UKCP
    User Interface](https://ukclimateprojections-ui.metoffice.gov.uk),
    draws together a set of pre-existing technologies in a responsive
    and scalable web-interface. 
",https://help.ceda.ac.uk/article/235-climate-data-in-ceda-archives#*uk climate-projections*,2905,375
CMIP5 Data,"The WCRP CMIP5 programme was one of the most comprehensive combined
climate modelling experiments ever performed in climate science and
produced huge quantities of data archived around the world by members of
the Earth System Grid Federation (ESGF).\
\
To assist use of the CMIP5 data, CEDA archives hold around 50% of the
total CMIP5 output within the archives and have built a comprehensive
series of data catalogue records covering these holdings.
This help article should assist users to :
-   [search for and discover specific model output](#search)
-   [browse to find related model output](#related)
-   [find related quality control statements and simulation
    details](#qc)
",https://help.ceda.ac.uk/article/4465-cmip5-data,685,104
Searching for model output {#search},,https://help.ceda.ac.uk/article/4465-cmip5-data#searching-for-model-output-{#search},0,0
Finding related model output {#related},"Each simulation dataset record in the catalogue should include details
in the abstract about various aspects of the data, such as :
-   realms
-   frequencies
-   ensemble members
Additionally, each dataset record is linked a \""Project\"" record
representing each modelling group\'s contribution to the CMIP5 project.
On those records you will find information about the modelling group
membership and also links to all the datasets that CEDA holds from the
group.  Each Dataset will also be found under the linked \""Dataset
Colleciton\"" which pulls together all the datasets produced from a given
model.
",https://help.ceda.ac.uk/article/4465-cmip5-data#finding-related-model-output-{#related},604,94
Quality control statements and Simulation details {#qc},"For a large number of the dataset records there are links to background
information which may be of use to users of the data. For example, on
the dataset records there may be links to data quality check information
held by DKRZ under the \""Docs\"" tab, though this isn\'t available for
all records. Similarly, under the \""Process\"" tab users will find
summary information about the simulation run, listing the component
model and experiment parts and, where possible, linking out to related
documentation held on the ES-DOC site.
",https://help.ceda.ac.uk/article/4465-cmip5-data#quality-control-statements-and-simulation-details-{#qc},529,87
CEDA GRIB Documentation,"*A brief introduction to the GRIB 1data format.*
",https://help.ceda.ac.uk/article/4426-grib#ceda-grib-documentation,49,8
Introduction,"This page gives a brief introduction to GRIB (GRIdded Binary) data with
particular emphasis on the GRIB files held at CEDA. The aim is to give
an indication of what to expect when faced with a GRIB file. More
detailed documentation is available, for example at 
[NCAR](http://dss.ucar.edu/docs/formats/grib/gribdoc/) and
the [ECMWF](https://confluence.ecmwf.int//display/CKB/What+are+GRIB+files).
GRIB is a WMO (World Meteorological Organisation) standard format for
archiving and exchanging gridded data. GRIB is a binary format, and the
data is packed to increase storage efficiency. GRIB data is also
self-describing, meaning that the information needed to read the file is
present within the file. This page outlines how the data and information
needed to interpret the data is stored in a GRIB file.
",https://help.ceda.ac.uk/article/4426-grib#introduction,805,117
GRIB Structure,"Each GRIB file is composed of a series of  **GRIB records**. One GRIB
record holds the gridded data for one parameter at one time and at one
level. Each GRIB record is composed of **6 GRIB sections**. Section 4
contains the data itself. The other sections give the information
required to read the GRIB record, and information on the meteorological
parameter contained in the record, the level the record refers to and
the type of grid the parameter is on. The sections are as follows:
**Section 0:** Indicator Section.\
**Section 1:** Product Definition Section.\
**Section 2:** Grid Description Section - Optional.\
**Section 3:** Bit Map Section - Optional.\
**Section 4:** Binary Data Section.\
**Section 5:** \'7777\' - ASCII Characters indicating end of GRID
record.
Section 1, known as the Product Definition Section (PDS), and section 2,
known as the Grid Definition Section (GDS), are the information sections
most frequently referred to by users of GRIB data. The PDS (section 1)
contains information about the parameter, level type, level and date of
the record. The GDS (section 2) contains information on the grid type
(such as whether the grid is regular or gaussian), and the resolution of
the grid.
",https://help.ceda.ac.uk/article/4426-grib#grib-structure,1215,198
GRIB codes,"Some of the information in the GRIB record information sections are
represented by integer codes. For instance, the name of the parameter in
the GRIB record is indicated using an integer known as the GRIB
parameter code, or parameter identifier. Code tables, which are not part
of the GRIB file but are held separately for each dataset using data in
GRIB format.
",https://help.ceda.ac.uk/article/4426-grib#grib-codes,363,62
Reading GRIB,"Being coded binary files, GRIB files are not readable without the use of
suitable software. There are utilities which can be used to decode GRIB
files. An example is the 
[XCONV/CONVSH](http://cms.ncas.ac.uk/documents/xconv/) package that can
be obtained
from [NCAS-CMS](http://cms.ncas.ac.uk/documents/xconv/download.html).
The  [NCAR Command Language](http://www.ncl.ucar.edu/) readily reads all
GRIB-1 and GRIB-2 files.
Other freely available software packages that can read GRIB via the 
[GrADS/GRIB](https://help.ceda.ac.uk/article/3799-grad-gribs) interface
are:
-   [CDAT (Climate Data Analysis
    Tools)](https://help.ceda.ac.uk/article/291-cdat)
-   [GrADS (Grid Analysis and Display
    System)](https://help.ceda.ac.uk/article/292-grads).
If you would prefer to write your own routines for reading GRIB data, a
complete description of the format is given in the  [Guide to
GRIB](https://artefacts.ceda.ac.uk/badc_datadocs/ecmwf-era/grib/gribdoc.txt).
",https://help.ceda.ac.uk/article/4426-grib#reading-grib,963,102
Example,"Using  [grib2brief]{.kbd} on one of the ECMWF 2.5° gridded data files
held at the BADC will produce an output similar to that displayed below.
The  *\[emphasised\]* text at the line ends has been added here to
highlight the most useful pieces of information in the GRIB information
sections. It is worth noting that in the Grid definition section
latitudes and longitudes are given in millidegrees.
    Section 0 - Indicator Section. 
    -------------------------------------
    Length of GRIB message (octets). 21132
    GRIB Edition Number. 1
    Section 1 - Product Definition Section.
    ---------------------------------------
    Code Table 2 Version Number. 128
    Originating centre identifier. 98 [98=ECMWF]
    Model identification. 190
    Grid definition. 255
    Flag.  10000000
    Parameter identifier . 129 [129=geopotential]
    Type of level. 100 [100=pressure levels]
    Value 1 of level. 1000 [Data on 1000 hPa]
    Value 2 of level. 0 [used for layer mean data]
    Year of reference time of data. 1999 [Date information]
    Month of reference time of data. 1 [Date information]
    Day of reference time of data. 1 [Date information]
    Hour of reference time of data. 0 [Date information]
    Minute of reference time of data. 0 [Date information]
    Time unit. 1 [Time unit used for Time ranges]
    Time range one. 0 [Data is at hour specified above]
    Time range two. 0 [used for time average or accumulated fields]
    Time range indicator. 0
    Number averaged. 0
    Number missing from average. 0
    Century of reference time of data. 20
    Sub-centre identifier. 0
    Units decimal scaling factor. 0
    ECMWF local usage identifier. 1
    (Mars labelling or ensemble forecast)
    Class. 1
    Type. 2
    Stream. 1025
    Version number or Experiment identifier. 1000
    Forecast number. 0
    Total number of forecasts. 0
    Section 2 - Grid Description Section.
    -------------------------------------
    (Southern latitudes and Western longitudes are negative.)
    Data represent type = lat/long  0 [0=lat/lon grid]
    Number of points along a parallel. 144
    Number of points along a meridian. 73
    Latitude of first grid point. 90000 [in milli degrees]
    Longitude of first grid point. 0 [in milli degrees]
    Resolution and components flag. 10000000 
    Latitude of last grid point. -90000 [in milli degrees]
    Longitude of last grid point. 357500 [in milli degrees]
    i direction (East-West) increment. 2500 [in milli degrees]
    j direction (North-South) increment. 2500 [in milli degrees]
    Scanning mode flags  00000000
    Number of vertical coordinate parameters. 0
    Section 4 - Binary Data Section.
    -------------------------------------
    Number of data values coded/decoded. 10512
    Number of bits per data value. 16
    Type of data (0=grid pt, 128=spectral). 0
    Type of packing (0=simple, 64=complex). 0
    Type of data (0=float, 32=integer). 0
    Additional flags (0=none, 16=present). 0
    Reserved. 0
    Number of values (0=single, 64=matrix). 0
    Secondary bit-maps (0=none, 32=present). 0
    Values width (0=constant, 16=variable). 0
    First 20 data values. [for scanning mode 00000000 longitudes 
    924.8220               vary most rapidly and scan from West to East
    924.8220               latitudes vary least rapidly and scan from 
    924.8220               North to South.  So the data opposite is
    924.8220               the first 20 longitudes for the north pole]
    924.8220 
    924.8220 
    924.8220 
    924.8220 
    924.8220 
    924.8220 
    924.8220 
    924.8220 
    924.8220 
    924.8220 
    924.8220 
    924.8220 
    924.8220 
    924.8220 
    924.8220 
    924.8220
",https://help.ceda.ac.uk/article/4426-grib#example,3716,495
Decoding Software,"ECMWF provides access and support for the 
[GRIB-API](https://confluence.ecmwf.int//display/GRIB/Home), an API that
is accessible from C, FORTRAN and Python for both GRIB edition 1 and 2
formatted data.
Another commonly used GRIB decoder, that also runs on PCs, is 
[wgrib](http://www.cpc.ncep.noaa.gov/products/wesley/wgrib.html). A copy
of wgrib and related documentation is available for users of ECMWF data
held in the CEDA archives (in
the [software](http://data.ceda.ac.uk/badc/ecmwf-era/software/decoder/wgrib/)
directory - registered users only).
",https://help.ceda.ac.uk/article/4426-grib#decoding-software,555,66
Selection and retention policy for data,"::: {#wikipage}
[Original trac
page](http://team.ceda.ac.uk//trac/ceda/wiki/opman/policyDataSelection)
",https://help.ceda.ac.uk/article/4318-selection-and-retention-policy-for-data,103,5
Intro {#Intro},"Why selection policy?
",https://help.ceda.ac.uk/article/4318-selection-and-retention-policy-for-data#intro-{#intro},22,3
Selection policy {#Selectionpolicy},"Why manage data at all? Data Managment is an active, difficult,
long-term and expensive process, it surely needs some convinving reasons
to engage with such a massive undertakeing. If the answer to one or more
of the following questions is yes, then data are candidates for
professional data management.
1.  **Primary use** : A project has produces and uses data to do its
    business. Some basic data managment needs to be done by the project
    itself to make sure its data are looked after within the project.
    The life time of this is the life of the project. Because the need
    to share is only within the project there is drive to more universal
    standards.
2.  **Use facilitation** : Is the management of the data by a project
    team likely to be too onerous for them or result in duplication of
    effort with other NERC funded activities? If so keep the data for a
    year after the project ends.
3.  **Community Re-Use** : Are there --- or are there likely to be in
    the future --- users from the subject community in which the data
    originated, who might use the data without having one of the
    original team involved as co-investigators (or authors)? If so keep
    the data for 5 years beyond the end of the project.
4.  **General Re-Use** : Is there --- or is there likely to be in the
    future --- a community of potential users who might use the data
    without having one of the original team involved ? If so then keep
    the data indefinitly.
5.  **Historical Reference** : Does the data have some historical
    importance. Some data may become landmarks, in some way, along the
    route of scientific knowledge. If so then keep the data
    indefinitely.
6.  **Legal Reference** : Is there a legal reason to keep the data? Data
    may have been quoted to make a statement that might be challenged or
    there may be specific legislation that demands certain retention
    periods. If so the data should be kept for the time specified in the
    legislation.
7.  **Academic Reference** : Is the data likely to be refereed to in
    academic publications? These publications might be challenged
    scientifically and the data cited should therefore be kept for
    evidential reasons. If so the data should be kept for 10 years.
",https://help.ceda.ac.uk/article/4318-selection-and-retention-policy-for-data#selection-policy-{#selectionpolicy},2279,385
Data management issues {#Datamanagementissues},"-   Backup - Bit level security of the data.
-   Fixity - Bit level verification of the data.
-   Identity - labelling data as belonging to a data set.
-   Format - Use of standardised formats to make data sharing more
    practical.
-   Metadata conventions - Use of standard vocabularies to make machine
    readable sharing practical.
-   Discovery - Creation and provision of metadata to enable data to be
    found.
-   View - Enabling a graphical view of data to aid selection.
-   Access - Access to data.
-   Licence - Licensing of data to comply with policy and law.
-   Access control - Applying access constraints so that authors can
    have confidence in licence enforcement and to track usage.
-   Reporting - Supplying authors and funders with numbers to backup
    their decisions.
-   Context metadata and documentation - Allowing independent use of
    data.
-   Retention - Review and removal of data to comply with policy and
    law.
-   Media migration - ensure data are still readable over time.
-   Format migration - ensure data are still usable over time.
-   Governance - Agree with suppliers and funders the correct rules to
    apply to the data.
",https://help.ceda.ac.uk/article/4318-selection-and-retention-policy-for-data#data-management-issues-{#datamanagementissues},1176,196
Data review and retention {#Datareviewandretention},"-   Selection and retention of Model data [Model
    data](http://ceda-internal.helpscoutdocs.com/article/4300-archiving-of-simulations-within-the-nerc-data-management-framework-badc-policy-and-guidelines){.wiki}
-   Selection and retention of Experimental data [Experimental
    data](http://ceda-internal.helpscoutdocs.com/article/4349-archiving-of-experimental-data-within-the-nerc-data-management-framework-badc-policy-and-guidelines){.wiki}
-   Selection and retention of Observational data [Observational
    data?](http://team.ceda.ac.uk//trac/ceda/wiki/opman/policyObsDataSelection){.missing
    .wiki}
:::
",https://help.ceda.ac.uk/article/4318-selection-and-retention-policy-for-data#data-review-and-retention-{#datareviewandretention},615,29
UKSSDC Data,"There are some great data about the near-Earth environment held by the
UKSSDC.
",https://help.ceda.ac.uk/article/275-ukssdc-data,79,13
UK Solar System Data Centre (UKSSDC),"-   [World Data Centre for Solar-Terrestrial
    Physics ](https://www.ukssdc.ac.uk/wdcc1/wdc_menu.html)- ([WDC
    Data](https://www.ukssdc.ac.uk/wdcc1/data_menu.html))\
    International archive of STP data.
-   [Solar Archives](https://www.ukssdc.ac.uk/solar/)\
    Archives of solar measurement data and modelling.
-   [Virtual Library of Solar-Terrestrial Physics and
    Chemistry ](https://www.ukssdc.ac.uk/spaceweb/index.html)\
    An actively maintained collection of space physics links.
-   [Instrument
    Locator](https://www.ukssdc.ac.uk/wdcc1/instruments.html)\
    Lists and interactive maps of world-wide ground based STP
    instruments.
-   [Space Environment Database and Analysis
    Tools](https://www.ukssdc.ac.uk/sedat/)\
    Facility to support analysis of charged-particle environment data.
-   [Ditton Park Archive](https://www.dittonpark-archive.rl.ac.uk/)\
    Home of UK Radio Research 1924 - 1979.
<!-- -->
-   [About the UKSSDC](https://www.ukssdc.ac.uk/about_us.html)
-   [Access Keys](https://www.ukssdc.ac.uk/access_keys.html)
",https://help.ceda.ac.uk/article/275-ukssdc-data#uk-solar-system-data-centre-(ukssdc),1062,91
